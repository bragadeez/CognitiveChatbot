[
  {
    "id": 0,
    "content": "Aur\u00e9lien G\u00e9ron\nHands-on Machine Learning with Scikit-Learn, Keras & TensorFlow\nConcepts, Tools, and Techniques to Build Intelligent Systems\nTM\n2nd Edition\nUpdated for TensorFlow 2\nAur\u00e9lien G\u00e9ron\nHands-On Machine Learning with\nScikit-Learn, Keras, and\nTensorFlow\nConcepts, Tools, and Techniques to\nBuild Intelligent Systems\nSECOND EDITION\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n978-1-492-03264-9\n[TI]\nHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\nby Aur\u00e9lien G\u00e9ron\nCopyright \u00a9 2019 Kiwisoft S.A.S. All rights reserved. Printed in Canada. Published by O\u2019Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. O\u2019Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles ( For more information, contact our corporate/institutional\nsales department: 800-998-9938 or corporate@oreilly.com. Editors: Rachel Roumeliotis and Nicole Tache\nProduction Editor: Kristen Brown\nCopyeditor: Amanda Kersey\nProofreader: Rachel Head\nIndexer: Judith McConville\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nSeptember 2019: Second Edition\nRevision History for the Second Edition\n2019-09-05: First Release\n2019-10-11: Second Release\n2019-11-22: Third Release\nSee  for release details. The O\u2019Reilly logo is a registered trademark of O\u2019Reilly Media, Inc. Hands-On Machine Learning with\nScikit-Learn, Keras, and TensorFlow, the cover image, and related trade dress are trademarks of O\u2019Reilly\nMedia, Inc. The views expressed in this work are those of the author, and do not represent the publisher\u2019s views. While the publisher and the author have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the author disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights. Table of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xv\nPart I. The Fundamentals of Machine Learning\n1. The Machine Learning Landscape. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nWhat Is Machine Learning? 2\nWhy Use Machine Learning?"
  },
  {
    "id": 1,
    "content": "2\nExamples of Applications 5\nTypes of Machine Learning Systems 7\nSupervised/Unsupervised Learning 7\nBatch and Online Learning 14\nInstance-Based Versus Model-Based Learning 17\nMain Challenges of Machine Learning 23\nInsufficient Quantity of Training Data 23\nNonrepresentative Training Data 25\nPoor-Quality Data 26\nIrrelevant Features 27\nOverfitting the Training Data 27\nUnderfitting the Training Data 29\nStepping Back 30\nTesting and Validating 30\nHyperparameter Tuning and Model Selection 31\nData Mismatch 32\nExercises 33\n2. End-to-End Machine Learning Project. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\nWorking with Real Data 35\niii\nLook at the Big Picture 37\nFrame the Problem 37\nSelect a Performance Measure 39\nCheck the Assumptions 42\nGet the Data 42\nCreate the Workspace 42\nDownload the Data 46\nTake a Quick Look at the Data Structure 47\nCreate a Test Set 51\nDiscover and Visualize the Data to Gain Insights 56\nVisualizing Geographical Data 56\nLooking for Correlations 58\nExperimenting with Attribute Combinations 61\nPrepare the Data for Machine Learning Algorithms 62\nData Cleaning 63\nHandling Text and Categorical Attributes 65\nCustom Transformers 68\nFeature Scaling 69\nTransformation Pipelines 70\nSelect and Train a Model 72\nTraining and Evaluating on the Training Set 72\nBetter Evaluation Using Cross-Validation 73\nFine-Tune Your Model 75\nGrid Search 76\nRandomized Search 78\nEnsemble Methods 78\nAnalyze the Best Models and Their Errors 78\nEvaluate Your System on the Test Set 79\nLaunch, Monitor, and Maintain Your System 80\nTry It Out! 83\nExercises 84\n3. Classification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nMNIST 85\nTraining a Binary Classifier 88\nPerformance Measures 88\nMeasuring Accuracy Using Cross-Validation 89\nConfusion Matrix 90\nPrecision and Recall 92\nPrecision/Recall Trade-off 93\nThe ROC Curve 97\nMulticlass Classification 100\niv | Table of Contents\nError Analysis 102\nMultilabel Classification 106\nMultioutput Classification 107\nExercises 108\n4. Training Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
  },
  {
    "id": 2,
    "content": "111\nLinear Regression 112\nThe Normal Equation 114\nComputational Complexity 117\nGradient Descent 118\nBatch Gradient Descent 121\nStochastic Gradient Descent 124\nMini-batch Gradient Descent 127\nPolynomial Regression 128\nLearning Curves 130\nRegularized Linear Models 134\nRidge Regression 135\nLasso Regression 137\nElastic Net 140\nEarly Stopping 141\nLogistic Regression 142\nEstimating Probabilities 143\nTraining and Cost Function 144\nDecision Boundaries 145\nSoftmax Regression 148\nExercises 151\n5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\nLinear SVM Classification 153\nSoft Margin Classification 154\nNonlinear SVM Classification 157\nPolynomial Kernel 158\nSimilarity Features 159\nGaussian RBF Kernel 160\nComputational Complexity 162\nSVM Regression 162\nUnder the Hood 164\nDecision Function and Predictions 165\nTraining Objective 166\nQuadratic Programming 167\nThe Dual Problem 168\nKernelized SVMs 169\nTable of Contents | v\nOnline SVMs 172\nExercises 174\n6. Decision Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\nTraining and Visualizing a Decision Tree 175\nMaking Predictions 176\nEstimating Class Probabilities 178\nThe CART Training Algorithm 179\nComputational Complexity 180\nGini Impurity or Entropy? 180\nRegularization Hyperparameters 181\nRegression 183\nInstability 185\nExercises 186\n7. Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189\nVoting Classifiers 189\nBagging and Pasting 192\nBagging and Pasting in Scikit-Learn 194\nOut-of-Bag Evaluation 195\nRandom Patches and Random Subspaces 196\nRandom Forests 197\nExtra-Trees 198\nFeature Importance 198\nBoosting 199\nAdaBoost 200\nGradient Boosting 203\nStacking 208\nExercises 211\n8. Dimensionality Reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\nThe Curse of Dimensionality 214\nMain Approaches for Dimensionality Reduction 215\nProjection 215\nManifold Learning 218\nPCA 219\nPreserving the Variance 219\nPrincipal Components 220\nProjecting Down to d Dimensions 221\nUsing Scikit-Learn 222\nExplained Variance Ratio 222\nChoosing the Right Number of Dimensions 223\nvi | Table of Contents\nPCA for Compression 224\nRandomized PCA 225\nIncremental PCA 225\nKernel PCA 226\nSelecting a Kernel and Tuning Hyperparameters 227\nLLE 230\nOther Dimensionality Reduction Techniques 232\nExercises 233\n9. Unsupervised Learning Techniques. . ."
  },
  {
    "id": 3,
    "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235\nClustering 236\nK-Means 238\nLimits of K-Means 248\nUsing Clustering for Image Segmentation 249\nUsing Clustering for Preprocessing 251\nUsing Clustering for Semi-Supervised Learning 253\nDBSCAN 255\nOther Clustering Algorithms 258\nGaussian Mixtures 260\nAnomaly Detection Using Gaussian Mixtures 266\nSelecting the Number of Clusters 267\nBayesian Gaussian Mixture Models 270\nOther Algorithms for Anomaly and Novelty Detection 274\nExercises 275\nPart II. Neural Networks and Deep Learning\n10. Introduction to Artificial Neural Networks with Keras. . . . . . . . . . . . . . . . . . . . . . . . . . 279\nFrom Biological to Artificial Neurons 280\nBiological Neurons 281\nLogical Computations with Neurons 283\nThe Perceptron 284\nThe Multilayer Perceptron and Backpropagation 289\nRegression MLPs 292\nClassification MLPs 294\nImplementing MLPs with Keras 295\nInstalling TensorFlow 2 296\nBuilding an Image Classifier Using the Sequential API 297\nBuilding a Regression MLP Using the Sequential API 307\nBuilding Complex Models Using the Functional API 308\nUsing the Subclassing API to Build Dynamic Models 313\nTable of Contents | vii\nSaving and Restoring a Model 314\nUsing Callbacks 315\nUsing TensorBoard for Visualization 317\nFine-Tuning Neural Network Hyperparameters 320\nNumber of Hidden Layers 323\nNumber of Neurons per Hidden Layer 324\nLearning Rate, Batch Size, and Other Hyperparameters 325\nExercises 327\n11. Training Deep Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331\nThe Vanishing/Exploding Gradients Problems 332\nGlorot and He Initialization 333\nNonsaturating Activation Functions 335\nBatch Normalization 338\nGradient Clipping 345\nReusing Pretrained Layers 345\nTransfer Learning with Keras 347\nUnsupervised Pretraining 349\nPretraining on an Auxiliary Task 350\nFaster Optimizers 351\nMomentum Optimization 351\nNesterov Accelerated Gradient 353\nAdaGrad 354\nRMSProp 355\nAdam and Nadam Optimization 356\nLearning Rate Scheduling 359\nAvoiding Overfitting Through Regularization 364\n\u21131 and \u21132 Regularization 364\nDropout 365\nMonte Carlo (MC) Dropout 368\nMax-Norm Regularization 370\nSummary and Practical Guidelines 371\nExercises 373\n12. Custom Models and Training with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
  },
  {
    "id": 4,
    "content": "375\nA Quick Tour of TensorFlow 376\nUsing TensorFlow like NumPy 379\nTensors and Operations 379\nTensors and NumPy 381\nType Conversions 381\nVariables 382\nOther Data Structures 383\nviii | Table of Contents\nCustomizing Models and Training Algorithms 384\nCustom Loss Functions 384\nSaving and Loading Models That Contain Custom Components 385\nCustom Activation Functions, Initializers, Regularizers, and Constraints 387\nCustom Metrics 388\nCustom Layers 391\nCustom Models 394\nLosses and Metrics Based on Model Internals 397\nComputing Gradients Using Autodiff 399\nCustom Training Loops 402\nTensorFlow Functions and Graphs 405\nAutoGraph and Tracing 407\nTF Function Rules 409\nExercises 410\n13. Loading and Preprocessing Data with TensorFlow. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 413\nThe Data API 414\nChaining Transformations 415\nShuffling the Data 416\nPreprocessing the Data 419\nPutting Everything Together 420\nPrefetching 421\nUsing the Dataset with tf.keras 423\nThe TFRecord Format 424\nCompressed TFRecord Files 425\nA Brief Introduction to Protocol Buffers 425\nTensorFlow Protobufs 427\nLoading and Parsing Examples 428\nHandling Lists of Lists Using the SequenceExample Protobuf 429\nPreprocessing the Input Features 430\nEncoding Categorical Features Using One-Hot Vectors 431\nEncoding Categorical Features Using Embeddings 433\nKeras Preprocessing Layers 437\nTF Transform 439\nThe TensorFlow Datasets (TFDS) Project 441\nExercises 442\n14. Deep Computer Vision Using Convolutional Neural Networks. . . . . . . . . . . . . . . . . . . 445\nThe Architecture of the Visual Cortex 446\nConvolutional Layers 448\nFilters 450\nStacking Multiple Feature Maps 451\nTable of Contents | ix\nTensorFlow Implementation 453\nMemory Requirements 456\nPooling Layers 456\nTensorFlow Implementation 458\nCNN Architectures 460\nLeNet-5 463\nAlexNet 464\nGoogLeNet 466\nVGGNet 470\nResNet 471\nXception 474\nSENet 476\nImplementing a ResNet-34 CNN Using Keras 478\nUsing Pretrained Models from Keras 479\nPretrained Models for Transfer Learning 481\nClassification and Localization 483\nObject Detection 485\nFully Convolutional Networks 487\nYou Only Look Once (YOLO) 489\nSemantic Segmentation 492\nExercises 496\n15. Processing Sequences Using RNNs and CNNs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497\nRecurrent Neurons and Layers 498\nMemory Cells 500\nInput and Output Sequences 501\nTraining RNNs 502\nForecasting a Time Series 503\nBaseline Metrics 505\nImplementing a Simple RNN 505\nDeep RNNs 506\nForecasting Several Time Steps Ahead 508\nHandling Long Sequences 511\nFighting the Unstable Gradients Problem 512\nTackling the Short-Term Memory Problem 514\nExercises 523\n16. Natural Language Processing with RNNs and Attention. . . . . . . . . . . . . . . . . . . . . . . ."
  },
  {
    "id": 5,
    "content": "525\nGenerating Shakespearean Text Using a Character RNN 526\nCreating the Training Dataset 527\nHow to Split a Sequential Dataset 527\nChopping the Sequential Dataset into Multiple Windows 528\nx | Table of Contents\nBuilding and Training the Char-RNN Model 530\nUsing the Char-RNN Model 531\nGenerating Fake Shakespearean Text 531\nStateful RNN 532\nSentiment Analysis 534\nMasking 538\nReusing Pretrained Embeddings 540\nAn Encoder\u2013Decoder Network for Neural Machine Translation 542\nBidirectional RNNs 546\nBeam Search 547\nAttention Mechanisms 549\nVisual Attention 552\nAttention Is All You Need: The Transformer Architecture 554\nRecent Innovations in Language Models 563\nExercises 565\n17. Representation Learning and Generative Learning Using Autoencoders and GANs. 567\nEfficient Data Representations 569\nPerforming PCA with an Undercomplete Linear Autoencoder 570\nStacked Autoencoders 572\nImplementing a Stacked Autoencoder Using Keras 572\nVisualizing the Reconstructions 574\nVisualizing the Fashion MNIST Dataset 574\nUnsupervised Pretraining Using Stacked Autoencoders 576\nTying Weights 577\nTraining One Autoencoder at a Time 578\nConvolutional Autoencoders 579\nRecurrent Autoencoders 580\nDenoising Autoencoders 581\nSparse Autoencoders 582\nVariational Autoencoders 586\nGenerating Fashion MNIST Images 590\nGenerative Adversarial Networks 592\nThe Difficulties of Training GANs 596\nDeep Convolutional GANs 598\nProgressive Growing of GANs 601\nStyleGANs 604\nExercises 607\n18. Reinforcement Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609\nLearning to Optimize Rewards 610\nPolicy Search 612\nTable of Contents | xi\nIntroduction to OpenAI Gym 613\nNeural Network Policies 617\nEvaluating Actions: The Credit Assignment Problem 619\nPolicy Gradients 620\nMarkov Decision Processes 625\nTemporal Difference Learning 629\nQ-Learning 630\nExploration Policies 632\nApproximate Q-Learning and Deep Q-Learning 633\nImplementing Deep Q-Learning 634\nDeep Q-Learning Variants 639\nFixed Q-Value Targets 639\nDouble DQN 640\nPrioritized Experience Replay 640\nDueling DQN 641\nThe TF-Agents Library 642\nInstalling TF-Agents 643\nTF-Agents Environments 643\nEnvironment Specifications 644\nEnvironment Wrappers and Atari Preprocessing 645\nTraining Architecture 649\nCreating the Deep Q-Network 650\nCreating the DQN Agent 652\nCreating the Replay Buffer and the Corresponding Observer 654\nCreating Training Metrics 655\nCreating the Collect Driver 656\nCreating the Dataset 658\nCreating the Training Loop 661\nOverview of Some Popular RL Algorithms 662\nExercises 664\n19. Training and Deploying TensorFlow Models at Scale. . . . . . . . . . . . . . . . . . . . . . . . . . ."
  },
  {
    "id": 6,
    "content": "667\nServing a TensorFlow Model 668\nUsing TensorFlow Serving 668\nCreating a Prediction Service on GCP AI Platform 677\nUsing the Prediction Service 682\nDeploying a Model to a Mobile or Embedded Device 685\nUsing GPUs to Speed Up Computations 689\nGetting Your Own GPU 690\nUsing a GPU-Equipped Virtual Machine 692\nColaboratory 693\nManaging the GPU RAM 694\nxii | Table of Contents\nPlacing Operations and Variables on Devices 697\nParallel Execution Across Multiple Devices 699\nTraining Models Across Multiple Devices 701\nModel Parallelism 701\nData Parallelism 704\nTraining at Scale Using the Distribution Strategies API 709\nTraining a Model on a TensorFlow Cluster 711\nRunning Large Training Jobs on Google Cloud AI Platform 714\nBlack Box Hyperparameter Tuning on AI Platform 716\nExercises 717\nThank You! 718\nA. Exercise Solutions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 719\nB. Machine Learning Project Checklist. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 755\nC. SVM Dual Problem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 761\nD. Autodiff. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 765\nE. Other Popular ANN Architectures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 773\nF. Special Data Structures. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 783\nG. TensorFlow Graphs. . . . . . . . . . . . . . . . . . . . . . . . ."
  },
  {
    "id": 7,
    "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 791\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 801\nTable of Contents | xiii\n1 Geoffrey E. Hinton et al., \u201cA Fast Learning Algorithm for Deep Belief Nets,\u201d Neural Computation 18 (2006):\n1527\u20131554. 2 Despite the fact that Yann LeCun\u2019s deep convolutional neural networks had worked well for image recognition\nsince the 1990s, although they were not as general-purpose. Preface\nThe Machine Learning Tsunami\nIn 2006, Geoffrey Hinton et al. published a paper1 showing how to train a deep neural\nnetwork capable of recognizing handwritten digits with state-of-the-art precision\n(>98%). They branded this technique \u201cDeep Learning.\u201d A deep neural network is a\n(very) simplified model of our cerebral cortex, composed of a stack of layers of artifi\u2010\ncial neurons. Training a deep neural net was widely considered impossible at the\ntime,2 and most researchers had abandoned the idea in the late 1990s. This paper\nrevived the interest of the scientific community, and before long many new papers\ndemonstrated that Deep Learning was not only possible, but capable of mind-\nblowing achievements that no other Machine Learning (ML) technique could hope to\nmatch (with the help of tremendous computing power and great amounts of data). This enthusiasm soon extended to many other areas of Machine Learning. A decade or so later, Machine Learning has conquered the industry: it is at the heart\nof much of the magic in today\u2019s high-tech products, ranking your web search results,\npowering your smartphone\u2019s speech recognition, recommending videos, and beating\nthe world champion at the game of Go. Before you know it, it will be driving your car. Machine Learning in Your Projects\nSo, naturally you are excited about Machine Learning and would love to join the\nparty! xv\nPerhaps you would like to give your homemade robot a brain of its own? Make it rec\u2010\nognize faces? Or learn to walk around? Or maybe your company has tons of data (user logs, financial data, production data,\nmachine sensor data, hotline stats, HR reports, etc. ), and more than likely you could\nunearth some hidden gems if you just knew where to look. With Machine Learning,\nyou could accomplish the following and more:\n\u2022 Segment customers and find the best marketing strategy for each group. \u2022 Recommend products for each client based on what similar clients bought. \u2022 Detect which transactions are likely to be fraudulent. \u2022 Forecast next year\u2019s revenue."
  },
  {
    "id": 8,
    "content": "Whatever the reason, you have decided to learn Machine Learning and implement it\nin your projects. Great idea! Objective and Approach\nThis book assumes that you know close to nothing about Machine Learning. Its goal\nis to give you the concepts, tools, and intuition you need to implement programs\ncapable of learning from data. We will cover a large number of techniques, from the simplest and most commonly\nused (such as Linear Regression) to some of the Deep Learning techniques that regu\u2010\nlarly win competitions. Rather than implementing our own toy versions of each algorithm, we will be using\nproduction-ready Python frameworks:\n\u2022 Scikit-Learn is very easy to use, yet it implements many Machine Learning algo\u2010\nrithms efficiently, so it makes for a great entry point to learning Machine Learn\u2010\ning. It was created by David Cournapeau in 2007, and is now led by a team of\nresearchers at the French Institute for Research in Computer Science and Auto\u2010\nmation (Inria). \u2022 TensorFlow is a more complex library for distributed numerical computation. It\nmakes it possible to train and run very large neural networks efficiently by dis\u2010\ntributing the computations across potentially hundreds of multi-GPU (graphics\nprocessing unit) servers. TensorFlow (TF) was created at Google and supports\nmany of its large-scale Machine Learning applications. It was open sourced in\nNovember 2015, and version 2.0 was released in September 2019. \u2022 Keras is a high-level Deep Learning API that makes it very simple to train and\nrun neural networks. It can run on top of either TensorFlow, Theano, or Micro\u2010\nsoft Cognitive Toolkit (formerly known as CNTK). TensorFlow comes with its\nxvi | Preface\nown implementation of this API, called tf.keras, which provides support for some\nadvanced TensorFlow features (e.g., the ability to efficiently load data). The book favors a hands-on approach, growing an intuitive understanding of\nMachine Learning through concrete working examples and just a little bit of theory. While you can read this book without picking up your laptop, I highly recommend\nyou experiment with the code examples available online as Jupyter notebooks at\n\nPrerequisites\nThis book assumes that you have some Python programming experience and that you\nare familiar with Python\u2019s main scientific libraries\u2014in particular, NumPy, pandas,\nand Matplotlib. Also, if you care about what\u2019s under the hood, you should have a reasonable under\u2010\nstanding of college-level math as well (calculus, linear algebra, probabilities, and sta\u2010\ntistics). If you don\u2019t know Python yet,  is a great place to start. The offi\u2010\ncial tutorial on Python.org is also quite good. If you have never used Jupyter, Chapter 2 will guide you through installation and the\nbasics: it is a powerful tool to have in your toolbox. If you are not familiar with Python\u2019s scientific libraries, the provided Jupyter note\u2010\nbooks include a few tutorials. There is also a quick math tutorial for linear algebra. Roadmap\nThis book is organized in two parts."
  },
  {
    "id": 9,
    "content": "Part I, The Fundamentals of Machine Learning,\ncovers the following topics:\n\u2022 What Machine Learning is, what problems it tries to solve, and the main cate\u2010\ngories and fundamental concepts of its systems\n\u2022 The steps in a typical Machine Learning project\n\u2022 Learning by fitting a model to data\n\u2022 Optimizing a cost function\n\u2022 Handling, cleaning, and preparing data\n\u2022 Selecting and engineering features\n\u2022 Selecting a model and tuning hyperparameters using cross-validation\n\u2022 The challenges of Machine Learning, in particular underfitting and overfitting\n(the bias/variance trade-off)\nPreface | xvii\n\u2022 The most common learning algorithms: Linear and Polynomial Regression,\nLogistic Regression, k-Nearest Neighbors, Support Vector Machines, Decision\nTrees, Random Forests, and Ensemble methods\n\u2022 Reducing the dimensionality of the training data to fight the \u201ccurse of dimen\u2010\nsionality\u201d\n\u2022 Other unsupervised learning techniques, including clustering, density estima\u2010\ntion, and anomaly detection\nPart II, Neural Networks and Deep Learning, covers the following topics:\n\u2022 What neural nets are and what they\u2019re good for\n\u2022 Building and training neural nets using TensorFlow and Keras\n\u2022 The most important neural net architectures: feedforward neural nets for tabular\ndata, convolutional nets for computer vision, recurrent nets and long short-term\nmemory (LSTM) nets for sequence processing, encoder/decoders and Trans\u2010\nformers for natural language processing, autoencoders and generative adversarial\nnetworks (GANs) for generative learning\n\u2022 Techniques for training deep neural nets\n\u2022 How to build an agent (e.g., a bot in a game) that can learn good strategies\nthrough trial and error, using Reinforcement Learning\n\u2022 Loading and preprocessing large amounts of data efficiently\n\u2022 Training and deploying TensorFlow models at scale\nThe first part is based mostly on Scikit-Learn, while the second part uses TensorFlow\nand Keras. Don\u2019t jump into deep waters too hastily: while Deep Learning is no\ndoubt one of the most exciting areas in Machine Learning, you\nshould master the fundamentals first. Moreover, most problems\ncan be solved quite well using simpler techniques such as Random\nForests and Ensemble methods (discussed in Part I). Deep Learn\u2010\ning is best suited for complex problems such as image recognition,\nspeech recognition, or natural language processing, provided you\nhave enough data, computing power, and patience. xviii | Preface\nChanges in the Second Edition\nThis second edition has six main objectives:\n1. Cover additional ML topics: more unsupervised learning techniques (including\nclustering, anomaly detection, density estimation, and mixture models); more\ntechniques for training deep nets (including self-normalized networks); addi\u2010\ntional computer vision techniques (including Xception, SENet, object detection\nwith YOLO, and semantic segmentation using R-CNN); handling sequences\nusing covolutional neural networks (CNNs, including WaveNet); natural lan\u2010\nguage processing using recurrent neural networks (RNNs), CNNs, and Trans\u2010\nformers; and GANs. 2. Cover additional libraries and APIs (Keras, the Data API, TF-Agents for Rein\u2010\nforcement Learning) and training and deploying TF models at scale using the\nDistribution Strategies API, TF-Serving, and Google Cloud AI Platform. Also\nbriefly introduce TF Transform, TFLite, TF Addons/Seq2Seq, and TensorFlow.js. 3."
  },
  {
    "id": 10,
    "content": "Discuss some of the latest important results from Deep Learning research. 4. Migrate all TensorFlow chapters to TensorFlow 2, and use TensorFlow\u2019s imple\u2010\nmentation of the Keras API (tf.keras) whenever possible. 5. Update the code examples to use the latest versions of Scikit-Learn, NumPy, pan\u2010\ndas, Matplotlib, and other libraries. 6. Clarify some sections and fix some errors, thanks to plenty of great feedback\nfrom readers. Some chapters were added, others were rewritten, and a few were reordered. See\n for more details on what changed in the second edition. Other Resources\nMany excellent resources are available to learn about Machine Learning. For example,\nAndrew Ng\u2019s ML course on Coursera is amazing, although it requires a significant\ntime investment (think months). There are also many interesting websites about Machine Learning, including of\ncourse Scikit-Learn\u2019s exceptional User Guide. You may also enjoy Dataquest, which\nprovides very nice interactive tutorials, and ML blogs such as those listed on Quora. Finally, the Deep Learning website has a good list of resources to check out to learn\nmore. There are many other introductory books about Machine Learning. In particular:\nPreface | xix\n\u2022 Joel Grus\u2019s Data Science from Scratch (O\u2019Reilly) presents the fundamentals of\nMachine Learning and implements some of the main algorithms in pure Python\n(from scratch, as the name suggests). \u2022 Stephen Marsland\u2019s Machine Learning: An Algorithmic Perspective (Chapman &\nHall) is a great introduction to Machine Learning, covering a wide range of topics\nin depth with code examples in Python (also from scratch, but using NumPy). \u2022 Sebastian Raschka\u2019s Python Machine Learning (Packt Publishing) is also a great\nintroduction to Machine Learning and leverages Python open source libraries\n(Pylearn 2 and Theano). \u2022 Fran\u00e7ois Chollet\u2019s Deep Learning with Python (Manning) is a very practical book\nthat covers a large range of topics in a clear and concise way, as you might expect\nfrom the author of the excellent Keras library. It favors code examples over math\u2010\nematical theory. \u2022 Andriy Burkov\u2019s The Hundred-Page Machine Learning Book is very short and cov\u2010\ners an impressive range of topics, introducing them in approachable terms\nwithout shying away from the math equations. \u2022 Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin\u2019s Learning from\nData (AMLBook) is a rather theoretical approach to ML that provides deep\ninsights, in particular on the bias/variance trade-off (see Chapter 4). \u2022 Stuart Russell and Peter Norvig\u2019s Artificial Intelligence: A Modern Approach, 3rd\nEdition (Pearson), is a great (and huge) book covering an incredible amount of\ntopics, including Machine Learning. It helps put ML into perspective. Finally, joining ML competition websites such as Kaggle.com will allow you to prac\u2010\ntice your skills on real-world problems, with help and insights from some of the best\nML professionals out there. Conventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions."
  },
  {
    "id": 11,
    "content": "Constant width\nUsed for program listings, as well as within paragraphs to refer to program ele\u2010\nments such as variable or function names, databases, data types, environment\nvariables, statements and keywords. Constant width bold\nShows commands or other text that should be typed literally by the user. xx | Preface\nConstant width italic\nShows text that should be replaced with user-supplied values or by values deter\u2010\nmined by context. This element signifies a tip or suggestion. This element signifies a general note. This element indicates a warning or caution. Code Examples\nThere is a series of Jupyter notebooks full of supplemental material, such as code\nexamples and exercises, available for download at \nml2. Some of the code examples in the book leave out repetitive sections or details that are\nobvious or unrelated to Machine Learning. This keeps the focus on the important\nparts of the code and saves space to cover more topics. If you want the full code\nexamples, they are all available in the Jupyter notebooks. Note that when the code examples display some outputs, these code examples are\nshown with Python prompts (>>> and ...), as in a Python shell, to clearly distinguish\nthe code from the outputs. For example, this code defines the square() function,\nthen it computes and displays the square of 3:\n>>> def square(x):\n... return x ** 2\n...\n>>> result = square(3)\n>>> result When code does not display anything, prompts are not used. However, the result may\nsometimes be shown as a comment, like this:\nPreface | xxi\ndef square(x): return x ** 2\nresult = square(3) # result is 9\nUsing Code Examples\nThis book is here to help you get your job done. In general, if example code is offered\nwith this book, you may use it in your programs and documentation. You do not\nneed to contact us for permission unless you\u2019re reproducing a significant portion of\nthe code. For example, writing a program that uses several chunks of code from this\nbook does not require permission. Selling or distributing a CD-ROM of examples\nfrom O\u2019Reilly books does require permission. Answering a question by citing this\nbook and quoting example code does not require permission. Incorporating a signifi\u2010\ncant amount of example code from this book into your product\u2019s documentation does\nrequire permission. We appreciate, but do not require, attribution. An attribution usually includes the\ntitle, author, publisher, and ISBN. For example: \u201cHands-On Machine Learning with\nScikit-Learn, Keras, and TensorFlow, 2nd Edition, by Aur\u00e9lien G\u00e9ron (O\u2019Reilly). Copyright 2019 Kiwisoft S.A.S., 978-1-492-03264-9.\u201d If you feel your use of code\nexamples falls outside fair use or the permission given above, feel free to contact us at\npermissions@oreilly.com. O\u2019Reilly Online Learning\nFor almost 40 years, O\u2019Reilly Media has provided technology\nand business training, knowledge, and insight to help compa\u2010\nnies succeed. Our unique network of experts and innovators share their knowledge and expertise\nthrough books, articles, conferences, and our online learning platform."
  },
  {
    "id": 12,
    "content": "O\u2019Reilly\u2019s\nonline learning platform gives you on-demand access to live training courses, in-\ndepth learning paths, interactive coding environments, and a vast collection of text\nand video from O\u2019Reilly and 200+ other publishers. For more information, please\nvisit \nxxii | Preface\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO\u2019Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at \nTo comment or ask technical questions about this book, send email to bookques\u2010\ntions@oreilly.com. For more information about our books, courses, conferences, and news, see our web\u2010\nsite at \nFind us on Facebook: \nFollow us on Twitter: \nWatch us on YouTube: \nAcknowledgments\nNever in my wildest dreams did I imagine that the first edition of this book would get\nsuch a large audience. I received so many messages from readers, many asking ques\u2010\ntions, some kindly pointing out errata, and most sending me encouraging words. I\ncannot express how grateful I am to all these readers for their tremendous support. Thank you all so very much! Please do not hesitate to file issues on GitHub if you find\nerrors in the code examples (or just to ask questions), or to submit errata if you find\nerrors in the text. Some readers also shared how this book helped them get their first\njob, or how it helped them solve a concrete problem they were working on. I find\nsuch feedback incredibly motivating. If you find this book helpful, I would love it if\nyou could share your story with me, either privately (e.g., via LinkedIn) or publicly\n(e.g., in a tweet or through an Amazon review). I am also incredibly thankful to all the amazing people who took time out of their\nbusy lives to review my book with such care. In particular, I would like to thank Fran\u2010\n\u00e7ois Chollet for reviewing all the chapters based on Keras and TensorFlow and giving\nme some great in-depth feedback. Since Keras is one of the main additions to this sec\u2010\nond edition, having its author review the book was invaluable. I highly recommend\nPreface | xxiii\nFran\u00e7ois\u2019s book Deep Learning with Python (Manning): it has the conciseness, clarity,\nand depth of the Keras library itself. Special thanks as well to Ankur Patel, who\nreviewed every chapter of this second edition and gave me excellent feedback, in par\u2010\nticular on Chapter 9, which covers unsupervised learning techniques. He could write\na whole book on the topic\u2026 oh, wait, he did! Do check out Hands-On Unsupervised\nLearning Using Python: How to Build Applied Machine Learning Solutions from Unla\u2010\nbeled Data (O\u2019Reilly). Huge thanks as well to Olzhas Akpambetov, who reviewed all\nthe chapters in the second part of the book, tested much of the code, and offered\nmany great suggestions."
  },
  {
    "id": 13,
    "content": "I\u2019m grateful to Mark Daoust, Jon Krohn, Dominic Monn,\nand Josh Patterson for reviewing the second part of this book so thoroughly and\noffering their expertise. They left no stone unturned and provided amazingly useful\nfeedback. While writing this second edition, I was fortunate enough to get plenty of help from\nmembers of the TensorFlow team\u2014in particular Martin Wicke, who tirelessly\nanswered dozens of my questions and dispatched the rest to the right people, includ\u2010\ning Karmel Allison, Paige Bailey, Eugene Brevdo, William Chargin, Daniel \u201cWolff\u201d\nDobson, Nick Felt, Bruce Fontaine, Goldie Gadde, Sandeep Gupta, Priya Gupta,\nKevin Haas, Konstantinos Katsiapis ,Viacheslav Kovalevskyi, Allen Lavoie, Clemens\nMewald, Dan Moldovan, Sean Morgan, Tom O\u2019Malley, Alexandre Passos, Andr\u00e9 Sus\u2010\nano Pinto, Anthony Platanios, Oscar Ramirez, Anna Revinskaya, Saurabh Saxena,\nRyan Sepassi, Jiri Simsa, Xiaodan Song, Christina Sorokin, Dustin Tran, Todd Wang,\nPete Warden (who also reviewed the first edition) Edd Wilder-James, and Yuefeng\nZhou, all of whom were tremendously helpful. Huge thanks to all of you, and to all\nother members of the TensorFlow team, not just for your help, but also for making\nsuch a great library! Special thanks to Irene Giannoumis and Robert Crowe of the\nTFX team for reviewing Chapters 13 and 19 in depth. Many thanks as well to O\u2019Reilly\u2019s fantastic staff, in particular Nicole Tach\u00e9, who gave\nme insightful feedback and was always cheerful, encouraging, and helpful: I could not\ndream of a better editor. Big thanks to Michele Cronin as well, who was very helpful\n(and patient) at the start of this second edition, and to Kristen Brown, the production\neditor for the second edition, who saw it through all the steps (she also coordinated\nfixes and updates for each reprint of the first edition). Thanks as well to Rachel Mon\u2010\naghan and Amanda Kersey for their thorough copyediting (respectively for the first\nand second edition), and to Johnny O\u2019Toole who managed the relationship with\nAmazon and answered many of my questions. Thanks to Marie Beaugureau, Ben\nLorica, Mike Loukides, and Laurel Ruma for believing in this project and helping me\ndefine its scope. Thanks to Matt Hacker and all of the Atlas team for answering all my\ntechnical questions regarding formatting, AsciiDoc, and LaTeX, and thanks to Nick\nAdams, Rebecca Demarest, Rachel Head, Judith McConville, Helen Monroe, Karen\nMontgomery, Rachel Roumeliotis, and everyone else at O\u2019Reilly who contributed to\nthis book. xxiv | Preface\nI would also like to thank my former Google colleagues, in particular the YouTube\nvideo classification team, for teaching me so much about Machine Learning. I could\nnever have started the first edition without them. Special thanks to my personal ML\ngurus: Cl\u00e9ment Courbet, Julien Dubois, Mathias Kende, Daniel Kitachewsky, James\nPack, Alexander Pak, Anosh Raj, Vitor Sessak, Wiktor Tomczak, Ingrid von Glehn,\nand Rich Washington. And thanks to everyone else I worked with at YouTube and in\nthe amazing Google research teams in Mountain View."
  },
  {
    "id": 14,
    "content": "Many thanks as well to Martin\nAndrews, Sam Witteveen, and Jason Zaman for welcoming me into their Google\nDeveloper Experts group in Singapore, with the kind support of Soonson Kwon, and\nfor all the great discussions we had about Deep Learning and TensorFlow. Anyone\ninterested in Deep Learning in Singapore should definitely join their Deep Learning\nSingapore meetup. Jason deserves special thanks for sharing some of his TFLite\nexpertise for Chapter 19! I will never forget the kind people who reviewed the first edition of this book, includ\u2010\ning David Andrzejewski, Lukas Biewald, Justin Francis, Vincent Guilbeau, Eddy\nHung, Karim Matrah, Gr\u00e9goire Mesnil, Salim S\u00e9maoune, Iain Smears, Michel Tessier,\nIngrid von Glehn, Pete Warden, and of course my dear brother Sylvain. Special\nthanks to Haesun Park, who gave me plenty of excellent feedback and caught several\nerrors while he was writing the Korean translation of the first edition of this book. He\nalso translated the Jupyter notebooks into Korean, not to mention TensorFlow\u2019s doc\u2010\numentation. I do not speak Korean, but judging by the quality of his feedback, all his\ntranslations must be truly excellent! Haesun also kindly contributed some of the solu\u2010\ntions to the exercises in this second edition. Last but not least, I am infinitely grateful to my beloved wife, Emmanuelle, and to our\nthree wonderful children, Alexandre, R\u00e9mi, and Gabrielle, for encouraging me to\nwork hard on this book. I\u2019m also thankful to them for their insatiable curiosity:\nexplaining some of the most difficult concepts in this book to my wife and children\nhelped me clarify my thoughts and directly improved many parts of it. And they keep\nbringing me cookies and coffee! What more can one dream of? Preface | xxv\nPART I\nThe Fundamentals of\nMachine Learning\nCHAPTER 1\nThe Machine Learning Landscape\nWhen most people hear \u201cMachine Learning,\u201d they picture a robot: a dependable but\u2010\nler or a deadly Terminator, depending on who you ask. But Machine Learning is not\njust a futuristic fantasy; it\u2019s already here. In fact, it has been around for decades in\nsome specialized applications, such as Optical Character Recognition (OCR). But the\nfirst ML application that really became mainstream, improving the lives of hundreds\nof millions of people, took over the world back in the 1990s: the spam filter. It\u2019s not\nexactly a self-aware Skynet, but it does technically qualify as Machine Learning (it has\nactually learned so well that you seldom need to flag an email as spam anymore). It\nwas followed by hundreds of ML applications that now quietly power hundreds of\nproducts and features that you use regularly, from better recommendations to voice\nsearch. Where does Machine Learning start and where does it end? What exactly does it\nmean for a machine to learn something? If I download a copy of Wikipedia, has my\ncomputer really learned something? Is it suddenly smarter? In this chapter we will\nstart by clarifying what Machine Learning is and why you may want to use it."
  },
  {
    "id": 15,
    "content": "Then, before we set out to explore the Machine Learning continent, we will take a\nlook at the map and learn about the main regions and the most notable landmarks:\nsupervised versus unsupervised learning, online versus batch learning, instance-\nbased versus model-based learning. Then we will look at the workflow of a typical ML\nproject, discuss the main challenges you may face, and cover how to evaluate and\nfine-tune a Machine Learning system. This chapter introduces a lot of fundamental concepts (and jargon) that every data\nscientist should know by heart. It will be a high-level overview (it\u2019s the only chapter\nwithout much code), all rather simple, but you should make sure everything is crystal\nclear to you before continuing on to the rest of the book. So grab a coffee and let\u2019s get\nstarted! If you already know all the Machine Learning basics, you may want\nto skip directly to Chapter 2. If you are not sure, try to answer all\nthe questions listed at the end of the chapter before moving on. What Is Machine Learning? Machine Learning is the science (and art) of programming computers so they can\nlearn from data. Here is a slightly more general definition:\n[Machine Learning is the] field of study that gives computers the ability to learn\nwithout being explicitly programmed. \u2014Arthur Samuel, 1959\nAnd a more engineering-oriented one:\nA computer program is said to learn from experience E with respect to some task T\nand some performance measure P, if its performance on T, as measured by P,\nimproves with experience E.\n\u2014Tom Mitchell, 1997\nYour spam filter is a Machine Learning program that, given examples of spam emails\n(e.g., flagged by users) and examples of regular (nonspam, also called \u201cham\u201d) emails,\ncan learn to flag spam. The examples that the system uses to learn are called the train\u2010\ning set. Each training example is called a training instance (or sample). In this case, the\ntask T is to flag spam for new emails, the experience E is the training data, and the\nperformance measure P needs to be defined; for example, you can use the ratio of\ncorrectly classified emails. This particular performance measure is called accuracy,\nand it is often used in classification tasks. If you just download a copy of Wikipedia, your computer has a lot more data, but it is\nnot suddenly better at any task. Thus, downloading a copy of Wikipedia is not\nMachine Learning. Why Use Machine Learning? Consider how you would write a spam filter using traditional programming techni\u2010\nques (Figure 1-1):\n1. First you would consider what spam typically looks like. You might notice that\nsome words or phrases (such as \u201c4U,\u201d \u201ccredit card,\u201d \u201cfree,\u201d and \u201camazing\u201d) tend to\ncome up a lot in the subject line. Perhaps you would also notice a few other pat\u2010\nterns in the sender\u2019s name, the email\u2019s body, and other parts of the email. | Chapter 1: The Machine Learning Landscape\n2."
  },
  {
    "id": 16,
    "content": "You would write a detection algorithm for each of the patterns that you noticed,\nand your program would flag emails as spam if a number of these patterns were\ndetected. 3. You would test your program and repeat steps 1 and 2 until it was good enough\nto launch. Figure 1-1. The traditional approach\nSince the problem is difficult, your program will likely become a long list of complex\nrules\u2014pretty hard to maintain. In contrast, a spam filter based on Machine Learning techniques automatically learns\nwhich words and phrases are good predictors of spam by detecting unusually fre\u2010\nquent patterns of words in the spam examples compared to the ham examples\n(Figure 1-2). The program is much shorter, easier to maintain, and most likely more\naccurate. What if spammers notice that all their emails containing \u201c4U\u201d are blocked? They\nmight start writing \u201cFor U\u201d instead. A spam filter using traditional programming\ntechniques would need to be updated to flag \u201cFor U\u201d emails. If spammers keep work\u2010\ning around your spam filter, you will need to keep writing new rules forever. In contrast, a spam filter based on Machine Learning techniques automatically noti\u2010\nces that \u201cFor U\u201d has become unusually frequent in spam flagged by users, and it starts\nflagging them without your intervention (Figure 1-3). Why Use Machine Learning? | Figure 1-2. The Machine Learning approach\nFigure 1-3. Automatically adapting to change\nAnother area where Machine Learning shines is for problems that either are too com\u2010\nplex for traditional approaches or have no known algorithm. For example, consider\nspeech recognition. Say you want to start simple and write a program capable of dis\u2010\ntinguishing the words \u201cone\u201d and \u201ctwo.\u201d You might notice that the word \u201ctwo\u201d starts\nwith a high-pitch sound (\u201cT\u201d), so you could hardcode an algorithm that measures\nhigh-pitch sound intensity and use that to distinguish ones and twos\u2014but obviously\nthis technique will not scale to thousands of words spoken by millions of very differ\u2010\nent people in noisy environments and in dozens of languages. The best solution (at\nleast today) is to write an algorithm that learns by itself, given many example record\u2010\nings for each word. Finally, Machine Learning can help humans learn (Figure 1-4). ML algorithms can be\ninspected to see what they have learned (although for some algorithms this can be\ntricky). For instance, once a spam filter has been trained on enough spam, it can\neasily be inspected to reveal the list of words and combinations of words that it\nbelieves are the best predictors of spam. Sometimes this will reveal unsuspected | Chapter 1: The Machine Learning Landscape\ncorrelations or new trends, and thereby lead to a better understanding of the prob\u2010\nlem. Applying ML techniques to dig into large amounts of data can help discover pat\u2010\nterns that were not immediately apparent. This is called data mining. Figure 1-4."
  },
  {
    "id": 17,
    "content": "Machine Learning can help humans learn\nTo summarize, Machine Learning is great for:\n\u2022 Problems for which existing solutions require a lot of fine-tuning or long lists of\nrules: one Machine Learning algorithm can often simplify code and perform bet\u2010\nter than the traditional approach. \u2022 Complex problems for which using a traditional approach yields no good solu\u2010\ntion: the best Machine Learning techniques can perhaps find a solution. \u2022 Fluctuating environments: a Machine Learning system can adapt to new data. \u2022 Getting insights about complex problems and large amounts of data. Examples of Applications\nLet\u2019s look at some concrete examples of Machine Learning tasks, along with the tech\u2010\nniques that can tackle them:\nAnalyzing images of products on a production line to automatically classify them\nThis is image classification, typically performed using convolutional neural net\u2010\nworks (CNNs; see Chapter 14). Examples of Applications | Detecting tumors in brain scans\nThis is semantic segmentation, where each pixel in the image is classified (as we\nwant to determine the exact location and shape of tumors), typically using CNNs\nas well. Automatically classifying news articles\nThis is natural language processing (NLP), and more specifically text classifica\u2010\ntion, which can be tackled using recurrent neural networks (RNNs), CNNs, or\nTransformers (see Chapter 16). Automatically flagging offensive comments on discussion forums\nThis is also text classification, using the same NLP tools. Summarizing long documents automatically\nThis is a branch of NLP called text summarization, again using the same tools. Creating a chatbot or a personal assistant\nThis involves many NLP components, including natural language understanding\n(NLU) and question-answering modules. Forecasting your company\u2019s revenue next year, based on many performance metrics\nThis is a regression task (i.e., predicting values) that may be tackled using any\nregression model, such as a Linear Regression or Polynomial Regression model\n(see Chapter 4), a regression SVM (see Chapter 5), a regression Random Forest\n(see Chapter 7), or an artificial neural network (see Chapter 10). If you want to\ntake into account sequences of past performance metrics, you may want to use\nRNNs, CNNs, or Transformers (see Chapters 15 and 16). Making your app react to voice commands\nThis is speech recognition, which requires processing audio samples: since they\nare long and complex sequences, they are typically processed using RNNs, CNNs,\nor Transformers (see Chapters 15 and 16). Detecting credit card fraud\nThis is anomaly detection (see Chapter 9). Segmenting clients based on their purchases so that you can design a different marketing\nstrategy for each segment\nThis is clustering (see Chapter 9). Representing a complex, high-dimensional dataset in a clear and insightful diagram\nThis is data visualization, often involving dimensionality reduction techniques\n(see Chapter 8). Recommending a product that a client may be interested in, based on past purchases\nThis is a recommender system."
  },
  {
    "id": 18,
    "content": "One approach is to feed past purchases (and\nother information about the client) to an artificial neural network (see Chap\u2010 | Chapter 1: The Machine Learning Landscape\nter 10), and get it to output the most likely next purchase. This neural net would\ntypically be trained on past sequences of purchases across all clients. Building an intelligent bot for a game\nThis is often tackled using Reinforcement Learning (RL; see Chapter 18), which\nis a branch of Machine Learning that trains agents (such as bots) to pick the\nactions that will maximize their rewards over time (e.g., a bot may get a reward\nevery time the player loses some life points), within a given environment (such as\nthe game). The famous AlphaGo program that beat the world champion at the\ngame of Go was built using RL. This list could go on and on, but hopefully it gives you a sense of the incredible\nbreadth and complexity of the tasks that Machine Learning can tackle, and the types\nof techniques that you would use for each task. Types of Machine Learning Systems\nThere are so many different types of Machine Learning systems that it is useful to\nclassify them in broad categories, based on the following criteria:\n\u2022 Whether or not they are trained with human supervision (supervised, unsuper\u2010\nvised, semisupervised, and Reinforcement Learning)\n\u2022 Whether or not they can learn incrementally on the fly (online versus batch\nlearning)\n\u2022 Whether they work by simply comparing new data points to known data points,\nor instead by detecting patterns in the training data and building a predictive\nmodel, much like scientists do (instance-based versus model-based learning)\nThese criteria are not exclusive; you can combine them in any way you like. For\nexample, a state-of-the-art spam filter may learn on the fly using a deep neural net\u2010\nwork model trained using examples of spam and ham; this makes it an online, model-\nbased, supervised learning system. Let\u2019s look at each of these criteria a bit more closely. Supervised/Unsupervised Learning\nMachine Learning systems can be classified according to the amount and type of\nsupervision they get during training. There are four major categories: supervised\nlearning, unsupervised learning, semisupervised learning, and Reinforcement\nLearning. Types of Machine Learning Systems | 1 Fun fact: this odd-sounding name is a statistics term introduced by Francis Galton while he was studying the\nfact that the children of tall people tend to be shorter than their parents. Since the children were shorter, he\ncalled this regression to the mean. This name was then applied to the methods he used to analyze correlations\nbetween variables. Supervised learning\nIn supervised learning, the training set you feed to the algorithm includes the desired\nsolutions, called labels (Figure 1-5). Figure 1-5. A labeled training set for spam classification (an example of supervised\nlearning)\nA typical supervised learning task is classification."
  },
  {
    "id": 19,
    "content": "The spam filter is a good example\nof this: it is trained with many example emails along with their class (spam or ham),\nand it must learn how to classify new emails. Another typical task is to predict a target numeric value, such as the price of a car,\ngiven a set of features (mileage, age, brand, etc.) called predictors. This sort of task is\ncalled regression (Figure 1-6).1 To train the system, you need to give it many examples\nof cars, including both their predictors and their labels (i.e., their prices). In Machine Learning an attribute is a data type (e.g., \u201cmileage\u201d),\nwhile a feature has several meanings, depending on the context, but\ngenerally means an attribute plus its value (e.g., \u201cmileage =\n15,000\u201d). Many people use the words attribute and feature inter\u2010\nchangeably. Note that some regression algorithms can be used for classification as well, and vice\nversa. For example, Logistic Regression is commonly used for classification, as it can\noutput a value that corresponds to the probability of belonging to a given class (e.g.,\n20% chance of being spam). | Chapter 1: The Machine Learning Landscape\n2 Some neural network architectures can be unsupervised, such as autoencoders and restricted Boltzmann\nmachines. They can also be semisupervised, such as in deep belief networks and unsupervised pretraining. Figure 1-6. A regression problem: predict a value, given an input feature (there are usu\u2010\nally multiple input features, and sometimes multiple output values)\nHere are some of the most important supervised learning algorithms (covered in this\nbook):\n\u2022 k-Nearest Neighbors\n\u2022 Linear Regression\n\u2022 Logistic Regression\n\u2022 Support Vector Machines (SVMs)\n\u2022 Decision Trees and Random Forests\n\u2022 Neural networks2\nUnsupervised learning\nIn unsupervised learning, as you might guess, the training data is unlabeled\n(Figure 1-7). The system tries to learn without a teacher. Types of Machine Learning Systems | Figure 1-7. An unlabeled training set for unsupervised learning\nHere are some of the most important unsupervised learning algorithms (most of\nthese are covered in Chapters 8 and 9):\n\u2022 Clustering\n\u2014 K-Means\n\u2014 DBSCAN\n\u2014 Hierarchical Cluster Analysis (HCA)\n\u2022 Anomaly detection and novelty detection\n\u2014 One-class SVM\n\u2014 Isolation Forest\n\u2022 Visualization and dimensionality reduction\n\u2014 Principal Component Analysis (PCA)\n\u2014 Kernel PCA\n\u2014 Locally Linear Embedding (LLE)\n\u2014 t-Distributed Stochastic Neighbor Embedding (t-SNE)\n\u2022 Association rule learning\n\u2014 Apriori\n\u2014 Eclat\nFor example, say you have a lot of data about your blog\u2019s visitors. You may want to\nrun a clustering algorithm to try to detect groups of similar visitors (Figure 1-8). At\nno point do you tell the algorithm which group a visitor belongs to: it finds those\nconnections without your help. For example, it might notice that 40% of your visitors\nare males who love comic books and generally read your blog in the evening, while\n20% are young sci-fi lovers who visit during the weekends. If you use a hierarchical\nclustering algorithm, it may also subdivide each group into smaller groups."
  },
  {
    "id": 20,
    "content": "This may\nhelp you target your posts for each group. | Chapter 1: The Machine Learning Landscape\n3 Notice how animals are rather well separated from vehicles and how horses are close to deer but far from\nbirds. Figure reproduced with permission from Richard Socher et al., \u201cZero-Shot Learning Through Cross-\nModal Transfer,\u201d Proceedings of the 26th International Conference on Neural Information Processing Systems 1\n(2013): 935\u2013943. Figure 1-8. Clustering\nVisualization algorithms are also good examples of unsupervised learning algorithms:\nyou feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep\u2010\nresentation of your data that can easily be plotted (Figure 1-9). These algorithms try\nto preserve as much structure as they can (e.g., trying to keep separate clusters in the\ninput space from overlapping in the visualization) so that you can understand how\nthe data is organized and perhaps identify unsuspected patterns. Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3\nTypes of Machine Learning Systems | A related task is dimensionality reduction, in which the goal is to simplify the data\nwithout losing too much information. One way to do this is to merge several correla\u2010\nted features into one. For example, a car\u2019s mileage may be strongly correlated with its\nage, so the dimensionality reduction algorithm will merge them into one feature that\nrepresents the car\u2019s wear and tear. This is called feature extraction. It is often a good idea to try to reduce the dimension of your train\u2010\ning data using a dimensionality reduction algorithm before you\nfeed it to another Machine Learning algorithm (such as a super\u2010\nvised learning algorithm). It will run much faster, the data will take\nup less disk and memory space, and in some cases it may also per\u2010\nform better. Yet another important unsupervised task is anomaly detection\u2014for example, detect\u2010\ning unusual credit card transactions to prevent fraud, catching manufacturing defects,\nor automatically removing outliers from a dataset before feeding it to another learn\u2010\ning algorithm. The system is shown mostly normal instances during training, so it\nlearns to recognize them; then, when it sees a new instance, it can tell whether it looks\nlike a normal one or whether it is likely an anomaly (see Figure 1-10). A very similar\ntask is novelty detection: it aims to detect new instances that look different from all\ninstances in the training set. This requires having a very \u201cclean\u201d training set, devoid of\nany instance that you would like the algorithm to detect. For example, if you have\nthousands of pictures of dogs, and 1% of these pictures represent Chihuahuas, then a\nnovelty detection algorithm should not treat new pictures of Chihuahuas as novelties. On the other hand, anomaly detection algorithms may consider these dogs as so rare\nand so different from other dogs that they would likely classify them as anomalies (no\noffense to Chihuahuas). Figure 1-10."
  },
  {
    "id": 21,
    "content": "Anomaly detection\nFinally, another common unsupervised task is association rule learning, in which the\ngoal is to dig into large amounts of data and discover interesting relations between | Chapter 1: The Machine Learning Landscape\n4 That\u2019s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes\nmixes up two people who look alike, so you may need to provide a few labels per person and manually clean\nup some clusters. attributes. For example, suppose you own a supermarket. Running an association rule\non your sales logs may reveal that people who purchase barbecue sauce and potato\nchips also tend to buy steak. Thus, you may want to place these items close to one\nanother. Semisupervised learning\nSince labeling data is usually time-consuming and costly, you will often have plenty of\nunlabeled instances, and few labeled instances. Some algorithms can deal with data\nthat\u2019s partially labeled. This is called semisupervised learning (Figure 1-11). Figure 1-11. Semisupervised learning with two classes (triangles and squares): the unla\u2010\nbeled examples (circles) help classify a new instance (the cross) into the triangle class\nrather than the square class, even though it is closer to the labeled squares\nSome photo-hosting services, such as Google Photos, are good examples of this. Once\nyou upload all your family photos to the service, it automatically recognizes that the\nsame person A shows up in photos 1, 5, and 11, while another person B shows up in\nphotos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now all\nthe system needs is for you to tell it who these people are. Just add one label per per\u2010\nson4 and it is able to name everyone in every photo, which is useful for searching\nphotos. Most semisupervised learning algorithms are combinations of unsupervised and\nsupervised algorithms. For example, deep belief networks (DBNs) are based on unsu\u2010\npervised components called restricted Boltzmann machines (RBMs) stacked on top of\none another. RBMs are trained sequentially in an unsupervised manner, and then the\nwhole system is fine-tuned using supervised learning techniques. Types of Machine Learning Systems | Reinforcement Learning\nReinforcement Learning is a very different beast. The learning system, called an agent\nin this context, can observe the environment, select and perform actions, and get\nrewards in return (or penalties in the form of negative rewards, as shown in\nFigure 1-12). It must then learn by itself what is the best strategy, called a policy, to get\nthe most reward over time. A policy defines what action the agent should choose\nwhen it is in a given situation. Figure 1-12. Reinforcement Learning\nFor example, many robots implement Reinforcement Learning algorithms to learn\nhow to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement\nLearning: it made the headlines in May 2017 when it beat the world champion Ke Jie\nat the game of Go."
  },
  {
    "id": 22,
    "content": "It learned its winning policy by analyzing millions of games, and\nthen playing many games against itself. Note that learning was turned off during the\ngames against the champion; AlphaGo was just applying the policy it had learned. Batch and Online Learning\nAnother criterion used to classify Machine Learning systems is whether or not the\nsystem can learn incrementally from a stream of incoming data. | Chapter 1: The Machine Learning Landscape\nBatch learning\nIn batch learning, the system is incapable of learning incrementally: it must be trained\nusing all the available data. This will generally take a lot of time and computing\nresources, so it is typically done offline. First the system is trained, and then it is\nlaunched into production and runs without learning anymore; it just applies what it\nhas learned. This is called offline learning. If you want a batch learning system to know about new data (such as a new type of\nspam), you need to train a new version of the system from scratch on the full dataset\n(not just the new data, but also the old data), then stop the old system and replace it\nwith the new one. Fortunately, the whole process of training, evaluating, and launching a Machine\nLearning system can be automated fairly easily (as shown in Figure 1-3), so even a\nbatch learning system can adapt to change. Simply update the data and train a new\nversion of the system from scratch as often as needed. This solution is simple and often works fine, but training using the full set of data can\ntake many hours, so you would typically train a new system only every 24 hours or\neven just weekly. If your system needs to adapt to rapidly changing data (e.g., to pre\u2010\ndict stock prices), then you need a more reactive solution. Also, training on the full set of data requires a lot of computing resources (CPU,\nmemory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and\nyou automate your system to train from scratch every day, it will end up costing you a\nlot of money. If the amount of data is huge, it may even be impossible to use a batch\nlearning algorithm. Finally, if your system needs to be able to learn autonomously and it has limited\nresources (e.g., a smartphone application or a rover on Mars), then carrying around\nlarge amounts of training data and taking up a lot of resources to train for hours\nevery day is a showstopper. Fortunately, a better option in all these cases is to use algorithms that are capable of\nlearning incrementally. Online learning\nIn online learning, you train the system incrementally by feeding it data instances\nsequentially, either individually or in small groups called mini-batches. Each learning\nstep is fast and cheap, so the system can learn about new data on the fly, as it arrives\n(see Figure 1-13). Types of Machine Learning Systems | Figure 1-13."
  },
  {
    "id": 23,
    "content": "In online learning, a model is trained and launched into production, and\nthen it keeps learning as new data comes in\nOnline learning is great for systems that receive data as a continuous flow (e.g., stock\nprices) and need to adapt to change rapidly or autonomously. It is also a good option\nif you have limited computing resources: once an online learning system has learned\nabout new data instances, it does not need them anymore, so you can discard them\n(unless you want to be able to roll back to a previous state and \u201creplay\u201d the data). This\ncan save a huge amount of space. Online learning algorithms can also be used to train systems on huge datasets that\ncannot fit in one machine\u2019s main memory (this is called out-of-core learning). The\nalgorithm loads part of the data, runs a training step on that data, and repeats the\nprocess until it has run on all of the data (see Figure 1-14). Out-of-core learning is usually done offline (i.e., not on the live\nsystem), so online learning can be a confusing name. Think of it as\nincremental learning. One important parameter of online learning systems is how fast they should adapt to\nchanging data: this is called the learning rate. If you set a high learning rate, then your\nsystem will rapidly adapt to new data, but it will also tend to quickly forget the old\ndata (you don\u2019t want a spam filter to flag only the latest kinds of spam it was shown). Conversely, if you set a low learning rate, the system will have more inertia; that is, it\nwill learn more slowly, but it will also be less sensitive to noise in the new data or to\nsequences of nonrepresentative data points (outliers). | Chapter 1: The Machine Learning Landscape\nFigure 1-14. Using online learning to handle huge datasets\nA big challenge with online learning is that if bad data is fed to the system, the sys\u2010\ntem\u2019s performance will gradually decline. If it\u2019s a live system, your clients will notice. For example, bad data could come from a malfunctioning sensor on a robot, or from\nsomeone spamming a search engine to try to rank high in search results. To reduce\nthis risk, you need to monitor your system closely and promptly switch learning off\n(and possibly revert to a previously working state) if you detect a drop in perfor\u2010\nmance. You may also want to monitor the input data and react to abnormal data (e.g.,\nusing an anomaly detection algorithm). Instance-Based Versus Model-Based Learning\nOne more way to categorize Machine Learning systems is by how they generalize. Most Machine Learning tasks are about making predictions. This means that given a\nnumber of training examples, the system needs to be able to make good predictions\nfor (generalize to) examples it has never seen before. Having a good performance\nmeasure on the training data is good, but insufficient; the true goal is to perform well\non new instances."
  },
  {
    "id": 24,
    "content": "There are two main approaches to generalization: instance-based learning and\nmodel-based learning. Instance-based learning\nPossibly the most trivial form of learning is simply to learn by heart. If you were to\ncreate a spam filter this way, it would just flag all emails that are identical to emails\nthat have already been flagged by users\u2014not the worst solution, but certainly not the\nbest. Types of Machine Learning Systems | Instead of just flagging emails that are identical to known spam emails, your spam\nfilter could be programmed to also flag emails that are very similar to known spam\nemails. This requires a measure of similarity between two emails. A (very basic) simi\u2010\nlarity measure between two emails could be to count the number of words they have\nin common. The system would flag an email as spam if it has many words in com\u2010\nmon with a known spam email. This is called instance-based learning: the system learns the examples by heart, then\ngeneralizes to new cases by using a similarity measure to compare them to the\nlearned examples (or a subset of them). For example, in Figure 1-15 the new instance\nwould be classified as a triangle because the majority of the most similar instances\nbelong to that class. Figure 1-15. Instance-based learning\nModel-based learning\nAnother way to generalize from a set of examples is to build a model of these exam\u2010\nples and then use that model to make predictions. This is called model-based learning\n(Figure 1-16). Figure 1-16. Model-based learning | Chapter 1: The Machine Learning Landscape\nFor example, suppose you want to know if money makes people happy, so you down\u2010\nload the Better Life Index data from the OECD\u2019s website and stats about gross domes\u2010\ntic product (GDP) per capita from the IMF\u2019s website. Then you join the tables and\nsort by GDP per capita. Table 1-1 shows an excerpt of what you get. Table 1-1. Does money make people happier? Country\nGDP per capita (USD)\nLife satisfaction\nHungary\n12,240\n4.9\nKorea\n27,195\n5.8\nFrance\n37,675\n6.5\nAustralia\n50,962\n7.3\nUnited States\n55,805\n7.2\nLet\u2019s plot the data for these countries (Figure 1-17). Figure 1-17. Do you see a trend here? There does seem to be a trend here! Although the data is noisy (i.e., partly random), it\nlooks like life satisfaction goes up more or less linearly as the country\u2019s GDP per cap\u2010\nita increases. So you decide to model life satisfaction as a linear function of GDP per\ncapita. This step is called model selection: you selected a linear model of life satisfac\u2010\ntion with just one attribute, GDP per capita (Equation 1-1). Equation 1-1. A simple linear model\nlife_satisfaction = \u03b80 + \u03b81 \u00d7 GDP_per_capita\nTypes of Machine Learning Systems | 5 By convention, the Greek letter \u03b8 (theta) is frequently used to represent model parameters."
  },
  {
    "id": 25,
    "content": "This model has two model parameters, \u03b80 and \u03b81.5 By tweaking these parameters, you\ncan make your model represent any linear function, as shown in Figure 1-18. Figure 1-18. A few possible linear models\nBefore you can use your model, you need to define the parameter values \u03b80 and \u03b81. How can you know which values will make your model perform best? To answer this\nquestion, you need to specify a performance measure. You can either define a utility\nfunction (or fitness function) that measures how good your model is, or you can define\na cost function that measures how bad it is. For Linear Regression problems, people\ntypically use a cost function that measures the distance between the linear model\u2019s\npredictions and the training examples; the objective is to minimize this distance. This is where the Linear Regression algorithm comes in: you feed it your training\nexamples, and it finds the parameters that make the linear model fit best to your data. This is called training the model. In our case, the algorithm finds that the optimal\nparameter values are \u03b80 = 4.85 and \u03b81 = 4.91 \u00d7 10\u20135. Confusingly, the same word \u201cmodel\u201d can refer to a type of model\n(e.g., Linear Regression), to a fully specified model architecture (e.g.,\nLinear Regression with one input and one output), or to the final\ntrained model ready to be used for predictions (e.g., Linear Regres\u2010\nsion with one input and one output, using \u03b80 = 4.85 and \u03b81 = 4.91 \u00d7\n10\u20135). Model selection consists in choosing the type of model and\nfully specifying its architecture. Training a model means running\nan algorithm to find the model parameters that will make it best fit\nthe training data (and hopefully make good predictions on new\ndata). | Chapter 1: The Machine Learning Landscape\n6 The prepare_country_stats() function\u2019s definition is not shown here (see this chapter\u2019s Jupyter notebook if\nyou want all the gory details). It\u2019s just boring pandas code that joins the life satisfaction data from the OECD\nwith the GDP per capita data from the IMF. 7 It\u2019s OK if you don\u2019t understand all the code yet; we will present Scikit-Learn in the following chapters. Now the model fits the training data as closely as possible (for a linear model), as you\ncan see in Figure 1-19. Figure 1-19. The linear model that fits the training data best\nYou are finally ready to run the model to make predictions. For example, say you\nwant to know how happy Cypriots are, and the OECD data does not have the answer. Fortunately, you can use your model to make a good prediction: you look up Cyprus\u2019s\nGDP per capita, find $22,587, and then apply your model and find that life satisfac\u2010\ntion is likely to be somewhere around 4.85 + 22,587 \u00d7 4.91 \u00d7 10-5 = 5.96."
  },
  {
    "id": 26,
    "content": "To whet your appetite, Example 1-1 shows the Python code that loads the data, pre\u2010\npares it,6 creates a scatterplot for visualization, and then trains a linear model and\nmakes a prediction.7\nExample 1-1. Training and running a linear model using Scikit-Learn\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport sklearn.linear_model\n# Load the data\noecd_bli = pd.read_csv(\"oecd_bli_2015.csv\", thousands=',')\ngdp_per_capita = pd.read_csv(\"gdp_per_capita.csv\",thousands=',',delimiter='\\t', encoding='latin1', na_values=\"n/a\")\nTypes of Machine Learning Systems | # Prepare the data\ncountry_stats = prepare_country_stats(oecd_bli, gdp_per_capita)\nX = np.c_[country_stats[\"GDP per capita\"]]\ny = np.c_[country_stats[\"Life satisfaction\"]]\n# Visualize the data\ncountry_stats.plot(kind='scatter', x=\"GDP per capita\", y='Life satisfaction')\nplt.show()\n# Select a linear model\nmodel = sklearn.linear_model.LinearRegression()\n# Train the model\nmodel.fit(X, y)\n# Make a prediction for Cyprus\nX_new = [[22587]] # Cyprus's GDP per capita\nprint(model.predict(X_new)) # outputs [[ 5.96242338]]\nIf you had used an instance-based learning algorithm instead, you\nwould have found that Slovenia has the closest GDP per capita to\nthat of Cyprus ($20,732), and since the OECD data tells us that\nSlovenians\u2019 life satisfaction is 5.7, you would have predicted a life\nsatisfaction of 5.7 for Cyprus. If you zoom out a bit and look at the\ntwo next-closest countries, you will find Portugal and Spain with\nlife satisfactions of 5.1 and 6.5, respectively. Averaging these three\nvalues, you get 5.77, which is pretty close to your model-based pre\u2010\ndiction. This simple algorithm is called k-Nearest Neighbors regres\u2010\nsion (in this example, k = 3). Replacing the Linear Regression model with k-Nearest Neighbors\nregression in the previous code is as simple as replacing these two\nlines:\nimport sklearn.linear_model\nmodel = sklearn.linear_model.LinearRegression()\nwith these two:\nimport sklearn.neighbors\nmodel = sklearn.neighbors.KNeighborsRegressor( n_neighbors=3)\nIf all went well, your model will make good predictions. If not, you may need to use\nmore attributes (employment rate, health, air pollution, etc. ), get more or better-\nquality training data, or perhaps select a more powerful model (e.g., a Polynomial\nRegression model). | Chapter 1: The Machine Learning Landscape\nIn summary:\n\u2022 You studied the data. \u2022 You selected a model. \u2022 You trained it on the training data (i.e., the learning algorithm searched for the\nmodel parameter values that minimize a cost function). \u2022 Finally, you applied the model to make predictions on new cases (this is called\ninference), hoping that this model will generalize well. This is what a typical Machine Learning project looks like. In Chapter 2 you will\nexperience this firsthand by going through a project end to end. We have covered a lot of ground so far: you now know what Machine Learning is\nreally about, why it is useful, what some of the most common categories of ML sys\u2010\ntems are, and what a typical project workflow looks like. Now let\u2019s look at what can go\nwrong in learning and prevent you from making accurate predictions."
  },
  {
    "id": 27,
    "content": "Main Challenges of Machine Learning\nIn short, since your main task is to select a learning algorithm and train it on some\ndata, the two things that can go wrong are \u201cbad algorithm\u201d and \u201cbad data.\u201d Let\u2019s start\nwith examples of bad data. Insufficient Quantity of Training Data\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and\nsay \u201capple\u201d (possibly repeating this procedure a few times). Now the child is able to\nrecognize apples in all sorts of colors and shapes. Genius. Machine Learning is not quite there yet; it takes a lot of data for most Machine Learn\u2010\ning algorithms to work properly. Even for very simple problems you typically need\nthousands of examples, and for complex problems such as image or speech recogni\u2010\ntion you may need millions of examples (unless you can reuse parts of an existing\nmodel). Main Challenges of Machine Learning | 8 For example, knowing whether to write \u201cto,\u201d \u201ctwo,\u201d or \u201ctoo,\u201d depending on the context. 9 Figure reproduced with permission from Michele Banko and Eric Brill, \u201cScaling to Very Very Large Corpora\nfor Natural Language Disambiguation,\u201d Proceedings of the 39th Annual Meeting of the Association for Compu\u2010\ntational Linguistics (2001): 26\u201333. 10 Peter Norvig et al., \u201cThe Unreasonable Effectiveness of Data,\u201d IEEE Intelligent Systems 24, no. 2 (2009): 8\u201312. The Unreasonable Effectiveness of Data\nIn a famous paper published in 2001, Microsoft researchers Michele Banko and Eric\nBrill showed that very different Machine Learning algorithms, including fairly simple\nones, performed almost identically well on a complex problem of natural language\ndisambiguation8 once they were given enough data (as you can see in Figure 1-20). Figure 1-20. The importance of data versus algorithms9\nAs the authors put it, \u201cthese results suggest that we may want to reconsider the trade-\noff between spending time and money on algorithm development versus spending it\non corpus development.\u201d\nThe idea that data matters more than algorithms for complex problems was further\npopularized by Peter Norvig et al. in a paper titled \u201cThe Unreasonable Effectiveness\nof Data\u201d, published in 2009.10 It should be noted, however, that small- and medium-\nsized datasets are still very common, and it is not always easy or cheap to get extra\ntraining data\u2014so don\u2019t abandon algorithms just yet. | Chapter 1: The Machine Learning Landscape\nNonrepresentative Training Data\nIn order to generalize well, it is crucial that your training data be representative of the\nnew cases you want to generalize to. This is true whether you use instance-based\nlearning or model-based learning. For example, the set of countries we used earlier for training the linear model was not\nperfectly representative; a few countries were missing. Figure 1-21 shows what the\ndata looks like when you add the missing countries. Figure 1-21. A more representative training sample\nIf you train a linear model on this data, you get the solid line, while the old model is\nrepresented by the dotted line."
  },
  {
    "id": 28,
    "content": "As you can see, not only does adding a few missing\ncountries significantly alter the model, but it makes it clear that such a simple linear\nmodel is probably never going to work well. It seems that very rich countries are not\nhappier than moderately rich countries (in fact, they seem unhappier), and con\u2010\nversely some poor countries seem happier than many rich countries. By using a nonrepresentative training set, we trained a model that is unlikely to make\naccurate predictions, especially for very poor and very rich countries. It is crucial to use a training set that is representative of the cases you want to general\u2010\nize to. This is often harder than it sounds: if the sample is too small, you will have\nsampling noise (i.e., nonrepresentative data as a result of chance), but even very large\nsamples can be nonrepresentative if the sampling method is flawed. This is called\nsampling bias. Main Challenges of Machine Learning | Examples of Sampling Bias\nPerhaps the most famous example of sampling bias happened during the US presi\u2010\ndential election in 1936, which pitted Landon against Roosevelt: the Literary Digest\nconducted a very large poll, sending mail to about 10 million people. It got 2.4 million\nanswers, and predicted with high confidence that Landon would get 57% of the votes. Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest\u2019s\nsampling method:\n\u2022 First, to obtain the addresses to send the polls to, the Literary Digest used tele\u2010\nphone directories, lists of magazine subscribers, club membership lists, and the\nlike. All of these lists tended to favor wealthier people, who were more likely to\nvote Republican (hence Landon). \u2022 Second, less than 25% of the people who were polled answered. Again this intro\u2010\nduced a sampling bias, by potentially ruling out people who didn\u2019t care much\nabout politics, people who didn\u2019t like the Literary Digest, and other key groups. This is a special type of sampling bias called nonresponse bias. Here is another example: say you want to build a system to recognize funk music vid\u2010\neos. One way to build your training set is to search for \u201cfunk music\u201d on YouTube and\nuse the resulting videos. But this assumes that YouTube\u2019s search engine returns a set of\nvideos that are representative of all the funk music videos on YouTube. In reality, the\nsearch results are likely to be biased toward popular artists (and if you live in Brazil\nyou will get a lot of \u201cfunk carioca\u201d videos, which sound nothing like James Brown). On the other hand, how else can you get a large training set? Poor-Quality Data\nObviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-\nquality measurements), it will make it harder for the system to detect the underlying\npatterns, so your system is less likely to perform well. It is often well worth the effort\nto spend time cleaning up your training data."
  },
  {
    "id": 29,
    "content": "The truth is, most data scientists spend\na significant part of their time doing just that. The following are a couple of examples\nof when you\u2019d want to clean up training data:\n\u2022 If some instances are clearly outliers, it may help to simply discard them or try to\nfix the errors manually. \u2022 If some instances are missing a few features (e.g., 5% of your customers did not\nspecify their age), you must decide whether you want to ignore this attribute alto\u2010\ngether, ignore these instances, fill in the missing values (e.g., with the median\nage), or train one model with the feature and one model without it. | Chapter 1: The Machine Learning Landscape\nIrrelevant Features\nAs the saying goes: garbage in, garbage out. Your system will only be capable of learn\u2010\ning if the training data contains enough relevant features and not too many irrelevant\nones. A critical part of the success of a Machine Learning project is coming up with a\ngood set of features to train on. This process, called feature engineering, involves the\nfollowing steps:\n\u2022 Feature selection (selecting the most useful features to train on among existing\nfeatures)\n\u2022 Feature extraction (combining existing features to produce a more useful one\u2014as\nwe saw earlier, dimensionality reduction algorithms can help)\n\u2022 Creating new features by gathering new data\nNow that we have looked at many examples of bad data, let\u2019s look at a couple of exam\u2010\nples of bad algorithms. Overfitting the Training Data\nSay you are visiting a foreign country and the taxi driver rips you off. You might be\ntempted to say that all taxi drivers in that country are thieves. Overgeneralizing is\nsomething that we humans do all too often, and unfortunately machines can fall into\nthe same trap if we are not careful. In Machine Learning this is called overfitting: it\nmeans that the model performs well on the training data, but it does not generalize\nwell. Figure 1-22 shows an example of a high-degree polynomial life satisfaction model\nthat strongly overfits the training data. Even though it performs much better on the\ntraining data than the simple linear model, would you really trust its predictions? Figure 1-22. Overfitting the training data\nMain Challenges of Machine Learning | Complex models such as deep neural networks can detect subtle patterns in the data,\nbut if the training set is noisy, or if it is too small (which introduces sampling noise),\nthen the model is likely to detect patterns in the noise itself. Obviously these patterns\nwill not generalize to new instances. For example, say you feed your life satisfaction\nmodel many more attributes, including uninformative ones such as the country\u2019s\nname. In that case, a complex model may detect patterns like the fact that all coun\u2010\ntries in the training data with a w in their name have a life satisfaction greater than 7:\nNew Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5)."
  },
  {
    "id": 30,
    "content": "How confident\nare you that the w-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously\nthis pattern occurred in the training data by pure chance, but the model has no way\nto tell whether a pattern is real or simply the result of noise in the data. Overfitting happens when the model is too complex relative to the\namount and noisiness of the training data. Here are possible solu\u2010\ntions:\n\u2022 Simplify the model by selecting one with fewer parameters\n(e.g., a linear model rather than a high-degree polynomial\nmodel), by reducing the number of attributes in the training\ndata, or by constraining the model. \u2022 Gather more training data. \u2022 Reduce the noise in the training data (e.g., fix data errors and\nremove outliers). Constraining a model to make it simpler and reduce the risk of overfitting is called\nregularization. For example, the linear model we defined earlier has two parameters,\n\u03b80 and \u03b81. This gives the learning algorithm two degrees of freedom to adapt the model\nto the training data: it can tweak both the height (\u03b80) and the slope (\u03b81) of the line. If\nwe forced \u03b81 = 0, the algorithm would have only one degree of freedom and would\nhave a much harder time fitting the data properly: all it could do is move the line up\nor down to get as close as possible to the training instances, so it would end up\naround the mean. A very simple model indeed! If we allow the algorithm to modify \u03b81\nbut we force it to keep it small, then the learning algorithm will effectively have some\u2010\nwhere in between one and two degrees of freedom. It will produce a model that\u2019s sim\u2010\npler than one with two degrees of freedom, but more complex than one with just one. You want to find the right balance between fitting the training data perfectly and\nkeeping the model simple enough to ensure that it will generalize well. Figure 1-23 shows three models. The dotted line represents the original model that\nwas trained on the countries represented as circles (without the countries represented\nas squares), the dashed line is our second model trained with all countries (circles and\nsquares), and the solid line is a model trained with the same data as the first model | Chapter 1: The Machine Learning Landscape\nbut with a regularization constraint. You can see that regularization forced the model\nto have a smaller slope: this model does not fit the training data (circles) as well as the\nfirst model, but it actually generalizes better to new examples that it did not see dur\u2010\ning training (squares). Figure 1-23. Regularization reduces the risk of overfitting\nThe amount of regularization to apply during learning can be controlled by a hyper\u2010\nparameter. A hyperparameter is a parameter of a learning algorithm (not of the\nmodel). As such, it is not affected by the learning algorithm itself; it must be set prior\nto training and remains constant during training."
  },
  {
    "id": 31,
    "content": "If you set the regularization hyper\u2010\nparameter to a very large value, you will get an almost flat model (a slope close to\nzero); the learning algorithm will almost certainly not overfit the training data, but it\nwill be less likely to find a good solution. Tuning hyperparameters is an important\npart of building a Machine Learning system (you will see a detailed example in the\nnext chapter). Underfitting the Training Data\nAs you might guess, underfitting is the opposite of overfitting: it occurs when your\nmodel is too simple to learn the underlying structure of the data. For example, a lin\u2010\near model of life satisfaction is prone to underfit; reality is just more complex than\nthe model, so its predictions are bound to be inaccurate, even on the training\nexamples. Here are the main options for fixing this problem:\n\u2022 Select a more powerful model, with more parameters. \u2022 Feed better features to the learning algorithm (feature engineering). \u2022 Reduce the constraints on the model (e.g., reduce the regularization hyperpara\u2010\nmeter). Main Challenges of Machine Learning | Stepping Back\nBy now you know a lot about Machine Learning. However, we went through so many\nconcepts that you may be feeling a little lost, so let\u2019s step back and look at the big\npicture:\n\u2022 Machine Learning is about making machines get better at some task by learning\nfrom data, instead of having to explicitly code rules. \u2022 There are many different types of ML systems: supervised or not, batch or online,\ninstance-based or model-based. \u2022 In an ML project you gather data in a training set, and you feed the training set to\na learning algorithm. If the algorithm is model-based, it tunes some parameters\nto fit the model to the training set (i.e., to make good predictions on the training\nset itself), and then hopefully it will be able to make good predictions on new\ncases as well. If the algorithm is instance-based, it just learns the examples by\nheart and generalizes to new instances by using a similarity measure to compare\nthem to the learned instances. \u2022 The system will not perform well if your training set is too small, or if the data is\nnot representative, is noisy, or is polluted with irrelevant features (garbage in,\ngarbage out). Lastly, your model needs to be neither too simple (in which case it\nwill underfit) nor too complex (in which case it will overfit). There\u2019s just one last important topic to cover: once you have trained a model, you\ndon\u2019t want to just \u201chope\u201d it generalizes to new cases. You want to evaluate it and fine-\ntune it if necessary. Let\u2019s see how to do that. Testing and Validating\nThe only way to know how well a model will generalize to new cases is to actually try\nit out on new cases. One way to do that is to put your model in production and moni\u2010\ntor how well it performs."
  },
  {
    "id": 32,
    "content": "This works well, but if your model is horribly bad, your\nusers will complain\u2014not the best idea. A better option is to split your data into two sets: the training set and the test set. As\nthese names imply, you train your model using the training set, and you test it using\nthe test set. The error rate on new cases is called the generalization error (or out-of-\nsample error), and by evaluating your model on the test set, you get an estimate of this\nerror. This value tells you how well your model will perform on instances it has never\nseen before. If the training error is low (i.e., your model makes few mistakes on the training set)\nbut the generalization error is high, it means that your model is overfitting the train\u2010\ning data. | Chapter 1: The Machine Learning Landscape\nIt is common to use 80% of the data for training and hold out 20%\nfor testing. However, this depends on the size of the dataset: if it\ncontains 10 million instances, then holding out 1% means your test\nset will contain 100,000 instances, probably more than enough to\nget a good estimate of the generalization error. Hyperparameter Tuning and Model Selection\nEvaluating a model is simple enough: just use a test set. But suppose you are hesitat\u2010\ning between two types of models (say, a linear model and a polynomial model): how\ncan you decide between them? One option is to train both and compare how well\nthey generalize using the test set. Now suppose that the linear model generalizes better, but you want to apply some\nregularization to avoid overfitting. The question is, how do you choose the value of\nthe regularization hyperparameter? One option is to train 100 different models using\n100 different values for this hyperparameter. Suppose you find the best hyperparame\u2010\nter value that produces a model with the lowest generalization error\u2014say, just 5%\nerror. You launch this model into production, but unfortunately it does not perform\nas well as expected and produces 15% errors. What just happened? The problem is that you measured the generalization error multiple times on the test\nset, and you adapted the model and hyperparameters to produce the best model for\nthat particular set. This means that the model is unlikely to perform as well on new\ndata. A common solution to this problem is called holdout validation: you simply hold out\npart of the training set to evaluate several candidate models and select the best one. The new held-out set is called the validation set (or sometimes the development set, or\ndev set). More specifically, you train multiple models with various hyperparameters\non the reduced training set (i.e., the full training set minus the validation set), and\nyou select the model that performs best on the validation set. After this holdout vali\u2010\ndation process, you train the best model on the full training set (including the valida\u2010\ntion set), and this gives you the final model."
  },
  {
    "id": 33,
    "content": "Lastly, you evaluate this final model on\nthe test set to get an estimate of the generalization error. This solution usually works quite well. However, if the validation set is too small, then\nmodel evaluations will be imprecise: you may end up selecting a suboptimal model by\nmistake. Conversely, if the validation set is too large, then the remaining training set\nwill be much smaller than the full training set. Why is this bad? Well, since the final\nmodel will be trained on the full training set, it is not ideal to compare candidate\nmodels trained on a much smaller training set. It would be like selecting the fastest\nsprinter to participate in a marathon. One way to solve this problem is to perform\nrepeated cross-validation, using many small validation sets. Each model is evaluated\nonce per validation set after it is trained on the rest of the data. By averaging out all\nTesting and Validating | the evaluations of a model, you get a much more accurate measure of its perfor\u2010\nmance. There is a drawback, however: the training time is multiplied by the number\nof validation sets. Data Mismatch\nIn some cases, it\u2019s easy to get a large amount of data for training, but this data proba\u2010\nbly won\u2019t be perfectly representative of the data that will be used in production. For\nexample, suppose you want to create a mobile app to take pictures of flowers and\nautomatically determine their species. You can easily download millions of pictures of\nflowers on the web, but they won\u2019t be perfectly representative of the pictures that will\nactually be taken using the app on a mobile device. Perhaps you only have 10,000 rep\u2010\nresentative pictures (i.e., actually taken with the app). In this case, the most important\nrule to remember is that the validation set and the test set must be as representative as\npossible of the data you expect to use in production, so they should be composed\nexclusively of representative pictures: you can shuffle them and put half in the valida\u2010\ntion set and half in the test set (making sure that no duplicates or near-duplicates end\nup in both sets). But after training your model on the web pictures, if you observe\nthat the performance of the model on the validation set is disappointing, you will not\nknow whether this is because your model has overfit the training set, or whether this\nis just due to the mismatch between the web pictures and the mobile app pictures. One solution is to hold out some of the training pictures (from the web) in yet\nanother set that Andrew Ng calls the train-dev set. After the model is trained (on the\ntraining set, not on the train-dev set), you can evaluate it on the train-dev set. If it\nperforms well, then the model is not overfitting the training set. If it performs poorly\non the validation set, the problem must be coming from the data mismatch."
  },
  {
    "id": 34,
    "content": "You can\ntry to tackle this problem by preprocessing the web images to make them look more\nlike the pictures that will be taken by the mobile app, and then retraining the model. Conversely, if the model performs poorly on the train-dev set, then it must have over\u2010\nfit the training set, so you should try to simplify or regularize the model, get more\ntraining data, and clean up the training data. | Chapter 1: The Machine Learning Landscape\n11 David Wolpert, \u201cThe Lack of A Priori Distinctions Between Learning Algorithms,\u201d Neural Computation 8, no. 7 (1996): 1341\u20131390. No Free Lunch Theorem\nA model is a simplified version of the observations. The simplifications are meant to\ndiscard the superfluous details that are unlikely to generalize to new instances. To\ndecide what data to discard and what data to keep, you must make assumptions. For\nexample, a linear model makes the assumption that the data is fundamentally linear\nand that the distance between the instances and the straight line is just noise, which\ncan safely be ignored. In a famous 1996 paper,11 David Wolpert demonstrated that if you make absolutely\nno assumption about the data, then there is no reason to prefer one model over any\nother. This is called the No Free Lunch (NFL) theorem. For some datasets the best\nmodel is a linear model, while for other datasets it is a neural network. There is no\nmodel that is a priori guaranteed to work better (hence the name of the theorem). The\nonly way to know for sure which model is best is to evaluate them all. Since this is not\npossible, in practice you make some reasonable assumptions about the data and eval\u2010\nuate only a few reasonable models. For example, for simple tasks you may evaluate\nlinear models with various levels of regularization, and for a complex problem you\nmay evaluate various neural networks. Exercises\nIn this chapter we have covered some of the most important concepts in Machine\nLearning. In the next chapters we will dive deeper and write more code, but before we\ndo, make sure you know how to answer the following questions:\n1. How would you define Machine Learning? 2. Can you name four types of problems where it shines? 3. What is a labeled training set? 4. What are the two most common supervised tasks? 5. Can you name four common unsupervised tasks? 6. What type of Machine Learning algorithm would you use to allow a robot to\nwalk in various unknown terrains? 7. What type of algorithm would you use to segment your customers into multiple\ngroups? 8. Would you frame the problem of spam detection as a supervised learning prob\u2010\nlem or an unsupervised learning problem? Exercises | 9. What is an online learning system? 10. What is out-of-core learning? 11. What type of learning algorithm relies on a similarity measure to make predic\u2010\ntions? 12. What is the difference between a model parameter and a learning algorithm\u2019s\nhyperparameter? 13."
  },
  {
    "id": 35,
    "content": "What do model-based learning algorithms search for? What is the most common\nstrategy they use to succeed? How do they make predictions? 14. Can you name four of the main challenges in Machine Learning? 15. If your model performs great on the training data but generalizes poorly to new\ninstances, what is happening? Can you name three possible solutions? 16. What is a test set, and why would you want to use it? 17. What is the purpose of a validation set? 18. What is the train-dev set, when do you need it, and how do you use it? 19. What can go wrong if you tune hyperparameters using the test set? Solutions to these exercises are available in Appendix A. | Chapter 1: The Machine Learning Landscape\n1 The example project is fictitious; the goal is to illustrate the main steps of a Machine Learning project, not to\nlearn anything about the real estate business. CHAPTER 2\nEnd-to-End Machine Learning Project\nIn this chapter you will work through an example project end to end, pretending to\nbe a recently hired data scientist at a real estate company.1 Here are the main steps\nyou will go through:\n1. Look at the big picture. 2. Get the data. 3. Discover and visualize the data to gain insights. 4. Prepare the data for Machine Learning algorithms. 5. Select a model and train it. 6. Fine-tune your model. 7. Present your solution. 8. Launch, monitor, and maintain your system. Working with Real Data\nWhen you are learning about Machine Learning, it is best to experiment with real-\nworld data, not artificial datasets. Fortunately, there are thousands of open datasets to\nchoose from, ranging across all sorts of domains. Here are a few places you can look\nto get data: 2 The original dataset appeared in R. Kelley Pace and Ronald Barry, \u201cSparse Spatial Autoregressions,\u201d Statistics\n& Probability Letters 33, no. 3 (1997): 291\u2013297. \u2022 Popular open data repositories\n\u2014 UC Irvine Machine Learning Repository\n\u2014 Kaggle datasets\n\u2014 Amazon\u2019s AWS datasets\n\u2022 Meta portals (they list open data repositories)\n\u2014 Data Portals\n\u2014 OpenDataMonitor\n\u2014 Quandl\n\u2022 Other pages listing many popular open data repositories\n\u2014 Wikipedia\u2019s list of Machine Learning datasets\n\u2014 Quora.com\n\u2014 The datasets subreddit\nIn this chapter we\u2019ll use the California Housing Prices dataset from the StatLib repos\u2010\nitory2 (see Figure 2-1). This dataset is based on data from the 1990 California census. It is not exactly recent (a nice house in the Bay Area was still affordable at the time),\nbut it has many qualities for learning, so we will pretend it is recent data. For teaching\npurposes I\u2019ve added a categorical attribute and removed a few features. Figure 2-1. California housing prices | Chapter 2: End-to-End Machine Learning Project\n3 A piece of information fed to a Machine Learning system is often called a signal, in reference to Claude Shan\u2010\nnon\u2019s information theory, which he developed at Bell Labs to improve telecommunications."
  },
  {
    "id": 36,
    "content": "His theory: you\nwant a high signal-to-noise ratio. Look at the Big Picture\nWelcome to the Machine Learning Housing Corporation! Your first task is to use Cal\u2010\nifornia census data to build a model of housing prices in the state. This data includes\nmetrics such as the population, median income, and median housing price for each\nblock group in California. Block groups are the smallest geographical unit for which\nthe US Census Bureau publishes sample data (a block group typically has a popula\u2010\ntion of 600 to 3,000 people). We will call them \u201cdistricts\u201d for short. Your model should learn from this data and be able to predict the median housing\nprice in any district, given all the other metrics. Since you are a well-organized data scientist, the first thing you\nshould do is pull out your Machine Learning project checklist. You\ncan start with the one in Appendix B; it should work reasonably\nwell for most Machine Learning projects, but make sure to adapt it\nto your needs. In this chapter we will go through many checklist\nitems, but we will also skip a few, either because they are self-\nexplanatory or because they will be discussed in later chapters. Frame the Problem\nThe first question to ask your boss is what exactly the business objective is. Building a\nmodel is probably not the end goal. How does the company expect to use and benefit\nfrom this model? Knowing the objective is important because it will determine how\nyou frame the problem, which algorithms you will select, which performance meas\u2010\nure you will use to evaluate your model, and how much effort you will spend tweak\u2010\ning it. Your boss answers that your model\u2019s output (a prediction of a district\u2019s median hous\u2010\ning price) will be fed to another Machine Learning system (see Figure 2-2), along\nwith many other signals.3 This downstream system will determine whether it is worth\ninvesting in a given area or not. Getting this right is critical, as it directly affects\nrevenue. Look at the Big Picture | Figure 2-2. A Machine Learning pipeline for real estate investments\nPipelines\nA sequence of data processing components is called a data pipeline. Pipelines are very\ncommon in Machine Learning systems, since there is a lot of data to manipulate and\nmany data transformations to apply. Components typically run asynchronously. Each component pulls in a large amount\nof data, processes it, and spits out the result in another data store. Then, some time\nlater, the next component in the pipeline pulls this data and spits out its own output. Each component is fairly self-contained: the interface between components is simply\nthe data store. This makes the system simple to grasp (with the help of a data flow\ngraph), and different teams can focus on different components. Moreover, if a com\u2010\nponent breaks down, the downstream components can often continue to run nor\u2010\nmally (at least for a while) by just using the last output from the broken component."
  },
  {
    "id": 37,
    "content": "This makes the architecture quite robust. On the other hand, a broken component can go unnoticed for some time if proper\nmonitoring is not implemented. The data gets stale and the overall system\u2019s perfor\u2010\nmance drops. The next question to ask your boss is what the current solution looks like (if any). The current situation will often give you a reference for performance, as well as\ninsights on how to solve the problem. Your boss answers that the district housing pri\u2010\nces are currently estimated manually by experts: a team gathers up-to-date informa\u2010\ntion about a district, and when they cannot get the median housing price, they\nestimate it using complex rules. This is costly and time-consuming, and their estimates are not great; in cases where\nthey manage to find out the actual median housing price, they often realize that their\nestimates were off by more than 20%. This is why the company thinks that it would\nbe useful to train a model to predict a district\u2019s median housing price, given other | Chapter 2: End-to-End Machine Learning Project\ndata about that district. The census data looks like a great dataset to exploit for this\npurpose, since it includes the median housing prices of thousands of districts, as well\nas other data. With all this information, you are now ready to start designing your system. First,\nyou need to frame the problem: is it supervised, unsupervised, or Reinforcement\nLearning? Is it a classification task, a regression task, or something else? Should you\nuse batch learning or online learning techniques? Before you read on, pause and try\nto answer these questions for yourself. Have you found the answers? Let\u2019s see: it is clearly a typical supervised learning task,\nsince you are given labeled training examples (each instance comes with the expected\noutput, i.e., the district\u2019s median housing price). It is also a typical regression task,\nsince you are asked to predict a value. More specifically, this is a multiple regression\nproblem, since the system will use multiple features to make a prediction (it will use\nthe district\u2019s population, the median income, etc.). It is also a univariate regression\nproblem, since we are only trying to predict a single value for each district. If we were\ntrying to predict multiple values per district, it would be a multivariate regression\nproblem. Finally, there is no continuous flow of data coming into the system, there is\nno particular need to adjust to changing data rapidly, and the data is small enough to\nfit in memory, so plain batch learning should do just fine. If the data were huge, you could either split your batch learning\nwork across multiple servers (using the MapReduce technique) or\nuse an online learning technique. Select a Performance Measure\nYour next step is to select a performance measure. A typical performance measure for\nregression problems is the Root Mean Square Error (RMSE)."
  },
  {
    "id": 38,
    "content": "It gives an idea of how\nmuch error the system typically makes in its predictions, with a higher weight for\nlarge errors. Equation 2-1 shows the mathematical formula to compute the RMSE. Equation 2-1. Root Mean Square Error (RMSE)\nRMSE X, h = m \u2211\ni = 1\nm\nh x i\n\u2212y i 2\nLook at the Big Picture | 4 Recall that the transpose operator flips a column vector into a row vector (and vice versa). Notations\nThis equation introduces several very common Machine Learning notations that we\nwill use throughout this book:\n\u2022 m is the number of instances in the dataset you are measuring the RMSE on. \u2014 For example, if you are evaluating the RMSE on a validation set of 2,000 dis\u2010\ntricts, then m = 2,000. \u2022 x(i) is a vector of all the feature values (excluding the label) of the ith instance in\nthe dataset, and y(i) is its label (the desired output value for that instance). \u2014 For example, if the first district in the dataset is located at longitude \u2013118.29\u00b0,\nlatitude 33.91\u00b0, and it has 1,416 inhabitants with a median income of $38,372,\nand the median house value is $156,400 (ignoring the other features for now),\nthen:\nx 1 =\n\u2212118.29\n33.91\n1,416\n38,372\nand:\ny 1 = 156,400\n\u2022 X is a matrix containing all the feature values (excluding labels) of all instances in\nthe dataset. There is one row per instance, and the ith row is equal to the trans\u2010\npose of x(i), noted (x(i))\u22ba.4\n\u2014 For example, if the first district is as just described, then the matrix X looks\nlike this:\nX =\nx 1 \u22ba\nx 2 \u22ba\n\u22ee\nx 1999 \u22ba\nx 2000 \u22ba\n= \u2212118.29 33.91 1,416 38,372\n\u22ee\n\u22ee\n\u22ee\n\u22ee | Chapter 2: End-to-End Machine Learning Project\n\u2022 h is your system\u2019s prediction function, also called a hypothesis. When your system\nis given an instance\u2019s feature vector x(i), it outputs a predicted value \u0177(i) = h(x(i))\nfor that instance (\u0177 is pronounced \u201cy-hat\u201d). \u2014 For example, if your system predicts that the median housing price in the first\ndistrict is $158,400, then \u0177(1) = h(x(1)) = 158,400. The prediction error for this\ndistrict is \u0177(1) \u2013 y(1) = 2,000. \u2022 RMSE(X,h) is the cost function measured on the set of examples using your\nhypothesis h.\nWe use lowercase italic font for scalar values (such as m or y(i)) and function names\n(such as h), lowercase bold font for vectors (such as x(i)), and uppercase bold font for\nmatrices (such as X). Even though the RMSE is generally the preferred performance measure for regression\ntasks, in some contexts you may prefer to use another function. For example, suppose\nthat there are many outlier districts. In that case, you may consider using the mean\nabsolute error (MAE, also called the average absolute deviation; see Equation 2-2):\nEquation 2-2."
  },
  {
    "id": 39,
    "content": "Mean absolute error (MAE)\nMAE X, h = 1\nm \u2211\ni = 1\nm\nh x i\n\u2212y i\nBoth the RMSE and the MAE are ways to measure the distance between two vectors:\nthe vector of predictions and the vector of target values. Various distance measures,\nor norms, are possible:\n\u2022 Computing the root of a sum of squares (RMSE) corresponds to the Euclidean\nnorm: this is the notion of distance you are familiar with. It is also called the \u21132\nnorm, noted \u2225 \u00b7 \u22252 (or just \u2225 \u00b7 \u2225). \u2022 Computing the sum of absolutes (MAE) corresponds to the \u21131 norm, noted \u2225 \u00b7 \u22251. This is sometimes called the Manhattan norm because it measures the distance\nbetween two points in a city if you can only travel along orthogonal city blocks. \u2022 More generally, the \u2113k norm of a vector v containing n elements is defined as \u2225v\u2225k\n= (|v0|k + |v1|k + ... + |vn|k)1/k. \u21130 gives the number of nonzero elements in the vec\u2010\ntor, and \u2113\u221e gives the maximum absolute value in the vector. \u2022 The higher the norm index, the more it focuses on large values and neglects small\nones. This is why the RMSE is more sensitive to outliers than the MAE. But when\noutliers are exponentially rare (like in a bell-shaped curve), the RMSE performs\nvery well and is generally preferred. Look at the Big Picture | 5 The latest version of Python 3 is recommended. Python 2.7+ may work too, but now that it\u2019s deprecated, all\nmajor scientific libraries are dropping support for it, so you should migrate to Python 3 as soon as possible. Check the Assumptions\nLastly, it is good practice to list and verify the assumptions that have been made so far\n(by you or others); this can help you catch serious issues early on. For example, the\ndistrict prices that your system outputs are going to be fed into a downstream\nMachine Learning system, and you assume that these prices are going to be used as\nsuch. But what if the downstream system converts the prices into categories (e.g.,\n\u201ccheap,\u201d \u201cmedium,\u201d or \u201cexpensive\u201d) and then uses those categories instead of the pri\u2010\nces themselves? In this case, getting the price perfectly right is not important at all;\nyour system just needs to get the category right. If that\u2019s so, then the problem should\nhave been framed as a classification task, not a regression task. You don\u2019t want to find\nthis out after working on a regression system for months. Fortunately, after talking with the team in charge of the downstream system, you are\nconfident that they do indeed need the actual prices, not just categories. Great! You\u2019re\nall set, the lights are green, and you can start coding now! Get the Data\nIt\u2019s time to get your hands dirty. Don\u2019t hesitate to pick up your laptop and walk\nthrough the following code examples in a Jupyter notebook."
  },
  {
    "id": 40,
    "content": "The full Jupyter note\u2010\nbook is available at \nCreate the Workspace\nFirst you will need to have Python installed. It is probably already installed on your\nsystem. If not, you can get it at \nNext you need to create a workspace directory for your Machine Learning code and\ndatasets. Open a terminal and type the following commands (after the $ prompts):\n$ export ML_PATH=\"$HOME/ml\" # You can change the path if you prefer\n$ mkdir -p $ML_PATH\nYou will need a number of Python modules: Jupyter, NumPy, pandas, Matplotlib, and\nScikit-Learn. If you already have Jupyter running with all these modules installed,\nyou can safely skip to \u201cDownload the Data\u201d on page 46. If you don\u2019t have them yet,\nthere are many ways to install them (and their dependencies). You can use your\nsystem\u2019s packaging system (e.g., apt-get on Ubuntu, or MacPorts or Homebrew on\nmacOS), install a Scientific Python distribution such as Anaconda and use its packag\u2010\ning system, or just use Python\u2019s own packaging system, pip, which is included by | Chapter 2: End-to-End Machine Learning Project\n6 I\u2019ll show the installation steps using pip in a bash shell on a Linux or macOS system. You may need to adapt\nthese commands to your own system. On Windows, I recommend installing Anaconda instead. 7 If you want to upgrade pip for all users on your machine rather than just your own user, you should remove\nthe --user option and make sure you have administrator rights (e.g., by adding sudo before the whole com\u2010\nmand on Linux or macOS). 8 Alternative tools include venv (very similar to virtualenv and included in the standard library), virtualenv\u2010\nwrapper (provides extra functionalities on top of virtualenv), pyenv (allows easy switching between Python\nversions), and pipenv (a great packaging tool by the same author as the popular requests library, built on top\nof pip and virtualenv). default with the Python binary installers (since Python 2.7.9).6 You can check to see if\npip is installed by typing the following command:\n$ python3 -m pip --version\npip 19.3.1 from [...]/lib/python3.7/site-packages/pip (python 3.7)\nYou should make sure you have a recent version of pip installed. To upgrade the pip\nmodule, type the following (the exact version may differ):7\n$ python3 -m pip install --user -U pip\nCollecting pip\n[...]\nSuccessfully installed pip-19.3.1\nCreating an Isolated Environment\nIf you would like to work in an isolated environment (which is strongly recom\u2010\nmended so that you can work on different projects without having conflicting library\nversions), install virtualenv8 by running the following pip command (again, if you\nwant virtualenv to be installed for all users on your machine, remove --user and run\nthis command with administrator rights):\n$ python3 -m pip install --user -U virtualenv\nCollecting virtualenv\n[...]\nSuccessfully installed virtualenv-16.7.6\nNow you can create an isolated Python environment by typing this:\n$ cd $ML_PATH\n$ python3 -m virtualenv my_env\nUsing base prefix '[...]'\nNew python executable in [...]/ml/my_env/bin/python3\nAlso creating executable in [...]/ml/my_env/bin/python\nInstalling setuptools, pip, wheel...done."
  },
  {
    "id": 41,
    "content": "Now every time you want to activate this environment, just open a terminal and type\nthe following:\nGet the Data | 9 Note that Jupyter can handle multiple versions of Python, and even many other languages such as R or\nOctave. $ cd $ML_PATH\n$ source my_env/bin/activate # on Linux or macOS\n$ .\\my_env\\Scripts\\activate # on Windows\nTo deactivate this environment, type deactivate. While the environment is active,\nany package you install using pip will be installed in this isolated environment, and\nPython will only have access to these packages (if you also want access to the system\u2019s\npackages, you should create the environment using virtualenv\u2019s --system-site-\npackages option). Check out virtualenv\u2019s documentation for more information. Now you can install all the required modules and their dependencies using this sim\u2010\nple pip command (if you are not using a virtualenv, you will need the --user option\nor administrator rights):\n$ python3 -m pip install -U jupyter matplotlib numpy pandas scipy scikit-learn\nCollecting jupyter Downloading \nCollecting matplotlib [...]\nIf you created a virtualenv, you need to register it to Jupyter and give it a name:\n$ python3 -m ipykernel install --user --name=python3\nNow you can fire up Jupyter by typing the following command:\n$ jupyter notebook\n[...] Serving notebooks from local directory: [...]/ml\n[...] The Jupyter Notebook is running at:\n[...] \n[...] or \n[...] Use Control-C to stop this server and shut down all kernels [...]\nA Jupyter server is now running in your terminal, listening to port 8888. You can visit\nthis server by opening your web browser to  (this usually hap\u2010\npens automatically when the server starts). You should see your empty workspace\ndirectory (containing only the env directory if you followed the preceding virtualenv\ninstructions). Now create a new Python notebook by clicking the New button and selecting the\nappropriate Python version9 (see Figure 2-3). Doing that will create a new notebook\nfile called Untitled.ipynb in your workspace, start a Jupyter Python kernel to run the\nnotebook, and open this notebook in a new tab. You should start by renaming this\nnotebook to \u201cHousing\u201d (this will automatically rename the file to Housing.ipynb) by\nclicking Untitled and typing the new name. | Chapter 2: End-to-End Machine Learning Project\nFigure 2-3. Your workspace in Jupyter\nA notebook contains a list of cells. Each cell can contain executable code or formatted\ntext. Right now the notebook contains only one empty code cell, labeled \u201cIn [1]:\u201d. Try\ntyping print(\"Hello world!\") in the cell and clicking the play button (see\nFigure 2-4) or pressing Shift-Enter. This sends the current cell to this notebook\u2019s\nPython kernel, which runs it and returns the output. The result is displayed below the\ncell, and since you\u2019ve reached the end of the notebook, a new cell is automatically cre\u2010\nated. Go through the User Interface Tour from Jupyter\u2019s Help menu to learn the\nbasics. Figure 2-4."
  },
  {
    "id": 42,
    "content": "Hello world Python notebook\nGet the Data | 10 You might also need to check legal constraints, such as private fields that should never be copied to unsafe\ndata stores. 11 In a real project you would save this code in a Python file, but for now you can just write it in your Jupyter\nnotebook. Download the Data\nIn typical environments your data would be available in a relational database (or\nsome other common data store) and spread across multiple tables/documents/files. To access it, you would first need to get your credentials and access authorizations10\nand familiarize yourself with the data schema. In this project, however, things are\nmuch simpler: you will just download a single compressed file, housing.tgz, which\ncontains a comma-separated values (CSV) file called housing.csv with all the data. You could use your web browser to download the file and run tar xzf housing.tgz\nto decompress it and extract the CSV file, but it is preferable to create a small func\u2010\ntion to do that. Having a function that downloads the data is useful in particular if the\ndata changes regularly: you can write a small script that uses the function to fetch the\nlatest data (or you can set up a scheduled job to do that automatically at regular inter\u2010\nvals). Automating the process of fetching the data is also useful if you need to install\nthe dataset on multiple machines. Here is the function to fetch the data:11\nimport os\nimport tarfile\nimport urllib\nDOWNLOAD_ROOT = \"\nHOUSING_PATH = os.path.join(\"datasets\", \"housing\")\nHOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\ndef fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): os.makedirs(housing_path, exist_ok=True) tgz_path = os.path.join(housing_path, \"housing.tgz\") urllib.request.urlretrieve(housing_url, tgz_path) housing_tgz = tarfile.open(tgz_path) housing_tgz.extractall(path=housing_path) housing_tgz.close()\nNow when you call fetch_housing_data(), it creates a datasets/housing directory in\nyour workspace, downloads the housing.tgz file, and extracts the housing.csv file from\nit in this directory. | Chapter 2: End-to-End Machine Learning Project\nNow let\u2019s load the data using pandas. Once again, you should write a small function\nto load the data:\nimport pandas as pd\ndef load_housing_data(housing_path=HOUSING_PATH): csv_path = os.path.join(housing_path, \"housing.csv\") return pd.read_csv(csv_path)\nThis function returns a pandas DataFrame object containing all the data. Take a Quick Look at the Data Structure\nLet\u2019s take a look at the top five rows using the DataFrame\u2019s head() method (see\nFigure 2-5). Figure 2-5. Top five rows in the dataset\nEach row represents one district. There are 10 attributes (you can see the first 6 in the\nscreenshot): longitude, latitude, housing_median_age, total_rooms, total_bed\nrooms, population, households, median_income, median_house_value, and\nocean_proximity. The info() method is useful to get a quick description of the data, in particular the\ntotal number of rows, each attribute\u2019s type, and the number of nonnull values (see\nFigure 2-6). Get the Data | Figure 2-6. Housing info\nThere are 20,640 instances in the dataset, which means that it is fairly small by\nMachine Learning standards, but it\u2019s perfect to get started. Notice that the total_bed\nrooms attribute has only 20,433 nonnull values, meaning that 207 districts are missing\nthis feature."
  },
  {
    "id": 43,
    "content": "We will need to take care of this later. All attributes are numerical, except the ocean_proximity field. Its type is object, so it\ncould hold any kind of Python object. But since you loaded this data from a CSV file,\nyou know that it must be a text attribute. When you looked at the top five rows, you\nprobably noticed that the values in the ocean_proximity column were repetitive,\nwhich means that it is probably a categorical attribute. You can find out what cate\u2010\ngories exist and how many districts belong to each category by using the\nvalue_counts() method:\n>>> housing[\"ocean_proximity\"].value_counts()\n<1H OCEAN 9136\nINLAND 6551\nNEAR OCEAN 2658\nNEAR BAY 2290\nISLAND 5\nName: ocean_proximity, dtype: int64\nLet\u2019s look at the other fields. The describe() method shows a summary of the\nnumerical attributes (Figure 2-7). | Chapter 2: End-to-End Machine Learning Project\n12 The standard deviation is generally denoted \u03c3 (the Greek letter sigma), and it is the square root of the var\u2010\niance, which is the average of the squared deviation from the mean. When a feature has a bell-shaped normal\ndistribution (also called a Gaussian distribution), which is very common, the \u201c68-95-99.7\u201d rule applies: about\n68% of the values fall within 1\u03c3 of the mean, 95% within 2\u03c3, and 99.7% within 3\u03c3. Figure 2-7. Summary of each numerical attribute\nThe count, mean, min, and max rows are self-explanatory. Note that the null values are\nignored (so, for example, the count of total_bedrooms is 20,433, not 20,640). The\nstd row shows the standard deviation, which measures how dispersed the values are.12\nThe 25%, 50%, and 75% rows show the corresponding percentiles: a percentile indi\u2010\ncates the value below which a given percentage of observations in a group of observa\u2010\ntions fall. For example, 25% of the districts have a housing_median_age lower than\n18, while 50% are lower than 29 and 75% are lower than 37. These are often called the\n25th percentile (or first quartile), the median, and the 75th percentile (or third\nquartile). Another quick way to get a feel of the type of data you are dealing with is to plot a\nhistogram for each numerical attribute. A histogram shows the number of instances\n(on the vertical axis) that have a given value range (on the horizontal axis). You can\neither plot this one attribute at a time, or you can call the hist() method on the\nwhole dataset (as shown in the following code example), and it will plot a histogram\nfor each numerical attribute (see Figure 2-8):\n%matplotlib inline # only in a Jupyter notebook\nimport matplotlib.pyplot as plt\nhousing.hist(bins=50, figsize=(20,15))\nplt.show()\nGet the Data | The hist() method relies on Matplotlib, which in turn relies on a\nuser-specified graphical backend to draw on your screen. So before\nyou can plot anything, you need to specify which backend Matplot\u2010\nlib should use. The simplest option is to use Jupyter\u2019s magic com\u2010\nmand %matplotlib inline."
  },
  {
    "id": 44,
    "content": "This tells Jupyter to set up Matplotlib\nso it uses Jupyter\u2019s own backend. Plots are then rendered within the\nnotebook itself. Note that calling show() is optional in a Jupyter\nnotebook, as Jupyter will automatically display plots when a cell is\nexecuted. Figure 2-8. A histogram for each numerical attribute\nThere are a few things you might notice in these histograms:\n1. First, the median income attribute does not look like it is expressed in US dollars\n(USD). After checking with the team that collected the data, you are told that the\ndata has been scaled and capped at 15 (actually, 15.0001) for higher median\nincomes, and at 0.5 (actually, 0.4999) for lower median incomes. The numbers\nrepresent roughly tens of thousands of dollars (e.g., 3 actually means about\n$30,000). Working with preprocessed attributes is common in Machine Learning, | Chapter 2: End-to-End Machine Learning Project\nand it is not necessarily a problem, but you should try to understand how the\ndata was computed. 2. The housing median age and the median house value were also capped. The lat\u2010\nter may be a serious problem since it is your target attribute (your labels). Your\nMachine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system\u2019s out\u2010\nput) to see if this is a problem or not. If they tell you that they need precise pre\u2010\ndictions even beyond $500,000, then you have two options:\na. Collect proper labels for the districts whose labels were capped. b. Remove those districts from the training set (and also from the test set, since\nyour system should not be evaluated poorly if it predicts values beyond\n$500,000). 3. These attributes have very different scales. We will discuss this later in this chap\u2010\nter, when we explore feature scaling. 4. Finally, many histograms are tail-heavy: they extend much farther to the right of\nthe median than to the left. This may make it a bit harder for some Machine\nLearning algorithms to detect patterns. We will try transforming these attributes\nlater on to have more bell-shaped distributions. Hopefully you now have a better understanding of the kind of data you are dealing\nwith. Wait! Before you look at the data any further, you need to create a\ntest set, put it aside, and never look at it. Create a Test Set\nIt may sound strange to voluntarily set aside part of the data at this stage. After all,\nyou have only taken a quick glance at the data, and surely you should learn a whole\nlot more about it before you decide what algorithms to use, right? This is true, but\nyour brain is an amazing pattern detection system, which means that it is highly\nprone to overfitting: if you look at the test set, you may stumble upon some seemingly\ninteresting pattern in the test data that leads you to select a particular kind of\nMachine Learning model."
  },
  {
    "id": 45,
    "content": "When you estimate the generalization error using the test\nset, your estimate will be too optimistic, and you will launch a system that will not\nperform as well as expected. This is called data snooping bias. Creating a test set is theoretically simple: pick some instances randomly, typically\n20% of the dataset (or less if your dataset is very large), and set them aside:\nGet the Data | 13 In this book, when a code example contains a mix of code and outputs, as is the case here, it is formatted like\nin the Python interpreter, for better readability: the code lines are prefixed with >>> (or ... for indented\nblocks), and the outputs have no prefix. 14 You will often see people set the random seed to 42. This number has no special property, other than to be the\nAnswer to the Ultimate Question of Life, the Universe, and Everything. import numpy as np\ndef split_train_test(data, test_ratio): shuffled_indices = np.random.permutation(len(data)) test_set_size = int(len(data) * test_ratio) test_indices = shuffled_indices[:test_set_size] train_indices = shuffled_indices[test_set_size:] return data.iloc[train_indices], data.iloc[test_indices]\nYou can then use this function like this:13\n>>> train_set, test_set = split_train_test(housing, 0.2)\n>>> len(train_set) >>> len(test_set) Well, this works, but it is not perfect: if you run the program again, it will generate a\ndifferent test set! Over time, you (or your Machine Learning algorithms) will get to\nsee the whole dataset, which is what you want to avoid. One solution is to save the test set on the first run and then load it in subsequent\nruns. Another option is to set the random number generator\u2019s seed (e.g., with np.ran\ndom.seed(42))14 before calling np.random.permutation() so that it always generates\nthe same shuffled indices. But both these solutions will break the next time you fetch an updated dataset. To\nhave a stable train/test split even after updating the dataset, a common solution is to\nuse each instance\u2019s identifier to decide whether or not it should go in the test set\n(assuming instances have a unique and immutable identifier). For example, you could\ncompute a hash of each instance\u2019s identifier and put that instance in the test set if the\nhash is lower than or equal to 20% of the maximum hash value. This ensures that the\ntest set will remain consistent across multiple runs, even if you refresh the dataset. The new test set will contain 20% of the new instances, but it will not contain any\ninstance that was previously in the training set. Here is a possible implementation:\nfrom zlib import crc32\ndef test_set_check(identifier, test_ratio): return crc32(np.int64(identifier)) & 0xffffffff < test_ratio * 2**32 | Chapter 2: End-to-End Machine Learning Project\n15 The location information is actually quite coarse, and as a result many districts will have the exact same ID, so\nthey will end up in the same set (test or train). This introduces some unfortunate sampling bias."
  },
  {
    "id": 46,
    "content": "def split_train_test_by_id(data, test_ratio, id_column): ids = data[id_column] in_test_set = ids.apply(lambda id_: test_set_check(id_, test_ratio)) return data.loc[~in_test_set], data.loc[in_test_set]\nUnfortunately, the housing dataset does not have an identifier column. The simplest\nsolution is to use the row index as the ID:\nhousing_with_id = housing.reset_index() # adds an `index` column\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"index\")\nIf you use the row index as a unique identifier, you need to make sure that new data\ngets appended to the end of the dataset and that no row ever gets deleted. If this is not\npossible, then you can try to use the most stable features to build a unique identifier. For example, a district\u2019s latitude and longitude are guaranteed to be stable for a few\nmillion years, so you could combine them into an ID like so:15\nhousing_with_id[\"id\"] = housing[\"longitude\"] * 1000 + housing[\"latitude\"]\ntrain_set, test_set = split_train_test_by_id(housing_with_id, 0.2, \"id\")\nScikit-Learn provides a few functions to split datasets into multiple subsets in various\nways. The simplest function is train_test_split(), which does pretty much the\nsame thing as the function split_train_test(), with a couple of additional features. First, there is a random_state parameter that allows you to set the random generator\nseed. Second, you can pass it multiple datasets with an identical number of rows, and\nit will split them on the same indices (this is very useful, for example, if you have a\nseparate DataFrame for labels):\nfrom sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)\nSo far we have considered purely random sampling methods. This is generally fine if\nyour dataset is large enough (especially relative to the number of attributes), but if it\nis not, you run the risk of introducing a significant sampling bias. When a survey\ncompany decides to call 1,000 people to ask them a few questions, they don\u2019t just pick\n1,000 people randomly in a phone book. They try to ensure that these 1,000 people\nare representative of the whole population. For example, the US population is 51.3%\nfemales and 48.7% males, so a well-conducted survey in the US would try to maintain\nthis ratio in the sample: 513 female and 487 male. This is called stratified sampling:\nthe population is divided into homogeneous subgroups called strata, and the right\nnumber of instances are sampled from each stratum to guarantee that the test set is\nrepresentative of the overall population. If the people running the survey used purely\nrandom sampling, there would be about a 12% chance of sampling a skewed test set\nGet the Data | that was either less than 49% female or more than 54% female. Either way, the survey\nresults would be significantly biased. Suppose you chatted with experts who told you that the median income is a very\nimportant attribute to predict median housing prices. You may want to ensure that\nthe test set is representative of the various categories of incomes in the whole dataset."
  },
  {
    "id": 47,
    "content": "Since the median income is a continuous numerical attribute, you first need to create\nan income category attribute. Let\u2019s look at the median income histogram more closely\n(back in Figure 2-8): most median income values are clustered around 1.5 to 6 (i.e.,\n$15,000\u2013$60,000), but some median incomes go far beyond 6. It is important to have\na sufficient number of instances in your dataset for each stratum, or else the estimate\nof a stratum\u2019s importance may be biased. This means that you should not have too\nmany strata, and each stratum should be large enough. The following code uses the\npd.cut() function to create an income category attribute with five categories (labeled\nfrom 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from\n1.5 to 3, and so on:\nhousing[\"income_cat\"] = pd.cut(housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])\nThese income categories are represented in Figure 2-9:\nhousing[\"income_cat\"].hist()\nFigure 2-9. Histogram of income categories\nNow you are ready to do stratified sampling based on the income category. For this\nyou can use Scikit-Learn\u2019s StratifiedShuffleSplit class: | Chapter 2: End-to-End Machine Learning Project\nfrom sklearn.model_selection import StratifiedShuffleSplit\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"income_cat\"]): strat_train_set = housing.loc[train_index] strat_test_set = housing.loc[test_index]\nLet\u2019s see if this worked as expected. You can start by looking at the income category\nproportions in the test set:\n>>> strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set)\n3 0.350533\n2 0.318798\n4 0.176357\n5 0.114583\n1 0.039729\nName: income_cat, dtype: float64\nWith similar code you can measure the income category proportions in the full data\u2010\nset. Figure 2-10 compares the income category proportions in the overall dataset, in\nthe test set generated with stratified sampling, and in a test set generated using purely\nrandom sampling. As you can see, the test set generated using stratified sampling has\nincome category proportions almost identical to those in the full dataset, whereas the\ntest set generated using purely random sampling is skewed. Figure 2-10. Sampling bias comparison of stratified versus purely random sampling\nNow you should remove the income_cat attribute so the data is back to its original\nstate:\nfor set_ in (strat_train_set, strat_test_set): set_.drop(\"income_cat\", axis=1, inplace=True)\nWe spent quite a bit of time on test set generation for a good reason: this is an often\nneglected but critical part of a Machine Learning project. Moreover, many of these\nideas will be useful later when we discuss cross-validation. Now it\u2019s time to move on\nto the next stage: exploring the data. Get the Data | Discover and Visualize the Data to Gain Insights\nSo far you have only taken a quick glance at the data to get a general understanding of\nthe kind of data you are manipulating. Now the goal is to go into a little more depth. First, make sure you have put the test set aside and you are only exploring the train\u2010\ning set."
  },
  {
    "id": 48,
    "content": "Also, if the training set is very large, you may want to sample an exploration\nset, to make manipulations easy and fast. In our case, the set is quite small, so you can\njust work directly on the full set. Let\u2019s create a copy so that you can play with it\nwithout harming the training set:\nhousing = strat_train_set.copy()\nVisualizing Geographical Data\nSince there is geographical information (latitude and longitude), it is a good idea to\ncreate a scatterplot of all districts to visualize the data (Figure 2-11):\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\")\nFigure 2-11. A geographical scatterplot of the data\nThis looks like California all right, but other than that it is hard to see any particular\npattern. Setting the alpha option to 0.1 makes it much easier to visualize the places\nwhere there is a high density of data points (Figure 2-12):\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1) | Chapter 2: End-to-End Machine Learning Project\n16 If you are reading this in grayscale, grab a red pen and scribble over most of the coastline from the Bay Area\ndown to San Diego (as you might expect). You can add a patch of yellow around Sacramento as well. Figure 2-12. A better visualization that highlights high-density areas\nNow that\u2019s much better: you can clearly see the high-density areas, namely the Bay\nArea and around Los Angeles and San Diego, plus a long line of fairly high density in\nthe Central Valley, in particular around Sacramento and Fresno. Our brains are very good at spotting patterns in pictures, but you may need to play\naround with visualization parameters to make the patterns stand out. Now let\u2019s look at the housing prices (Figure 2-13). The radius of each circle represents\nthe district\u2019s population (option s), and the color represents the price (option c). We\nwill use a predefined color map (option cmap) called jet, which ranges from blue\n(low values) to red (high prices):16\nhousing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4, s=housing[\"population\"]/100, label=\"population\", figsize=(10,7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n)\nplt.legend()\nDiscover and Visualize the Data to Gain Insights | Figure 2-13. California housing prices: red is expensive, blue is cheap, larger circles indi\u2010\ncate areas with a larger population\nThis image tells you that the housing prices are very much related to the location\n(e.g., close to the ocean) and to the population density, as you probably knew already. A clustering algorithm should be useful for detecting the main cluster and for adding\nnew features that measure the proximity to the cluster centers. The ocean proximity\nattribute may be useful as well, although in Northern California the housing prices in\ncoastal districts are not too high, so it is not a simple rule."
  },
  {
    "id": 49,
    "content": "Looking for Correlations\nSince the dataset is not too large, you can easily compute the standard correlation\ncoefficient (also called Pearson\u2019s r) between every pair of attributes using the corr()\nmethod:\ncorr_matrix = housing.corr()\nNow let\u2019s look at how much each attribute correlates with the median house value:\n>>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\nmedian_house_value 1.000000\nmedian_income 0.687170\ntotal_rooms 0.135231\nhousing_median_age 0.114220 | Chapter 2: End-to-End Machine Learning Project\nhouseholds 0.064702\ntotal_bedrooms 0.047865\npopulation -0.026699\nlongitude -0.047279\nlatitude -0.142826\nName: median_house_value, dtype: float64\nThe correlation coefficient ranges from \u20131 to 1. When it is close to 1, it means that\nthere is a strong positive correlation; for example, the median house value tends to go\nup when the median income goes up. When the coefficient is close to \u20131, it means\nthat there is a strong negative correlation; you can see a small negative correlation\nbetween the latitude and the median house value (i.e., prices have a slight tendency to\ngo down when you go north). Finally, coefficients close to 0 mean that there is no\nlinear correlation. Figure 2-14 shows various plots along with the correlation coeffi\u2010\ncient between their horizontal and vertical axes. Figure 2-14. Standard correlation coefficient of various datasets (source: Wikipedia;\npublic domain image)\nThe correlation coefficient only measures linear correlations (\u201cif x\ngoes up, then y generally goes up/down\u201d). It may completely miss\nout on nonlinear relationships (e.g., \u201cif x is close to 0, then y gener\u2010\nally goes up\u201d). Note how all the plots of the bottom row have a cor\u2010\nrelation coefficient equal to 0, despite the fact that their axes are\nclearly not independent: these are examples of nonlinear relation\u2010\nships. Also, the second row shows examples where the correlation\ncoefficient is equal to 1 or \u20131; notice that this has nothing to do\nwith the slope. For example, your height in inches has a correlation\ncoefficient of 1 with your height in feet or in nanometers. Another way to check for correlation between attributes is to use the pandas\nscatter_matrix() function, which plots every numerical attribute against every\nDiscover and Visualize the Data to Gain Insights | other numerical attribute. Since there are now 11 numerical attributes, you would get\n112 = 121 plots, which would not fit on a page\u2014so let\u2019s just focus on a few promising\nattributes that seem most correlated with the median housing value (Figure 2-15):\nfrom pandas.plotting import scatter_matrix\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))\nFigure 2-15. This scatter matrix plots every numerical attribute against every other\nnumerical attribute, plus a histogram of each numerical attribute\nThe main diagonal (top left to bottom right) would be full of straight lines if pandas\nplotted each variable against itself, which would not be very useful. So instead pandas\ndisplays a histogram of each attribute (other options are available; see the pandas\ndocumentation for more details)."
  },
  {
    "id": 50,
    "content": "The most promising attribute to predict the median house value is the median\nincome, so let\u2019s zoom in on their correlation scatterplot (Figure 2-16):\nhousing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1) | Chapter 2: End-to-End Machine Learning Project\nFigure 2-16. Median income versus median house value\nThis plot reveals a few things. First, the correlation is indeed very strong; you can\nclearly see the upward trend, and the points are not too dispersed. Second, the price\ncap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this\nplot reveals other less obvious straight lines: a horizontal line around $450,000,\nanother around $350,000, perhaps one around $280,000, and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms\nfrom learning to reproduce these data quirks. Experimenting with Attribute Combinations\nHopefully the previous sections gave you an idea of a few ways you can explore the\ndata and gain insights. You identified a few data quirks that you may want to clean up\nbefore feeding the data to a Machine Learning algorithm, and you found interesting\ncorrelations between attributes, in particular with the target attribute. You also\nnoticed that some attributes have a tail-heavy distribution, so you may want to trans\u2010\nform them (e.g., by computing their logarithm). Of course, your mileage will vary\nconsiderably with each project, but the general ideas are similar. One last thing you may want to do before preparing the data for Machine Learning\nalgorithms is to try out various attribute combinations. For example, the total num\u2010\nber of rooms in a district is not very useful if you don\u2019t know how many households\nthere are. What you really want is the number of rooms per household. Similarly, the\ntotal number of bedrooms by itself is not very useful: you probably want to compare\nit to the number of rooms. And the population per household also seems like an\ninteresting attribute combination to look at. Let\u2019s create these new attributes:\nDiscover and Visualize the Data to Gain Insights | housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]\nAnd now let\u2019s look at the correlation matrix again:\n>>> corr_matrix = housing.corr()\n>>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)\nmedian_house_value 1.000000\nmedian_income 0.687160\nrooms_per_household 0.146285\ntotal_rooms 0.135097\nhousing_median_age 0.114110\nhouseholds 0.064506\ntotal_bedrooms 0.047689\npopulation_per_household -0.021985\npopulation -0.026920\nlongitude -0.047432\nlatitude -0.142724\nbedrooms_per_room -0.259984\nName: median_house_value, dtype: float64\nHey, not bad! The new bedrooms_per_room attribute is much more correlated with\nthe median house value than the total number of rooms or bedrooms. Apparently\nhouses with a lower bedroom/room ratio tend to be more expensive. The number of\nrooms per household is also more informative than the total number of rooms in a\ndistrict\u2014obviously the larger the houses, the more expensive they are. This round of exploration does not have to be absolutely thorough; the point is to\nstart off on the right foot and quickly gain insights that will help you get a first rea\u2010\nsonably good prototype."
  },
  {
    "id": 51,
    "content": "But this is an iterative process: once you get a prototype up\nand running, you can analyze its output to gain more insights and come back to this\nexploration step. Prepare the Data for Machine Learning Algorithms\nIt\u2019s time to prepare the data for your Machine Learning algorithms. Instead of doing\nthis manually, you should write functions for this purpose, for several good reasons:\n\u2022 This will allow you to reproduce these transformations easily on any dataset (e.g.,\nthe next time you get a fresh dataset). \u2022 You will gradually build a library of transformation functions that you can reuse\nin future projects. \u2022 You can use these functions in your live system to transform the new data before\nfeeding it to your algorithms. | Chapter 2: End-to-End Machine Learning Project\n\u2022 This will make it possible for you to easily try various transformations and see\nwhich combination of transformations works best. But first let\u2019s revert to a clean training set (by copying strat_train_set once again). Let\u2019s also separate the predictors and the labels, since we don\u2019t necessarily want to\napply the same transformations to the predictors and the target values (note that\ndrop() creates a copy of the data and does not affect strat_train_set):\nhousing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()\nData Cleaning\nMost Machine Learning algorithms cannot work with missing features, so let\u2019s create\na few functions to take care of them. We saw earlier that the total_bedrooms\nattribute has some missing values, so let\u2019s fix this. You have three options:\n1. Get rid of the corresponding districts. 2. Get rid of the whole attribute. 3. Set the values to some value (zero, the mean, the median, etc.). You can accomplish these easily using DataFrame\u2019s dropna(), drop(), and fillna()\nmethods:\nhousing.dropna(subset=[\"total_bedrooms\"]) # option 1\nhousing.drop(\"total_bedrooms\", axis=1) # option 2\nmedian = housing[\"total_bedrooms\"].median() # option 3\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)\nIf you choose option 3, you should compute the median value on the training set and\nuse it to fill the missing values in the training set. Don\u2019t forget to save the median\nvalue that you have computed. You will need it later to replace missing values in the\ntest set when you want to evaluate your system, and also once the system goes live to\nreplace missing values in new data. Scikit-Learn provides a handy class to take care of missing values: SimpleImputer. Here is how to use it. First, you need to create a SimpleImputer instance, specifying\nthat you want to replace each attribute\u2019s missing values with the median of that\nattribute:\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")\nSince the median can only be computed on numerical attributes, you need to create a\ncopy of the data without the text attribute ocean_proximity:\nhousing_num = housing.drop(\"ocean_proximity\", axis=1)\nPrepare the Data for Machine Learning Algorithms | 17 For more details on the design principles, see Lars Buitinck et al., \u201cAPI Design for Machine Learning Software:\nExperiences from the Scikit-Learn Project\u201d ,\u201d arXiv preprint arXiv:1309.0238 (2013)."
  },
  {
    "id": 52,
    "content": "Now you can fit the imputer instance to the training data using the fit() method:\nimputer.fit(housing_num)\nThe imputer has simply computed the median of each attribute and stored the result\nin its statistics_ instance variable. Only the total_bedrooms attribute had missing\nvalues, but we cannot be sure that there won\u2019t be any missing values in new data after\nthe system goes live, so it is safer to apply the imputer to all the numerical attributes:\n>>> imputer.statistics_\narray([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\n>>> housing_num.median().values\narray([ -118.51 , 34.26 , 29. , 2119.5 , 433. , 1164. , 408. , 3.5409])\nNow you can use this \u201ctrained\u201d imputer to transform the training set by replacing\nmissing values with the learned medians:\nX = imputer.transform(housing_num)\nThe result is a plain NumPy array containing the transformed features. If you want to\nput it back into a pandas DataFrame, it\u2019s simple:\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)\nScikit-Learn Design\nScikit-Learn\u2019s API is remarkably well designed. These are the main design principles:17\nConsistency\nAll objects share a consistent and simple interface:\nEstimators\nAny object that can estimate some parameters based on a dataset is called an\nestimator (e.g., an imputer is an estimator). The estimation itself is per\u2010\nformed by the fit() method, and it takes only a dataset as a parameter (or\ntwo for supervised learning algorithms; the second dataset contains the\nlabels). Any other parameter needed to guide the estimation process is con\u2010\nsidered a hyperparameter (such as an imputer\u2019s strategy), and it must be\nset as an instance variable (generally via a constructor parameter). Transformers\nSome estimators (such as an imputer) can also transform a dataset; these are\ncalled transformers. Once again, the API is simple: the transformation is\nperformed by the transform() method with the dataset to transform as a | Chapter 2: End-to-End Machine Learning Project\n18 Some predictors also provide methods to measure the confidence of their predictions. parameter. It returns the transformed dataset. This transformation generally\nrelies on the learned parameters, as is the case for an imputer. All transform\u2010\ners also have a convenience method called fit_transform() that is equiva\u2010\nlent to calling fit() and then transform() (but sometimes\nfit_transform() is optimized and runs much faster). Predictors\nFinally, some estimators, given a dataset, are capable of making predictions;\nthey are called predictors. For example, the LinearRegression model in the\nprevious chapter was a predictor: given a country\u2019s GDP per capita, it pre\u2010\ndicted life satisfaction. A predictor has a predict() method that takes a\ndataset of new instances and returns a dataset of corresponding predictions. It also has a score() method that measures the quality of the predictions,\ngiven a test set (and the corresponding labels, in the case of supervised learn\u2010\ning algorithms).18\nInspection\nAll the estimator\u2019s hyperparameters are accessible directly via public instance\nvariables (e.g., imputer.strategy), and all the estimator\u2019s learned parameters are\naccessible via public instance variables with an underscore suffix (e.g.,\nimputer.statistics_)."
  },
  {
    "id": 53,
    "content": "Nonproliferation of classes\nDatasets are represented as NumPy arrays or SciPy sparse matrices, instead of\nhomemade classes. Hyperparameters are just regular Python strings or numbers. Composition\nExisting building blocks are reused as much as possible. For example, it is easy to\ncreate a Pipeline estimator from an arbitrary sequence of transformers followed\nby a final estimator, as we will see. Sensible defaults\nScikit-Learn provides reasonable default values for most parameters, making it\neasy to quickly create a baseline working system. Handling Text and Categorical Attributes\nSo far we have only dealt with numerical attributes, but now let\u2019s look at text\nattributes. In this dataset, there is just one: the ocean_proximity attribute. Let\u2019s look\nat its value for the first 10 instances:\nPrepare the Data for Machine Learning Algorithms | 19 This class is available in Scikit-Learn 0.20 and later. If you use an earlier version, please consider upgrading, or\nuse the pandas Series.factorize() method. >>> housing_cat = housing[[\"ocean_proximity\"]]\n>>> housing_cat.head(10) ocean_proximity\n17606 <1H OCEAN\n18632 <1H OCEAN\n14650 NEAR OCEAN\n3230 INLAND\n3555 <1H OCEAN\n19480 INLAND\n8879 <1H OCEAN\n13685 INLAND\n4937 <1H OCEAN\n4861 <1H OCEAN\nIt\u2019s not arbitrary text: there are a limited number of possible values, each of which\nrepresents a category. So this attribute is a categorical attribute. Most Machine Learn\u2010\ning algorithms prefer to work with numbers, so let\u2019s convert these categories from\ntext to numbers. For this, we can use Scikit-Learn\u2019s OrdinalEncoder class:19\n>>> from sklearn.preprocessing import OrdinalEncoder\n>>> ordinal_encoder = OrdinalEncoder()\n>>> housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)\n>>> housing_cat_encoded[:10]\narray([[0. ], [0. ], [4. ], [1. ], [0. ], [1. ], [0. ], [1. ], [0. ], [0.]]) You can get the list of categories using the categories_ instance variable. It is a list\ncontaining a 1D array of categories for each categorical attribute (in this case, a list\ncontaining a single array since there is just one categorical attribute):\n>>> ordinal_encoder.categories_\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)]\nOne issue with this representation is that ML algorithms will assume that two nearby\nvalues are more similar than two distant values. This may be fine in some cases (e.g.,\nfor ordered categories such as \u201cbad,\u201d \u201caverage,\u201d \u201cgood,\u201d and \u201cexcellent\u201d), but it is obvi\u2010\nously not the case for the ocean_proximity column (for example, categories 0 and 4\nare clearly more similar than categories 0 and 1). To fix this issue, a common solution | Chapter 2: End-to-End Machine Learning Project\n20 Before Scikit-Learn 0.20, the method could only encode integer categorical values, but since 0.20 it can also\nhandle other types of inputs, including text categorical inputs. 21 See SciPy\u2019s documentation for more details. is to create one binary attribute per category: one attribute equal to 1 when the cate\u2010\ngory is \u201c<1H OCEAN\u201d (and 0 otherwise), another attribute equal to 1 when the cate\u2010\ngory is \u201cINLAND\u201d (and 0 otherwise), and so on."
  },
  {
    "id": 54,
    "content": "This is called one-hot encoding,\nbecause only one attribute will be equal to 1 (hot), while the others will be 0 (cold). The new attributes are sometimes called dummy attributes. Scikit-Learn provides a\nOneHotEncoder class to convert categorical values into one-hot vectors:20\n>>> from sklearn.preprocessing import OneHotEncoder\n>>> cat_encoder = OneHotEncoder()\n>>> housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n>>> housing_cat_1hot\n<16512x5 sparse matrix of type '<class 'numpy.float64'>' with 16512 stored elements in Compressed Sparse Row format>\nNotice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very\nuseful when you have categorical attributes with thousands of categories. After one-\nhot encoding, we get a matrix with thousands of columns, and the matrix is full of 0s\nexcept for a single 1 per row. Using up tons of memory mostly to store zeros would\nbe very wasteful, so instead a sparse matrix only stores the location of the nonzero\nelements. You can use it mostly like a normal 2D array,21 but if you really want to con\u2010\nvert it to a (dense) NumPy array, just call the toarray() method:\n>>> housing_cat_1hot.toarray()\narray([[1., 0., 0., 0., 0. ], [1., 0., 0., 0., 0. ], [0., 0., 0., 0., 1. ], ..., [0., 1., 0., 0., 0. ], [1., 0., 0., 0., 0. ], [0., 0., 0., 1., 0.]]) Once again, you can get the list of categories using the encoder\u2019s categories_\ninstance variable:\n>>> cat_encoder.categories_\n[array(['<1H OCEAN', 'INLAND', 'ISLAND', 'NEAR BAY', 'NEAR OCEAN'], dtype=object)]\nPrepare the Data for Machine Learning Algorithms | If a categorical attribute has a large number of possible categories\n(e.g., country code, profession, species), then one-hot encoding will\nresult in a large number of input features. This may slow down\ntraining and degrade performance. If this happens, you may want\nto replace the categorical input with useful numerical features\nrelated to the categories: for example, you could replace the\nocean_proximity feature with the distance to the ocean (similarly,\na country code could be replaced with the country\u2019s population and\nGDP per capita). Alternatively, you could replace each category\nwith a learnable, low-dimensional vector called an embedding. Each\ncategory\u2019s representation would be learned during training. This is\nan example of representation learning (see Chapters 13 and 17 for\nmore details). Custom Transformers\nAlthough Scikit-Learn provides many useful transformers, you will need to write\nyour own for tasks such as custom cleanup operations or combining specific\nattributes. You will want your transformer to work seamlessly with Scikit-Learn func\u2010\ntionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher\u2010\nitance), all you need to do is create a class and implement three methods: fit()\n(returning self), transform(), and fit_transform(). You can get the last one for free by simply adding TransformerMixin as a base class. If you add BaseEstimator as a base class (and avoid *args and **kargs in your con\u2010\nstructor), you will also get two extra methods (get_params() and set_params()) that\nwill be useful for automatic hyperparameter tuning."
  },
  {
    "id": 55,
    "content": "For example, here is a small transformer class that adds the combined attributes we\ndiscussed earlier:\nfrom sklearn.base import BaseEstimator, TransformerMixin\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin): def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs self.add_bedrooms_per_room = add_bedrooms_per_room def fit(self, X, y=None): return self # nothing else to do def transform(self, X): rooms_per_household = X[:, rooms_ix] / X[:, households_ix] population_per_household = X[:, population_ix] / X[:, households_ix] if self.add_bedrooms_per_room: bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix] return np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room] | Chapter 2: End-to-End Machine Learning Project else: return np.c_[X, rooms_per_household, population_per_household]\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)\nIn this example the transformer has one hyperparameter, add_bedrooms_per_room,\nset to True by default (it is often helpful to provide sensible defaults). This hyperpara\u2010\nmeter will allow you to easily find out whether adding this attribute helps the\nMachine Learning algorithms or not. More generally, you can add a hyperparameter\nto gate any data preparation step that you are not 100% sure about. The more you\nautomate these data preparation steps, the more combinations you can automatically\ntry out, making it much more likely that you will find a great combination (and sav\u2010\ning you a lot of time). Feature Scaling\nOne of the most important transformations you need to apply to your data is feature\nscaling. With few exceptions, Machine Learning algorithms don\u2019t perform well when\nthe input numerical attributes have very different scales. This is the case for the hous\u2010\ning data: the total number of rooms ranges from about 6 to 39,320, while the median\nincomes only range from 0 to 15. Note that scaling the target values is generally not\nrequired. There are two common ways to get all attributes to have the same scale: min-max\nscaling and standardization. Min-max scaling (many people call this normalization) is the simplest: values are shif\u2010\nted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting\nthe min value and dividing by the max minus the min. Scikit-Learn provides a trans\u2010\nformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets\nyou change the range if, for some reason, you don\u2019t want 0\u20131. Standardization is different: first it subtracts the mean value (so standardized values\nalways have a zero mean), and then it divides by the standard deviation so that the\nresulting distribution has unit variance. Unlike min-max scaling, standardization\ndoes not bound values to a specific range, which may be a problem for some algo\u2010\nrithms (e.g., neural networks often expect an input value ranging from 0 to 1). How\u2010\never, standardization is much less affected by outliers. For example, suppose a district\nhad a median income equal to 100 (by mistake). Min-max scaling would then crush\nall the other values from 0\u201315 down to 0\u20130.15, whereas standardization would not be\nmuch affected. Scikit-Learn provides a transformer called StandardScaler for\nstandardization."
  },
  {
    "id": 56,
    "content": "Prepare the Data for Machine Learning Algorithms | As with all the transformations, it is important to fit the scalers to\nthe training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the\ntest set (and new data). Transformation Pipelines\nAs you can see, there are many data transformation steps that need to be executed in\nthe right order. Fortunately, Scikit-Learn provides the Pipeline class to help with\nsuch sequences of transformations. Here is a small pipeline for the numerical\nattributes:\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nnum_pipeline = Pipeline([ ('imputer', SimpleImputer(strategy=\"median\")), ('attribs_adder', CombinedAttributesAdder()), ('std_scaler', StandardScaler()), ])\nhousing_num_tr = num_pipeline.fit_transform(housing_num)\nThe Pipeline constructor takes a list of name/estimator pairs defining a sequence of\nsteps. All but the last estimator must be transformers (i.e., they must have a\nfit_transform() method). The names can be anything you like (as long as they are\nunique and don\u2019t contain double underscores, __); they will come in handy later for\nhyperparameter tuning. When you call the pipeline\u2019s fit() method, it calls fit_transform() sequentially on\nall transformers, passing the output of each call as the parameter to the next call until\nit reaches the final estimator, for which it calls the fit() method. The pipeline exposes the same methods as the final estimator. In this example, the last\nestimator is a StandardScaler, which is a transformer, so the pipeline has a trans\nform() method that applies all the transforms to the data in sequence (and of course\nalso a fit_transform() method, which is the one we used). So far, we have handled the categorical columns and the numerical columns sepa\u2010\nrately. It would be more convenient to have a single transformer able to handle all col\u2010\numns, applying the appropriate transformations to each column. In version 0.20,\nScikit-Learn introduced the ColumnTransformer for this purpose, and the good news\nis that it works great with pandas DataFrames. Let\u2019s use it to apply all the transforma\u2010\ntions to the housing data: | Chapter 2: End-to-End Machine Learning Project\n22 Just like for pipelines, the name can be anything as long as it does not contain double underscores. from sklearn.compose import ColumnTransformer\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\nfull_pipeline = ColumnTransformer([ (\"num\", num_pipeline, num_attribs), (\"cat\", OneHotEncoder(), cat_attribs), ])\nhousing_prepared = full_pipeline.fit_transform(housing)\nFirst we import the ColumnTransformer class, next we get the list of numerical col\u2010\numn names and the list of categorical column names, and then we construct a Colum\nnTransformer. The constructor requires a list of tuples, where each tuple contains a\nname,22 a transformer, and a list of names (or indices) of columns that the trans\u2010\nformer should be applied to. In this example, we specify that the numerical columns\nshould be transformed using the num_pipeline that we defined earlier, and the cate\u2010\ngorical columns should be transformed using a OneHotEncoder."
  },
  {
    "id": 57,
    "content": "Finally, we apply this\nColumnTransformer to the housing data: it applies each transformer to the appropri\u2010\nate columns and concatenates the outputs along the second axis (the transformers\nmust return the same number of rows). Note that the OneHotEncoder returns a sparse matrix, while the num_pipeline returns\na dense matrix. When there is such a mix of sparse and dense matrices, the Colum\nnTransformer estimates the density of the final matrix (i.e., the ratio of nonzero\ncells), and it returns a sparse matrix if the density is lower than a given threshold (by\ndefault, sparse_threshold=0.3). In this example, it returns a dense matrix. And\nthat\u2019s it! We have a preprocessing pipeline that takes the full housing data and applies\nthe appropriate transformations to each column. Instead of using a transformer, you can specify the string \"drop\" if\nyou want the columns to be dropped, or you can specify \"pass\nthrough\" if you want the columns to be left untouched. By default,\nthe remaining columns (i.e., the ones that were not listed) will be\ndropped, but you can set the remainder hyperparameter to any\ntransformer (or to \"passthrough\") if you want these columns to be\nhandled differently. If you are using Scikit-Learn 0.19 or earlier, you can use a third-party library such as\nsklearn-pandas, or you can roll out your own custom transformer to get the same\nfunctionality as the ColumnTransformer. Alternatively, you can use the FeatureUnion\nPrepare the Data for Machine Learning Algorithms | class, which can apply different transformers and concatenate their outputs. But you\ncannot specify different columns for each transformer; they all apply to the whole\ndata. It is possible to work around this limitation using a custom transformer for col\u2010\numn selection (see the Jupyter notebook for an example). Select and Train a Model\nAt last! You framed the problem, you got the data and explored it, you sampled a\ntraining set and a test set, and you wrote transformation pipelines to clean up and\nprepare your data for Machine Learning algorithms automatically. You are now ready\nto select and train a Machine Learning model. Training and Evaluating on the Training Set\nThe good news is that thanks to all these previous steps, things are now going to be\nmuch simpler than you might think. Let\u2019s first train a Linear Regression model, like\nwe did in the previous chapter:\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared, housing_labels)\nDone! You now have a working Linear Regression model. Let\u2019s try it out on a few\ninstances from the training set:\n>>> some_data = housing.iloc[:5]\n>>> some_labels = housing_labels.iloc[:5]\n>>> some_data_prepared = full_pipeline.transform(some_data)\n>>> print(\"Predictions:\", lin_reg.predict(some_data_prepared))\nPredictions: [ 210644.6045 317768.8069 210956.4333 59218.9888 189747.5584]\n>>> print(\"Labels:\", list(some_labels))\nLabels: [286600.0, 340600.0, 196900.0, 46300.0, 254500.0]\nIt works, although the predictions are not exactly accurate (e.g., the first prediction is\noff by close to 40%!)."
  },
  {
    "id": 58,
    "content": "Let\u2019s measure this regression model\u2019s RMSE on the whole train\u2010\ning set using Scikit-Learn\u2019s mean_squared_error() function:\n>>> from sklearn.metrics import mean_squared_error\n>>> housing_predictions = lin_reg.predict(housing_prepared)\n>>> lin_mse = mean_squared_error(housing_labels, housing_predictions)\n>>> lin_rmse = np.sqrt(lin_mse)\n>>> lin_rmse\n68628.19819848922\nThis is better than nothing, but clearly not a great score: most districts\u2019 median_hous\ning_values range between $120,000 and $265,000, so a typical prediction error of\n$68,628 is not very satisfying. This is an example of a model underfitting the training\ndata. When this happens it can mean that the features do not provide enough | Chapter 2: End-to-End Machine Learning Project\ninformation to make good predictions, or that the model is not powerful enough. As\nwe saw in the previous chapter, the main ways to fix underfitting are to select a more\npowerful model, to feed the training algorithm with better features, or to reduce the\nconstraints on the model. This model is not regularized, which rules out the last\noption. You could try to add more features (e.g., the log of the population), but first\nlet\u2019s try a more complex model to see how it does. Let\u2019s train a DecisionTreeRegressor. This is a powerful model, capable of finding\ncomplex nonlinear relationships in the data (Decision Trees are presented in more\ndetail in Chapter 6). The code should look familiar by now:\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)\nNow that the model is trained, let\u2019s evaluate it on the training set:\n>>> housing_predictions = tree_reg.predict(housing_prepared)\n>>> tree_mse = mean_squared_error(housing_labels, housing_predictions)\n>>> tree_rmse = np.sqrt(tree_mse)\n>>> tree_rmse\n0.0\nWait, what!? No error at all? Could this model really be absolutely perfect? Of course,\nit is much more likely that the model has badly overfit the data. How can you be sure? As we saw earlier, you don\u2019t want to touch the test set until you are ready to launch a\nmodel you are confident about, so you need to use part of the training set for training\nand part of it for model validation. Better Evaluation Using Cross-Validation\nOne way to evaluate the Decision Tree model would be to use the\ntrain_test_split() function to split the training set into a smaller training set and a\nvalidation set, then train your models against the smaller training set and evaluate\nthem against the validation set. It\u2019s a bit of work, but nothing too difficult, and it\nwould work fairly well. A great alternative is to use Scikit-Learn\u2019s K-fold cross-validation feature. The follow\u2010\ning code randomly splits the training set into 10 distinct subsets called folds, then it\ntrains and evaluates the Decision Tree model 10 times, picking a different fold for\nevaluation every time and training on the other 9 folds."
  },
  {
    "id": 59,
    "content": "The result is an array con\u2010\ntaining the 10 evaluation scores:\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)\nSelect and Train a Model | Scikit-Learn\u2019s cross-validation features expect a utility function\n(greater is better) rather than a cost function (lower is better), so\nthe scoring function is actually the opposite of the MSE (i.e., a neg\u2010\native value), which is why the preceding code computes -scores\nbefore calculating the square root. Let\u2019s look at the results:\n>>> def display_scores(scores):\n... print(\"Scores:\", scores)\n... print(\"Mean:\", scores.mean())\n... print(\"Standard deviation:\", scores.std())\n...\n>>> display_scores(tree_rmse_scores)\nScores: [70194.33680785 66855.16363941 72432.58244769 70758.73896782 71115.88230639 75585.14172901 70262.86139133 70273.6325285 75366.87952553 71231.65726027]\nMean: 71407.68766037929\nStandard deviation: 2439.4345041191004\nNow the Decision Tree doesn\u2019t look as good as it did earlier. In fact, it seems to per\u2010\nform worse than the Linear Regression model! Notice that cross-validation allows\nyou to get not only an estimate of the performance of your model, but also a measure\nof how precise this estimate is (i.e., its standard deviation). The Decision Tree has a\nscore of approximately 71,407, generally \u00b12,439. You would not have this information\nif you just used one validation set. But cross-validation comes at the cost of training\nthe model several times, so it is not always possible. Let\u2019s compute the same scores for the Linear Regression model just to be sure:\n>>> lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,\n... scoring=\"neg_mean_squared_error\", cv=10)\n...\n>>> lin_rmse_scores = np.sqrt(-lin_scores)\n>>> display_scores(lin_rmse_scores)\nScores: [66782.73843989 66960.118071 70347.95244419 74739.57052552 68031.13388938 71193.84183426 64969.63056405 68281.61137997 71552.91566558 67665.10082067]\nMean: 69052.46136345083\nStandard deviation: 2731.674001798348\nThat\u2019s right: the Decision Tree model is overfitting so badly that it performs worse\nthan the Linear Regression model. Let\u2019s try one last model now: the RandomForestRegressor. As we will see in Chap\u2010\nter 7, Random Forests work by training many Decision Trees on random subsets of\nthe features, then averaging out their predictions. Building a model on top of many\nother models is called Ensemble Learning, and it is often a great way to push ML algo\u2010\nrithms even further. We will skip most of the code since it is essentially the same as\nfor the other models: | Chapter 2: End-to-End Machine Learning Project\n>>> from sklearn.ensemble import RandomForestRegressor\n>>> forest_reg = RandomForestRegressor()\n>>> forest_reg.fit(housing_prepared, housing_labels)\n>>> [...]\n>>> forest_rmse\n18603.515021376355\n>>> display_scores(forest_rmse_scores)\nScores: [49519.80364233 47461.9115823 50029.02762854 52325.28068953 49308.39426421 53446.37892622 48634.8036574 47585.73832311 53490.10699751 50021.5852922 ]\nMean: 50182.303100336096\nStandard deviation: 2097.0810550985693\nWow, this is much better: Random Forests look very promising. However, note that\nthe score on the training set is still much lower than on the validation sets, meaning\nthat the model is still overfitting the training set. Possible solutions for overfitting are\nto simplify the model, constrain it (i.e., regularize it), or get a lot more training data."
  },
  {
    "id": 60,
    "content": "Before you dive much deeper into Random Forests, however, you should try out\nmany other models from various categories of Machine Learning algorithms (e.g.,\nseveral Support Vector Machines with different kernels, and possibly a neural net\u2010\nwork), without spending too much time tweaking the hyperparameters. The goal is to\nshortlist a few (two to five) promising models. You should save every model you experiment with so that you can\ncome back easily to any model you want. Make sure you save both\nthe hyperparameters and the trained parameters, as well as the\ncross-validation scores and perhaps the actual predictions as well. This will allow you to easily compare scores across model types,\nand compare the types of errors they make. You can easily save\nScikit-Learn models by using Python\u2019s pickle module or by using\nthe joblib library, which is more efficient at serializing large\nNumPy arrays (you can install this library using pip):\nimport joblib\njoblib.dump(my_model, \"my_model.pkl\")\n# and later...\nmy_model_loaded = joblib.load(\"my_model.pkl\")\nFine-Tune Your Model\nLet\u2019s assume that you now have a shortlist of promising models. You now need to\nfine-tune them. Let\u2019s look at a few ways you can do that. Fine-Tune Your Model | Grid Search\nOne option would be to fiddle with the hyperparameters manually, until you find a\ngreat combination of hyperparameter values. This would be very tedious work, and\nyou may not have time to explore many combinations. Instead, you should get Scikit-Learn\u2019s GridSearchCV to search for you. All you need\nto do is tell it which hyperparameters you want it to experiment with and what values\nto try out, and it will use cross-validation to evaluate all the possible combinations of\nhyperparameter values. For example, the following code searches for the best combi\u2010\nnation of hyperparameter values for the RandomForestRegressor:\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = [ {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]}, {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]}, ]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)\nWhen you have no idea what value a hyperparameter should have,\na simple approach is to try out consecutive powers of 10 (or a\nsmaller number if you want a more fine-grained search, as shown\nin this example with the n_estimators hyperparameter). This param_grid tells Scikit-Learn to first evaluate all 3 \u00d7 4 = 12 combinations of\nn_estimators and max_features hyperparameter values specified in the first dict\n(don\u2019t worry about what these hyperparameters mean for now; they will be explained\nin Chapter 7), then try all 2 \u00d7 3 = 6 combinations of hyperparameter values in the\nsecond dict, but this time with the bootstrap hyperparameter set to False instead of\nTrue (which is the default value for this hyperparameter). The grid search will explore 12 + 6 = 18 combinations of RandomForestRegressor\nhyperparameter values, and it will train each model 5 times (since we are using five-\nfold cross validation)."
  },
  {
    "id": 61,
    "content": "In other words, all in all, there will be 18 \u00d7 5 = 90 rounds of\ntraining! It may take quite a long time, but when it is done you can get the best com\u2010\nbination of parameters like this:\n>>> grid_search.best_params_\n{'max_features': 8, 'n_estimators': 30} | Chapter 2: End-to-End Machine Learning Project\nSince 8 and 30 are the maximum values that were evaluated, you\nshould probably try searching again with higher values; the score\nmay continue to improve. You can also get the best estimator directly:\n>>> grid_search.best_estimator_\nRandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None, max_features=8, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=30, n_jobs=None, oob_score=False, random_state=None, verbose=0, warm_start=False)\nIf GridSearchCV is initialized with refit=True (which is the\ndefault), then once it finds the best estimator using cross-\nvalidation, it retrains it on the whole training set. This is usually a\ngood idea, since feeding it more data will likely improve its\nperformance. And of course the evaluation scores are also available:\n>>> cvres = grid_search.cv_results_\n>>> for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n... print(np.sqrt(-mean_score), params)\n...\n63669.05791727153 {'max_features': 2, 'n_estimators': 3}\n55627.16171305252 {'max_features': 2, 'n_estimators': 10}\n53384.57867637289 {'max_features': 2, 'n_estimators': 30}\n60965.99185930139 {'max_features': 4, 'n_estimators': 3}\n52740.98248528835 {'max_features': 4, 'n_estimators': 10}\n50377.344409590376 {'max_features': 4, 'n_estimators': 30}\n58663.84733372485 {'max_features': 6, 'n_estimators': 3}\n52006.15355973719 {'max_features': 6, 'n_estimators': 10}\n50146.465964159885 {'max_features': 6, 'n_estimators': 30}\n57869.25504027614 {'max_features': 8, 'n_estimators': 3}\n51711.09443660957 {'max_features': 8, 'n_estimators': 10}\n49682.25345942335 {'max_features': 8, 'n_estimators': 30}\n62895.088889905004 {'bootstrap': False, 'max_features': 2, 'n_estimators': 3}\n54658.14484390074 {'bootstrap': False, 'max_features': 2, 'n_estimators': 10}\n59470.399594730654 {'bootstrap': False, 'max_features': 3, 'n_estimators': 3}\n52725.01091081235 {'bootstrap': False, 'max_features': 3, 'n_estimators': 10}\n57490.612956065226 {'bootstrap': False, 'max_features': 4, 'n_estimators': 3}\n51009.51445842374 {'bootstrap': False, 'max_features': 4, 'n_estimators': 10}\nIn this example, we obtain the best solution by setting the max_features hyperpara\u2010\nmeter to 8 and the n_estimators hyperparameter to 30. The RMSE score for this\ncombination is 49,682, which is slightly better than the score you got earlier using the\nFine-Tune Your Model | default hyperparameter values (which was 50,182). Congratulations, you have suc\u2010\ncessfully fine-tuned your best model! Don\u2019t forget that you can treat some of the data preparation steps as\nhyperparameters. For example, the grid search will automatically\nfind out whether or not to add a feature you were not sure about\n(e.g., using the add_bedrooms_per_room hyperparameter of your\nCombinedAttributesAdder transformer). It may similarly be used\nto automatically find the best way to handle outliers, missing fea\u2010\ntures, feature selection, and more. Randomized Search\nThe grid search approach is fine when you are exploring relatively few combinations,\nlike in the previous example, but when the hyperparameter search space is large, it is\noften preferable to use RandomizedSearchCV instead. This class can be used in much\nthe same way as the GridSearchCV class, but instead of trying out all possible combi\u2010\nnations, it evaluates a given number of random combinations by selecting a random\nvalue for each hyperparameter at every iteration."
  },
  {
    "id": 62,
    "content": "This approach has two main\nbenefits:\n\u2022 If you let the randomized search run for, say, 1,000 iterations, this approach will\nexplore 1,000 different values for each hyperparameter (instead of just a few val\u2010\nues per hyperparameter with the grid search approach). \u2022 Simply by setting the number of iterations, you have more control over the com\u2010\nputing budget you want to allocate to hyperparameter search. Ensemble Methods\nAnother way to fine-tune your system is to try to combine the models that perform\nbest. The group (or \u201censemble\u201d) will often perform better than the best individual\nmodel (just like Random Forests perform better than the individual Decision Trees\nthey rely on), especially if the individual models make very different types of errors. We will cover this topic in more detail in Chapter 7. Analyze the Best Models and Their Errors\nYou will often gain good insights on the problem by inspecting the best models. For\nexample, the RandomForestRegressor can indicate the relative importance of each\nattribute for making accurate predictions:\n>>> feature_importances = grid_search.best_estimator_.feature_importances_\n>>> feature_importances\narray([7.33442355e-02, 6.29090705e-02, 4.11437985e-02, 1.46726854e-02, | Chapter 2: End-to-End Machine Learning Project 1.41064835e-02, 1.48742809e-02, 1.42575993e-02, 3.66158981e-01, 5.64191792e-02, 1.08792957e-01, 5.33510773e-02, 1.03114883e-02, 1.64780994e-01, 6.02803867e-05, 1.96041560e-03, 2.85647464e-03])\nLet\u2019s display these importance scores next to their corresponding attribute names:\n>>> extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n>>> cat_encoder = full_pipeline.named_transformers_[\"cat\"]\n>>> cat_one_hot_attribs = list(cat_encoder.categories_[0])\n>>> attributes = num_attribs + extra_attribs + cat_one_hot_attribs\n>>> sorted(zip(feature_importances, attributes), reverse=True)\n[(0.3661589806181342, 'median_income'), (0.1647809935615905, 'INLAND'), (0.10879295677551573, 'pop_per_hhold'), (0.07334423551601242, 'longitude'), (0.0629090704826203, 'latitude'), (0.05641917918195401, 'rooms_per_hhold'), (0.05335107734767581, 'bedrooms_per_room'), (0.041143798478729635, 'housing_median_age'), (0.014874280890402767, 'population'), (0.014672685420543237, 'total_rooms'), (0.014257599323407807, 'households'), (0.014106483453584102, 'total_bedrooms'), (0.010311488326303787, '<1H OCEAN'), (0.002856474637320158, 'NEAR OCEAN'), (0.00196041559947807, 'NEAR BAY'), (6.028038672736599e-05, 'ISLAND')]\nWith this information, you may want to try dropping some of the less useful features\n(e.g., apparently only one ocean_proximity category is really useful, so you could try\ndropping the others). You should also look at the specific errors that your system makes, then try to under\u2010\nstand why it makes them and what could fix the problem (adding extra features or\ngetting rid of uninformative ones, cleaning up outliers, etc.). Evaluate Your System on the Test Set\nAfter tweaking your models for a while, you eventually have a system that performs\nsufficiently well. Now is the time to evaluate the final model on the test set. There is\nnothing special about this process; just get the predictors and the labels from your\ntest set, run your full_pipeline to transform the data (call transform(), not\nfit_transform()\u2014you do not want to fit the test set! ), and evaluate the final model\non the test set:\nfinal_model = grid_search.best_estimator_\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\nX_test_prepared = full_pipeline.transform(X_test)\nFine-Tune Your Model | final_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse) # => evaluates to 47,730.2\nIn some cases, such a point estimate of the generalization error will not be quite\nenough to convince you to launch: what if it is just 0.1% better than the model cur\u2010\nrently in production?"
  },
  {
    "id": 63,
    "content": "You might want to have an idea of how precise this estimate is. For this, you can compute a 95% confidence interval for the generalization error using\nscipy.stats.t.interval():\n>>> from scipy import stats\n>>> confidence = 0.95\n>>> squared_errors = (final_predictions - y_test) ** 2\n>>> np.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n... loc=squared_errors.mean(),\n... scale=stats.sem(squared_errors)))\n...\narray([45685.10470776, 49691.25001878])\nIf you did a lot of hyperparameter tuning, the performance will usually be slightly\nworse than what you measured using cross-validation (because your system ends up\nfine-tuned to perform well on the validation data and will likely not perform as well\non unknown datasets). It is not the case in this example, but when this happens you\nmust resist the temptation to tweak the hyperparameters to make the numbers look\ngood on the test set; the improvements would be unlikely to generalize to new data. Now comes the project prelaunch phase: you need to present your solution (high\u2010\nlighting what you have learned, what worked and what did not, what assumptions\nwere made, and what your system\u2019s limitations are), document everything, and create\nnice presentations with clear visualizations and easy-to-remember statements (e.g.,\n\u201cthe median income is the number one predictor of housing prices\u201d). In this Califor\u2010\nnia housing example, the final performance of the system is not better than the\nexperts\u2019 price estimates, which were often off by about 20%, but it may still be a good\nidea to launch it, especially if this frees up some time for the experts so they can work\non more interesting and productive tasks. Launch, Monitor, and Maintain Your System\nPerfect, you got approval to launch! You now need to get your solution ready for pro\u2010\nduction (e.g., polish the code, write documentation and tests, and so on). Then you\ncan deploy your model to your production environment. One way to do this is to save\nthe trained Scikit-Learn model (e.g., using joblib), including the full preprocessing\nand prediction pipeline, then load this trained model within your production envi\u2010\nronment and use it to make predictions by calling its predict() method. For exam\u2010\nple, perhaps the model will be used within a website: the user will type in some data | Chapter 2: End-to-End Machine Learning Project\n23 In a nutshell, a REST (or RESTful) API is an HTTP-based API that follows some conventions, such as using\nstandard HTTP verbs to read, update, create, or delete resources (GET, POST, PUT, and DELETE) and using\nJSON for the inputs and outputs. about a new district and click the Estimate Price button. This will send a query con\u2010\ntaining the data to the web server, which will forward it to your web application, and\nfinally your code will simply call the model\u2019s predict() method (you want to load the\nmodel upon server startup, rather than every time the model is used). Alternatively,\nyou can wrap the model within a dedicated web service that your web application can\nquery through a REST API23 (see Figure 2-17)."
  },
  {
    "id": 64,
    "content": "This makes it easier to upgrade your\nmodel to new versions without interrupting the main application. It also simplifies\nscaling, since you can start as many web services as needed and load-balance the\nrequests coming from your web application across these web services. Moreover, it\nallows your web application to use any language, not just Python. Figure 2-17. A model deployed as a web service and used by a web application\nAnother popular strategy is to deploy your model on the cloud, for example on Goo\u2010\ngle Cloud AI Platform (formerly known as Google Cloud ML Engine): just save your\nmodel using joblib and upload it to Google Cloud Storage (GCS), then head over to\nGoogle Cloud AI Platform and create a new model version, pointing it to the GCS\nfile. That\u2019s it! This gives you a simple web service that takes care of load balancing and\nscaling for you. It take JSON requests containing the input data (e.g., of a district) and\nreturns JSON responses containing the predictions. You can then use this web service\nin your website (or whatever production environment you are using). As we will see\nin Chapter 19, deploying TensorFlow models on AI Platform is not much different\nfrom deploying Scikit-Learn models. But deployment is not the end of the story. You also need to write monitoring code to\ncheck your system\u2019s live performance at regular intervals and trigger alerts when it\ndrops. This could be a steep drop, likely due to a broken component in your infra\u2010\nstructure, but be aware that it could also be a gentle decay that could easily go unno\u2010\nticed for a long time. This is quite common because models tend to \u201crot\u201d over time:\nindeed, the world changes, so if the model was trained with last year\u2019s data, it may not\nbe adapted to today\u2019s data. Launch, Monitor, and Maintain Your System | 24 A captcha is a test to ensure a user is not a robot. These tests have often been used as a cheap way to label\ntraining data. Even a model trained to classify pictures of cats and dogs may need\nto be retrained regularly, not because cats and dogs will mutate\novernight, but because cameras keep changing, along with image\nformats, sharpness, brightness, and size ratios. Moreover, people\nmay love different breeds next year, or they may decide to dress\ntheir pets with tiny hats\u2014who knows? So you need to monitor your model\u2019s live performance. But how do you that? Well, it\ndepends. In some cases, the model\u2019s performance can be inferred from downstream\nmetrics. For example, if your model is part of a recommender system and it suggests\nproducts that the users may be interested in, then it\u2019s easy to monitor the number of\nrecommended products sold each day. If this number drops (compared to non-\nrecommended products), then the prime suspect is the model."
  },
  {
    "id": 65,
    "content": "This may be because\nthe data pipeline is broken, or perhaps the model needs to be retrained on fresh data\n(as we will discuss shortly). However, it\u2019s not always possible to determine the model\u2019s performance without any\nhuman analysis. For example, suppose you trained an image classification model (see\nChapter 3) to detect several product defects on a production line. How can you get an\nalert if the model\u2019s performance drops, before thousands of defective products get\nshipped to your clients? One solution is to send to human raters a sample of all the\npictures that the model classified (especially pictures that the model wasn\u2019t so sure\nabout). Depending on the task, the raters may need to be experts, or they could be\nnonspecialists, such as workers on a crowdsourcing platform (e.g., Amazon Mechani\u2010\ncal Turk). In some applications they could even be the users themselves, responding\nfor example via surveys or repurposed captchas.24\nEither way, you need to put in place a monitoring system (with or without human\nraters to evaluate the live model), as well as all the relevant processes to define what to\ndo in case of failures and how to prepare for them. Unfortunately, this can be a lot of\nwork. In fact, it is often much more work than building and training a model. If the data keeps evolving, you will need to update your datasets and retrain your\nmodel regularly. You should probably automate the whole process as much as possi\u2010\nble. Here are a few things you can automate:\n\u2022 Collect fresh data regularly and label it (e.g., using human raters). \u2022 Write a script to train the model and fine-tune the hyperparameters automati\u2010\ncally. This script could run automatically, for example every day or every week,\ndepending on your needs. | Chapter 2: End-to-End Machine Learning Project\n\u2022 Write another script that will evaluate both the new model and the previous\nmodel on the updated test set, and deploy the model to production if the perfor\u2010\nmance has not decreased (if it did, make sure you investigate why). You should also make sure you evaluate the model\u2019s input data quality. Sometimes\nperformance will degrade slightly because of a poor-quality signal (e.g., a malfunc\u2010\ntioning sensor sending random values, or another team\u2019s output becoming stale), but\nit may take a while before your system\u2019s performance degrades enough to trigger an\nalert. If you monitor your model\u2019s inputs, you may catch this earlier. For example, you\ncould trigger an alert if more and more inputs are missing a feature, or if its mean or\nstandard deviation drifts too far from the training set, or a categorical feature starts\ncontaining new categories. Finally, make sure you keep backups of every model you create and have the process\nand tools in place to roll back to a previous model quickly, in case the new model\nstarts failing badly for some reason. Having backups also makes it possible to easily\ncompare new models with previous ones."
  },
  {
    "id": 66,
    "content": "Similarly, you should keep backups of every\nversion of your datasets so that you can roll back to a previous dataset if the new one\never gets corrupted (e.g., if the fresh data that gets added to it turns out to be full of\noutliers). Having backups of your datasets also allows you to evaluate any model\nagainst any previous dataset. You may want to create several subsets of the test set in order to\nevaluate how well your model performs on specific parts of the\ndata. For example, you may want to have a subset containing only\nthe most recent data, or a test set for specific kinds of inputs (e.g.,\ndistricts located inland versus districts located near the ocean). This will give you a deeper understanding of your model\u2019s\nstrengths and weaknesses. As you can see, Machine Learning involves quite a lot of infrastructure, so don\u2019t be\nsurprised if your first ML project takes a lot of effort and time to build and deploy to\nproduction. Fortunately, once all the infrastructure is in place, going from idea to\nproduction will be much faster. Try It Out! Hopefully this chapter gave you a good idea of what a Machine Learning project\nlooks like as well as showing you some of the tools you can use to train a great system. As you can see, much of the work is in the data preparation step: building monitoring\ntools, setting up human evaluation pipelines, and automating regular model training. The Machine Learning algorithms are important, of course, but it is probably prefera\u2010\nTry It Out! | ble to be comfortable with the overall process and know three or four algorithms well\nrather than to spend all your time exploring advanced algorithms. So, if you have not already done so, now is a good time to pick up a laptop, select a\ndataset that you are interested in, and try to go through the whole process from A to\nZ. A good place to start is on a competition website such as  you\nwill have a dataset to play with, a clear goal, and people to share the experience with. Have fun! Exercises\nThe following exercises are all based on this chapter\u2019s housing dataset:\n1. Try a Support Vector Machine regressor (sklearn.svm.SVR) with various hyper\u2010\nparameters, such as kernel=\"linear\" (with various values for the C hyperpara\u2010\nmeter) or kernel=\"rbf\" (with various values for the C and gamma\nhyperparameters). Don\u2019t worry about what these hyperparameters mean for now. How does the best SVR predictor perform? 2. Try replacing GridSearchCV with RandomizedSearchCV. 3. Try adding a transformer in the preparation pipeline to select only the most\nimportant attributes. 4. Try creating a single pipeline that does the full data preparation plus the final\nprediction. 5. Automatically explore some preparation options using GridSearchCV. Solutions to these exercises can be found in the Jupyter notebooks available at \ngithub.com/ageron/handson-ml2."
  },
  {
    "id": 67,
    "content": "| Chapter 2: End-to-End Machine Learning Project\n1 By default Scikit-Learn caches downloaded datasets in a directory called $HOME/scikit_learn_data. CHAPTER 3\nClassification\nIn Chapter 1 I mentioned that the most common supervised learning tasks are\nregression (predicting values) and classification (predicting classes). In Chapter 2 we\nexplored a regression task, predicting housing values, using various algorithms such\nas Linear Regression, Decision Trees, and Random Forests (which will be explained\nin further detail in later chapters). Now we will turn our attention to classification\nsystems. MNIST\nIn this chapter we will be using the MNIST dataset, which is a set of 70,000 small\nimages of digits handwritten by high school students and employees of the US Cen\u2010\nsus Bureau. Each image is labeled with the digit it represents. This set has been stud\u2010\nied so much that it is often called the \u201chello world\u201d of Machine Learning: whenever\npeople come up with a new classification algorithm they are curious to see how it will\nperform on MNIST, and anyone who learns Machine Learning tackles this dataset\nsooner or later. Scikit-Learn provides many helper functions to download popular datasets. MNIST is\none of them. The following code fetches the MNIST dataset:1\n>>> from sklearn.datasets import fetch_openml\n>>> mnist = fetch_openml('mnist_784', version=1)\n>>> mnist.keys()\ndict_keys(['data', 'target', 'feature_names', 'DESCR', 'details', 'categories', 'url']) Datasets loaded by Scikit-Learn generally have a similar dictionary structure, includ\u2010\ning the following:\n\u2022 A DESCR key describing the dataset\n\u2022 A data key containing an array with one row per instance and one column per\nfeature\n\u2022 A target key containing an array with the labels\nLet\u2019s look at these arrays:\n>>> X, y = mnist[\"data\"], mnist[\"target\"]\n>>> X.shape\n(70000, 784)\n>>> y.shape\n(70000,)\nThere are 70,000 images, and each image has 784 features. This is because each image\nis 28 \u00d7 28 pixels, and each feature simply represents one pixel\u2019s intensity, from 0\n(white) to 255 (black). Let\u2019s take a peek at one digit from the dataset. All you need to\ndo is grab an instance\u2019s feature vector, reshape it to a 28 \u00d7 28 array, and display it\nusing Matplotlib\u2019s imshow() function:\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nsome_digit = X[0]\nsome_digit_image = some_digit.reshape(28, 28)\nplt.imshow(some_digit_image, cmap=\"binary\")\nplt.axis(\"off\")\nplt.show()\nThis looks like a 5, and indeed that\u2019s what the label tells us:\n>>> y[0]\n'5'\nNote that the label is a string. Most ML algorithms expect numbers, so let\u2019s cast y to\ninteger:\n>>> y = y.astype(np.uint8) | Chapter 3: Classification\n2 Shuffling may be a bad idea in some contexts\u2014for example, if you are working on time series data (such as\nstock market prices or weather conditions). We will explore this in the next chapters. To give you a feel for the complexity of the classification task, Figure 3-1 shows a few\nmore images from the MNIST dataset. Figure 3-1. Digits from the MNIST dataset\nBut wait! You should always create a test set and set it aside before inspecting the data\nclosely."
  },
  {
    "id": 68,
    "content": "The MNIST dataset is actually already split into a training set (the first 60,000\nimages) and a test set (the last 10,000 images):\nX_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\nThe training set is already shuffled for us, which is good because this guarantees that\nall cross-validation folds will be similar (you don\u2019t want one fold to be missing some\ndigits). Moreover, some learning algorithms are sensitive to the order of the training\ninstances, and they perform poorly if they get many similar instances in a row. Shuf\u2010\nfling the dataset ensures that this won\u2019t happen.2\nMNIST | Training a Binary Classifier\nLet\u2019s simplify the problem for now and only try to identify one digit\u2014for example,\nthe number 5. This \u201c5-detector\u201d will be an example of a binary classifier, capable of\ndistinguishing between just two classes, 5 and not-5. Let\u2019s create the target vectors for\nthis classification task:\ny_train_5 = (y_train == 5) # True for all 5s, False for all other digits\ny_test_5 = (y_test == 5)\nNow let\u2019s pick a classifier and train it. A good place to start is with a Stochastic Gradi\u2010\nent Descent (SGD) classifier, using Scikit-Learn\u2019s SGDClassifier class. This classifier\nhas the advantage of being capable of handling very large datasets efficiently. This is\nin part because SGD deals with training instances independently, one at a time\n(which also makes SGD well suited for online learning), as we will see later. Let\u2019s cre\u2010\nate an SGDClassifier and train it on the whole training set:\nfrom sklearn.linear_model import SGDClassifier\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train_5)\nThe SGDClassifier relies on randomness during training (hence\nthe name \u201cstochastic\u201d). If you want reproducible results, you\nshould set the random_state parameter. Now we can use it to detect images of the number 5:\n>>> sgd_clf.predict([some_digit])\narray([ True])\nThe classifier guesses that this image represents a 5 (True). Looks like it guessed right\nin this particular case! Now, let\u2019s evaluate this model\u2019s performance. Performance Measures\nEvaluating a classifier is often significantly trickier than evaluating a regressor, so we\nwill spend a large part of this chapter on this topic. There are many performance\nmeasures available, so grab another coffee and get ready to learn many new concepts\nand acronyms! | Chapter 3: Classification\nMeasuring Accuracy Using Cross-Validation\nA good way to evaluate a model is to use cross-validation, just as you did in Chap\u2010\nter 2. Implementing Cross-Validation\nOccasionally you will need more control over the cross-validation process than what\nScikit-Learn provides off the shelf. In these cases, you can implement cross-validation\nyourself."
  },
  {
    "id": 69,
    "content": "The following code does roughly the same thing as Scikit-Learn\u2019s\ncross_val_score() function, and it prints the same result:\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.base import clone\nskfolds = StratifiedKFold(n_splits=3, random_state=42)\nfor train_index, test_index in skfolds.split(X_train, y_train_5): clone_clf = clone(sgd_clf) X_train_folds = X_train[train_index] y_train_folds = y_train_5[train_index] X_test_fold = X_train[test_index] y_test_fold = y_train_5[test_index] clone_clf.fit(X_train_folds, y_train_folds) y_pred = clone_clf.predict(X_test_fold) n_correct = sum(y_pred == y_test_fold) print(n_correct / len(y_pred)) # prints 0.9502, 0.96565, and 0.96495\nThe StratifiedKFold class performs stratified sampling (as explained in Chapter 2)\nto produce folds that contain a representative ratio of each class. At each iteration the\ncode creates a clone of the classifier, trains that clone on the training folds, and makes\npredictions on the test fold. Then it counts the number of correct predictions and\noutputs the ratio of correct predictions. Let\u2019s use the cross_val_score() function to evaluate our SGDClassifier model,\nusing K-fold cross-validation with three folds. Remember that K-fold cross-validation\nmeans splitting the training set into K folds (in this case, three), then making predic\u2010\ntions and evaluating them on each fold using a model trained on the remaining folds\n(see Chapter 2):\n>>> from sklearn.model_selection import cross_val_score\n>>> cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\narray([0.96355, 0.93795, 0.95615])\nWow! Above 93% accuracy (ratio of correct predictions) on all cross-validation folds? This looks amazing, doesn\u2019t it? Well, before you get too excited, let\u2019s look at a very\ndumb classifier that just classifies every single image in the \u201cnot-5\u201d class:\nPerformance Measures | from sklearn.base import BaseEstimator\nclass Never5Classifier(BaseEstimator): def fit(self, X, y=None): return self def predict(self, X): return np.zeros((len(X), 1), dtype=bool)\nCan you guess this model\u2019s accuracy? Let\u2019s find out:\n>>> never_5_clf = Never5Classifier()\n>>> cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")\narray([0.91125, 0.90855, 0.90915])\nThat\u2019s right, it has over 90% accuracy! This is simply because only about 10% of the\nimages are 5s, so if you always guess that an image is not a 5, you will be right about\n90% of the time. Beats Nostradamus. This demonstrates why accuracy is generally not the preferred performance measure\nfor classifiers, especially when you are dealing with skewed datasets (i.e., when some\nclasses are much more frequent than others). Confusion Matrix\nA much better way to evaluate the performance of a classifier is to look at the confu\u2010\nsion matrix. The general idea is to count the number of times instances of class A are\nclassified as class B. For example, to know the number of times the classifier confused\nimages of 5s with 3s, you would look in the fifth row and third column of the confu\u2010\nsion matrix. To compute the confusion matrix, you first need to have a set of predictions so that\nthey can be compared to the actual targets. You could make predictions on the test\nset, but let\u2019s keep it untouched for now (remember that you want to use the test set\nonly at the very end of your project, once you have a classifier that you are ready to\nlaunch)."
  },
  {
    "id": 70,
    "content": "Instead, you can use the cross_val_predict() function:\nfrom sklearn.model_selection import cross_val_predict\ny_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)\nJust like the cross_val_score() function, cross_val_predict() performs K-fold\ncross-validation, but instead of returning the evaluation scores, it returns the predic\u2010\ntions made on each test fold. This means that you get a clean prediction for each\ninstance in the training set (\u201cclean\u201d meaning that the prediction is made by a model\nthat never saw the data during training). Now you are ready to get the confusion matrix using the confusion_matrix() func\u2010\ntion. Just pass it the target classes (y_train_5) and the predicted classes\n(y_train_pred): | Chapter 3: Classification\n>>> from sklearn.metrics import confusion_matrix\n>>> confusion_matrix(y_train_5, y_train_pred)\narray([[53057, 1522], [ 1325, 4096]])\nEach row in a confusion matrix represents an actual class, while each column repre\u2010\nsents a predicted class. The first row of this matrix considers non-5 images (the nega\u2010\ntive class): 53,057 of them were correctly classified as non-5s (they are called true\nnegatives), while the remaining 1,522 were wrongly classified as 5s (false positives). The second row considers the images of 5s (the positive class): 1,325 were wrongly\nclassified as non-5s (false negatives), while the remaining 4,096 were correctly classi\u2010\nfied as 5s (true positives). A perfect classifier would have only true positives and true\nnegatives, so its confusion matrix would have nonzero values only on its main diago\u2010\nnal (top left to bottom right):\n>>> y_train_perfect_predictions = y_train_5 # pretend we reached perfection\n>>> confusion_matrix(y_train_5, y_train_perfect_predictions)\narray([[54579, 0], [ 0, 5421]])\nThe confusion matrix gives you a lot of information, but sometimes you may prefer a\nmore concise metric. An interesting one to look at is the accuracy of the positive pre\u2010\ndictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision\nprecision =\nTP\nTP + FP\nTP is the number of true positives, and FP is the number of false positives. A trivial way to have perfect precision is to make one single positive prediction and\nensure it is correct (precision = 1/1 = 100%). But this would not be very useful, since\nthe classifier would ignore all but one positive instance. So precision is typically used\nalong with another metric named recall, also called sensitivity or the true positive rate\n(TPR): this is the ratio of positive instances that are correctly detected by the classifier\n(Equation 3-2). Equation 3-2. Recall\nrecall =\nTP\nTP + FN\nFN is, of course, the number of false negatives. If you are confused about the confusion matrix, Figure 3-2 may help. Performance Measures | Figure 3-2."
  },
  {
    "id": 71,
    "content": "An illustrated confusion matrix shows examples of true negatives (top left),\nfalse positives (top right), false negatives (lower left), and true positives (lower right)\nPrecision and Recall\nScikit-Learn provides several functions to compute classifier metrics, including preci\u2010\nsion and recall:\n>>> from sklearn.metrics import precision_score, recall_score\n>>> precision_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1522)\n0.7290850836596654\n>>> recall_score(y_train_5, y_train_pred) # == 4096 / (4096 + 1325)\n0.7555801512636044\nNow your 5-detector does not look as shiny as it did when you looked at its accuracy. When it claims an image represents a 5, it is correct only 72.9% of the time. More\u2010\nover, it only detects 75.6% of the 5s. It is often convenient to combine precision and recall into a single metric called the F1\nscore, in particular if you need a simple way to compare two classifiers. The F1 score is\nthe harmonic mean of precision and recall (Equation 3-3). Whereas the regular mean\ntreats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F1 score if both recall and precision are\nhigh. Equation 3-3. F1\nF1 = precision + recall\n= 2 \u00d7 precision \u00d7 recall\nprecision + recall =\nTP\nTP + FN + FP | Chapter 3: Classification\nTo compute the F1 score, simply call the f1_score() function:\n>>> from sklearn.metrics import f1_score\n>>> f1_score(y_train_5, y_train_pred)\n0.7420962043663375\nThe F1 score favors classifiers that have similar precision and recall. This is not always\nwhat you want: in some contexts you mostly care about precision, and in other con\u2010\ntexts you really care about recall. For example, if you trained a classifier to detect vid\u2010\neos that are safe for kids, you would probably prefer a classifier that rejects many\ngood videos (low recall) but keeps only safe ones (high precision), rather than a clas\u2010\nsifier that has a much higher recall but lets a few really bad videos show up in your\nproduct (in such cases, you may even want to add a human pipeline to check the clas\u2010\nsifier\u2019s video selection). On the other hand, suppose you train a classifier to detect\nshoplifters in surveillance images: it is probably fine if your classifier has only 30%\nprecision as long as it has 99% recall (sure, the security guards will get a few false\nalerts, but almost all shoplifters will get caught). Unfortunately, you can\u2019t have it both ways: increasing precision reduces recall, and\nvice versa. This is called the precision/recall trade-off. Precision/Recall Trade-off\nTo understand this trade-off, let\u2019s look at how the SGDClassifier makes its classifica\u2010\ntion decisions. For each instance, it computes a score based on a decision function. If\nthat score is greater than a threshold, it assigns the instance to the positive class;\notherwise it assigns it to the negative class. Figure 3-3 shows a few digits positioned\nfrom the lowest score on the left to the highest score on the right."
  },
  {
    "id": 72,
    "content": "Suppose the deci\u2010\nsion threshold is positioned at the central arrow (between the two 5s): you will find 4\ntrue positives (actual 5s) on the right of that threshold, and 1 false positive (actually a\n6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6\nactual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). If you raise the\nthreshold (move it to the arrow on the right), the false positive (the 6) becomes a true\nnegative, thereby increasing the precision (up to 100% in this case), but one true posi\u2010\ntive becomes a false negative, decreasing recall down to 50%. Conversely, lowering\nthe threshold increases recall and reduces precision. Performance Measures | Figure 3-3. In this precision/recall trade-off, images are ranked by their classifier score,\nand those above the chosen decision threshold are considered positive; the higher the\nthreshold, the lower the recall, but (in general) the higher the precision\nScikit-Learn does not let you set the threshold directly, but it does give you access to\nthe decision scores that it uses to make predictions. Instead of calling the classifier\u2019s\npredict() method, you can call its decision_function() method, which returns a\nscore for each instance, and then use any threshold you want to make predictions\nbased on those scores:\n>>> y_scores = sgd_clf.decision_function([some_digit])\n>>> y_scores\narray([2412.53175101])\n>>> threshold = 0\n>>> y_some_digit_pred = (y_scores > threshold)\narray([ True])\nThe SGDClassifier uses a threshold equal to 0, so the previous code returns the same\nresult as the predict() method (i.e., True). Let\u2019s raise the threshold:\n>>> threshold = 8000\n>>> y_some_digit_pred = (y_scores > threshold)\n>>> y_some_digit_pred\narray([False])\nThis confirms that raising the threshold decreases recall. The image actually repre\u2010\nsents a 5, and the classifier detects it when the threshold is 0, but it misses it when the\nthreshold is increased to 8,000. How do you decide which threshold to use? First, use the cross_val_predict()\nfunction to get the scores of all instances in the training set, but this time specify that\nyou want to return decision scores instead of predictions:\ny_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3, method=\"decision_function\")\nWith these scores, use the precision_recall_curve() function to compute precision\nand recall for all possible thresholds: | Chapter 3: Classification\nfrom sklearn.metrics import precision_recall_curve\nprecisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)\nFinally, use Matplotlib to plot precision and recall as functions of the threshold value\n(Figure 3-4):\ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds): plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\") plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\") [...] # highlight the threshold and add the legend, axis label, and grid\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()\nFigure 3-4. Precision and recall versus the decision threshold\nYou may wonder why the precision curve is bumpier than the recall\ncurve in Figure 3-4. The reason is that precision may sometimes go\ndown when you raise the threshold (although in general it will go\nup)."
  },
  {
    "id": 73,
    "content": "To understand why, look back at Figure 3-3 and notice what\nhappens when you start from the central threshold and move it just\none digit to the right: precision goes from 4/5 (80%) down to 3/4\n(75%). On the other hand, recall can only go down when the thres\u2010\nhold is increased, which explains why its curve looks smooth. Another way to select a good precision/recall trade-off is to plot precision directly\nagainst recall, as shown in Figure 3-5 (the same threshold as earlier is highlighted). Performance Measures | Figure 3-5. Precision versus recall\nYou can see that precision really starts to fall sharply around 80% recall. You will\nprobably want to select a precision/recall trade-off just before that drop\u2014for exam\u2010\nple, at around 60% recall. But of course, the choice depends on your project. Suppose you decide to aim for 90% precision. You look up the first plot and find that\nyou need to use a threshold of about 8,000. To be more precise you can search for the\nlowest threshold that gives you at least 90% precision (np.argmax() will give you the\nfirst index of the maximum value, which in this case means the first True value):\nthreshold_90_precision = thresholds[np.argmax(precisions >= 0.90)] # ~7816\nTo make predictions (on the training set for now), instead of calling the classifier\u2019s\npredict() method, you can run this code:\ny_train_pred_90 = (y_scores >= threshold_90_precision)\nLet\u2019s check these predictions\u2019 precision and recall:\n>>> precision_score(y_train_5, y_train_pred_90)\n0.9000380083618396\n>>> recall_score(y_train_5, y_train_pred_90)\n0.4368197749492714\nGreat, you have a 90% precision classifier! As you can see, it is fairly easy to create a\nclassifier with virtually any precision you want: just set a high enough threshold, and\nyou\u2019re done. But wait, not so fast. A high-precision classifier is not very useful if its\nrecall is too low! | Chapter 3: Classification\nIf someone says, \u201cLet\u2019s reach 99% precision,\u201d you should ask, \u201cAt\nwhat recall?\u201d\nThe ROC Curve\nThe receiver operating characteristic (ROC) curve is another common tool used with\nbinary classifiers. It is very similar to the precision/recall curve, but instead of plot\u2010\nting precision versus recall, the ROC curve plots the true positive rate (another name\nfor recall) against the false positive rate (FPR). The FPR is the ratio of negative instan\u2010\nces that are incorrectly classified as positive. It is equal to 1 \u2013 the true negative rate\n(TNR), which is the ratio of negative instances that are correctly classified as negative. The TNR is also called specificity. Hence, the ROC curve plots sensitivity (recall) ver\u2010\nsus 1 \u2013 specificity. To plot the ROC curve, you first use the roc_curve() function to compute the TPR\nand FPR for various threshold values:\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_train_5, y_scores)\nThen you can plot the FPR against the TPR using Matplotlib."
  },
  {
    "id": 74,
    "content": "This code produces the\nplot in Figure 3-6:\ndef plot_roc_curve(fpr, tpr, label=None): plt.plot(fpr, tpr, linewidth=2, label=label) plt.plot([0, 1], [0, 1], 'k--') # Dashed diagonal [...] # Add axis labels and grid\nplot_roc_curve(fpr, tpr)\nplt.show()\nOnce again there is a trade-off: the higher the recall (TPR), the more false positives\n(FPR) the classifier produces. The dotted line represents the ROC curve of a purely\nrandom classifier; a good classifier stays as far away from that line as possible (toward\nthe top-left corner). Performance Measures | Figure 3-6. This ROC curve plots the false positive rate against the true positive rate for\nall possible thresholds; the red circle highlights the chosen ratio (at 43.68% recall)\nOne way to compare classifiers is to measure the area under the curve (AUC). A per\u2010\nfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will\nhave a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC\nAUC:\n>>> from sklearn.metrics import roc_auc_score\n>>> roc_auc_score(y_train_5, y_scores)\n0.9611778893101814\nSince the ROC curve is so similar to the precision/recall (PR)\ncurve, you may wonder how to decide which one to use. As a rule\nof thumb, you should prefer the PR curve whenever the positive\nclass is rare or when you care more about the false positives than\nthe false negatives. Otherwise, use the ROC curve. For example,\nlooking at the previous ROC curve (and the ROC AUC score), you\nmay think that the classifier is really good. But this is mostly\nbecause there are few positives (5s) compared to the negatives\n(non-5s). In contrast, the PR curve makes it clear that the classifier\nhas room for improvement (the curve could be closer to the top-\nleft corner). Let\u2019s now train a RandomForestClassifier and compare its ROC curve and ROC\nAUC score to those of the SGDClassifier. First, you need to get scores for each\ninstance in the training set. But due to the way it works (see Chapter 7), the Random\nForestClassifier class does not have a decision_function() method. Instead, it | Chapter 3: Classification\nhas a predict_proba() method. Scikit-Learn classifiers generally have one or the\nother, or both. The predict_proba() method returns an array containing a row per\ninstance and a column per class, each containing the probability that the given\ninstance belongs to the given class (e.g., 70% chance that the image represents a 5):\nfrom sklearn.ensemble import RandomForestClassifier\nforest_clf = RandomForestClassifier(random_state=42)\ny_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5, cv=3, method=\"predict_proba\")\nThe roc_curve() function expects labels and scores, but instead of scores you can\ngive it class probabilities. Let\u2019s use the positive class\u2019s probability as the score:\ny_scores_forest = y_probas_forest[:, 1] # score = proba of positive class\nfpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest)\nNow you are ready to plot the ROC curve. It is useful to plot the first ROC curve as\nwell to see how they compare (Figure 3-7):\nplt.plot(fpr, tpr, \"b:\", label=\"SGD\")\nplot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\nplt.legend(loc=\"lower right\")\nplt.show()\nFigure 3-7."
  },
  {
    "id": 75,
    "content": "Comparing ROC curves: the Random Forest classifier is superior to the SGD\nclassifier because its ROC curve is much closer to the top-left corner, and it has a greater\nAUC\nPerformance Measures | As you can see in Figure 3-7, the RandomForestClassifier\u2019s ROC curve looks much\nbetter than the SGDClassifier\u2019s: it comes much closer to the top-left corner. As a\nresult, its ROC AUC score is also significantly better:\n>>> roc_auc_score(y_train_5, y_scores_forest)\n0.9983436731328145\nTry measuring the precision and recall scores: you should find 99.0% precision and\n86.6% recall. Not too bad! You now know how to train binary classifiers, choose the appropriate metric for your\ntask, evaluate your classifiers using cross-validation, select the precision/recall trade-\noff that fits your needs, and use ROC curves and ROC AUC scores to compare vari\u2010\nous models. Now let\u2019s try to detect more than just the 5s. Multiclass Classification\nWhereas binary classifiers distinguish between two classes, multiclass classifiers (also\ncalled multinomial classifiers) can distinguish between more than two classes. Some algorithms (such as SGD classifiers, Random Forest classifiers, and naive Bayes\nclassifiers) are capable of handling multiple classes natively. Others (such as Logistic\nRegression or Support Vector Machine classifiers) are strictly binary classifiers. How\u2010\never, there are various strategies that you can use to perform multiclass classification\nwith multiple binary classifiers. One way to create a system that can classify the digit images into 10 classes (from 0 to\n9) is to train 10 binary classifiers, one for each digit (a 0-detector, a 1-detector, a 2-\ndetector, and so on). Then when you want to classify an image, you get the decision\nscore from each classifier for that image and you select the class whose classifier out\u2010\nputs the highest score. This is called the one-versus-the-rest (OvR) strategy (also called\none-versus-all). Another strategy is to train a binary classifier for every pair of digits: one to distin\u2010\nguish 0s and 1s, another to distinguish 0s and 2s, another for 1s and 2s, and so on. This is called the one-versus-one (OvO) strategy. If there are N classes, you need to\ntrain N \u00d7 (N \u2013 1) / 2 classifiers. For the MNIST problem, this means training 45\nbinary classifiers! When you want to classify an image, you have to run the image\nthrough all 45 classifiers and see which class wins the most duels. The main advan\u2010\ntage of OvO is that each classifier only needs to be trained on the part of the training\nset for the two classes that it must distinguish. Some algorithms (such as Support Vector Machine classifiers) scale poorly with the\nsize of the training set. For these algorithms OvO is preferred because it is faster to\ntrain many classifiers on small training sets than to train few classifiers on large train\u2010\ning sets. For most binary classification algorithms, however, OvR is preferred."
  },
  {
    "id": 76,
    "content": "| Chapter 3: Classification\nScikit-Learn detects when you try to use a binary classification algorithm for a multi\u2010\nclass classification task, and it automatically runs OvR or OvO, depending on the\nalgorithm. Let\u2019s try this with a Support Vector Machine classifier (see Chapter 5),\nusing the sklearn.svm.SVC class:\n>>> from sklearn.svm import SVC\n>>> svm_clf = SVC()\n>>> svm_clf.fit(X_train, y_train) # y_train, not y_train_5\n>>> svm_clf.predict([some_digit])\narray([5], dtype=uint8)\nThat was easy! This code trains the SVC on the training set using the original target\nclasses from 0 to 9 (y_train), instead of the 5-versus-the-rest target classes\n(y_train_5). Then it makes a prediction (a correct one in this case). Under the hood,\nScikit-Learn actually used the OvO strategy: it trained 45 binary classifiers, got their\ndecision scores for the image, and selected the class that won the most duels. If you call the decision_function() method, you will see that it returns 10 scores\nper instance (instead of just 1). That\u2019s one score per class:\n>>> some_digit_scores = svm_clf.decision_function([some_digit])\n>>> some_digit_scores\narray([[ 2.92492871, 7.02307409, 3.93648529, 0.90117363, 5.96945908, 9.5 , 1.90718593, 8.02755089, -0.13202708, 4.94216947]])\nThe highest score is indeed the one corresponding to class 5:\n>>> np.argmax(some_digit_scores) >>> svm_clf.classes_\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)\n>>> svm_clf.classes_[5] When a classifier is trained, it stores the list of target classes in its\nclasses_ attribute, ordered by value. In this case, the index of each\nclass in the classes_ array conveniently matches the class itself\n(e.g., the class at index 5 happens to be class 5), but in general you\nwon\u2019t be so lucky. If you want to force Scikit-Learn to use one-versus-one or one-versus-the-rest, you\ncan use the OneVsOneClassifier or OneVsRestClassifier classes. Simply create an\ninstance and pass a classifier to its constructor (it does not even have to be a binary\nclassifier). For example, this code creates a multiclass classifier using the OvR strat\u2010\negy, based on an SVC:\n>>> from sklearn.multiclass import OneVsRestClassifier\n>>> ovr_clf = OneVsRestClassifier(SVC())\n>>> ovr_clf.fit(X_train, y_train)\nMulticlass Classification | >>> ovr_clf.predict([some_digit])\narray([5], dtype=uint8)\n>>> len(ovr_clf.estimators_) Training an SGDClassifier (or a RandomForestClassifier) is just as easy:\n>>> sgd_clf.fit(X_train, y_train)\n>>> sgd_clf.predict([some_digit])\narray([5], dtype=uint8)\nThis time Scikit-Learn did not have to run OvR or OvO because SGD classifiers can\ndirectly classify instances into multiple classes. The decision_function() method\nnow returns one value per class. Let\u2019s look at the score that the SGD classifier assigned\nto each class:\n>>> sgd_clf.decision_function([some_digit])\narray([[-15955.22628, -38080.96296, -13326.66695, 573.52692, -17680.68466, 2412.53175, -25526.86498, -12290.15705, -7946.05205, -10631.35889]])\nYou can see that the classifier is fairly confident about its prediction: almost all scores\nare largely negative, while class 5 has a score of 2412.5. The model has a slight doubt\nregarding class 3, which gets a score of 573.5. Now of course you want to evaluate this\nclassifier. As usual, you can use cross-validation. Use the cross_val_score() func\u2010\ntion to evaluate the SGDClassifier\u2019s accuracy:\n>>> cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring=\"accuracy\")\narray([0.8489802 , 0.87129356, 0.86988048])\nIt gets over 84% on all test folds."
  },
  {
    "id": 77,
    "content": "If you used a random classifier, you would get 10%\naccuracy, so this is not such a bad score, but you can still do much better. Simply scal\u2010\ning the inputs (as discussed in Chapter 2) increases accuracy above 89%:\n>>> from sklearn.preprocessing import StandardScaler\n>>> scaler = StandardScaler()\n>>> X_train_scaled = scaler.fit_transform(X_train.astype(np.float64))\n>>> cross_val_score(sgd_clf, X_train_scaled, y_train, cv=3, scoring=\"accuracy\")\narray([0.89707059, 0.8960948 , 0.90693604])\nError Analysis\nIf this were a real project, you would now follow the steps in your Machine Learning\nproject checklist (see Appendix B). You\u2019d explore data preparation options, try out\nmultiple models (shortlisting the best ones and fine-tuning their hyperparameters\nusing GridSearchCV), and automate as much as possible. Here, we will assume that\nyou have found a promising model and you want to find ways to improve it. One way\nto do this is to analyze the types of errors it makes. | Chapter 3: Classification\nFirst, look at the confusion matrix. You need to make predictions using the\ncross_val_predict() function, then call the confusion_matrix() function, just like\nyou did earlier:\n>>> y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3)\n>>> conf_mx = confusion_matrix(y_train, y_train_pred)\n>>> conf_mx\narray([[5578, 0, 22, 7, 8, 45, 35, 5, 222, 1], [ 0, 6410, 35, 26, 4, 44, 4, 8, 198, 13], [ 28, 27, 5232, 100, 74, 27, 68, 37, 354, 11], [ 23, 18, 115, 5254, 2, 209, 26, 38, 373, 73], [ 11, 14, 45, 12, 5219, 11, 33, 26, 299, 172], [ 26, 16, 31, 173, 54, 4484, 76, 14, 482, 65], [ 31, 17, 45, 2, 42, 98, 5556, 3, 123, 1], [ 20, 10, 53, 27, 50, 13, 3, 5696, 173, 220], [ 17, 64, 47, 91, 3, 125, 24, 11, 5421, 48], [ 24, 18, 29, 67, 116, 39, 1, 174, 329, 5152]])\nThat\u2019s a lot of numbers. It\u2019s often more convenient to look at an image representation\nof the confusion matrix, using Matplotlib\u2019s matshow() function:\nplt.matshow(conf_mx, cmap=plt.cm.gray)\nplt.show()\nThis confusion matrix looks pretty good, since most images are on the main diago\u2010\nnal, which means that they were classified correctly. The 5s look slightly darker than\nthe other digits, which could mean that there are fewer images of 5s in the dataset or\nthat the classifier does not perform as well on 5s as on other digits. In fact, you can\nverify that both are the case. Let\u2019s focus the plot on the errors. First, you need to divide each value in the confusion\nmatrix by the number of images in the corresponding class so that you can compare\nerror rates instead of absolute numbers of errors (which would make abundant\nclasses look unfairly bad):\nError Analysis | row_sums = conf_mx.sum(axis=1, keepdims=True)\nnorm_conf_mx = conf_mx / row_sums\nFill the diagonal with zeros to keep only the errors, and plot the result:\nnp.fill_diagonal(norm_conf_mx, 0)\nplt.matshow(norm_conf_mx, cmap=plt.cm.gray)\nplt.show()\nYou can clearly see the kinds of errors the classifier makes. Remember that rows rep\u2010\nresent actual classes, while columns represent predicted classes."
  },
  {
    "id": 78,
    "content": "The column for class\n8 is quite bright, which tells you that many images get misclassified as 8s. However,\nthe row for class 8 is not that bad, telling you that actual 8s in general get properly\nclassified as 8s. As you can see, the confusion matrix is not necessarily symmetrical. You can also see that 3s and 5s often get confused (in both directions). Analyzing the confusion matrix often gives you insights into ways to improve your\nclassifier. Looking at this plot, it seems that your efforts should be spent on reducing\nthe false 8s. For example, you could try to gather more training data for digits that\nlook like 8s (but are not) so that the classifier can learn to distinguish them from real\n8s. Or you could engineer new features that would help the classifier\u2014for example,\nwriting an algorithm to count the number of closed loops (e.g., 8 has two, 6 has one, 5\nhas none). Or you could preprocess the images (e.g., using Scikit-Image, Pillow, or\nOpenCV) to make some patterns, such as closed loops, stand out more. Analyzing individual errors can also be a good way to gain insights on what your\nclassifier is doing and why it is failing, but it is more difficult and time-consuming. For example, let\u2019s plot examples of 3s and 5s (the plot_digits() function just uses\nMatplotlib\u2019s imshow() function; see this chapter\u2019s Jupyter notebook for details):\ncl_a, cl_b = 3, 5\nX_aa = X_train[(y_train == cl_a) & (y_train_pred == cl_a)]\nX_ab = X_train[(y_train == cl_a) & (y_train_pred == cl_b)]\nX_ba = X_train[(y_train == cl_b) & (y_train_pred == cl_a)]\nX_bb = X_train[(y_train == cl_b) & (y_train_pred == cl_b)] | Chapter 3: Classification\n3 But remember that our brain is a fantastic pattern recognition system, and our visual system does a lot of\ncomplex preprocessing before any information reaches our consciousness, so the fact that it feels simple does\nnot mean that it is. plt.figure(figsize=(8,8))\nplt.subplot(221); plot_digits(X_aa[:25], images_per_row=5)\nplt.subplot(222); plot_digits(X_ab[:25], images_per_row=5)\nplt.subplot(223); plot_digits(X_ba[:25], images_per_row=5)\nplt.subplot(224); plot_digits(X_bb[:25], images_per_row=5)\nplt.show()\nThe two 5 \u00d7 5 blocks on the left show digits classified as 3s, and the two 5 \u00d7 5 blocks\non the right show images classified as 5s. Some of the digits that the classifier gets\nwrong (i.e., in the bottom-left and top-right blocks) are so badly written that even a\nhuman would have trouble classifying them (e.g., the 5 in the first row and second\ncolumn truly looks like a badly written 3). However, most misclassified images seem\nlike obvious errors to us, and it\u2019s hard to understand why the classifier made the mis\u2010\ntakes it did.3 The reason is that we used a simple SGDClassifier, which is a linear\nmodel. All it does is assign a weight per class to each pixel, and when it sees a new\nimage it just sums up the weighted pixel intensities to get a score for each class. So\nsince 3s and 5s differ only by a few pixels, this model will easily confuse them."
  },
  {
    "id": 79,
    "content": "The main difference between 3s and 5s is the position of the small line that joins the\ntop line to the bottom arc. If you draw a 3 with the junction slightly shifted to the left,\nthe classifier might classify it as a 5, and vice versa. In other words, this classifier is\nquite sensitive to image shifting and rotation. So one way to reduce the 3/5 confusion\nwould be to preprocess the images to ensure that they are well centered and not too\nrotated. This will probably help reduce other errors as well. Error Analysis | Multilabel Classification\nUntil now each instance has always been assigned to just one class. In some cases you\nmay want your classifier to output multiple classes for each instance. Consider a face-\nrecognition classifier: what should it do if it recognizes several people in the same\npicture? It should attach one tag per person it recognizes. Say the classifier has been\ntrained to recognize three faces, Alice, Bob, and Charlie. Then when the classifier is\nshown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning \u201cAlice yes,\nBob no, Charlie yes\u201d). Such a classification system that outputs multiple binary tags is\ncalled a multilabel classification system. We won\u2019t go into face recognition just yet, but let\u2019s look at a simpler example, just for\nillustration purposes:\nfrom sklearn.neighbors import KNeighborsClassifier\ny_train_large = (y_train >= 7)\ny_train_odd = (y_train % 2 == 1)\ny_multilabel = np.c_[y_train_large, y_train_odd]\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train, y_multilabel)\nThis code creates a y_multilabel array containing two target labels for each digit\nimage: the first indicates whether or not the digit is large (7, 8, or 9), and the second\nindicates whether or not it is odd. The next lines create a KNeighborsClassifier\ninstance (which supports multilabel classification, though not all classifiers do), and\nwe train it using the multiple targets array. Now you can make a prediction, and\nnotice that it outputs two labels:\n>>> knn_clf.predict([some_digit])\narray([[False, True]])\nAnd it gets it right! The digit 5 is indeed not large (False) and odd (True). There are many ways to evaluate a multilabel classifier, and selecting the right metric\nreally depends on your project. One approach is to measure the F1 score for each\nindividual label (or any other binary classifier metric discussed earlier), then simply\ncompute the average score. This code computes the average F1 score across all labels:\n>>> y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=3)\n>>> f1_score(y_multilabel, y_train_knn_pred, average=\"macro\")\n0.976410265560605\nThis assumes that all labels are equally important, however, which may not be the\ncase. In particular, if you have many more pictures of Alice than of Bob or Charlie,\nyou may want to give more weight to the classifier\u2019s score on pictures of Alice. One\nsimple option is to give each label a weight equal to its support (i.e., the number of | Chapter 3: Classification\n4 Scikit-Learn offers a few other averaging options and multilabel classifier metrics; see the documentation for\nmore details."
  },
  {
    "id": 80,
    "content": "instances with that target label). To do this, simply set average=\"weighted\" in the\npreceding code.4\nMultioutput Classification\nThe last type of classification task we are going to discuss here is called multioutput\u2013\nmulticlass classification (or simply multioutput classification). It is simply a generaliza\u2010\ntion of multilabel classification where each label can be multiclass (i.e., it can have\nmore than two possible values). To illustrate this, let\u2019s build a system that removes noise from images. It will take as\ninput a noisy digit image, and it will (hopefully) output a clean digit image, repre\u2010\nsented as an array of pixel intensities, just like the MNIST images. Notice that the\nclassifier\u2019s output is multilabel (one label per pixel) and each label can have multiple\nvalues (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput\nclassification system. The line between classification and regression is sometimes blurry,\nsuch as in this example. Arguably, predicting pixel intensity is more\nakin to regression than to classification. Moreover, multioutput\nsystems are not limited to classification tasks; you could even have\na system that outputs multiple labels per instance, including both\nclass labels and value labels. Let\u2019s start by creating the training and test sets by taking the MNIST images and\nadding noise to their pixel intensities with NumPy\u2019s randint() function. The target\nimages will be the original images:\nnoise = np.random.randint(0, 100, (len(X_train), 784))\nX_train_mod = X_train + noise\nnoise = np.random.randint(0, 100, (len(X_test), 784))\nX_test_mod = X_test + noise\ny_train_mod = X_train\ny_test_mod = X_test\nLet\u2019s take a peek at an image from the test set (yes, we\u2019re snooping on the test data, so\nyou should be frowning right now):\nMultioutput Classification | 5 You can use the shift() function from the scipy.ndimage.interpolation module. For example,\nshift(image, [2, 1], cval=0) shifts the image two pixels down and one pixel to the right. On the left is the noisy input image, and on the right is the clean target image. Now\nlet\u2019s train the classifier and make it clean this image:\nknn_clf.fit(X_train_mod, y_train_mod)\nclean_digit = knn_clf.predict([X_test_mod[some_index]])\nplot_digit(clean_digit)\nLooks close enough to the target! This concludes our tour of classification. You\nshould now know how to select good metrics for classification tasks, pick the appro\u2010\npriate precision/recall trade-off, compare classifiers, and more generally build good\nclassification systems for a variety of tasks. Exercises\n1. Try to build a classifier for the MNIST dataset that achieves over 97% accuracy\non the test set. Hint: the KNeighborsClassifier works quite well for this task;\nyou just need to find good hyperparameter values (try a grid search on the\nweights and n_neighbors hyperparameters). 2. Write a function that can shift an MNIST image in any direction (left, right, up,\nor down) by one pixel.5 Then, for each image in the training set, create four shif\u2010\nted copies (one per direction) and add them to the training set. Finally, train your\nbest model on this expanded training set and measure its accuracy on the test set."
  },
  {
    "id": 81,
    "content": "You should observe that your model performs even better now! This technique of\nartificially growing the training set is called data augmentation or training set\nexpansion. | Chapter 3: Classification\n3. Tackle the Titanic dataset. A great place to start is on Kaggle. 4. Build a spam classifier (a more challenging exercise):\n\u2022 Download examples of spam and ham from Apache SpamAssassin\u2019s public\ndatasets. \u2022 Unzip the datasets and familiarize yourself with the data format. \u2022 Split the datasets into a training set and a test set. \u2022 Write a data preparation pipeline to convert each email into a feature vector. Your preparation pipeline should transform an email into a (sparse) vector that\nindicates the presence or absence of each possible word. For example, if all\nemails only ever contain four words, \u201cHello,\u201d \u201chow,\u201d \u201care,\u201d \u201cyou,\u201d then the email\n\u201cHello you Hello Hello you\u201d would be converted into a vector [1, 0, 0, 1]\n(meaning [\u201cHello\u201d is present, \u201chow\u201d is absent, \u201care\u201d is absent, \u201cyou\u201d is\npresent]), or [3, 0, 0, 2] if you prefer to count the number of occurrences of\neach word. You may want to add hyperparameters to your preparation pipeline to control\nwhether or not to strip off email headers, convert each email to lowercase,\nremove punctuation, replace all URLs with \u201cURL,\u201d replace all numbers with\n\u201cNUMBER,\u201d or even perform stemming (i.e., trim off word endings; there are\nPython libraries available to do this). Finally, try out several classifiers and see if you can build a great spam classi\u2010\nfier, with both high recall and high precision. Solutions to these exercises can be found in the Jupyter notebooks available at \ngithub.com/ageron/handson-ml2. Exercises | CHAPTER 4\nTraining Models\nSo far we have treated Machine Learning models and their training algorithms mostly\nlike black boxes. If you went through some of the exercises in the previous chapters,\nyou may have been surprised by how much you can get done without knowing any\u2010\nthing about what\u2019s under the hood: you optimized a regression system, you improved\na digit image classifier, and you even built a spam classifier from scratch, all this\nwithout knowing how they actually work. Indeed, in many situations you don\u2019t really\nneed to know the implementation details. However, having a good understanding of how things work can help you quickly\nhome in on the appropriate model, the right training algorithm to use, and a good set\nof hyperparameters for your task. Understanding what\u2019s under the hood will also help\nyou debug issues and perform error analysis more efficiently. Lastly, most of the top\u2010\nics discussed in this chapter will be essential in understanding, building, and training\nneural networks (discussed in Part II of this book). In this chapter we will start by looking at the Linear Regression model, one of the\nsimplest models there is."
  },
  {
    "id": 82,
    "content": "We will discuss two very different ways to train it:\n\u2022 Using a direct \u201cclosed-form\u201d equation that directly computes the model parame\u2010\nters that best fit the model to the training set (i.e., the model parameters that\nminimize the cost function over the training set). \u2022 Using an iterative optimization approach called Gradient Descent (GD) that\ngradually tweaks the model parameters to minimize the cost function over the\ntraining set, eventually converging to the same set of parameters as the first\nmethod. We will look at a few variants of Gradient Descent that we will use again\nand again when we study neural networks in Part II: Batch GD, Mini-batch GD,\nand Stochastic GD. Next we will look at Polynomial Regression, a more complex model that can fit non\u2010\nlinear datasets. Since this model has more parameters than Linear Regression, it is\nmore prone to overfitting the training data, so we will look at how to detect whether\nor not this is the case using learning curves, and then we will look at several regulari\u2010\nzation techniques that can reduce the risk of overfitting the training set. Finally, we will look at two more models that are commonly used for classification\ntasks: Logistic Regression and Softmax Regression. There will be quite a few math equations in this chapter, using basic\nnotions of linear algebra and calculus. To understand these equa\u2010\ntions, you will need to know what vectors and matrices are; how to\ntranspose them, multiply them, and inverse them; and what partial\nderivatives are. If you are unfamiliar with these concepts, please go\nthrough the linear algebra and calculus introductory tutorials avail\u2010\nable as Jupyter notebooks in the online supplemental material. For\nthose who are truly allergic to mathematics, you should still go\nthrough this chapter and simply skip the equations; hopefully, the\ntext will be sufficient to help you understand most of the concepts. Linear Regression\nIn Chapter 1 we looked at a simple regression model of life satisfaction: life_satisfac\u2010\ntion = \u03b80 + \u03b81 \u00d7 GDP_per_capita. This model is just a linear function of the input feature GDP_per_capita. \u03b80 and \u03b81 are\nthe model\u2019s parameters. More generally, a linear model makes a prediction by simply computing a weighted\nsum of the input features, plus a constant called the bias term (also called the intercept\nterm), as shown in Equation 4-1. Equation 4-1. Linear Regression model prediction\ny = \u03b80 + \u03b81x1 + \u03b82x2 + \u22ef+ \u03b8nxn\nIn this equation:\n\u2022 \u0177 is the predicted value. \u2022 n is the number of features. \u2022 xi is the ith feature value. \u2022 \u03b8j is the jth model parameter (including the bias term \u03b80 and the feature weights\n\u03b81, \u03b82, \u22ef, \u03b8n). | Chapter 4: Training Models\n1 It is often the case that a learning algorithm will try to optimize a different function than the performance\nmeasure used to evaluate the final model."
  },
  {
    "id": 83,
    "content": "This is generally because that function is easier to compute, because\nit has useful differentiation properties that the performance measure lacks, or because we want to constrain\nthe model during training, as you will see when we discuss regularization. This can be written much more concisely using a vectorized form, as shown in Equa\u2010\ntion 4-2. Equation 4-2. Linear Regression model prediction (vectorized form)\ny = h\u03b8 x = \u03b8 \u00b7 x\nIn this equation:\n\u2022 \u03b8 is the model\u2019s parameter vector, containing the bias term \u03b80 and the feature\nweights \u03b81 to \u03b8n. \u2022 x is the instance\u2019s feature vector, containing x0 to xn, with x0 always equal to 1. \u2022 \u03b8 \u00b7 x is the dot product of the vectors \u03b8 and x, which is of course equal to \u03b80x0 +\n\u03b81x1 + \u03b82x2 + ... + \u03b8nxn. \u2022 h\u03b8 is the hypothesis function, using the model parameters \u03b8. In Machine Learning, vectors are often represented as column vec\u2010\ntors, which are 2D arrays with a single column. If \u03b8 and x are col\u2010\numn vectors, then the prediction is y = \u03b8\u22bax, where \u03b8\u22ba is the\ntranspose of \u03b8 (a row vector instead of a column vector) and \u03b8\u22bax is\nthe matrix multiplication of \u03b8\u22ba and x. It is of course the same pre\u2010\ndiction, except that it is now represented as a single-cell matrix\nrather than a scalar value. In this book I will use this notation to\navoid switching between dot products and matrix multiplications. OK, that\u2019s the Linear Regression model\u2014but how do we train it? Well, recall that\ntraining a model means setting its parameters so that the model best fits the training\nset. For this purpose, we first need a measure of how well (or poorly) the model fits\nthe training data. In Chapter 2 we saw that the most common performance measure\nof a regression model is the Root Mean Square Error (RMSE) (Equation 2-1). There\u2010\nfore, to train a Linear Regression model, we need to find the value of \u03b8 that minimi\u2010\nzes the RMSE. In practice, it is simpler to minimize the mean squared error (MSE)\nthan the RMSE, and it leads to the same result (because the value that minimizes a\nfunction also minimizes its square root).1\nLinear Regression | The MSE of a Linear Regression hypothesis h\u03b8 on a training set X is calculated using\nEquation 4-3. Equation 4-3. MSE cost function for a Linear Regression model\nMSE X, h\u03b8 = 1\nm \u2211\ni = 1\nm\n\u03b8\u22bax i \u2212y i 2\nMost of these notations were presented in Chapter 2 (see \u201cNotations\u201d on page 40). The only difference is that we write h\u03b8 instead of just h to make it clear that the model\nis parametrized by the vector \u03b8. To simplify notations, we will just write MSE(\u03b8)\ninstead of MSE(X, h\u03b8)."
  },
  {
    "id": 84,
    "content": "The Normal Equation\nTo find the value of \u03b8 that minimizes the cost function, there is a closed-form solution\n\u2014in other words, a mathematical equation that gives the result directly. This is called\nthe Normal Equation (Equation 4-4). Equation 4-4. Normal Equation\n\u03b8 = X\u22baX\n\u22121 X\u22ba y\nIn this equation:\n\u2022 \u03b8 is the value of \u03b8 that minimizes the cost function. \u2022 y is the vector of target values containing y(1) to y(m). Let\u2019s generate some linear-looking data to test this equation on (Figure 4-1):\nimport numpy as np\nX = 2 * np.random.rand(100, 1)\ny = 4 + 3 * X + np.random.randn(100, 1) | Chapter 4: Training Models\nFigure 4-1. Randomly generated linear dataset\nNow let\u2019s compute \u03b8 using the Normal Equation. We will use the inv() function from\nNumPy\u2019s linear algebra module (np.linalg) to compute the inverse of a matrix, and\nthe dot() method for matrix multiplication:\nX_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\ntheta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\nThe function that we used to generate the data is y = 4 + 3x1 + Gaussian noise. Let\u2019s\nsee what the equation found:\n>>> theta_best\narray([[4.21509616], [2.77011339]])\nWe would have hoped for \u03b80 = 4 and \u03b81 = 3 instead of \u03b80 = 4.215 and \u03b81 = 2.770. Close\nenough, but the noise made it impossible to recover the exact parameters of the origi\u2010\nnal function. Now we can make predictions using \u03b8:\n>>> X_new = np.array([[0], [2]])\n>>> X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n>>> y_predict = X_new_b.dot(theta_best)\n>>> y_predict\narray([[4.21509616], [9.75532293]])\nLinear Regression | 2 Note that Scikit-Learn separates the bias term (intercept_) from the feature weights (coef_). Let\u2019s plot this model\u2019s predictions (Figure 4-2):\nplt.plot(X_new, y_predict, \"r-\")\nplt.plot(X, y, \"b.\") plt.axis([0, 2, 0, 15])\nplt.show()\nFigure 4-2. Linear Regression model predictions\nPerforming Linear Regression using Scikit-Learn is simple:2\n>>> from sklearn.linear_model import LinearRegression\n>>> lin_reg = LinearRegression()\n>>> lin_reg.fit(X, y)\n>>> lin_reg.intercept_, lin_reg.coef_\n(array([4.21509616]), array([[2.77011339]]))\n>>> lin_reg.predict(X_new)\narray([[4.21509616], [9.75532293]])\nThe LinearRegression class is based on the scipy.linalg.lstsq() function (the\nname stands for \u201cleast squares\u201d), which you could call directly:\n>>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n>>> theta_best_svd\narray([[4.21509616], [2.77011339]])\nThis function computes \u03b8 = X+y, where \ufffd+ is the pseudoinverse of X (specifically,\nthe Moore-Penrose inverse). You can use np.linalg.pinv() to compute the\npseudoinverse directly: | Chapter 4: Training Models\n>>> np.linalg.pinv(X_b).dot(y)\narray([[4.21509616], [2.77011339]])\nThe pseudoinverse itself is computed using a standard matrix factorization technique\ncalled Singular Value Decomposition (SVD) that can decompose the training set\nmatrix X into the matrix multiplication of three matrices U \u03a3 V\u22ba (see\nnumpy.linalg.svd()). The pseudoinverse is computed as X+ = V\u03a3+U\u22ba. To compute\nthe matrix \u03a3+, the algorithm takes \u03a3 and sets to zero all values smaller than a tiny\nthreshold value, then it replaces all the nonzero values with their inverse, and finally\nit transposes the resulting matrix."
  },
  {
    "id": 85,
    "content": "This approach is more efficient than computing the\nNormal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may\nnot work if the matrix X\u22baX is not invertible (i.e., singular), such as if m < n or if some\nfeatures are redundant, but the pseudoinverse is always defined. Computational Complexity\nThe Normal Equation computes the inverse of X\u22ba X, which is an (n + 1) \u00d7 (n + 1)\nmatrix (where n is the number of features). The computational complexity of inverting\nsuch a matrix is typically about O(n2.4) to O(n3), depending on the implementation. In\nother words, if you double the number of features, you multiply the computation\ntime by roughly 22.4 = 5.3 to 23 = 8. The SVD approach used by Scikit-Learn\u2019s LinearRegression class is about O(n2). If\nyou double the number of features, you multiply the computation time by roughly 4. Both the Normal Equation and the SVD approach get very slow\nwhen the number of features grows large (e.g., 100,000). On the\npositive side, both are linear with regard to the number of instances\nin the training set (they are O(m)), so they handle large training\nsets efficiently, provided they can fit in memory. Also, once you have trained your Linear Regression model (using the Normal Equa\u2010\ntion or any other algorithm), predictions are very fast: the computational complexity\nis linear with regard to both the number of instances you want to make predictions\non and the number of features. In other words, making predictions on twice as many\ninstances (or twice as many features) will take roughly twice as much time. Now we will look at a very different way to train a Linear Regression model, which is\nbetter suited for cases where there are a large number of features or too many training\ninstances to fit in memory. Linear Regression | Gradient Descent\nGradient Descent is a generic optimization algorithm capable of finding optimal solu\u2010\ntions to a wide range of problems. The general idea of Gradient Descent is to tweak\nparameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog, and you can only feel the slope\nof the ground below your feet. A good strategy to get to the bottom of the valley\nquickly is to go downhill in the direction of the steepest slope. This is exactly what\nGradient Descent does: it measures the local gradient of the error function with\nregard to the parameter vector \u03b8, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum! Concretely, you start by filling \u03b8 with random values (this is called random initializa\u2010\ntion). Then you improve it gradually, taking one baby step at a time, each step\nattempting to decrease the cost function (e.g., the MSE), until the algorithm converges\nto a minimum (see Figure 4-3). Figure 4-3."
  },
  {
    "id": 86,
    "content": "In this depiction of Gradient Descent, the model parameters are initialized\nrandomly and get tweaked repeatedly to minimize the cost function; the learning step\nsize is proportional to the slope of the cost function, so the steps gradually get smaller as\nthe parameters approach the minimum\nAn important parameter in Gradient Descent is the size of the steps, determined by\nthe learning rate hyperparameter. If the learning rate is too small, then the algorithm\nwill have to go through many iterations to converge, which will take a long time (see\nFigure 4-4). | Chapter 4: Training Models\nFigure 4-4. The learning rate is too small\nOn the other hand, if the learning rate is too high, you might jump across the valley\nand end up on the other side, possibly even higher up than you were before. This\nmight make the algorithm diverge, with larger and larger values, failing to find a good\nsolution (see Figure 4-5). Figure 4-5. The learning rate is too large\nFinally, not all cost functions look like nice, regular bowls. There may be holes, ridges,\nplateaus, and all sorts of irregular terrains, making convergence to the minimum dif\u2010\nficult. Figure 4-6 shows the two main challenges with Gradient Descent. If the ran\u2010\ndom initialization starts the algorithm on the left, then it will converge to a local\nminimum, which is not as good as the global minimum. If it starts on the right, then it\nwill take a very long time to cross the plateau. And if you stop too early, you will\nnever reach the global minimum. Gradient Descent | 3 Technically speaking, its derivative is Lipschitz continuous. 4 Since feature 1 is smaller, it takes a larger change in \u03b81 to affect the cost function, which is why the bowl is\nelongated along the \u03b81 axis. Figure 4-6. Gradient Descent pitfalls\nFortunately, the MSE cost function for a Linear Regression model happens to be a\nconvex function, which means that if you pick any two points on the curve, the line\nsegment joining them never crosses the curve. This implies that there are no local\nminima, just one global minimum. It is also a continuous function with a slope that\nnever changes abruptly.3 These two facts have a great consequence: Gradient Descent\nis guaranteed to approach arbitrarily close the global minimum (if you wait long\nenough and if the learning rate is not too high). In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if\nthe features have very different scales. Figure 4-7 shows Gradient Descent on a train\u2010\ning set where features 1 and 2 have the same scale (on the left), and on a training set\nwhere feature 1 has much smaller values than feature 2 (on the right).4\nFigure 4-7."
  },
  {
    "id": 87,
    "content": "Gradient Descent with (left) and without (right) feature scaling | Chapter 4: Training Models\nAs you can see, on the left the Gradient Descent algorithm goes straight toward the\nminimum, thereby reaching it quickly, whereas on the right it first goes in a direction\nalmost orthogonal to the direction of the global minimum, and it ends with a long\nmarch down an almost flat valley. It will eventually reach the minimum, but it will\ntake a long time. When using Gradient Descent, you should ensure that all features\nhave a similar scale (e.g., using Scikit-Learn\u2019s StandardScaler\nclass), or else it will take much longer to converge. This diagram also illustrates the fact that training a model means searching for a\ncombination of model parameters that minimizes a cost function (over the training\nset). It is a search in the model\u2019s parameter space: the more parameters a model has,\nthe more dimensions this space has, and the harder the search is: searching for a nee\u2010\ndle in a 300-dimensional haystack is much trickier than in 3 dimensions. Fortunately,\nsince the cost function is convex in the case of Linear Regression, the needle is simply\nat the bottom of the bowl. Batch Gradient Descent\nTo implement Gradient Descent, you need to compute the gradient of the cost func\u2010\ntion with regard to each model parameter \u03b8j. In other words, you need to calculate\nhow much the cost function will change if you change \u03b8j just a little bit. This is called\na partial derivative. It is like asking \u201cWhat is the slope of the mountain under my feet\nif I face east?\u201d and then asking the same question facing north (and so on for all other\ndimensions, if you can imagine a universe with more than three dimensions). Equa\u2010\ntion 4-5 computes the partial derivative of the cost function with regard to parameter\n\u03b8j, noted \u2202 MSE(\u03b8) / \u2202\u03b8j. Equation 4-5. Partial derivatives of the cost function\n\u2202\n\u2202\u03b8j\nMSE \u03b8 = 2\nm \u2211\ni = 1\nm\n\u03b8\u22bax i \u2212y i xj\ni\nInstead of computing these partial derivatives individually, you can use Equation 4-6\nto compute them all in one go. The gradient vector, noted \u2207\u03b8MSE(\u03b8), contains all the\npartial derivatives of the cost function (one for each model parameter). Gradient Descent | 5 Eta (\u03b7) is the seventh letter of the Greek alphabet. Equation 4-6. Gradient vector of the cost function\n\u2207\u03b8 MSE \u03b8 =\n\u2202\n\u2202\u03b80\nMSE \u03b8\n\u2202\n\u2202\u03b81\nMSE \u03b8\n\u22ee\n\u2202\n\u2202\u03b8n\nMSE \u03b8\n= 2\nmX\u22baX\u03b8 \u2212y\nNotice that this formula involves calculations over the full training\nset X, at each Gradient Descent step! This is why the algorithm is\ncalled Batch Gradient Descent: it uses the whole batch of training\ndata at every step (actually, Full Gradient Descent would probably\nbe a better name). As a result it is terribly slow on very large train\u2010\ning sets (but we will see much faster Gradient Descent algorithms\nshortly)."
  },
  {
    "id": 88,
    "content": "However, Gradient Descent scales well with the number of\nfeatures; training a Linear Regression model when there are hun\u2010\ndreds of thousands of features is much faster using Gradient\nDescent than using the Normal Equation or SVD decomposition. Once you have the gradient vector, which points uphill, just go in the opposite direc\u2010\ntion to go downhill. This means subtracting \u2207\u03b8MSE(\u03b8) from \u03b8. This is where the\nlearning rate \u03b7 comes into play:5 multiply the gradient vector by \u03b7 to determine the\nsize of the downhill step (Equation 4-7). Equation 4-7. Gradient Descent step\n\u03b8 next step = \u03b8 \u2212\u03b7\u2207\u03b8 MSE \u03b8\nLet\u2019s look at a quick implementation of this algorithm:\neta = 0.1 # learning rate\nn_iterations = 1000\nm = 100\ntheta = np.random.randn(2,1) # random initialization\nfor iteration in range(n_iterations): gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) theta = theta - eta * gradients | Chapter 4: Training Models\nThat wasn\u2019t too hard! Let\u2019s look at the resulting theta:\n>>> theta\narray([[4.21509616], [2.77011339]])\nHey, that\u2019s exactly what the Normal Equation found! Gradient Descent worked per\u2010\nfectly. But what if you had used a different learning rate eta? Figure 4-8 shows the\nfirst 10 steps of Gradient Descent using three different learning rates (the dashed line\nrepresents the starting point). Figure 4-8. Gradient Descent with various learning rates\nOn the left, the learning rate is too low: the algorithm will eventually reach the solu\u2010\ntion, but it will take a long time. In the middle, the learning rate looks pretty good: in\njust a few iterations, it has already converged to the solution. On the right, the learn\u2010\ning rate is too high: the algorithm diverges, jumping all over the place and actually\ngetting further and further away from the solution at every step. To find a good learning rate, you can use grid search (see Chapter 2). However, you\nmay want to limit the number of iterations so that grid search can eliminate models\nthat take too long to converge. You may wonder how to set the number of iterations. If it is too low, you will still be\nfar away from the optimal solution when the algorithm stops; but if it is too high, you\nwill waste time while the model parameters do not change anymore. A simple solu\u2010\ntion is to set a very large number of iterations but to interrupt the algorithm when the\ngradient vector becomes tiny\u2014that is, when its norm becomes smaller than a tiny\nnumber \u03f5 (called the tolerance)\u2014because this happens when Gradient Descent has\n(almost) reached the minimum. Gradient Descent | Convergence Rate\nWhen the cost function is convex and its slope does not change abruptly (as is the\ncase for the MSE cost function), Batch Gradient Descent with a fixed learning rate\nwill eventually converge to the optimal solution, but you may have to wait a while: it\ncan take O(1/\u03f5) iterations to reach the optimum within a range of \u03f5, depending on the\nshape of the cost function."
  },
  {
    "id": 89,
    "content": "If you divide the tolerance by 10 to have a more precise\nsolution, then the algorithm may have to run about 10 times longer. Stochastic Gradient Descent\nThe main problem with Batch Gradient Descent is the fact that it uses the whole\ntraining set to compute the gradients at every step, which makes it very slow when\nthe training set is large. At the opposite extreme, Stochastic Gradient Descent picks a\nrandom instance in the training set at every step and computes the gradients based\nonly on that single instance. Obviously, working on a single instance at a time makes\nthe algorithm much faster because it has very little data to manipulate at every itera\u2010\ntion. It also makes it possible to train on huge training sets, since only one instance\nneeds to be in memory at each iteration (Stochastic GD can be implemented as an\nout-of-core algorithm; see Chapter 1). On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much\nless regular than Batch Gradient Descent: instead of gently decreasing until it reaches\nthe minimum, the cost function will bounce up and down, decreasing only on aver\u2010\nage. Over time it will end up very close to the minimum, but once it gets there it will\ncontinue to bounce around, never settling down (see Figure 4-9). So once the algo\u2010\nrithm stops, the final parameter values are good, but not optimal. Figure 4-9. With Stochastic Gradient Descent, each training step is much faster but also\nmuch more stochastic than when using Batch Gradient Descent | Chapter 4: Training Models\nWhen the cost function is very irregular (as in Figure 4-6), this can actually help the\nalgorithm jump out of local minima, so Stochastic Gradient Descent has a better\nchance of finding the global minimum than Batch Gradient Descent does. Therefore, randomness is good to escape from local optima, but bad because it means\nthat the algorithm can never settle at the minimum. One solution to this dilemma is\nto gradually reduce the learning rate. The steps start out large (which helps make\nquick progress and escape local minima), then get smaller and smaller, allowing the\nalgorithm to settle at the global minimum. This process is akin to simulated anneal\u2010\ning, an algorithm inspired from the process in metallurgy of annealing, where molten\nmetal is slowly cooled down. The function that determines the learning rate at each\niteration is called the learning schedule. If the learning rate is reduced too quickly, you\nmay get stuck in a local minimum, or even end up frozen halfway to the minimum. If\nthe learning rate is reduced too slowly, you may jump around the minimum for a\nlong time and end up with a suboptimal solution if you halt training too early."
  },
  {
    "id": 90,
    "content": "This code implements Stochastic Gradient Descent using a simple learning schedule:\nn_epochs = 50\nt0, t1 = 5, 50 # learning schedule hyperparameters\ndef learning_schedule(t): return t0 / (t + t1)\ntheta = np.random.randn(2,1) # random initialization\nfor epoch in range(n_epochs): for i in range(m): random_index = np.random.randint(m) xi = X_b[random_index:random_index+1] yi = y[random_index:random_index+1] gradients = 2 * xi.T.dot(xi.dot(theta) - yi) eta = learning_schedule(epoch * m + i) theta = theta - eta * gradients\nBy convention we iterate by rounds of m iterations; each round is called an epoch. While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010\ning set, this code goes through the training set only 50 times and reaches a pretty\ngood solution:\n>>> theta\narray([[4.21076011], [2.74856079]])\nFigure 4-10 shows the first 20 steps of training (notice how irregular the steps are). Gradient Descent | Figure 4-10. The first 20 steps of Stochastic Gradient Descent\nNote that since instances are picked randomly, some instances may be picked several\ntimes per epoch, while others may not be picked at all. If you want to be sure that the\nalgorithm goes through every instance at each epoch, another approach is to shuffle\nthe training set (making sure to shuffle the input features and the labels jointly), then\ngo through it instance by instance, then shuffle it again, and so on. However, this\napproach generally converges more slowly. When using Stochastic Gradient Descent, the training instances\nmust be independent and identically distributed (IID) to ensure\nthat the parameters get pulled toward the global optimum, on aver\u2010\nage. A simple way to ensure this is to shuffle the instances during\ntraining (e.g., pick each instance randomly, or shuffle the training\nset at the beginning of each epoch). If you do not shuffle the\ninstances\u2014for example, if the instances are sorted by label\u2014then\nSGD will start by optimizing for one label, then the next, and so on,\nand it will not settle close to the global minimum. To perform Linear Regression using Stochastic GD with Scikit-Learn, you can use the\nSGDRegressor class, which defaults to optimizing the squared error cost function. The following code runs for maximum 1,000 epochs or until the loss drops by less\nthan 0.001 during one epoch (max_iter=1000, tol=1e-3). It starts with a learning rate\nof 0.1 (eta0=0.1), using the default learning schedule (different from the preceding\none). Lastly, it does not use any regularization (penalty=None; more details on this\nshortly): | Chapter 4: Training Models\nfrom sklearn.linear_model import SGDRegressor\nsgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\nsgd_reg.fit(X, y.ravel())\nOnce again, you find a solution quite close to the one returned by the Normal\nEquation:\n>>> sgd_reg.intercept_, sgd_reg.coef_\n(array([4.24365286]), array([2.8250878]))\nMini-batch Gradient Descent\nThe last Gradient Descent algorithm we will look at is called Mini-batch Gradient\nDescent."
  },
  {
    "id": 91,
    "content": "It is simple to understand once you know Batch and Stochastic Gradient\nDescent: at each step, instead of computing the gradients based on the full training set\n(as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD\ncomputes the gradients on small random sets of instances called mini-batches. The\nmain advantage of Mini-batch GD over Stochastic GD is that you can get a perfor\u2010\nmance boost from hardware optimization of matrix operations, especially when using\nGPUs. The algorithm\u2019s progress in parameter space is less erratic than with Stochastic GD,\nespecially with fairly large mini-batches. As a result, Mini-batch GD will end up walk\u2010\ning around a bit closer to the minimum than Stochastic GD\u2014but it may be harder for\nit to escape from local minima (in the case of problems that suffer from local minima,\nunlike Linear Regression). Figure 4-11 shows the paths taken by the three Gradient\nDescent algorithms in parameter space during training. They all end up near the\nminimum, but Batch GD\u2019s path actually stops at the minimum, while both Stochastic\nGD and Mini-batch GD continue to walk around. However, don\u2019t forget that Batch\nGD takes a lot of time to take each step, and Stochastic GD and Mini-batch GD\nwould also reach the minimum if you used a good learning schedule. Figure 4-11. Gradient Descent paths in parameter space\nGradient Descent | 6 While the Normal Equation can only perform Linear Regression, the Gradient Descent algorithms can be\nused to train many other models, as we will see. 7 A quadratic equation is of the form y = ax2 + bx + c.\nLet\u2019s compare the algorithms we\u2019ve discussed so far for Linear Regression6 (recall that\nm is the number of training instances and n is the number of features); see Table 4-1. Table 4-1. Comparison of algorithms for Linear Regression\nAlgorithm\nLarge m\nOut-of-core support\nLarge n\nHyperparams\nScaling required\nScikit-Learn\nNormal Equation\nFast\nNo\nSlow No\nN/A\nSVD\nFast\nNo\nSlow No\nLinearRegression\nBatch GD\nSlow\nNo\nFast Yes\nSGDRegressor\nStochastic GD\nFast\nYes\nFast\n\u22652\nYes\nSGDRegressor\nMini-batch GD\nFast\nYes\nFast\n\u22652\nYes\nSGDRegressor\nThere is almost no difference after training: all these algorithms\nend up with very similar models and make predictions in exactly\nthe same way. Polynomial Regression\nWhat if your data is more complex than a straight line? Surprisingly, you can use a\nlinear model to fit nonlinear data. A simple way to do this is to add powers of each\nfeature as new features, then train a linear model on this extended set of features. This\ntechnique is called Polynomial Regression. Let\u2019s look at an example. First, let\u2019s generate some nonlinear data, based on a simple\nquadratic equation7 (plus some noise; see Figure 4-12):\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1) | Chapter 4: Training Models\nFigure 4-12."
  },
  {
    "id": 92,
    "content": "Generated nonlinear and noisy dataset\nClearly, a straight line will never fit this data properly. So let\u2019s use Scikit-Learn\u2019s Poly\nnomialFeatures class to transform our training data, adding the square (second-\ndegree polynomial) of each feature in the training set as a new feature (in this case\nthere is just one feature):\n>>> from sklearn.preprocessing import PolynomialFeatures\n>>> poly_features = PolynomialFeatures(degree=2, include_bias=False)\n>>> X_poly = poly_features.fit_transform(X)\n>>> X[0]\narray([-0.75275929])\n>>> X_poly[0]\narray([-0.75275929, 0.56664654])\nX_poly now contains the original feature of X plus the square of this feature. Now you\ncan fit a LinearRegression model to this extended training data (Figure 4-13):\n>>> lin_reg = LinearRegression()\n>>> lin_reg.fit(X_poly, y)\n>>> lin_reg.intercept_, lin_reg.coef_\n(array([1.78134581]), array([[0.93366893, 0.56456263]]))\nPolynomial Regression | Figure 4-13. Polynomial Regression model predictions\nNot bad: the model estimates y = 0.56x1\n2 + 0.93x1 + 1.78 when in fact the original\nfunction was y = 0.5x1\n2 + 1.0x1 + 2.0 + Gaussian noise. Note that when there are multiple features, Polynomial Regression is capable of find\u2010\ning relationships between features (which is something a plain Linear Regression\nmodel cannot do). This is made possible by the fact that PolynomialFeatures also\nadds all combinations of features up to the given degree. For example, if there were\ntwo features a and b, PolynomialFeatures with degree=3 would not only add the\nfeatures a2, a3, b2, and b3, but also the combinations ab, a2b, and ab2. PolynomialFeatures(degree=d) transforms an array containing n\nfeatures into an array containing (n + d)! / d!n! features, where n! is\nthe factorial of n, equal to 1 \u00d7 2 \u00d7 3 \u00d7 \u22ef \u00d7 n. Beware of the combi\u2010\nnatorial explosion of the number of features! Learning Curves\nIf you perform high-degree Polynomial Regression, you will likely fit the training\ndata much better than with plain Linear Regression. For example, Figure 4-14 applies\na 300-degree polynomial model to the preceding training data, and compares the\nresult with a pure linear model and a quadratic model (second-degree polynomial). Notice how the 300-degree polynomial model wiggles around to get as close as possi\u2010\nble to the training instances. | Chapter 4: Training Models\nFigure 4-14. High-degree Polynomial Regression\nThis high-degree Polynomial Regression model is severely overfitting the training\ndata, while the linear model is underfitting it. The model that will generalize best in\nthis case is the quadratic model, which makes sense because the data was generated\nusing a quadratic model. But in general you won\u2019t know what function generated the\ndata, so how can you decide how complex your model should be? How can you tell\nthat your model is overfitting or underfitting the data? In Chapter 2 you used cross-validation to get an estimate of a model\u2019s generalization\nperformance. If a model performs well on the training data but generalizes poorly\naccording to the cross-validation metrics, then your model is overfitting. If it per\u2010\nforms poorly on both, then it is underfitting. This is one way to tell when a model is\ntoo simple or too complex."
  },
  {
    "id": 93,
    "content": "Another way to tell is to look at the learning curves: these are plots of the model\u2019s per\u2010\nformance on the training set and the validation set as a function of the training set\nsize (or the training iteration). To generate the plots, train the model several times on\ndifferent sized subsets of the training set. The following code defines a function that,\ngiven some training data, plots the learning curves of a model:\nLearning Curves | from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\ndef plot_learning_curves(model, X, y): X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2) train_errors, val_errors = [], [] for m in range(1, len(X_train)): model.fit(X_train[:m], y_train[:m]) y_train_predict = model.predict(X_train[:m]) y_val_predict = model.predict(X_val) train_errors.append(mean_squared_error(y_train[:m], y_train_predict)) val_errors.append(mean_squared_error(y_val, y_val_predict)) plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\") plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")\nLet\u2019s look at the learning curves of the plain Linear Regression model (a straight line;\nsee Figure 4-15):\nlin_reg = LinearRegression()\nplot_learning_curves(lin_reg, X, y)\nFigure 4-15. Learning curves\nThis model that\u2019s underfitting deserves a bit of explanation. First, let\u2019s look at the per\u2010\nformance on the training data: when there are just one or two instances in the train\u2010\ning set, the model can fit them perfectly, which is why the curve starts at zero. But as\nnew instances are added to the training set, it becomes impossible for the model to fit\nthe training data perfectly, both because the data is noisy and because it is not linear\nat all. So the error on the training data goes up until it reaches a plateau, at which\npoint adding new instances to the training set doesn\u2019t make the average error much\nbetter or worse. Now let\u2019s look at the performance of the model on the validation\ndata. When the model is trained on very few training instances, it is incapable of gen\u2010\neralizing properly, which is why the validation error is initially quite big. Then, as the | Chapter 4: Training Models\nmodel is shown more training examples, it learns, and thus the validation error\nslowly goes down. However, once again a straight line cannot do a good job modeling\nthe data, so the error ends up at a plateau, very close to the other curve. These learning curves are typical of a model that\u2019s underfitting. Both curves have\nreached a plateau; they are close and fairly high. If your model is underfitting the training data, adding more train\u2010\ning examples will not help. You need to use a more complex model\nor come up with better features. Now let\u2019s look at the learning curves of a 10th-degree polynomial model on the same\ndata (Figure 4-16):\nfrom sklearn.pipeline import Pipeline\npolynomial_regression = Pipeline([ (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)), (\"lin_reg\", LinearRegression()), ])\nplot_learning_curves(polynomial_regression, X, y)\nFigure 4-16. Learning curves for the 10th-degree polynomial model\nThese learning curves look a bit like the previous ones, but there are two very impor\u2010\ntant differences:\n\u2022 The error on the training data is much lower than with the Linear Regression\nmodel."
  },
  {
    "id": 94,
    "content": "Learning Curves | 8 This notion of bias is not to be confused with the bias term of linear models. \u2022 There is a gap between the curves. This means that the model performs signifi\u2010\ncantly better on the training data than on the validation data, which is the hall\u2010\nmark of an overfitting model. If you used a much larger training set, however, the\ntwo curves would continue to get closer. One way to improve an overfitting model is to feed it more training\ndata until the validation error reaches the training error. The Bias/Variance Trade-off\nAn important theoretical result of statistics and Machine Learning is the fact that a\nmodel\u2019s generalization error can be expressed as the sum of three very different\nerrors:\nBias\nThis part of the generalization error is due to wrong assumptions, such as assum\u2010\ning that the data is linear when it is actually quadratic. A high-bias model is most\nlikely to underfit the training data.8\nVariance\nThis part is due to the model\u2019s excessive sensitivity to small variations in the\ntraining data. A model with many degrees of freedom (such as a high-degree pol\u2010\nynomial model) is likely to have high variance and thus overfit the training data. Irreducible error\nThis part is due to the noisiness of the data itself. The only way to reduce this\npart of the error is to clean up the data (e.g., fix the data sources, such as broken\nsensors, or detect and remove outliers). Increasing a model\u2019s complexity will typically increase its variance and reduce its bias. Conversely, reducing a model\u2019s complexity increases its bias and reduces its variance. This is why it is called a trade-off. Regularized Linear Models\nAs we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the\nmodel (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be | Chapter 4: Training Models\n9 It is common to use the notation J(\u03b8) for cost functions that don\u2019t have a short name; we will often use this\nnotation throughout the rest of this book. The context will make it clear which cost function is being dis\u2010\ncussed. for it to overfit the data. A simple way to regularize a polynomial model is to reduce\nthe number of polynomial degrees. For a linear model, regularization is typically achieved by constraining the weights of\nthe model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net,\nwhich implement three different ways to constrain the weights. Ridge Regression\nRidge Regression (also called Tikhonov regularization) is a regularized version of Lin\u2010\near Regression: a regularization term equal to \u03b1\u2211i = 1\nn\n\u03b8i\n2 is added to the cost function. This forces the learning algorithm to not only fit the data but also keep the model\nweights as small as possible. Note that the regularization term should only be added\nto the cost function during training."
  },
  {
    "id": 95,
    "content": "Once the model is trained, you want to use the\nunregularized performance measure to evaluate the model\u2019s performance. It is quite common for the cost function used during training to be\ndifferent from the performance measure used for testing. Apart\nfrom regularization, another reason they might be different is that a\ngood training cost function should have optimization-friendly\nderivatives, while the performance measure used for testing should\nbe as close as possible to the final objective. For example, classifiers\nare often trained using a cost function such as the log loss (dis\u2010\ncussed in a moment) but evaluated using precision/recall. The hyperparameter \u03b1 controls how much you want to regularize the model. If \u03b1 = 0,\nthen Ridge Regression is just Linear Regression. If \u03b1 is very large, then all weights end\nup very close to zero and the result is a flat line going through the data\u2019s mean. Equa\u2010\ntion 4-8 presents the Ridge Regression cost function.9\nEquation 4-8. Ridge Regression cost function\nJ \u03b8 = MSE \u03b8 + \u03b11\n2 \u2211i = 1\nn\n\u03b8i Note that the bias term \u03b80 is not regularized (the sum starts at i = 1, not 0). If we\ndefine w as the vector of feature weights (\u03b81 to \u03b8n), then the regularization term is\nRegularized Linear Models | 10 Norms are discussed in Chapter 2.\nequal to \u00bd(\u2225 w \u22252)2, where \u2225 w \u22252 represents the \u21132 norm of the weight vector.10 For\nGradient Descent, just add \u03b1w to the MSE gradient vector (Equation 4-6). It is important to scale the data (e.g., using a StandardScaler)\nbefore performing Ridge Regression, as it is sensitive to the scale of\nthe input features. This is true of most regularized models. Figure 4-17 shows several Ridge models trained on some linear data using different \u03b1\nvalues. On the left, plain Ridge models are used, leading to linear predictions. On the\nright, the data is first expanded using PolynomialFeatures(degree=10), then it is\nscaled using a StandardScaler, and finally the Ridge models are applied to the result\u2010\ning features: this is Polynomial Regression with Ridge regularization. Note how\nincreasing \u03b1 leads to flatter (i.e., less extreme, more reasonable) predictions, thus\nreducing the model\u2019s variance but increasing its bias. Figure 4-17. A linear model (left) and a polynomial model (right), both with various lev\u2010\nels of Ridge regularization\nAs with Linear Regression, we can perform Ridge Regression either by computing a\nclosed-form equation or by performing Gradient Descent. The pros and cons are the | Chapter 4: Training Models\n11 A square matrix full of 0s except for 1s on the main diagonal (top left to bottom right). 12 Alternatively you can use the Ridge class with the \"sag\" solver. Stochastic Average GD is a variant of Stochas\u2010\ntic GD. For more details, see the presentation \u201cMinimizing Finite Sums with the Stochastic Average Gradient\nAlgorithm\u201d by Mark Schmidt et al. from the University of British Columbia. same."
  },
  {
    "id": 96,
    "content": "Equation 4-9 shows the closed-form solution, where A is the (n + 1) \u00d7 (n + 1)\nidentity matrix,11 except with a 0 in the top-left cell, corresponding to the bias term. Equation 4-9. Ridge Regression closed-form solution\n\u03b8 = X\u22baX + \u03b1A\n\u22121 X\u22ba y\nHere is how to perform Ridge Regression with Scikit-Learn using a closed-form solu\u2010\ntion (a variant of Equation 4-9 that uses a matrix factorization technique by Andr\u00e9-\nLouis Cholesky):\n>>> from sklearn.linear_model import Ridge\n>>> ridge_reg = Ridge(alpha=1, solver=\"cholesky\")\n>>> ridge_reg.fit(X, y)\n>>> ridge_reg.predict([[1.5]])\narray([[1.55071465]])\nAnd using Stochastic Gradient Descent:12\n>>> sgd_reg = SGDRegressor(penalty=\"l2\")\n>>> sgd_reg.fit(X, y.ravel())\n>>> sgd_reg.predict([[1.5]])\narray([1.47012588])\nThe penalty hyperparameter sets the type of regularization term to use. Specifying\n\"l2\" indicates that you want SGD to add a regularization term to the cost function\nequal to half the square of the \u21132 norm of the weight vector: this is simply Ridge\nRegression. Lasso Regression\nLeast Absolute Shrinkage and Selection Operator Regression (usually simply called\nLasso Regression) is another regularized version of Linear Regression: just like Ridge\nRegression, it adds a regularization term to the cost function, but it uses the \u21131 norm\nof the weight vector instead of half the square of the \u21132 norm (see Equation 4-10). Equation 4-10. Lasso Regression cost function\nJ \u03b8 = MSE \u03b8 + \u03b1\u2211i = 1\nn\n\u03b8i\nRegularized Linear Models | Figure 4-18 shows the same thing as Figure 4-17 but replaces Ridge models with\nLasso models and uses smaller \u03b1 values. Figure 4-18. A linear model (left) and a polynomial model (right), both using various\nlevels of Lasso regularization\nAn important characteristic of Lasso Regression is that it tends to eliminate the\nweights of the least important features (i.e., set them to zero). For example, the\ndashed line in the righthand plot in Figure 4-18 (with \u03b1 = 10-7) looks quadratic,\nalmost linear: all the weights for the high-degree polynomial features are equal to\nzero. In other words, Lasso Regression automatically performs feature selection and\noutputs a sparse model (i.e., with few nonzero feature weights). You can get a sense of why this is the case by looking at Figure 4-19: the axes repre\u2010\nsent two model parameters, and the background contours represent different loss\nfunctions. In the top-left plot, the contours represent the \u21131 loss (|\u03b81| + |\u03b82|), which\ndrops linearly as you get closer to any axis. For example, if you initialize the model\nparameters to \u03b81 = 2 and \u03b82 = 0.5, running Gradient Descent will decrement both\nparameters equally (as represented by the dashed yellow line); therefore \u03b82 will reach\n0 first (since it was closer to 0 to begin with). After that, Gradient Descent will roll\ndown the gutter until it reaches \u03b81 = 0 (with a bit of bouncing around, since the gradi\u2010\nents of \u21131 never get close to 0: they are either \u20131 or 1 for each parameter)."
  },
  {
    "id": 97,
    "content": "In the top-\nright plot, the contours represent Lasso\u2019s cost function (i.e., an MSE cost function plus\nan \u21131 loss). The small white circles show the path that Gradient Descent takes to opti\u2010\nmize some model parameters that were initialized around \u03b81 = 0.25 and \u03b82 = \u20131:\nnotice once again how the path quickly reaches \u03b82 = 0, then rolls down the gutter and\nends up bouncing around the global optimum (represented by the red square). If we\nincreased \u03b1, the global optimum would move left along the dashed yellow line, while | Chapter 4: Training Models\nif we decreased \u03b1, the global optimum would move right (in this example, the optimal\nparameters for the unregularized MSE are \u03b81 = 2 and \u03b82 = 0.5). Figure 4-19. Lasso versus Ridge regularization\nThe two bottom plots show the same thing but with an \u21132 penalty instead. In the\nbottom-left plot, you can see that the \u21132 loss decreases with the distance to the origin,\nso Gradient Descent just takes a straight path toward that point. In the bottom-right\nplot, the contours represent Ridge Regression\u2019s cost function (i.e., an MSE cost func\u2010\ntion plus an \u21132 loss). There are two main differences with Lasso. First, the gradients\nget smaller as the parameters approach the global optimum, so Gradient Descent nat\u2010\nurally slows down, which helps convergence (as there is no bouncing around). Sec\u2010\nond, the optimal parameters (represented by the red square) get closer and closer to\nthe origin when you increase \u03b1, but they never get eliminated entirely. To avoid Gradient Descent from bouncing around the optimum at\nthe end when using Lasso, you need to gradually reduce the learn\u2010\ning rate during training (it will still bounce around the optimum,\nbut the steps will get smaller and smaller, so it will converge). Regularized Linear Models | 13 You can think of a subgradient vector at a nondifferentiable point as an intermediate vector between the gra\u2010\ndient vectors around that point. The Lasso cost function is not differentiable at \u03b8i = 0 (for i = 1, 2, \u22ef, n), but Gradient\nDescent still works fine if you use a subgradient vector g13 instead when any \u03b8i = 0. Equation 4-11 shows a subgradient vector equation you can use for Gradient Descent\nwith the Lasso cost function. Equation 4-11. Lasso Regression subgradient vector\ng \u03b8, J = \u2207\u03b8 MSE \u03b8 + \u03b1\nsign \u03b81\nsign \u03b82\n\u22ee\nsign \u03b8n where sign \u03b8i =\n\u22121 if\u00a0\u03b8i < 0\n0 if\u00a0\u03b8i = 0\n+1 if\u00a0\u03b8i > 0\nHere is a small Scikit-Learn example using the Lasso class:\n>>> from sklearn.linear_model import Lasso\n>>> lasso_reg = Lasso(alpha=0.1)\n>>> lasso_reg.fit(X, y)\n>>> lasso_reg.predict([[1.5]])\narray([1.53788174])\nNote that you could instead use SGDRegressor(penalty=\"l1\"). Elastic Net\nElastic Net is a middle ground between Ridge Regression and Lasso Regression."
  },
  {
    "id": 98,
    "content": "The\nregularization term is a simple mix of both Ridge and Lasso\u2019s regularization terms,\nand you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge\nRegression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12). Equation 4-12. Elastic Net cost function\nJ \u03b8 = MSE \u03b8 + r\u03b1\u2211i = 1\nn\n\u03b8i + 1 \u2212r\n2 \u03b1\u2211i = 1\nn\n\u03b8i So when should you use plain Linear Regression (i.e., without any regularization),\nRidge, Lasso, or Elastic Net? It is almost always preferable to have at least a little bit of\nregularization, so generally you should avoid plain Linear Regression. Ridge is a good\ndefault, but if you suspect that only a few features are useful, you should prefer Lasso\nor Elastic Net because they tend to reduce the useless features\u2019 weights down to zero,\nas we have discussed. In general, Elastic Net is preferred over Lasso because Lasso | Chapter 4: Training Models\nmay behave erratically when the number of features is greater than the number of\ntraining instances or when several features are strongly correlated. Here is a short example that uses Scikit-Learn\u2019s ElasticNet (l1_ratio corresponds to\nthe mix ratio r):\n>>> from sklearn.linear_model import ElasticNet\n>>> elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)\n>>> elastic_net.fit(X, y)\n>>> elastic_net.predict([[1.5]])\narray([1.54333232])\nEarly Stopping\nA very different way to regularize iterative learning algorithms such as Gradient\nDescent is to stop training as soon as the validation error reaches a minimum. This is\ncalled early stopping. Figure 4-20 shows a complex model (in this case, a high-degree\nPolynomial Regression model) being trained with Batch Gradient Descent. As the\nepochs go by the algorithm learns, and its prediction error (RMSE) on the training\nset goes down, along with its prediction error on the validation set. After a while\nthough, the validation error stops decreasing and starts to go back up. This indicates\nthat the model has started to overfit the training data. With early stopping you just\nstop training as soon as the validation error reaches the minimum. It is such a simple\nand efficient regularization technique that Geoffrey Hinton called it a \u201cbeautiful free\nlunch.\u201d\nFigure 4-20. Early stopping regularization\nRegularized Linear Models | With Stochastic and Mini-batch Gradient Descent, the curves are\nnot so smooth, and it may be hard to know whether you have\nreached the minimum or not. One solution is to stop only after the\nvalidation error has been above the minimum for some time (when\nyou are confident that the model will not do any better), then roll\nback the model parameters to the point where the validation error\nwas at a minimum."
  },
  {
    "id": 99,
    "content": "Here is a basic implementation of early stopping:\nfrom sklearn.base import clone\n# prepare the data\npoly_scaler = Pipeline([ (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)), (\"std_scaler\", StandardScaler()) ])\nX_train_poly_scaled = poly_scaler.fit_transform(X_train)\nX_val_poly_scaled = poly_scaler.transform(X_val)\nsgd_reg = SGDRegressor(max_iter=1, tol=-np.infty, warm_start=True, penalty=None, learning_rate=\"constant\", eta0=0.0005)\nminimum_val_error = float(\"inf\")\nbest_epoch = None\nbest_model = None\nfor epoch in range(1000): sgd_reg.fit(X_train_poly_scaled, y_train) # continues where it left off y_val_predict = sgd_reg.predict(X_val_poly_scaled) val_error = mean_squared_error(y_val, y_val_predict) if val_error < minimum_val_error: minimum_val_error = val_error best_epoch = epoch best_model = clone(sgd_reg)\nNote that with warm_start=True, when the fit() method is called it continues train\u2010\ning where it left off, instead of restarting from scratch. Logistic Regression\nAs we discussed in Chapter 1, some regression algorithms can be used for classifica\u2010\ntion (and vice versa). Logistic Regression (also called Logit Regression) is commonly\nused to estimate the probability that an instance belongs to a particular class (e.g.,\nwhat is the probability that this email is spam?). If the estimated probability is greater\nthan 50%, then the model predicts that the instance belongs to that class (called the\npositive class, labeled \u201c1\u201d), and otherwise it predicts that it does not (i.e., it belongs to\nthe negative class, labeled \u201c0\u201d). This makes it a binary classifier. | Chapter 4: Training Models\nEstimating Probabilities\nSo how does Logistic Regression work? Just like a Linear Regression model, a Logistic\nRegression model computes a weighted sum of the input features (plus a bias term),\nbut instead of outputting the result directly like the Linear Regression model does, it\noutputs the logistic of this result (see Equation 4-13). Equation 4-13. Logistic Regression model estimated probability (vectorized form)\np = h\u03b8 x = \u03c3 x\u22ba\u03b8\nThe logistic\u2014noted \u03c3(\u00b7)\u2014is a sigmoid function (i.e., S-shaped) that outputs a number\nbetween 0 and 1. It is defined as shown in Equation 4-14 and Figure 4-21. Equation 4-14. Logistic function\n\u03c3 t = 1 + exp\n\u2212t\nFigure 4-21. Logistic function\nOnce the Logistic Regression model has estimated the probability p = h\u03b8(x) that an\ninstance x belongs to the positive class, it can make its prediction \u0177 easily (see Equa\u2010\ntion 4-15). Equation 4-15. Logistic Regression model prediction\ny = 0 if p < 0.5\n1 if p \u22650.5\nNotice that \u03c3(t) < 0.5 when t < 0, and \u03c3(t) \u2265 0.5 when t \u2265 0, so a Logistic Regression\nmodel predicts 1 if x\u22ba \u03b8 is positive and 0 if it is negative. Logistic Regression | The score t is often called the logit. The name comes from the fact\nthat the logit function, defined as logit(p) = log(p / (1 \u2013 p)), is the\ninverse of the logistic function. Indeed, if you compute the logit of\nthe estimated probability p, you will find that the result is t. The\nlogit is also called the log-odds, since it is the log of the ratio\nbetween the estimated probability for the positive class and the\nestimated probability for the negative class."
  },
  {
    "id": 100,
    "content": "Training and Cost Function\nNow you know how a Logistic Regression model estimates probabilities and makes\npredictions. But how is it trained? The objective of training is to set the parameter\nvector \u03b8 so that the model estimates high probabilities for positive instances (y = 1)\nand low probabilities for negative instances (y = 0). This idea is captured by the cost\nfunction shown in Equation 4-16 for a single training instance x. Equation 4-16. Cost function of a single training instance\nc \u03b8 =\n\u2212log p\nif\u00a0y = 1\n\u2212log 1 \u2212p if\u00a0y = 0\nThis cost function makes sense because \u2013log(t) grows very large when t approaches 0,\nso the cost will be large if the model estimates a probability close to 0 for a positive\ninstance, and it will also be very large if the model estimates a probability close to 1\nfor a negative instance. On the other hand, \u2013log(t) is close to 0 when t is close to 1, so\nthe cost will be close to 0 if the estimated probability is close to 0 for a negative\ninstance or close to 1 for a positive instance, which is precisely what we want. The cost function over the whole training set is the average cost over all training\ninstances. It can be written in a single expression called the log loss, shown in Equa\u2010\ntion 4-17. Equation 4-17. Logistic Regression cost function (log loss)\nJ \u03b8 = \u22121\nm \u2211i = 1\nm\ny i log p i\n+ 1 \u2212y i log 1 \u2212p i\nThe bad news is that there is no known closed-form equation to compute the value of\n\u03b8 that minimizes this cost function (there is no equivalent of the Normal Equation). The good news is that this cost function is convex, so Gradient Descent (or any other\noptimization algorithm) is guaranteed to find the global minimum (if the learning | Chapter 4: Training Models\n14 Photos reproduced from the corresponding Wikipedia pages. Iris virginica photo by Frank Mayfield (Creative\nCommons BY-SA 2.0), Iris versicolor photo by D. Gordon E. Robertson (Creative Commons BY-SA 3.0), Iris\nsetosa photo public domain. rate is not too large and you wait long enough). The partial derivatives of the cost\nfunction with regard to the jth model parameter \u03b8j are given by Equation 4-18. Equation 4-18. Logistic cost function partial derivatives\n\u2202\n\u2202\u03b8j\nJ \u03b8 = 1\nm \u2211\ni = 1\nm\n\u03c3 \u03b8\u22bax i\n\u2212y i xj\ni\nThis equation looks very much like Equation 4-5: for each instance it computes the\nprediction error and multiplies it by the jth feature value, and then it computes the\naverage over all training instances. Once you have the gradient vector containing all\nthe partial derivatives, you can use it in the Batch Gradient Descent algorithm. That\u2019s\nit: you now know how to train a Logistic Regression model."
  },
  {
    "id": 101,
    "content": "For Stochastic GD you\nwould take one instance at a time, and for Mini-batch GD you would use a mini-\nbatch at a time. Decision Boundaries\nLet\u2019s use the iris dataset to illustrate Logistic Regression. This is a famous dataset that\ncontains the sepal and petal length and width of 150 iris flowers of three different\nspecies: Iris setosa, Iris versicolor, and Iris virginica (see Figure 4-22). Figure 4-22. Flowers of three iris plant species14\nLogistic Regression | 15 NumPy\u2019s reshape() function allows one dimension to be \u20131, which means \u201cunspecified\u201d: the value is inferred\nfrom the length of the array and the remaining dimensions. Let\u2019s try to build a classifier to detect the Iris virginica type based only on the petal\nwidth feature. First let\u2019s load the data:\n>>> from sklearn import datasets\n>>> iris = datasets.load_iris()\n>>> list(iris.keys())\n['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']\n>>> X = iris[\"data\"][:, 3:] # petal width\n>>> y = (iris[\"target\"] == 2).astype(np.int) # 1 if Iris virginica, else 0\nNow let\u2019s train a Logistic Regression model:\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\nLet\u2019s look at the model\u2019s estimated probabilities for flowers with petal widths varying\nfrom 0 cm to 3 cm (Figure 4-23):15\nX_new = np.linspace(0, 3, 1000).reshape(-1, 1)\ny_proba = log_reg.predict_proba(X_new)\nplt.plot(X_new, y_proba[:, 1], \"g-\", label=\"Iris virginica\")\nplt.plot(X_new, y_proba[:, 0], \"b--\", label=\"Not Iris virginica\")\n# + more Matplotlib code to make the image look pretty\nFigure 4-23. Estimated probabilities and decision boundary\nThe petal width of Iris virginica flowers (represented by triangles) ranges from 1.4 cm\nto 2.5 cm, while the other iris flowers (represented by squares) generally have a\nsmaller petal width, ranging from 0.1 cm to 1.8 cm. Notice that there is a bit of over\u2010\nlap. Above about 2 cm the classifier is highly confident that the flower is an Iris virgin\u2010\nica (it outputs a high probability for that class), while below 1 cm it is highly\nconfident that it is not an Iris virginica (high probability for the \u201cNot Iris virginica\u201d | Chapter 4: Training Models\n16 It is the the set of points x such that \u03b80 + \u03b81x1 + \u03b82x2 = 0, which defines a straight line. class). In between these extremes, the classifier is unsure. However, if you ask it to\npredict the class (using the predict() method rather than the predict_proba()\nmethod), it will return whichever class is the most likely. Therefore, there is a decision\nboundary at around 1.6 cm where both probabilities are equal to 50%: if the petal\nwidth is higher than 1.6 cm, the classifier will predict that the flower is an Iris virgin\u2010\nica, and otherwise it will predict that it is not (even if it is not very confident):\n>>> log_reg.predict([[1.7], [1.5]])\narray([1, 0])\nFigure 4-24 shows the same dataset, but this time displaying two features: petal width\nand length. Once trained, the Logistic Regression classifier can, based on these two\nfeatures, estimate the probability that a new flower is an Iris virginica."
  },
  {
    "id": 102,
    "content": "The dashed line\nrepresents the points where the model estimates a 50% probability: this is the model\u2019s\ndecision boundary. Note that it is a linear boundary.16 Each parallel line represents the\npoints where the model outputs a specific probability, from 15% (bottom left) to 90%\n(top right). All the flowers beyond the top-right line have an over 90% chance of\nbeing Iris virginica, according to the model. Figure 4-24. Linear decision boundary\nJust like the other linear models, Logistic Regression models can be regularized using\n\u21131 or \u21132 penalties. Scikit-Learn actually adds an \u21132 penalty by default. The hyperparameter controlling the regularization strength of a\nScikit-Learn LogisticRegression model is not alpha (as in other\nlinear models), but its inverse: C. The higher the value of C, the less\nthe model is regularized. Logistic Regression | Softmax Regression\nThe Logistic Regression model can be generalized to support multiple classes directly,\nwithout having to train and combine multiple binary classifiers (as discussed in\nChapter 3). This is called Softmax Regression, or Multinomial Logistic Regression. The idea is simple: when given an instance x, the Softmax Regression model first\ncomputes a score sk(x) for each class k, then estimates the probability of each class by\napplying the softmax function (also called the normalized exponential) to the scores. The equation to compute sk(x) should look familiar, as it is just like the equation for\nLinear Regression prediction (see Equation 4-19). Equation 4-19. Softmax score for class k\nsk x = x\u22ba\u03b8 k\nNote that each class has its own dedicated parameter vector \u03b8(k). All these vectors are\ntypically stored as rows in a parameter matrix \u0398. Once you have computed the score of every class for the instance x, you can estimate\nthe probability pk that the instance belongs to class k by running the scores through\nthe softmax function (Equation 4-20). The function computes the exponential of\nevery score, then normalizes them (dividing by the sum of all the exponentials). The\nscores are generally called logits or log-odds (although they are actually unnormal\u2010\nized log-odds). Equation 4-20. Softmax function\npk = \u03c3 s x\nk =\nexp sk x\n\u2211j = 1\nK\nexp sj x\nIn this equation:\n\u2022 K is the number of classes. \u2022 s(x) is a vector containing the scores of each class for the instance x. \u2022 \u03c3(s(x))k is the estimated probability that the instance x belongs to class k, given\nthe scores of each class for that instance. | Chapter 4: Training Models\nJust like the Logistic Regression classifier, the Softmax Regression classifier predicts\nthe class with the highest estimated probability (which is simply the class with the\nhighest score), as shown in Equation 4-21. Equation 4-21. Softmax Regression classifier prediction\ny = argmax\nk\n\u03c3 s x\nk = argmax\nk\nsk x = argmax\nk\n\u03b8 k \u22bax\nThe argmax operator returns the value of a variable that maximizes a function."
  },
  {
    "id": 103,
    "content": "In this\nequation, it returns the value of k that maximizes the estimated probability \u03c3(s(x))k.\nThe Softmax Regression classifier predicts only one class at a time\n(i.e., it is multiclass, not multioutput), so it should be used only\nwith mutually exclusive classes, such as different types of plants. You cannot use it to recognize multiple people in one picture. Now that you know how the model estimates probabilities and makes predictions,\nlet\u2019s take a look at training. The objective is to have a model that estimates a high\nprobability for the target class (and consequently a low probability for the other\nclasses). Minimizing the cost function shown in Equation 4-22, called the cross\nentropy, should lead to this objective because it penalizes the model when it estimates\na low probability for a target class. Cross entropy is frequently used to measure how\nwell a set of estimated class probabilities matches the target classes. Equation 4-22. Cross entropy cost function\nJ \u0398 = \u22121\nm \u2211i = 1\nm\n\u2211k = 1\nK\nyk\ni log pk\ni\nIn this equation:\n\u2022 yk\ni is the target probability that the ith instance belongs to class k. In general, it is\neither equal to 1 or 0, depending on whether the instance belongs to the class or\nnot. Notice that when there are just two classes (K = 2), this cost function is equivalent to\nthe Logistic Regression\u2019s cost function (log loss; see Equation 4-17). Logistic Regression | Cross Entropy\nCross entropy originated from information theory. Suppose you want to efficiently\ntransmit information about the weather every day. If there are eight options (sunny,\nrainy, etc. ), you could encode each option using three bits because 23 = 8. However, if\nyou think it will be sunny almost every day, it would be much more efficient to code\n\u201csunny\u201d on just one bit (0) and the other seven options on four bits (starting with a\n1). Cross entropy measures the average number of bits you actually send per option. If your assumption about the weather is perfect, cross entropy will be equal to the\nentropy of the weather itself (i.e., its intrinsic unpredictability). But if your assump\u2010\ntions are wrong (e.g., if it rains often), cross entropy will be greater by an amount\ncalled the Kullback\u2013Leibler (KL) divergence. The cross entropy between two probability distributions p and q is defined as H(p,q)\n= \u2014\u03a3x p(x) log q(x) (at least when the distributions are discrete). For more details,\ncheck out my video on the subject. The gradient vector of this cost function with regard to \u03b8(k) is given by Equation 4-23. Equation 4-23. Cross entropy gradient vector for class k\n\u2207\n\u03b8 k J \u0398 = 1\nm \u2211\ni = 1\nm\npk\ni \u2212yk\ni x i\nNow you can compute the gradient vector for every class, then use Gradient Descent\n(or any other optimization algorithm) to find the parameter matrix \u0398 that minimizes\nthe cost function."
  },
  {
    "id": 104,
    "content": "Let\u2019s use Softmax Regression to classify the iris flowers into all three classes. Scikit-\nLearn\u2019s LogisticRegression uses one-versus-the-rest by default when you train it on\nmore than two classes, but you can set the multi_class hyperparameter to \"multino\nmial\" to switch it to Softmax Regression. You must also specify a solver that supports\nSoftmax Regression, such as the \"lbfgs\" solver (see Scikit-Learn\u2019s documentation for\nmore details). It also applies \u21132 regularization by default, which you can control using\nthe hyperparameter C:\nX = iris[\"data\"][:, (2, 3)] # petal length, petal width\ny = iris[\"target\"]\nsoftmax_reg = LogisticRegression(multi_class=\"multinomial\",solver=\"lbfgs\", C=10)\nsoftmax_reg.fit(X, y)\nSo the next time you find an iris with petals that are 5 cm long and 2 cm wide, you\ncan ask your model to tell you what type of iris it is, and it will answer Iris virginica\n(class 2) with 94.2% probability (or Iris versicolor with 5.8% probability): | Chapter 4: Training Models\n>>> softmax_reg.predict([[5, 2]])\narray([2])\n>>> softmax_reg.predict_proba([[5, 2]])\narray([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])\nFigure 4-25 shows the resulting decision boundaries, represented by the background\ncolors. Notice that the decision boundaries between any two classes are linear. The\nfigure also shows the probabilities for the Iris versicolor class, represented by the\ncurved lines (e.g., the line labeled with 0.450 represents the 45% probability bound\u2010\nary). Notice that the model can predict a class that has an estimated probability below\n50%. For example, at the point where all decision boundaries meet, all classes have an\nequal estimated probability of 33%. Figure 4-25. Softmax Regression decision boundaries\nExercises\n1. Which Linear Regression training algorithm can you use if you have a training\nset with millions of features? 2. Suppose the features in your training set have very different scales. Which algo\u2010\nrithms might suffer from this, and how? What can you do about it? 3. Can Gradient Descent get stuck in a local minimum when training a Logistic\nRegression model? 4. Do all Gradient Descent algorithms lead to the same model, provided you let\nthem run long enough? 5. Suppose you use Batch Gradient Descent and you plot the validation error at\nevery epoch. If you notice that the validation error consistently goes up, what is\nlikely going on? How can you fix this? 6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali\u2010\ndation error goes up? Exercises | 7. Which Gradient Descent algorithm (among those we discussed) will reach the\nvicinity of the optimal solution the fastest? Which will actually converge? How\ncan you make the others converge as well? 8. Suppose you are using Polynomial Regression. You plot the learning curves and\nyou notice that there is a large gap between the training error and the validation\nerror. What is happening? What are three ways to solve this? 9. Suppose you are using Ridge Regression and you notice that the training error\nand the validation error are almost equal and fairly high."
  },
  {
    "id": 105,
    "content": "Would you say that the\nmodel suffers from high bias or high variance? Should you increase the regulari\u2010\nzation hyperparameter \u03b1 or reduce it? 10. Why would you want to use:\na. Ridge Regression instead of plain Linear Regression (i.e., without any regula\u2010\nrization)? b. Lasso instead of Ridge Regression? c. Elastic Net instead of Lasso? 11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement two Logistic Regression classifiers or one Softmax Regres\u2010\nsion classifier? 12. Implement Batch Gradient Descent with early stopping for Softmax Regression\n(without using Scikit-Learn). Solutions to these exercises are available in Appendix A. | Chapter 4: Training Models\nCHAPTER 5\nSupport Vector Machines\nA Support Vector Machine (SVM) is a powerful and versatile Machine Learning\nmodel, capable of performing linear or nonlinear classification, regression, and even\noutlier detection. It is one of the most popular models in Machine Learning, and any\u2010\none interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010\nularly well suited for classification of complex small- or medium-sized datasets. This chapter will explain the core concepts of SVMs, how to use them, and how they\nwork. Linear SVM Classification\nThe fundamental idea behind SVMs is best explained with some pictures. Figure 5-1\nshows part of the iris dataset that was introduced at the end of Chapter 4. The two\nclasses can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The\nmodel whose decision boundary is represented by the dashed line is so bad that it\ndoes not even separate the classes properly. The other two models work perfectly on\nthis training set, but their decision boundaries come so close to the instances that\nthese models will probably not perform as well on new instances. In contrast, the\nsolid line in the plot on the right represents the decision boundary of an SVM classi\u2010\nfier; this line not only separates the two classes but also stays as far away from the\nclosest training instances as possible. You can think of an SVM classifier as fitting the\nwidest possible street (represented by the parallel dashed lines) between the classes. This is called large margin classification. Figure 5-1. Large margin classification\nNotice that adding more training instances \u201coff the street\u201d will not affect the decision\nboundary at all: it is fully determined (or \u201csupported\u201d) by the instances located on the\nedge of the street. These instances are called the support vectors (they are circled in\nFigure 5-1). Figure 5-2. Sensitivity to feature scales\nSVMs are sensitive to the feature scales, as you can see in\nFigure 5-2: in the left plot, the vertical scale is much larger than the\nhorizontal scale, so the widest possible street is close to horizontal. After feature scaling (e.g., using Scikit-Learn\u2019s StandardScaler),\nthe decision boundary in the right plot looks much better."
  },
  {
    "id": 106,
    "content": "Soft Margin Classification\nIf we strictly impose that all instances must be off the street and on the right side, this\nis called hard margin classification. There are two main issues with hard margin clas\u2010\nsification. First, it only works if the data is linearly separable. Second, it is sensitive to\noutliers. Figure 5-3 shows the iris dataset with just one additional outlier: on the left,\nit is impossible to find a hard margin; on the right, the decision boundary ends up\nvery different from the one we saw in Figure 5-1 without the outlier, and it will prob\u2010\nably not generalize as well. | Chapter 5: Support Vector Machines\nFigure 5-3. Hard margin sensitivity to outliers\nTo avoid these issues, use a more flexible model. The objective is to find a good bal\u2010\nance between keeping the street as large as possible and limiting the margin violations\n(i.e., instances that end up in the middle of the street or even on the wrong side). This\nis called soft margin classification. When creating an SVM model using Scikit-Learn, we can specify a number of hyper\u2010\nparameters. C is one of those hyperparameters. If we set it to a low value, then we end\nup with the model on the left of Figure 5-4. With a high value, we get the model on\nthe right. Margin violations are bad. It\u2019s usually better to have few of them. However,\nin this case the model on the left has a lot of margin violations but will probably gen\u2010\neralize better. Figure 5-4. Large margin (left) versus fewer margin violations (right)\nIf your SVM model is overfitting, you can try regularizing it by\nreducing C.\nThe following Scikit-Learn code loads the iris dataset, scales the features, and then\ntrains a linear SVM model (using the LinearSVC class with C=1 and the hinge loss\nfunction, described shortly) to detect Iris virginica flowers:\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.pipeline import Pipeline\nLinear SVM Classification | from sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\niris = datasets.load_iris()\nX = iris[\"data\"][:, (2, 3)] # petal length, petal width\ny = (iris[\"target\"] == 2).astype(np.float64) # Iris virginica\nsvm_clf = Pipeline([ (\"scaler\", StandardScaler()), (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\")), ])\nsvm_clf.fit(X, y)\nThe resulting model is represented on the left in Figure 5-4. Then, as usual, you can use the model to make predictions:\n>>> svm_clf.predict([[5.5, 1.7]])\narray([1.]) Unlike Logistic Regression classifiers, SVM classifiers do not out\u2010\nput probabilities for each class. Instead of using the LinearSVC class, we could use the SVC class with a linear kernel. When creating the SVC model, we would write SVC(kernel=\"linear\", C=1). Or we\ncould use the SGDClassifier class, with SGDClassifier(loss=\"hinge\", alpha=1/\n(m*C)). This applies regular Stochastic Gradient Descent (see Chapter 4) to train a\nlinear SVM classifier. It does not converge as fast as the LinearSVC class, but it can be\nuseful to handle online classification tasks or huge datasets that do not fit in memory\n(out-of-core training)."
  },
  {
    "id": 107,
    "content": "The LinearSVC class regularizes the bias term, so you should center\nthe training set first by subtracting its mean. This is automatic if\nyou scale the data using the StandardScaler. Also make sure you\nset the loss hyperparameter to \"hinge\", as it is not the default\nvalue. Finally, for better performance, you should set the dual\nhyperparameter to False, unless there are more features than\ntraining instances (we will discuss duality later in the chapter). | Chapter 5: Support Vector Machines\nNonlinear SVM Classification\nAlthough linear SVM classifiers are efficient and work surprisingly well in many\ncases, many datasets are not even close to being linearly separable. One approach to\nhandling nonlinear datasets is to add more features, such as polynomial features (as\nyou did in Chapter 4); in some cases this can result in a linearly separable dataset. Consider the left plot in Figure 5-5: it represents a simple dataset with just one fea\u2010\nture, x1. This dataset is not linearly separable, as you can see. But if you add a second\nfeature x2 = (x1)2, the resulting 2D dataset is perfectly linearly separable. Figure 5-5. Adding features to make a dataset linearly separable\nTo implement this idea using Scikit-Learn, create a Pipeline containing a Polyno\nmialFeatures transformer (discussed in \u201cPolynomial Regression\u201d on page 128), fol\u2010\nlowed by a StandardScaler and a LinearSVC. Let\u2019s test this on the moons dataset: this\nis a toy dataset for binary classification in which the data points are shaped as two\ninterleaving half circles (see Figure 5-6). You can generate this dataset using the\nmake_moons() function:\nfrom sklearn.datasets import make_moons\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nX, y = make_moons(n_samples=100, noise=0.15)\npolynomial_svm_clf = Pipeline([ (\"poly_features\", PolynomialFeatures(degree=3)), (\"scaler\", StandardScaler()), (\"svm_clf\", LinearSVC(C=10, loss=\"hinge\")) ])\npolynomial_svm_clf.fit(X, y)\nNonlinear SVM Classification | Figure 5-6. Linear SVM classifier using polynomial features\nPolynomial Kernel\nAdding polynomial features is simple to implement and can work great with all sorts\nof Machine Learning algorithms (not just SVMs). That said, at a low polynomial\ndegree, this method cannot deal with very complex datasets, and with a high polyno\u2010\nmial degree it creates a huge number of features, making the model too slow. Fortunately, when using SVMs you can apply an almost miraculous mathematical\ntechnique called the kernel trick (explained in a moment). The kernel trick makes it\npossible to get the same result as if you had added many polynomial features, even\nwith very high-degree polynomials, without actually having to add them. So there is\nno combinatorial explosion of the number of features because you don\u2019t actually add\nany features. This trick is implemented by the SVC class. Let\u2019s test it on the moons\ndataset:\nfrom sklearn.svm import SVC\npoly_kernel_svm_clf = Pipeline([ (\"scaler\", StandardScaler()), (\"svm_clf\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5)) ])\npoly_kernel_svm_clf.fit(X, y)\nThis code trains an SVM classifier using a third-degree polynomial kernel. It is repre\u2010\nsented on the left in Figure 5-7. On the right is another SVM classifier using a 10th-\ndegree polynomial kernel."
  },
  {
    "id": 108,
    "content": "Obviously, if your model is overfitting, you might want to\nreduce the polynomial degree. Conversely, if it is underfitting, you can try increasing\nit. The hyperparameter coef0 controls how much the model is influenced by high-\ndegree polynomials versus low-degree polynomials. | Chapter 5: Support Vector Machines\nFigure 5-7. SVM classifiers with a polynomial kernel\nA common approach to finding the right hyperparameter values is\nto use grid search (see Chapter 2). It is often faster to first do a very\ncoarse grid search, then a finer grid search around the best values\nfound. Having a good sense of what each hyperparameter actually\ndoes can also help you search in the right part of the hyperparame\u2010\nter space. Similarity Features\nAnother technique to tackle nonlinear problems is to add features computed using a\nsimilarity function, which measures how much each instance resembles a particular\nlandmark. For example, let\u2019s take the 1D dataset discussed earlier and add two land\u2010\nmarks to it at x1 = \u20132 and x1 = 1 (see the left plot in Figure 5-8). Next, let\u2019s define the\nsimilarity function to be the Gaussian Radial Basis Function (RBF) with \u03b3 = 0.3 (see\nEquation 5-1). Equation 5-1. Gaussian RBF\n\u03d5\u03b3 x, \u2113= exp \u2212\u03b3\u2225x \u2212\u2113\u22252\nThis is a bell-shaped function varying from 0 (very far away from the landmark) to 1\n(at the landmark). Now we are ready to compute the new features. For example, let\u2019s\nlook at the instance x1 = \u20131: it is located at a distance of 1 from the first landmark and\n2 from the second landmark. Therefore its new features are x2 = exp(\u20130.3 \u00d7 12) \u2248 0.74\nand x3 = exp(\u20130.3 \u00d7 22) \u2248 0.30. The plot on the right in Figure 5-8 shows the trans\u2010\nformed dataset (dropping the original features). As you can see, it is now linearly\nseparable. Nonlinear SVM Classification | Figure 5-8. Similarity features using the Gaussian RBF\nYou may wonder how to select the landmarks. The simplest approach is to create a\nlandmark at the location of each and every instance in the dataset. Doing that creates\nmany dimensions and thus increases the chances that the transformed training set\nwill be linearly separable. The downside is that a training set with m instances and n\nfeatures gets transformed into a training set with m instances and m features (assum\u2010\ning you drop the original features). If your training set is very large, you end up with\nan equally large number of features. Gaussian RBF Kernel\nJust like the polynomial features method, the similarity features method can be useful\nwith any Machine Learning algorithm, but it may be computationally expensive to\ncompute all the additional features, especially on large training sets. Once again the\nkernel trick does its SVM magic, making it possible to obtain a similar result as if you\nhad added many similarity features."
  },
  {
    "id": 109,
    "content": "Let\u2019s try the SVC class with the Gaussian RBF\nkernel:\nrbf_kernel_svm_clf = Pipeline([ (\"scaler\", StandardScaler()), (\"svm_clf\", SVC(kernel=\"rbf\", gamma=5, C=0.001)) ])\nrbf_kernel_svm_clf.fit(X, y)\nThis model is represented at the bottom left in Figure 5-9. The other plots show mod\u2010\nels trained with different values of hyperparameters gamma (\u03b3) and C. Increasing gamma\nmakes the bell-shaped curve narrower (see the lefthand plots in Figure 5-8). As a\nresult, each instance\u2019s range of influence is smaller: the decision boundary ends up\nbeing more irregular, wiggling around individual instances. Conversely, a small gamma\nvalue makes the bell-shaped curve wider: instances have a larger range of influence,\nand the decision boundary ends up smoother. So \u03b3 acts like a regularization | Chapter 5: Support Vector Machines\nhyperparameter: if your model is overfitting, you should reduce it; if it is underfitting,\nyou should increase it (similar to the C hyperparameter). Figure 5-9. SVM classifiers using an RBF kernel\nOther kernels exist but are used much more rarely. Some kernels are specialized for\nspecific data structures. String kernels are sometimes used when classifying text docu\u2010\nments or DNA sequences (e.g., using the string subsequence kernel or kernels based on\nthe Levenshtein distance). With so many kernels to choose from, how can you decide which\none to use? As a rule of thumb, you should always try the linear\nkernel first (remember that LinearSVC is much faster than SVC(ker\nnel=\"linear\")), especially if the training set is very large or if it\nhas plenty of features. If the training set is not too large, you should\nalso try the Gaussian RBF kernel; it works well in most cases. Then\nif you have spare time and computing power, you can experiment\nwith a few other kernels, using cross-validation and grid search. You\u2019d want to experiment like that especially if there are kernels\nspecialized for your training set\u2019s data structure. Nonlinear SVM Classification | 1 Chih-Jen Lin et al., \u201cA Dual Coordinate Descent Method for Large-Scale Linear SVM,\u201d Proceedings of the 25th\nInternational Conference on Machine Learning (2008): 408\u2013415. 2 John Platt, \u201cSequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines\u201d\n(Microsoft Research technical report, April 21, 1998), \nuploads/2016/02/tr-98-14.pdf. Computational Complexity\nThe LinearSVC class is based on the liblinear library, which implements an opti\u2010\nmized algorithm for linear SVMs.1 It does not support the kernel trick, but it scales\nalmost linearly with the number of training instances and the number of features. Its\ntraining time complexity is roughly O(m \u00d7 n). The algorithm takes longer if you require very high precision. This is controlled by\nthe tolerance hyperparameter \u03f5 (called tol in Scikit-Learn). In most classification\ntasks, the default tolerance is fine. The SVC class is based on the libsvm library, which implements an algorithm that\nsupports the kernel trick.2 The training time complexity is usually between O(m2 \u00d7 n)\nand O(m3 \u00d7 n). Unfortunately, this means that it gets dreadfully slow when the num\u2010\nber of training instances gets large (e.g., hundreds of thousands of instances)."
  },
  {
    "id": 110,
    "content": "This\nalgorithm is perfect for complex small or medium-sized training sets. It scales well\nwith the number of features, especially with sparse features (i.e., when each instance\nhas few nonzero features). In this case, the algorithm scales roughly with the average\nnumber of nonzero features per instance. Table 5-1 compares Scikit-Learn\u2019s SVM\nclassification classes. Table 5-1. Comparison of Scikit-Learn classes for SVM classification\nClass\nTime complexity\nOut-of-core support\nScaling required\nKernel trick\nLinearSVC\nO(m \u00d7 n)\nNo\nYes\nNo\nSGDClassifier O(m \u00d7 n)\nYes\nYes\nNo\nSVC\nO(m\u00b2 \u00d7 n) to O(m\u00b3 \u00d7 n)\nNo\nYes\nYes\nSVM Regression\nAs mentioned earlier, the SVM algorithm is versatile: not only does it support linear\nand nonlinear classification, but it also supports linear and nonlinear regression. To\nuse SVMs for regression instead of classification, the trick is to reverse the objective:\ninstead of trying to fit the largest possible street between two classes while limiting\nmargin violations, SVM Regression tries to fit as many instances as possible on the\nstreet while limiting margin violations (i.e., instances off the street). The width of the\nstreet is controlled by a hyperparameter, \u03f5. Figure 5-10 shows two linear SVM | Chapter 5: Support Vector Machines\nRegression models trained on some random linear data, one with a large margin (\u03f5 =\n1.5) and the other with a small margin (\u03f5 = 0.5). Figure 5-10. SVM Regression\nAdding more training instances within the margin does not affect the model\u2019s predic\u2010\ntions; thus, the model is said to be \u03f5-insensitive. You can use Scikit-Learn\u2019s LinearSVR class to perform linear SVM Regression. The\nfollowing code produces the model represented on the left in Figure 5-10 (the train\u2010\ning data should be scaled and centered first):\nfrom sklearn.svm import LinearSVR\nsvm_reg = LinearSVR(epsilon=1.5)\nsvm_reg.fit(X, y)\nTo tackle nonlinear regression tasks, you can use a kernelized SVM model. Figure 5-11 shows SVM Regression on a random quadratic training set, using a\nsecond-degree polynomial kernel. There is little regularization in the left plot (i.e., a\nlarge C value), and much more regularization in the right plot (i.e., a small C value). SVM Regression | Figure 5-11. SVM Regression using a second-degree polynomial kernel\nThe following code uses Scikit-Learn\u2019s SVR class (which supports the kernel trick) to\nproduce the model represented on the left in Figure 5-11:\nfrom sklearn.svm import SVR\nsvm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\nsvm_poly_reg.fit(X, y)\nThe SVR class is the regression equivalent of the SVC class, and the LinearSVR class is\nthe regression equivalent of the LinearSVC class. The LinearSVR class scales linearly\nwith the size of the training set (just like the LinearSVC class), while the SVR class gets\nmuch too slow when the training set grows large (just like the SVC class). SVMs can also be used for outlier detection; see Scikit-Learn\u2019s doc\u2010\numentation for more details. Under the Hood\nThis section explains how SVMs make predictions and how their training algorithms\nwork, starting with linear SVM classifiers."
  },
  {
    "id": 111,
    "content": "If you are just getting started with Machine\nLearning, you can safely skip it and go straight to the exercises at the end of this chap\u2010\nter, and come back later when you want to get a deeper understanding of SVMs. First, a word about notations. In Chapter 4 we used the convention of putting all the\nmodel parameters in one vector \u03b8, including the bias term \u03b80 and the input feature\nweights \u03b81 to \u03b8n, and adding a bias input x0 = 1 to all instances. In this chapter we will\nuse a convention that is more convenient (and more common) when dealing with | Chapter 5: Support Vector Machines\n3 More generally, when there are n features, the decision function is an n-dimensional hyperplane, and the deci\u2010\nsion boundary is an (n \u2013 1)-dimensional hyperplane. SVMs: the bias term will be called b, and the feature weights vector will be called w.\nNo bias feature will be added to the input feature vectors. Decision Function and Predictions\nThe linear SVM classifier model predicts the class of a new instance x by simply com\u2010\nputing the decision function w\u22ba x + b = w1 x1 + \u22ef + wn xn + b. If the result is positive,\nthe predicted class \u0177 is the positive class (1), and otherwise it is the negative class (0);\nsee Equation 5-2. Equation 5-2. Linear SVM classifier prediction\ny = 0 if w\u22bax + b < 0,\n1 if w\u22bax + b \u22650\nFigure 5-12 shows the decision function that corresponds to the model in the left in\nFigure 5-4: it is a 2D plane because this dataset has two features (petal width and petal\nlength). The decision boundary is the set of points where the decision function is\nequal to 0: it is the intersection of two planes, which is a straight line (represented by\nthe thick solid line).3\nFigure 5-12. Decision function for the iris dataset\nUnder the Hood | The dashed lines represent the points where the decision function is equal to 1 or \u20131:\nthey are parallel and at equal distance to the decision boundary, and they form a mar\u2010\ngin around it. Training a linear SVM classifier means finding the values of w and b\nthat make this margin as wide as possible while avoiding margin violations (hard\nmargin) or limiting them (soft margin). Training Objective\nConsider the slope of the decision function: it is equal to the norm of the weight vec\u2010\ntor, \u2225 w \u2225. If we divide this slope by 2, the points where the decision function is equal\nto \u00b11 are going to be twice as far away from the decision boundary. In other words,\ndividing the slope by 2 will multiply the margin by 2. This may be easier to visualize\nin 2D, as shown in Figure 5-13. The smaller the weight vector w, the larger the\nmargin. Figure 5-13."
  },
  {
    "id": 112,
    "content": "A smaller weight vector results in a larger margin\nSo we want to minimize \u2225 w \u2225 to get a large margin. If we also want to avoid any\nmargin violations (hard margin), then we need the decision function to be greater\nthan 1 for all positive training instances and lower than \u20131 for negative training\ninstances. If we define t(i) = \u20131 for negative instances (if y(i) = 0) and t(i) = 1 for positive\ninstances (if y(i) = 1), then we can express this constraint as t(i)(w\u22ba x(i) + b) \u2265 1 for all\ninstances. We can therefore express the hard margin linear SVM classifier objective as the con\u2010\nstrained optimization problem in Equation 5-3. Equation 5-3. Hard margin linear SVM classifier objective\nminimize\nw, b 2w\u22baw\nsubject to\nt i w\u22bax i + b \u22651\nfor i = 1, 2, \u22ef, m | Chapter 5: Support Vector Machines\n4 Zeta (\u03b6) is the sixth letter of the Greek alphabet. 5 To learn more about Quadratic Programming, you can start by reading Stephen Boyd and Lieven Vandenber\u2010\nghe\u2019s book Convex Optimization (Cambridge University Press, 2004) or watch Richard Brown\u2019s series of video\nlectures. We are minimizing \u00bd w\u22ba w, which is equal to \u00bd\u2225 w \u22252, rather than\nminimizing \u2225 w \u2225. Indeed, \u00bd\u2225 w \u22252 has a nice, simple derivative (it\nis just w), while \u2225 w \u2225 is not differentiable at w = 0. Optimization\nalgorithms work much better on differentiable functions. To get the soft margin objective, we need to introduce a slack variable \u03b6(i) \u2265 0 for each\ninstance:4 \u03b6(i) measures how much the ith instance is allowed to violate the margin. We\nnow have two conflicting objectives: make the slack variables as small as possible to\nreduce the margin violations, and make \u00bd w\u22ba w as small as possible to increase the\nmargin. This is where the C hyperparameter comes in: it allows us to define the trade\u2010\noff between these two objectives. This gives us the constrained optimization problem\nin Equation 5-4. Equation 5-4. Soft margin linear SVM classifier objective\nminimize\nw, b, \u03b6 2w\u22baw + C \u2211\ni = 1\nm\n\u03b6 i\nsubject to\nt i w\u22bax i + b \u22651 \u2212\u03b6 i\nand\n\u03b6 i \u22650\nfor i = 1, 2, \u22ef, m\nQuadratic Programming\nThe hard margin and soft margin problems are both convex quadratic optimization\nproblems with linear constraints. Such problems are known as Quadratic Program\u2010\nming (QP) problems. Many off-the-shelf solvers are available to solve QP problems\nby using a variety of techniques that are outside the scope of this book.5\nUnder the Hood | The general problem formulation is given by Equation 5-5. Equation 5-5."
  },
  {
    "id": 113,
    "content": "Quadratic Programming problem\nMinimize\np 2p\u22baHp\n+\nf\u22bap\nsubject to\nAp \u2264b\nwhere\np\nis an np\u2010dimensional vector (np = number of parameters),\nH\nis an np \u00d7 np matrix,\nf\nis an np\u2010dimensional vector,\nA\nis an nc \u00d7 np matrix (nc = number of constraints),\nb\nis an nc\u2010dimensional vector. Note that the expression A p \u2264 b defines nc constraints: p\u22ba a(i) \u2264 b(i) for i = 1, 2, \u22ef, nc,\nwhere a(i) is the vector containing the elements of the ith row of A and b(i) is the ith\nelement of b. You can easily verify that if you set the QP parameters in the following way, you get\nthe hard margin linear SVM classifier objective:\n\u2022 np = n + 1, where n is the number of features (the +1 is for the bias term). \u2022 nc = m, where m is the number of training instances. \u2022 H is the np \u00d7 np identity matrix, except with a zero in the top-left cell (to ignore\nthe bias term). \u2022 f = 0, an np-dimensional vector full of 0s. \u2022 b = \u20131, an nc-dimensional vector full of \u20131s. \u2022 a(i) = \u2013t(i) x\u02d9(i), where x\u02d9(i) is equal to x(i) with an extra bias feature x\u02d90 = 1. One way to train a hard margin linear SVM classifier is to use an off-the-shelf QP\nsolver and pass it the preceding parameters. The resulting vector p will contain the\nbias term b = p0 and the feature weights wi = pi for i = 1, 2, \u22ef, n. Similarly, you can\nuse a QP solver to solve the soft margin problem (see the exercises at the end of the\nchapter). To use the kernel trick, we are going to look at a different constrained optimization\nproblem. The Dual Problem\nGiven a constrained optimization problem, known as the primal problem, it is possi\u2010\nble to express a different but closely related problem, called its dual problem. The | Chapter 5: Support Vector Machines\n6 The objective function is convex, and the inequality constraints are continuously differentiable and convex\nfunctions. solution to the dual problem typically gives a lower bound to the solution of the pri\u2010\nmal problem, but under some conditions it can have the same solution as the primal\nproblem. Luckily, the SVM problem happens to meet these conditions,6 so you can\nchoose to solve the primal problem or the dual problem; both will have the same sol\u2010\nution. Equation 5-6 shows the dual form of the linear SVM objective (if you are inter\u2010\nested in knowing how to derive the dual problem from the primal problem, see\nAppendix C). Equation 5-6."
  },
  {
    "id": 114,
    "content": "Dual form of the linear SVM objective\nminimize\n\u03b1 2 \u2211\ni = 1\nm\n\u2211\nj = 1\nm\n\u03b1 i \u03b1 j t i t j x i \u22bax j\n\u2212\n\u2211\ni = 1\nm\n\u03b1 i\nsubject to\n\u03b1 i \u22650\nfor i = 1, 2, \u22ef, m\nOnce you find the vector \u03b1 that minimizes this equation (using a QP solver), use\nEquation 5-7 to compute w and b that minimize the primal problem. Equation 5-7. From the dual solution to the primal solution\nw = \u2211\ni = 1\nm\n\u03b1 i t i x i\nb = 1\nns \u2211\ni = 1\n\u03b1 i > 0\nm\nt i \u2212w\u22bax i\nThe dual problem is faster to solve than the primal one when the number of training\ninstances is smaller than the number of features. More importantly, the dual problem\nmakes the kernel trick possible, while the primal does not. So what is this kernel trick,\nanyway? Kernelized SVMs\nSuppose you want to apply a second-degree polynomial transformation to a two-\ndimensional training set (such as the moons training set), then train a linear SVM\nclassifier on the transformed training set. Equation 5-8 shows the second-degree pol\u2010\nynomial mapping function \u03d5 that you want to apply. Under the Hood | 7 As explained in Chapter 4, the dot product of two vectors a and b is normally noted a \u00b7 b. However, in\nMachine Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the\ndot product is achieved by computing a\u22bab. To remain consistent with the rest of the book, we will use this\nnotation here, ignoring the fact that this technically results in a single-cell matrix rather than a scalar value. Equation 5-8. Second-degree polynomial mapping\n\u03d5 x = \u03d5\nx1\nx2\n=\nx1 2 x1x2\nx2 Notice that the transformed vector is 3D instead of 2D. Now let\u2019s look at what hap\u2010\npens to a couple of 2D vectors, a and b, if we apply this second-degree polynomial\nmapping and then compute the dot product7 of the transformed vectors (See Equa\u2010\ntion 5-9). Equation 5-9. Kernel trick for a second-degree polynomial mapping\n\u03d5 a \u22ba\u03d5 b\n=\na1 2 a1a2\na2 \u22ba\nb1 2 b1b2\nb2 = a1\n2b1\n2 + 2a1b1a2b2 + a2\n2b2 = a1b1 + a2b2\n2 =\na1\na2\n\u22bab1\nb2 = a\u22bab How about that? The dot product of the transformed vectors is equal to the square of\nthe dot product of the original vectors: \u03d5(a)\u22ba \u03d5(b) = (a\u22ba b)2. Here is the key insight: if you apply the transformation \u03d5 to all training instances,\nthen the dual problem (see Equation 5-6) will contain the dot product \u03d5(x(i))\u22ba \u03d5(x(j)). But if \u03d5 is the second-degree polynomial transformation defined in Equation 5-8,\nthen you can replace this dot product of transformed vectors simply by x i \u22bax j 2\n."
  },
  {
    "id": 115,
    "content": "So,\nyou don\u2019t need to transform the training instances at all; just replace the dot product\nby its square in Equation 5-6. The result will be strictly the same as if you had gone\nthrough the trouble of transforming the training set then fitting a linear SVM algo\u2010\nrithm, but this trick makes the whole process much more computationally efficient. The function K(a, b) = (a\u22ba b)2 is a second-degree polynomial kernel. In Machine\nLearning, a kernel is a function capable of computing the dot product \u03d5(a)\u22ba \u03d5(b), | Chapter 5: Support Vector Machines\nbased only on the original vectors a and b, without having to compute (or even to\nknow about) the transformation \u03d5. Equation 5-10 lists some of the most commonly\nused kernels. Equation 5-10. Common kernels\nLinear:\nK a, b = a\u22bab\nPolynomial:\nK a, b = \u03b3a\u22bab + r\nd\nGaussian RBF:\nK a, b = exp \u2212\u03b3\u2225a \u2212b \u22252\nSigmoid:\nK a, b = tanh \u03b3a\u22bab + r\nMercer\u2019s Theorem\nAccording to Mercer\u2019s theorem, if a function K(a, b) respects a few mathematical con\u2010\nditions called Mercer\u2019s conditions (e.g., K must be continuous and symmetric in its\narguments so that K(a, b) = K(b, a), etc. ), then there exists a function \u03d5 that maps a\nand b into another space (possibly with much higher dimensions) such that K(a, b) =\n\u03d5(a)\u22ba \u03d5(b). You can use K as a kernel because you know \u03d5 exists, even if you don\u2019t\nknow what \u03d5 is. In the case of the Gaussian RBF kernel, it can be shown that \u03d5 maps\neach training instance to an infinite-dimensional space, so it\u2019s a good thing you don\u2019t\nneed to actually perform the mapping! Note that some frequently used kernels (such as the sigmoid kernel) don\u2019t respect all\nof Mercer\u2019s conditions, yet they generally work well in practice. There is still one loose end we must tie up. Equation 5-7 shows how to go from the\ndual solution to the primal solution in the case of a linear SVM classifier. But if you\napply the kernel trick, you end up with equations that include \u03d5(x(i)). In fact, w must\nhave the same number of dimensions as \u03d5(x(i)), which may be huge or even infinite,\nso you can\u2019t compute it. But how can you make predictions without knowing w? Well,\nthe good news is that you can plug the formula for w from Equation 5-7 into the deci\u2010\nsion function for a new instance x(n), and you get an equation with only dot products\nbetween input vectors. This makes it possible to use the kernel trick (Equation 5-11). Under the Hood | Equation 5-11."
  },
  {
    "id": 116,
    "content": "Making predictions with a kernelized SVM\nhw, b \u03d5 x n\n= w\u22ba\u03d5 x n\n+ b = \u2211\ni = 1\nm\n\u03b1 i t i \u03d5 x i\n\u22ba\n\u03d5 x n\n+ b\n= \u2211\ni = 1\nm\n\u03b1 i t i \u03d5 x i \u22ba\u03d5 x n\n+ b\n=\n\u2211\ni = 1\n\u03b1 i > 0\nm\n\u03b1 i t i K x i , x n\n+ b\nNote that since \u03b1(i) \u2260 0 only for support vectors, making predictions involves comput\u2010\ning the dot product of the new input vector x(n) with only the support vectors, not all\nthe training instances. Of course, you need to use the same trick to compute the bias\nterm b (Equation 5-12). Equation 5-12. Using the kernel trick to compute the bias term\nb = 1\nns \u2211\ni = 1\n\u03b1 i > 0\nm\nt i \u2212w\u22ba\u03d5 x i\n= 1\nns \u2211\ni = 1\n\u03b1 i > 0\nm\nt i \u2212\u2211\nj = 1\nm\n\u03b1 j t j \u03d5 x j\n\u22ba\n\u03d5 x i\n= 1\nns \u2211\ni = 1\n\u03b1 i > 0\nm\nt i \u2212\n\u2211\nj = 1\n\u03b1 j > 0\nm\n\u03b1 j t j K x i , x j\nIf you are starting to get a headache, it\u2019s perfectly normal: it\u2019s an unfortunate side\neffect of the kernel trick. Online SVMs\nBefore concluding this chapter, let\u2019s take a quick look at online SVM classifiers (recall\nthat online learning means learning incrementally, typically as new instances arrive). For linear SVM classifiers, one method for implementing an online SVM classifier is\nto use Gradient Descent (e.g., using SGDClassifier) to minimize the cost function in\nEquation 5-13, which is derived from the primal problem. Unfortunately, Gradient\nDescent converges much more slowly than the methods based on QP. | Chapter 5: Support Vector Machines\n8 Gert Cauwenberghs and Tomaso Poggio, \u201cIncremental and Decremental Support Vector Machine Learning,\u201d\nProceedings of the 13th International Conference on Neural Information Processing Systems (2000): 388\u2013394. 9 Antoine Bordes et al., \u201cFast Kernel Classifiers with Online and Active Learning,\u201d Journal of Machine Learning\nResearch 6 (2005): 1579\u20131619. Equation 5-13. Linear SVM classifier cost function\nJ w, b = 1\n2w\u22baw\n+\nC \u2211\ni = 1\nm\nmax 0, 1 \u2212t i w\u22bax i + b\nThe first sum in the cost function will push the model to have a small weight vector\nw, leading to a larger margin. The second sum computes the total of all margin viola\u2010\ntions. An instance\u2019s margin violation is equal to 0 if it is located off the street and on\nthe correct side, or else it is proportional to the distance to the correct side of the\nstreet. Minimizing this term ensures that the model makes the margin violations as\nsmall and as few as possible."
  },
  {
    "id": 117,
    "content": "Hinge Loss\nThe function max(0, 1 \u2013 t) is called the hinge loss function (see the following image). It is equal to 0 when t \u2265 1. Its derivative (slope) is equal to \u20131 if t < 1 and 0 if t > 1. It is\nnot differentiable at t = 1, but just like for Lasso Regression (see \u201cLasso Regression\u201d\non page 137), you can still use Gradient Descent using any subderivative at t = 1 (i.e.,\nany value between \u20131 and 0). It is also possible to implement online kernelized SVMs, as described in the papers\n\u201cIncremental and Decremental Support Vector Machine Learning\u201d8 and \u201cFast Kernel\nClassifiers with Online and Active Learning\u201d.9 These kernelized SVMs are imple\u2010\nUnder the Hood | mented in Matlab and C++. For large-scale nonlinear problems, you may want to\nconsider using neural networks instead (see Part II). Exercises\n1. What is the fundamental idea behind Support Vector Machines? 2. What is a support vector? 3. Why is it important to scale the inputs when using SVMs? 4. Can an SVM classifier output a confidence score when it classifies an instance? What about a probability? 5. Should you use the primal or the dual form of the SVM problem to train a model\non a training set with millions of instances and hundreds of features? 6. Say you\u2019ve trained an SVM classifier with an RBF kernel, but it seems to underfit\nthe training set. Should you increase or decrease \u03b3 (gamma)? What about C? 7. How should you set the QP parameters (H, f, A, and b) to solve the soft margin\nlinear SVM classifier problem using an off-the-shelf QP solver? 8. Train a LinearSVC on a linearly separable dataset. Then train an SVC and a\nSGDClassifier on the same dataset. See if you can get them to produce roughly\nthe same model. 9. Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary\nclassifiers, you will need to use one-versus-the-rest to classify all 10 digits. You\nmay want to tune the hyperparameters using small validation sets to speed up the\nprocess. What accuracy can you reach? 10. Train an SVM regressor on the California housing dataset. Solutions to these exercises are available in Appendix A. | Chapter 5: Support Vector Machines\nCHAPTER 6\nDecision Trees\nLike SVMs, Decision Trees are versatile Machine Learning algorithms that can per\u2010\nform both classification and regression tasks, and even multioutput tasks. They are\npowerful algorithms, capable of fitting complex datasets. For example, in Chapter 2\nyou trained a DecisionTreeRegressor model on the California housing dataset, fit\u2010\nting it perfectly (actually, overfitting it). Decision Trees are also the fundamental components of Random Forests (see Chap\u2010\nter 7), which are among the most powerful Machine Learning algorithms available\ntoday. In this chapter we will start by discussing how to train, visualize, and make predic\u2010\ntions with Decision Trees."
  },
  {
    "id": 118,
    "content": "Then we will go through the CART training algorithm\nused by Scikit-Learn, and we will discuss how to regularize trees and use them for\nregression tasks. Finally, we will discuss some of the limitations of Decision Trees. Training and Visualizing a Decision Tree\nTo understand Decision Trees, let\u2019s build one and take a look at how it makes predic\u2010\ntions. The following code trains a DecisionTreeClassifier on the iris dataset (see\nChapter 4):\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\niris = load_iris()\nX = iris.data[:, 2:] # petal length and width\ny = iris.target\ntree_clf = DecisionTreeClassifier(max_depth=2)\ntree_clf.fit(X, y) 1 Graphviz is an open source graph visualization software package, available at \nYou can visualize the trained Decision Tree by first using the export_graphviz()\nmethod to output a graph definition file called iris_tree.dot:\nfrom sklearn.tree import export_graphviz\nexport_graphviz( tree_clf, out_file=image_path(\"iris_tree.dot\"), feature_names=iris.feature_names[2:], class_names=iris.target_names, rounded=True, filled=True )\nThen you can use the dot command-line tool from the Graphviz package to convert\nthis .dot file to a variety of formats, such as PDF or PNG.1 This command line con\u2010\nverts the .dot file to a .png image file:\n$ dot -Tpng iris_tree.dot -o iris_tree.png\nYour first Decision Tree looks like Figure 6-1. Figure 6-1. Iris Decision Tree\nMaking Predictions\nLet\u2019s see how the tree represented in Figure 6-1 makes predictions. Suppose you find\nan iris flower and you want to classify it. You start at the root node (depth 0, at the\ntop): this node asks whether the flower\u2019s petal length is smaller than 2.45 cm. If it is,\nthen you move down to the root\u2019s left child node (depth 1, left). In this case, it is a leaf | Chapter 6: Decision Trees\nnode (i.e., it does not have any child nodes), so it does not ask any questions: simply\nlook at the predicted class for that node, and the Decision Tree predicts that your\nflower is an Iris setosa (class=setosa). Now suppose you find another flower, and this time the petal length is greater than\n2.45 cm. You must move down to the root\u2019s right child node (depth 1, right), which is\nnot a leaf node, so the node asks another question: is the petal width smaller than\n1.75 cm? If it is, then your flower is most likely an Iris versicolor (depth 2, left). If not,\nit is likely an Iris virginica (depth 2, right). It\u2019s really that simple. One of the many qualities of Decision Trees is that they require\nvery little data preparation. In fact, they don\u2019t require feature scal\u2010\ning or centering at all. A node\u2019s samples attribute counts how many training instances it applies to. For\nexample, 100 training instances have a petal length greater than 2.45 cm (depth 1,\nright), and of those 100, 54 have a petal width smaller than 1.75 cm (depth 2, left)."
  },
  {
    "id": 119,
    "content": "A\nnode\u2019s value attribute tells you how many training instances of each class this node\napplies to: for example, the bottom-right node applies to 0 Iris setosa, 1 Iris versicolor,\nand 45 Iris virginica. Finally, a node\u2019s gini attribute measures its impurity: a node is\n\u201cpure\u201d (gini=0) if all training instances it applies to belong to the same class. For\nexample, since the depth-1 left node applies only to Iris setosa training instances, it is\npure and its gini score is 0. Equation 6-1 shows how the training algorithm com\u2010\nputes the gini score Gi of the ith node. The depth-2 left node has a gini score equal to\n1 \u2013 (0/54)2 \u2013 (49/54)2 \u2013 (5/54)2 \u2248 0.168. Equation 6-1. Gini impurity\nGi = 1 \u2212\u2211\nk = 1\nn\npi, k In this equation:\n\u2022 pi,k is the ratio of class k instances among the training instances in the ith node. Scikit-Learn uses the CART algorithm, which produces only binary\ntrees: nonleaf nodes always have two children (i.e., questions only\nhave yes/no answers). However, other algorithms such as ID3 can\nproduce Decision Trees with nodes that have more than two\nchildren. Making Predictions | Figure 6-2 shows this Decision Tree\u2019s decision boundaries. The thick vertical line rep\u2010\nresents the decision boundary of the root node (depth 0): petal length = 2.45 cm. Since the lefthand area is pure (only Iris setosa), it cannot be split any further. How\u2010\never, the righthand area is impure, so the depth-1 right node splits it at petal width =\n1.75 cm (represented by the dashed line). Since max_depth was set to 2, the Decision\nTree stops right there. If you set max_depth to 3, then the two depth-2 nodes would\neach add another decision boundary (represented by the dotted lines). Figure 6-2. Decision Tree decision boundaries\nModel Interpretation: White Box Versus Black Box\nDecision Trees are intuitive, and their decisions are easy to interpret. Such models are\noften called white box models. In contrast, as we will see, Random Forests or neural\nnetworks are generally considered black box models. They make great predictions,\nand you can easily check the calculations that they performed to make these predic\u2010\ntions; nevertheless, it is usually hard to explain in simple terms why the predictions\nwere made. For example, if a neural network says that a particular person appears on\na picture, it is hard to know what contributed to this prediction: did the model recog\u2010\nnize that person\u2019s eyes? Their mouth? Their nose? Their shoes? Or even the couch\nthat they were sitting on? Conversely, Decision Trees provide nice, simple classifica\u2010\ntion rules that can even be applied manually if need be (e.g., for flower classification). Estimating Class Probabilities\nA Decision Tree can also estimate the probability that an instance belongs to a partic\u2010\nular class k. First it traverses the tree to find the leaf node for this instance, and then it\nreturns the ratio of training instances of class k in this node."
  },
  {
    "id": 120,
    "content": "For example, suppose\nyou have found a flower whose petals are 5 cm long and 1.5 cm wide. The | Chapter 6: Decision Trees\ncorresponding leaf node is the depth-2 left node, so the Decision Tree should output\nthe following probabilities: 0% for Iris setosa (0/54), 90.7% for Iris versicolor (49/54),\nand 9.3% for Iris virginica (5/54). And if you ask it to predict the class, it should out\u2010\nput Iris versicolor (class 1) because it has the highest probability. Let\u2019s check this:\n>>> tree_clf.predict_proba([[5, 1.5]])\narray([[0. , 0.90740741, 0.09259259]])\n>>> tree_clf.predict([[5, 1.5]])\narray([1])\nPerfect! Notice that the estimated probabilities would be identical anywhere else in\nthe bottom-right rectangle of Figure 6-2\u2014for example, if the petals were 6 cm long\nand 1.5 cm wide (even though it seems obvious that it would most likely be an Iris\nvirginica in this case). The CART Training Algorithm\nScikit-Learn uses the Classification and Regression Tree (CART) algorithm to train\nDecision Trees (also called \u201cgrowing\u201d trees). The algorithm works by first splitting the\ntraining set into two subsets using a single feature k and a threshold tk (e.g., \u201cpetal\nlength \u2264 2.45 cm\u201d). How does it choose k and tk? It searches for the pair (k, tk) that\nproduces the purest subsets (weighted by their size). Equation 6-2 gives the cost func\u2010\ntion that the algorithm tries to minimize. Equation 6-2. CART cost function for classification\nJ k, tk =\nmleft\nm Gleft +\nmright\nm\nGright\nwhere\nGleft/right measures the impurity of the left/right subset,\nmleft/right is the number of instances in the left/right subset. Once the CART algorithm has successfully split the training set in two, it splits the\nsubsets using the same logic, then the sub-subsets, and so on, recursively. It stops\nrecursing once it reaches the maximum depth (defined by the max_depth hyperpara\u2010\nmeter), or if it cannot find a split that will reduce impurity. A few other hyperparame\u2010\nters (described in a moment) control additional stopping conditions\n(min_samples_split, min_samples_leaf, min_weight_fraction_leaf, and\nmax_leaf_nodes). The CART Training Algorithm | 2 P is the set of problems that can be solved in polynomial time. NP is the set of problems whose solutions can\nbe verified in polynomial time. An NP-Hard problem is a problem to which any NP problem can be reduced\nin polynomial time. An NP-Complete problem is both NP and NP-Hard. A major open mathematical ques\u2010\ntion is whether or not P = NP. If P \u2260 NP (which seems likely), then no polynomial algorithm will ever be\nfound for any NP-Complete problem (except perhaps on a quantum computer). 3 log2 is the binary logarithm. It is equal to log2(m) = log(m) / log(2). 4 A reduction of entropy is often called an information gain. As you can see, the CART algorithm is a greedy algorithm: it greed\u2010\nily searches for an optimum split at the top level, then repeats the\nprocess at each subsequent level."
  },
  {
    "id": 121,
    "content": "It does not check whether or not\nthe split will lead to the lowest possible impurity several levels\ndown. A greedy algorithm often produces a solution that\u2019s reasona\u2010\nbly good but not guaranteed to be optimal. Unfortunately, finding the optimal tree is known to be an NP-\nComplete problem:2 it requires O(exp(m)) time, making the prob\u2010\nlem intractable even for small training sets. This is why we must\nsettle for a \u201creasonably good\u201d solution. Computational Complexity\nMaking predictions requires traversing the Decision Tree from the root to a leaf. Decision Trees generally are approximately balanced, so traversing the Decision Tree\nrequires going through roughly O(log2(m)) nodes.3 Since each node only requires\nchecking the value of one feature, the overall prediction complexity is O(log2(m)),\nindependent of the number of features. So predictions are very fast, even when deal\u2010\ning with large training sets. The training algorithm compares all features (or less if max_features is set) on all\nsamples at each node. Comparing all features on all samples at each node results in a\ntraining complexity of O(n \u00d7 m log2(m)). For small training sets (less than a few thou\u2010\nsand instances), Scikit-Learn can speed up training by presorting the data (set pre\nsort=True), but doing that slows down training considerably for larger training sets. Gini Impurity or Entropy? By default, the Gini impurity measure is used, but you can select the entropy impurity\nmeasure instead by setting the criterion hyperparameter to \"entropy\". The concept\nof entropy originated in thermodynamics as a measure of molecular disorder:\nentropy approaches zero when molecules are still and well ordered. Entropy later\nspread to a wide variety of domains, including Shannon\u2019s information theory, where it\nmeasures the average information content of a message:4 entropy is zero when all\nmessages are identical. In Machine Learning, entropy is frequently used as an | Chapter 6: Decision Trees\n5 See Sebastian Raschka\u2019s interesting analysis for more details. impurity measure: a set\u2019s entropy is zero when it contains instances of only one class. Equation 6-3 shows the definition of the entropy of the ith node. For example, the\ndepth-2 left node in Figure 6-1 has an entropy equal to \u2013(49/54) log2 (49/54) \u2013 (5/54)\nlog2 (5/54) \u2248 0.445. Equation 6-3. Entropy\nHi = \u2212\n\u2211\nk = 1\npi, k \u22600\nn\npi, k log2 pi, k\nSo, should you use Gini impurity or entropy? The truth is, most of the time it does\nnot make a big difference: they lead to similar trees. Gini impurity is slightly faster to\ncompute, so it is a good default. However, when they differ, Gini impurity tends to\nisolate the most frequent class in its own branch of the tree, while entropy tends to\nproduce slightly more balanced trees.5\nRegularization Hyperparameters\nDecision Trees make very few assumptions about the training data (as opposed to lin\u2010\near models, which assume that the data is linear, for example)."
  },
  {
    "id": 122,
    "content": "If left unconstrained,\nthe tree structure will adapt itself to the training data, fitting it very closely\u2014indeed,\nmost likely overfitting it. Such a model is often called a nonparametric model, not\nbecause it does not have any parameters (it often has a lot) but because the number of\nparameters is not determined prior to training, so the model structure is free to stick\nclosely to the data. In contrast, a parametric model, such as a linear model, has a pre\u2010\ndetermined number of parameters, so its degree of freedom is limited, reducing the\nrisk of overfitting (but increasing the risk of underfitting). To avoid overfitting the training data, you need to restrict the Decision Tree\u2019s freedom\nduring training. As you know by now, this is called regularization. The regularization\nhyperparameters depend on the algorithm used, but generally you can at least restrict\nthe maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the\nmax_depth hyperparameter (the default value is None, which means unlimited). Reducing max_depth will regularize the model and thus reduce the risk of overfitting. The DecisionTreeClassifier class has a few other parameters that similarly restrict\nthe shape of the Decision Tree: min_samples_split (the minimum number of sam\u2010\nples a node must have before it can be split), min_samples_leaf (the minimum num\u2010\nber of samples a leaf node must have), min_weight_fraction_leaf (same as\nRegularization Hyperparameters | min_samples_leaf but expressed as a fraction of the total number of weighted\ninstances), max_leaf_nodes (the maximum number of leaf nodes), and max_features\n(the maximum number of features that are evaluated for splitting at each node). Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize\nthe model. Other algorithms work by first training the Decision Tree without\nrestrictions, then pruning (deleting) unnecessary nodes. A node\nwhose children are all leaf nodes is considered unnecessary if the\npurity improvement it provides is not statistically significant. Stan\u2010\ndard statistical tests, such as the \u03c72 test (chi-squared test), are used\nto estimate the probability that the improvement is purely the\nresult of chance (which is called the null hypothesis). If this proba\u2010\nbility, called the p-value, is higher than a given threshold (typically\n5%, controlled by a hyperparameter), then the node is considered\nunnecessary and its children are deleted. The pruning continues\nuntil all unnecessary nodes have been pruned. Figure 6-3 shows two Decision Trees trained on the moons dataset (introduced in\nChapter 5). On the left the Decision Tree is trained with the default hyperparameters\n(i.e., no restrictions), and on the right it\u2019s trained with min_samples_leaf=4. It is\nquite obvious that the model on the left is overfitting, and the model on the right will\nprobably generalize better. Figure 6-3. Regularization using min_samples_leaf | Chapter 6: Decision Trees\nRegression\nDecision Trees are also capable of performing regression tasks."
  },
  {
    "id": 123,
    "content": "Let\u2019s build a regres\u2010\nsion tree using Scikit-Learn\u2019s DecisionTreeRegressor class, training it on a noisy\nquadratic dataset with max_depth=2:\nfrom sklearn.tree import DecisionTreeRegressor\ntree_reg = DecisionTreeRegressor(max_depth=2)\ntree_reg.fit(X, y)\nThe resulting tree is represented in Figure 6-4. Figure 6-4. A Decision Tree for regression\nThis tree looks very similar to the classification tree you built earlier. The main differ\u2010\nence is that instead of predicting a class in each node, it predicts a value. For example,\nsuppose you want to make a prediction for a new instance with x1 = 0.6. You traverse\nthe tree starting at the root, and you eventually reach the leaf node that predicts\nvalue=0.111. This prediction is the average target value of the 110 training instances\nassociated with this leaf node, and it results in a mean squared error equal to 0.015\nover these 110 instances. This model\u2019s predictions are represented on the left in Figure 6-5. If you set\nmax_depth=3, you get the predictions represented on the right. Notice how the pre\u2010\ndicted value for each region is always the average target value of the instances in that\nregion. The algorithm splits each region in a way that makes most training instances\nas close as possible to that predicted value. Regression | Figure 6-5. Predictions of two Decision Tree regression models\nThe CART algorithm works mostly the same way as earlier, except that instead of try\u2010\ning to split the training set in a way that minimizes impurity, it now tries to split the\ntraining set in a way that minimizes the MSE. Equation 6-4 shows the cost function\nthat the algorithm tries to minimize. Equation 6-4. CART cost function for regression\nJ k, tk =\nmleft\nm MSEleft +\nmright\nm\nMSEright\nwhere\nMSEnode =\n\u2211\ni \u2208node\nynode \u2212y i 2\nynode = mnode\n\u2211\ni \u2208node\ny i\nJust like for classification tasks, Decision Trees are prone to overfitting when dealing\nwith regression tasks. Without any regularization (i.e., using the default hyperpara\u2010\nmeters), you get the predictions on the left in Figure 6-6. These predictions are obvi\u2010\nously overfitting the training set very badly. Just setting min_samples_leaf=10 results\nin a much more reasonable model, represented on the right in Figure 6-6. Figure 6-6. Regularizing a Decision Tree regressor | Chapter 6: Decision Trees\n6 It randomly selects the set of features to evaluate at each node. Instability\nHopefully by now you are convinced that Decision Trees have a lot going for them:\nthey are simple to understand and interpret, easy to use, versatile, and powerful. However, they do have a few limitations. First, as you may have noticed, Decision\nTrees love orthogonal decision boundaries (all splits are perpendicular to an axis),\nwhich makes them sensitive to training set rotation. For example, Figure 6-7 shows a\nsimple linearly separable dataset: on the left, a Decision Tree can split it easily, while\non the right, after the dataset is rotated by 45\u00b0, the decision boundary looks unneces\u2010\nsarily convoluted."
  },
  {
    "id": 124,
    "content": "Although both Decision Trees fit the training set perfectly, it is very\nlikely that the model on the right will not generalize well. One way to limit this prob\u2010\nlem is to use Principal Component Analysis (see Chapter 8), which often results in a\nbetter orientation of the training data. Figure 6-7. Sensitivity to training set rotation\nMore generally, the main issue with Decision Trees is that they are very sensitive to\nsmall variations in the training data. For example, if you just remove the widest Iris\nversicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide)\nand train a new Decision Tree, you may get the model represented in Figure 6-8. As\nyou can see, it looks very different from the previous Decision Tree (Figure 6-2). Actually, since the training algorithm used by Scikit-Learn is stochastic,6 you may\nget very different models even on the same training data (unless you set the\nrandom_state hyperparameter). Instability | Figure 6-8. Sensitivity to training set details\nRandom Forests can limit this instability by averaging predictions over many trees, as\nwe will see in the next chapter. Exercises\n1. What is the approximate depth of a Decision Tree trained (without restrictions)\non a training set with one million instances? 2. Is a node\u2019s Gini impurity generally lower or greater than its parent\u2019s? Is it gener\u2010\nally lower/greater, or always lower/greater? 3. If a Decision Tree is overfitting the training set, is it a good idea to try decreasing\nmax_depth? 4. If a Decision Tree is underfitting the training set, is it a good idea to try scaling\nthe input features? 5. If it takes one hour to train a Decision Tree on a training set containing 1 million\ninstances, roughly how much time will it take to train another Decision Tree on a\ntraining set containing 10 million instances? 6. If your training set contains 100,000 instances, will setting presort=True speed\nup training? 7. Train and fine-tune a Decision Tree for the moons dataset by following these\nsteps:\na. Use make_moons(n_samples=10000, noise=0.4) to generate a moons dataset. b. Use train_test_split() to split the dataset into a training set and a test set. | Chapter 6: Decision Trees\nc. Use grid search with cross-validation (with the help of the GridSearchCV\nclass) to find good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes. d. Train it on the full training set using these hyperparameters, and measure\nyour model\u2019s performance on the test set. You should get roughly 85% to 87%\naccuracy. 8. Grow a forest by following these steps:\na. Continuing the previous exercise, generate 1,000 subsets of the training set,\neach containing 100 instances selected randomly. Hint: you can use Scikit-\nLearn\u2019s ShuffleSplit class for this. b. Train one Decision Tree on each subset, using the best hyperparameter values\nfound in the previous exercise. Evaluate these 1,000 Decision Trees on the test\nset."
  },
  {
    "id": 125,
    "content": "Since they were trained on smaller sets, these Decision Trees will likely\nperform worse than the first Decision Tree, achieving only about 80%\naccuracy. c. Now comes the magic. For each test set instance, generate the predictions of\nthe 1,000 Decision Trees, and keep only the most frequent prediction (you can\nuse SciPy\u2019s mode() function for this). This approach gives you majority-vote\npredictions over the test set. d. Evaluate these predictions on the test set: you should obtain a slightly higher\naccuracy than your first model (about 0.5 to 1.5% higher). Congratulations,\nyou have trained a Random Forest classifier! Solutions to these exercises are available in Appendix A. Exercises | CHAPTER 7\nEnsemble Learning and Random Forests\nSuppose you pose a complex question to thousands of random people, then aggregate\ntheir answers. In many cases you will find that this aggregated answer is better than\nan expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate\nthe predictions of a group of predictors (such as classifiers or regressors), you will\noften get better predictions than with the best individual predictor. A group of pre\u2010\ndictors is called an ensemble; thus, this technique is called Ensemble Learning, and an\nEnsemble Learning algorithm is called an Ensemble method. As an example of an Ensemble method, you can train a group of Decision Tree classi\u2010\nfiers, each on a different random subset of the training set. To make predictions, you\nobtain the predictions of all the individual trees, then predict the class that gets the\nmost votes (see the last exercise in Chapter 6). Such an ensemble of Decision Trees is\ncalled a Random Forest, and despite its simplicity, this is one of the most powerful\nMachine Learning algorithms available today. As discussed in Chapter 2, you will often use Ensemble methods near the end of a\nproject, once you have already built a few good predictors, to combine them into an\neven better predictor. In fact, the winning solutions in Machine Learning competi\u2010\ntions often involve several Ensemble methods (most famously in the Netflix Prize\ncompetition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010\nging, boosting, and stacking. We will also explore Random Forests. Voting Classifiers\nSuppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest\nclassifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). Figure 7-1. Training diverse classifiers\nA very simple way to create an even better classifier is to aggregate the predictions of\neach classifier and predict the class that gets the most votes. This majority-vote classi\u2010\nfier is called a hard voting classifier (see Figure 7-2). Figure 7-2. Hard voting classifier predictions\nSomewhat surprisingly, this voting classifier often achieves a higher accuracy than the\nbest classifier in the ensemble."
  },
  {
    "id": 126,
    "content": "In fact, even if each classifier is a weak learner (mean\u2010\ning it does only slightly better than random guessing), the ensemble can still be a\nstrong learner (achieving high accuracy), provided there are a sufficient number of\nweak learners and they are sufficiently diverse. | Chapter 7: Ensemble Learning and Random Forests\nHow is this possible? The following analogy can help shed some light on this mystery. Suppose you have a slightly biased coin that has a 51% chance of coming up heads\nand 49% chance of coming up tails. If you toss it 1,000 times, you will generally get\nmore or less 510 heads and 490 tails, and hence a majority of heads. If you do the\nmath, you will find that the probability of obtaining a majority of heads after 1,000\ntosses is close to 75%. The more you toss the coin, the higher the probability (e.g.,\nwith 10,000 tosses, the probability climbs over 97%). This is due to the law of large\nnumbers: as you keep tossing the coin, the ratio of heads gets closer and closer to the\nprobability of heads (51%). Figure 7-3 shows 10 series of biased coin tosses. You can\nsee that as the number of tosses increases, the ratio of heads approaches 51%. Eventu\u2010\nally all 10 series end up so close to 51% that they are consistently above 50%. Figure 7-3. The law of large numbers\nSimilarly, suppose you build an ensemble containing 1,000 classifiers that are individ\u2010\nually correct only 51% of the time (barely better than random guessing). If you pre\u2010\ndict the majority voted class, you can hope for up to 75% accuracy! However, this is\nonly true if all classifiers are perfectly independent, making uncorrelated errors,\nwhich is clearly not the case because they are trained on the same data. They are likely\nto make the same types of errors, so there will be many majority votes for the wrong\nclass, reducing the ensemble\u2019s accuracy. Ensemble methods work best when the predictors are as independ\u2010\nent from one another as possible. One way to get diverse classifiers\nis to train them using very different algorithms. This increases the\nchance that they will make very different types of errors, improving\nthe ensemble\u2019s accuracy. The following code creates and trains a voting classifier in Scikit-Learn, composed of\nthree diverse classifiers (the training set is the moons dataset, introduced in Chap\u2010\nter 5):\nVoting Classifiers | from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nlog_clf = LogisticRegression()\nrnd_clf = RandomForestClassifier()\nsvm_clf = SVC()\nvoting_clf = VotingClassifier( estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)], voting='hard')\nvoting_clf.fit(X_train, y_train)\nLet\u2019s look at each classifier\u2019s accuracy on the test set:\n>>> from sklearn.metrics import accuracy_score\n>>> for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n... clf.fit(X_train, y_train)\n... y_pred = clf.predict(X_test)\n... print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n...\nLogisticRegression 0.864\nRandomForestClassifier 0.896\nSVC 0.888\nVotingClassifier 0.904\nThere you have it! The voting classifier slightly outperforms all the individual\nclassifiers."
  },
  {
    "id": 127,
    "content": "If all classifiers are able to estimate class probabilities (i.e., they all have a pre\ndict_proba() method), then you can tell Scikit-Learn to predict the class with the\nhighest class probability, averaged over all the individual classifiers. This is called soft\nvoting. It often achieves higher performance than hard voting because it gives more\nweight to highly confident votes. All you need to do is replace voting=\"hard\" with\nvoting=\"soft\" and ensure that all classifiers can estimate class probabilities. This is\nnot the case for the SVC class by default, so you need to set its probability hyper\u2010\nparameter to True (this will make the SVC class use cross-validation to estimate class\nprobabilities, slowing down training, and it will add a predict_proba() method). If\nyou modify the preceding code to use soft voting, you will find that the voting classi\u2010\nfier achieves over 91.2% accuracy! Bagging and Pasting\nOne way to get a diverse set of classifiers is to use very different training algorithms,\nas just discussed. Another approach is to use the same training algorithm for every\npredictor and train them on different random subsets of the training set. When sam\u2010 | Chapter 7: Ensemble Learning and Random Forests\n1 Leo Breiman, \u201cBagging Predictors,\u201d Machine Learning 24, no. 2 (1996): 123\u2013140. 2 In statistics, resampling with replacement is called bootstrapping. 3 Leo Breiman, \u201cPasting Small Votes for Classification in Large Databases and On-Line,\u201d Machine Learning 36,\nno. 1\u20132 (1999): 85\u2013103. 4 Bias and variance were introduced in Chapter 4.\npling is performed with replacement, this method is called bagging1 (short for boot\u2010\nstrap aggregating2). When sampling is performed without replacement, it is called\npasting.3\nIn other words, both bagging and pasting allow training instances to be sampled sev\u2010\neral times across multiple predictors, but only bagging allows training instances to be\nsampled several times for the same predictor. This sampling and training process is\nrepresented in Figure 7-4. Figure 7-4. Bagging and pasting involves training several predictors on different random\nsamples of the training set\nOnce all predictors are trained, the ensemble can make a prediction for a new\ninstance by simply aggregating the predictions of all predictors. The aggregation\nfunction is typically the statistical mode (i.e., the most frequent prediction, just like a\nhard voting classifier) for classification, or the average for regression. Each individual\npredictor has a higher bias than if it were trained on the original training set, but\naggregation reduces both bias and variance.4 Generally, the net result is that the\nensemble has a similar bias but a lower variance than a single predictor trained on the\noriginal training set. Bagging and Pasting | 5 max_samples can alternatively be set to a float between 0.0 and 1.0, in which case the max number of instances\nto sample is equal to the size of the training set times max_samples. As you can see in Figure 7-4, predictors can all be trained in parallel, via different\nCPU cores or even different servers."
  },
  {
    "id": 128,
    "content": "Similarly, predictions can be made in parallel. This is one of the reasons bagging and pasting are such popular methods: they scale\nvery well. Bagging and Pasting in Scikit-Learn\nScikit-Learn offers a simple API for both bagging and pasting with the BaggingClas\nsifier class (or BaggingRegressor for regression). The following code trains an\nensemble of 500 Decision Tree classifiers:5 each is trained on 100 training instances\nrandomly sampled from the training set with replacement (this is an example of bag\u2010\nging, but if you want to use pasting instead, just set bootstrap=False). The n_jobs\nparameter tells Scikit-Learn the number of CPU cores to use for training and predic\u2010\ntions (\u20131 tells Scikit-Learn to use all available cores):\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nbag_clf = BaggingClassifier( DecisionTreeClassifier(), n_estimators=500, max_samples=100, bootstrap=True, n_jobs=-1)\nbag_clf.fit(X_train, y_train)\ny_pred = bag_clf.predict(X_test)\nThe BaggingClassifier automatically performs soft voting\ninstead of hard voting if the base classifier can estimate class proba\u2010\nbilities (i.e., if it has a predict_proba() method), which is the case\nwith Decision Tree classifiers. Figure 7-5 compares the decision boundary of a single Decision Tree with the deci\u2010\nsion boundary of a bagging ensemble of 500 trees (from the preceding code), both\ntrained on the moons dataset. As you can see, the ensemble\u2019s predictions will likely\ngeneralize much better than the single Decision Tree\u2019s predictions: the ensemble has a\ncomparable bias but a smaller variance (it makes roughly the same number of errors\non the training set, but the decision boundary is less irregular). | Chapter 7: Ensemble Learning and Random Forests\n6 As m grows, this ratio approaches 1 \u2013 exp(\u20131) \u2248 63.212%. Figure 7-5. A single Decision Tree (left) versus a bagging ensemble of 500 trees (right)\nBootstrapping introduces a bit more diversity in the subsets that each predictor is\ntrained on, so bagging ends up with a slightly higher bias than pasting; but the extra\ndiversity also means that the predictors end up being less correlated, so the ensemble\u2019s\nvariance is reduced. Overall, bagging often results in better models, which explains\nwhy it is generally preferred. However, if you have spare time and CPU power, you\ncan use cross-validation to evaluate both bagging and pasting and select the one that\nworks best. Out-of-Bag Evaluation\nWith bagging, some instances may be sampled several times for any given predictor,\nwhile others may not be sampled at all. By default a BaggingClassifier samples m\ntraining instances with replacement (bootstrap=True), where m is the size of the\ntraining set. This means that only about 63% of the training instances are sampled on\naverage for each predictor.6 The remaining 37% of the training instances that are not\nsampled are called out-of-bag (oob) instances. Note that they are not the same 37%\nfor all predictors. Since a predictor never sees the oob instances during training, it can be evaluated on\nthese instances, without the need for a separate validation set. You can evaluate the\nensemble itself by averaging out the oob evaluations of each predictor."
  },
  {
    "id": 129,
    "content": "In Scikit-Learn, you can set oob_score=True when creating a BaggingClassifier to\nrequest an automatic oob evaluation after training. The following code demonstrates\nthis. The resulting evaluation score is available through the oob_score_ variable:\nBagging and Pasting | 7 Gilles Louppe and Pierre Geurts, \u201cEnsembles on Random Patches,\u201d Lecture Notes in Computer Science 7523\n(2012): 346\u2013361. >>> bag_clf = BaggingClassifier(\n... DecisionTreeClassifier(), n_estimators=500,\n... bootstrap=True, n_jobs=-1, oob_score=True)\n...\n>>> bag_clf.fit(X_train, y_train)\n>>> bag_clf.oob_score_\n0.90133333333333332\nAccording to this oob evaluation, this BaggingClassifier is likely to achieve about\n90.1% accuracy on the test set. Let\u2019s verify this:\n>>> from sklearn.metrics import accuracy_score\n>>> y_pred = bag_clf.predict(X_test)\n>>> accuracy_score(y_test, y_pred)\n0.91200000000000003\nWe get 91.2% accuracy on the test set\u2014close enough! The oob decision function for each training instance is also available through the\noob_decision_function_ variable. In this case (since the base estimator has a pre\ndict_proba() method), the decision function returns the class probabilities for each\ntraining instance. For example, the oob evaluation estimates that the first training\ninstance has a 68.25% probability of belonging to the positive class (and 31.75% of\nbelonging to the negative class):\n>>> bag_clf.oob_decision_function_\narray([[0.31746032, 0.68253968], [0.34117647, 0.65882353], [1. , 0. ], ... [1. , 0. ], [0.03108808, 0.96891192], [0.57291667, 0.42708333]])\nRandom Patches and Random Subspaces\nThe BaggingClassifier class supports sampling the features as well. Sampling is\ncontrolled by two hyperparameters: max_features and bootstrap_features. They\nwork the same way as max_samples and bootstrap, but for feature sampling instead\nof instance sampling. Thus, each predictor will be trained on a random subset of the\ninput features. This technique is particularly useful when you are dealing with high-dimensional\ninputs (such as images). Sampling both training instances and features is called the\nRandom Patches method.7 Keeping all training instances (by setting bootstrap=False | Chapter 7: Ensemble Learning and Random Forests\n8 Tin Kam Ho, \u201cThe Random Subspace Method for Constructing Decision Forests,\u201d IEEE Transactions on Pat\u2010\ntern Analysis and Machine Intelligence 20, no. 8 (1998): 832\u2013844. 9 Tin Kam Ho, \u201cRandom Decision Forests,\u201d Proceedings of the Third International Conference on Document\nAnalysis and Recognition 1 (1995): 278. 10 The BaggingClassifier class remains useful if you want a bag of something other than Decision Trees. 11 There are a few notable exceptions: splitter is absent (forced to \"random\"), presort is absent (forced to\nFalse), max_samples is absent (forced to 1.0), and base_estimator is absent (forced to DecisionTreeClassi\nfier with the provided hyperparameters). and max_samples=1.0) but sampling features (by setting bootstrap_features to\nTrue and/or max_features to a value smaller than 1.0) is called the Random Subspa\u2010\nces method.8\nSampling features results in even more predictor diversity, trading a bit more bias for\na lower variance. Random Forests\nAs we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally\ntrained via the bagging method (or sometimes pasting), typically with max_samples\nset to the size of the training set."
  },
  {
    "id": 130,
    "content": "Instead of building a BaggingClassifier and pass\u2010\ning it a DecisionTreeClassifier, you can instead use the RandomForestClassifier\nclass, which is more convenient and optimized for Decision Trees10 (similarly, there is\na RandomForestRegressor class for regression tasks). The following code uses all\navailable CPU cores to train a Random Forest classifier with 500 trees (each limited\nto maximum 16 nodes):\nfrom sklearn.ensemble import RandomForestClassifier\nrnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\nrnd_clf.fit(X_train, y_train)\ny_pred_rf = rnd_clf.predict(X_test)\nWith a few exceptions, a RandomForestClassifier has all the hyperparameters of a\nDecisionTreeClassifier (to control how trees are grown), plus all the hyperpara\u2010\nmeters of a BaggingClassifier to control the ensemble itself.11\nThe Random Forest algorithm introduces extra randomness when growing trees;\ninstead of searching for the very best feature when splitting a node (see Chapter 6), it\nsearches for the best feature among a random subset of features. The algorithm\nresults in greater tree diversity, which (again) trades a higher bias for a lower var\u2010\niance, generally yielding an overall better model. The following BaggingClassifier\nis roughly equivalent to the previous RandomForestClassifier:\nRandom Forests | 12 Pierre Geurts et al., \u201cExtremely Randomized Trees,\u201d Machine Learning 63, no. 1 (2006): 3\u201342. bag_clf = BaggingClassifier( DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16), n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)\nExtra-Trees\nWhen you are growing a tree in a Random Forest, at each node only a random subset\nof the features is considered for splitting (as discussed earlier). It is possible to make\ntrees even more random by also using random thresholds for each feature rather than\nsearching for the best possible thresholds (like regular Decision Trees do). A forest of such extremely random trees is called an Extremely Randomized Trees\nensemble12 (or Extra-Trees for short). Once again, this technique trades more bias for\na lower variance. It also makes Extra-Trees much faster to train than regular Random\nForests, because finding the best possible threshold for each feature at every node is\none of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier\nclass. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\nTreesRegressor class has the same API as the RandomForestRegressor class. It is hard to tell in advance whether a RandomForestClassifier\nwill perform better or worse than an ExtraTreesClassifier. Gen\u2010\nerally, the only way to know is to try both and compare them using\ncross-validation (tuning the hyperparameters using grid search). Feature Importance\nYet another great quality of Random Forests is that they make it easy to measure the\nrelative importance of each feature. Scikit-Learn measures a feature\u2019s importance by\nlooking at how much the tree nodes that use that feature reduce impurity on average\n(across all trees in the forest). More precisely, it is a weighted average, where each\nnode\u2019s weight is equal to the number of training samples that are associated with it\n(see Chapter 6). Scikit-Learn computes this score automatically for each feature after training, then it\nscales the results so that the sum of all importances is equal to 1."
  },
  {
    "id": 131,
    "content": "You can access the\nresult using the feature_importances_ variable. For example, the following code\ntrains a RandomForestClassifier on the iris dataset (introduced in Chapter 4) and\noutputs each feature\u2019s importance. It seems that the most important features are the\npetal length (44%) and width (42%), while sepal length and width are rather unim\u2010\nportant in comparison (11% and 2%, respectively): | Chapter 7: Ensemble Learning and Random Forests\n>>> from sklearn.datasets import load_iris\n>>> iris = load_iris()\n>>> rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n>>> rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n>>> for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n... print(name, score)\n...\nsepal length (cm) 0.112492250999\nsepal width (cm) 0.0231192882825\npetal length (cm) 0.441030464364\npetal width (cm) 0.423357996355\nSimilarly, if you train a Random Forest classifier on the MNIST dataset (introduced\nin Chapter 3) and plot each pixel\u2019s importance, you get the image represented in\nFigure 7-6. Figure 7-6. MNIST pixel importance (according to a Random Forest classifier)\nRandom Forests are very handy to get a quick understanding of what features\nactually matter, in particular if you need to perform feature selection. Boosting\nBoosting (originally called hypothesis boosting) refers to any Ensemble method that\ncan combine several weak learners into a strong learner. The general idea of most\nboosting methods is to train predictors sequentially, each trying to correct its prede\u2010\ncessor. There are many boosting methods available, but by far the most popular are\nBoosting | 13 Yoav Freund and Robert E. Schapire, \u201cA Decision-Theoretic Generalization of On-Line Learning and an\nApplication to Boosting,\u201d Journal of Computer and System Sciences 55, no. 1 (1997): 119\u2013139. 14 This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost; they are slow\nand tend to be unstable with it. AdaBoost13 (short for Adaptive Boosting) and Gradient Boosting. Let\u2019s start with Ada\u2010\nBoost. AdaBoost\nOne way for a new predictor to correct its predecessor is to pay a bit more attention\nto the training instances that the predecessor underfitted. This results in new predic\u2010\ntors focusing more and more on the hard cases. This is the technique used by\nAdaBoost. For example, when training an AdaBoost classifier, the algorithm first trains a base\nclassifier (such as a Decision Tree) and uses it to make predictions on the training set. The algorithm then increases the relative weight of misclassified training instances. Then it trains a second classifier, using the updated weights, and again makes predic\u2010\ntions on the training set, updates the instance weights, and so on (see Figure 7-7). Figure 7-7. AdaBoost sequential training with instance weight updates\nFigure 7-8 shows the decision boundaries of five consecutive predictors on the\nmoons dataset (in this example, each predictor is a highly regularized SVM classifier\nwith an RBF kernel14). The first classifier gets many instances wrong, so their weights | Chapter 7: Ensemble Learning and Random Forests\nget boosted. The second classifier therefore does a better job on these instances, and\nso on."
  },
  {
    "id": 132,
    "content": "The plot on the right represents the same sequence of predictors, except that\nthe learning rate is halved (i.e., the misclassified instance weights are boosted half as\nmuch at every iteration). As you can see, this sequential learning technique has some\nsimilarities with Gradient Descent, except that instead of tweaking a single predictor\u2019s\nparameters to minimize a cost function, AdaBoost adds predictors to the ensemble,\ngradually making it better. Figure 7-8. Decision boundaries of consecutive predictors\nOnce all predictors are trained, the ensemble makes predictions very much like bag\u2010\nging or pasting, except that predictors have different weights depending on their\noverall accuracy on the weighted training set. There is one important drawback to this sequential learning techni\u2010\nque: it cannot be parallelized (or only partially), since each predic\u2010\ntor can only be trained after the previous predictor has been\ntrained and evaluated. As a result, it does not scale as well as bag\u2010\nging or pasting. Let\u2019s take a closer look at the AdaBoost algorithm. Each instance weight w(i) is initially\nset to 1/m. A first predictor is trained, and its weighted error rate r1 is computed on\nthe training set; see Equation 7-1. Equation 7-1. Weighted error rate of the jth predictor\nrj =\n\u2211\ni = 1\ny j\ni \u2260y i\nm\nw i\n\u2211\ni = 1\nm\nw i\nwhere y j\ni is the jth predictor\u2019s prediction for the ith instance. Boosting | 15 The original AdaBoost algorithm does not use a learning rate hyperparameter. The predictor\u2019s weight \u03b1j is then computed using Equation 7-2, where \u03b7 is the learn\u2010\ning rate hyperparameter (defaults to 1).15 The more accurate the predictor is, the\nhigher its weight will be. If it is just guessing randomly, then its weight will be close to\nzero. However, if it is most often wrong (i.e., less accurate than random guessing),\nthen its weight will be negative. Equation 7-2. Predictor weight\n\u03b1j = \u03b7 log\n1 \u2212rj\nrj\nNext, the AdaBoost algorithm updates the instance weights, using Equation 7-3,\nwhich boosts the weights of the misclassified instances. Equation 7-3. Weight update rule\nfor i = 1, 2, \u22ef, m\nw i\nw i\nif yj\ni = y i\nw i exp \u03b1j if yj\ni \u2260y i\nThen all the instance weights are normalized (i.e., divided by \u2211i = 1\nm\nw i ). Finally, a new predictor is trained using the updated weights, and the whole process is\nrepeated (the new predictor\u2019s weight is computed, the instance weights are updated,\nthen another predictor is trained, and so on). The algorithm stops when the desired\nnumber of predictors is reached, or when a perfect predictor is found. To make predictions, AdaBoost simply computes the predictions of all the predictors\nand weighs them using the predictor weights \u03b1j. The predicted class is the one that\nreceives the majority of weighted votes (see Equation 7-4). Equation 7-4."
  },
  {
    "id": 133,
    "content": "AdaBoost predictions\ny x = argmax\nk\n\u2211\nj = 1\ny j x = k\nN\n\u03b1j\nwhere N is the number of predictors. | Chapter 7: Ensemble Learning and Random Forests\n16 For more details, see Ji Zhu et al., \u201cMulti-Class AdaBoost,\u201d Statistics and Its Interface 2, no. 3 (2009): 349\u2013360. 17 Gradient Boosting was first introduced in Leo Breiman\u2019s 1997 paper \u201cArcing the Edge\u201d and was further devel\u2010\noped in the 1999 paper \u201cGreedy Function Approximation: A Gradient Boosting Machine\u201d by Jerome H. Fried\u2010\nman. Scikit-Learn uses a multiclass version of AdaBoost called SAMME16 (which stands for\nStagewise Additive Modeling using a Multiclass Exponential loss function). When there\nare just two classes, SAMME is equivalent to AdaBoost. If the predictors can estimate\nclass probabilities (i.e., if they have a predict_proba() method), Scikit-Learn can use\na variant of SAMME called SAMME.R (the R stands for \u201cReal\u201d), which relies on class\nprobabilities rather than predictions and generally performs better. The following code trains an AdaBoost classifier based on 200 Decision Stumps using\nScikit-Learn\u2019s AdaBoostClassifier class (as you might expect, there is also an Ada\nBoostRegressor class). A Decision Stump is a Decision Tree with max_depth=1\u2014in\nother words, a tree composed of a single decision node plus two leaf nodes. This is\nthe default base estimator for the AdaBoostClassifier class:\nfrom sklearn.ensemble import AdaBoostClassifier\nada_clf = AdaBoostClassifier( DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm=\"SAMME.R\", learning_rate=0.5)\nada_clf.fit(X_train, y_train)\nIf your AdaBoost ensemble is overfitting the training set, you can\ntry reducing the number of estimators or more strongly regulariz\u2010\ning the base estimator. Gradient Boosting\nAnother very popular boosting algorithm is Gradient Boosting.17 Just like AdaBoost,\nGradient Boosting works by sequentially adding predictors to an ensemble, each one\ncorrecting its predecessor. However, instead of tweaking the instance weights at every\niteration like AdaBoost does, this method tries to fit the new predictor to the residual\nerrors made by the previous predictor. Let\u2019s go through a simple regression example, using Decision Trees as the base predic\u2010\ntors (of course, Gradient Boosting also works great with regression tasks). This is\ncalled Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT). First, let\u2019s\nfit a DecisionTreeRegressor to the training set (for example, a noisy quadratic train\u2010\ning set):\nBoosting | from sklearn.tree import DecisionTreeRegressor\ntree_reg1 = DecisionTreeRegressor(max_depth=2)\ntree_reg1.fit(X, y)\nNext, we\u2019ll train a second DecisionTreeRegressor on the residual errors made by the\nfirst predictor:\ny2 = y - tree_reg1.predict(X)\ntree_reg2 = DecisionTreeRegressor(max_depth=2)\ntree_reg2.fit(X, y2)\nThen we train a third regressor on the residual errors made by the second predictor:\ny3 = y2 - tree_reg2.predict(X)\ntree_reg3 = DecisionTreeRegressor(max_depth=2)\ntree_reg3.fit(X, y3)\nNow we have an ensemble containing three trees. It can make predictions on a new\ninstance simply by adding up the predictions of all the trees:\ny_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\nFigure 7-9 represents the predictions of these three trees in the left column, and the\nensemble\u2019s predictions in the right column."
  },
  {
    "id": 134,
    "content": "In the first row, the ensemble has just one\ntree, so its predictions are exactly the same as the first tree\u2019s predictions. In the second\nrow, a new tree is trained on the residual errors of the first tree. On the right you can\nsee that the ensemble\u2019s predictions are equal to the sum of the predictions of the first\ntwo trees. Similarly, in the third row another tree is trained on the residual errors of\nthe second tree. You can see that the ensemble\u2019s predictions gradually get better as\ntrees are added to the ensemble. A simpler way to train GBRT ensembles is to use Scikit-Learn\u2019s GradientBoostingRe\ngressor class. Much like the RandomForestRegressor class, it has hyperparameters to\ncontrol the growth of Decision Trees (e.g., max_depth, min_samples_leaf), as well as\nhyperparameters to control the ensemble training, such as the number of trees\n(n_estimators). The following code creates the same ensemble as the previous one:\nfrom sklearn.ensemble import GradientBoostingRegressor\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\ngbrt.fit(X, y) | Chapter 7: Ensemble Learning and Random Forests\nFigure 7-9. In this depiction of Gradient Boosting, the first predictor (top left) is trained\nnormally, then each consecutive predictor (middle left and lower left) is trained on the\nprevious predictor\u2019s residuals; the right column shows the resulting ensemble\u2019s predictions\nThe learning_rate hyperparameter scales the contribution of each tree. If you set it\nto a low value, such as 0.1, you will need more trees in the ensemble to fit the train\u2010\ning set, but the predictions will usually generalize better. This is a regularization tech\u2010\nnique called shrinkage. Figure 7-10 shows two GBRT ensembles trained with a low\nlearning rate: the one on the left does not have enough trees to fit the training set,\nwhile the one on the right has too many trees and overfits the training set. Boosting | Figure 7-10. GBRT ensembles with not enough predictors (left) and too many (right)\nIn order to find the optimal number of trees, you can use early stopping (see Chap\u2010\nter 4). A simple way to implement this is to use the staged_predict() method: it\nreturns an iterator over the predictions made by the ensemble at each stage of train\u2010\ning (with one tree, two trees, etc.). The following code trains a GBRT ensemble with\n120 trees, then measures the validation error at each stage of training to find the opti\u2010\nmal number of trees, and finally trains another GBRT ensemble using the optimal\nnumber of trees:\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nX_train, X_val, y_train, y_val = train_test_split(X, y)\ngbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\ngbrt.fit(X_train, y_train)\nerrors = [mean_squared_error(y_val, y_pred) for y_pred in gbrt.staged_predict(X_val)]\nbst_n_estimators = np.argmin(errors) + 1\ngbrt_best = GradientBoostingRegressor(max_depth=2,n_estimators=bst_n_estimators)\ngbrt_best.fit(X_train, y_train)\nThe validation errors are represented on the left of Figure 7-11, and the best model\u2019s\npredictions are represented on the right. | Chapter 7: Ensemble Learning and Random Forests\nFigure 7-11."
  },
  {
    "id": 135,
    "content": "Tuning the number of trees using early stopping\nIt is also possible to implement early stopping by actually stopping training early\n(instead of training a large number of trees first and then looking back to find the\noptimal number). You can do so by setting warm_start=True, which makes Scikit-\nLearn keep existing trees when the fit() method is called, allowing incremental\ntraining. The following code stops training when the validation error does not\nimprove for five iterations in a row:\ngbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\nmin_val_error = float(\"inf\")\nerror_going_up = 0\nfor n_estimators in range(1, 120): gbrt.n_estimators = n_estimators gbrt.fit(X_train, y_train) y_pred = gbrt.predict(X_val) val_error = mean_squared_error(y_val, y_pred) if val_error < min_val_error: min_val_error = val_error error_going_up = 0 else: error_going_up += 1 if error_going_up == 5: break # early stopping\nThe GradientBoostingRegressor class also supports a subsample hyperparameter,\nwhich specifies the fraction of training instances to be used for training each tree. For\nexample, if subsample=0.25, then each tree is trained on 25% of the training instan\u2010\nces, selected randomly. As you can probably guess by now, this technique trades a\nhigher bias for a lower variance. It also speeds up training considerably. This is called\nStochastic Gradient Boosting. Boosting | 18 David H. Wolpert, \u201cStacked Generalization,\u201d Neural Networks 5, no. 2 (1992): 241\u2013259. It is possible to use Gradient Boosting with other cost functions. This is controlled by the loss hyperparameter (see Scikit-Learn\u2019s\ndocumentation for more details). It is worth noting that an optimized implementation of Gradient Boosting is available\nin the popular Python library XGBoost, which stands for Extreme Gradient Boosting. This package was initially developed by Tianqi Chen as part of the Distributed (Deep)\nMachine Learning Community (DMLC), and it aims to be extremely fast, scalable,\nand portable. In fact, XGBoost is often an important component of the winning\nentries in ML competitions. XGBoost\u2019s API is quite similar to Scikit-Learn\u2019s:\nimport xgboost\nxgb_reg = xgboost.XGBRegressor()\nxgb_reg.fit(X_train, y_train)\ny_pred = xgb_reg.predict(X_val)\nXGBoost also offers several nice features, such as automatically taking care of early\nstopping:\nxgb_reg.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=2)\ny_pred = xgb_reg.predict(X_val)\nYou should definitely check it out! Stacking\nThe last Ensemble method we will discuss in this chapter is called stacking (short for\nstacked generalization).18 It is based on a simple idea: instead of using trivial functions\n(such as hard voting) to aggregate the predictions of all predictors in an ensemble,\nwhy don\u2019t we train a model to perform this aggregation? Figure 7-12 shows such an\nensemble performing a regression task on a new instance. Each of the bottom three\npredictors predicts a different value (3.1, 2.7, and 2.9), and then the final predictor\n(called a blender, or a meta learner) takes these predictions as inputs and makes the\nfinal prediction (3.0). | Chapter 7: Ensemble Learning and Random Forests\n19 Alternatively, it is possible to use out-of-fold predictions. In some contexts this is called stacking, while using a\nhold-out set is called blending. For many people these terms are synonymous. Figure 7-12."
  },
  {
    "id": 136,
    "content": "Aggregating predictions using a blending predictor\nTo train the blender, a common approach is to use a hold-out set.19 Let\u2019s see how it\nworks. First, the training set is split into two subsets. The first subset is used to train\nthe predictors in the first layer (see Figure 7-13). Figure 7-13. Training the first layer\nNext, the first layer\u2019s predictors are used to make predictions on the second (held-\nout) set (see Figure 7-14). This ensures that the predictions are \u201cclean,\u201d since the pre\u2010\ndictors never saw these instances during training. For each instance in the hold-out\nStacking | set, there are three predicted values. We can create a new training set using these pre\u2010\ndicted values as input features (which makes this new training set 3D), and keeping\nthe target values. The blender is trained on this new training set, so it learns to pre\u2010\ndict the target value, given the first layer\u2019s predictions. Figure 7-14. Training the blender\nIt is actually possible to train several different blenders this way (e.g., one using Lin\u2010\near Regression, another using Random Forest Regression), to get a whole layer of\nblenders. The trick is to split the training set into three subsets: the first one is used to\ntrain the first layer, the second one is used to create the training set used to train the\nsecond layer (using predictions made by the predictors of the first layer), and the\nthird one is used to create the training set to train the third layer (using predictions\nmade by the predictors of the second layer). Once this is done, we can make a predic\u2010\ntion for a new instance by going through each layer sequentially, as shown in\nFigure 7-15. | Chapter 7: Ensemble Learning and Random Forests\nFigure 7-15. Predictions in a multilayer stacking ensemble\nUnfortunately, Scikit-Learn does not support stacking directly, but it is not too hard\nto roll out your own implementation (see the following exercises). Alternatively, you\ncan use an open source implementation such as DESlib. Exercises\n1. If you have trained five different models on the exact same training data, and\nthey all achieve 95% precision, is there any chance that you can combine these\nmodels to get better results? If so, how? If not, why? 2. What is the difference between hard and soft voting classifiers? 3. Is it possible to speed up training of a bagging ensemble by distributing it across\nmultiple servers? What about pasting ensembles, boosting ensembles, Random\nForests, or stacking ensembles? 4. What is the benefit of out-of-bag evaluation? 5. What makes Extra-Trees more random than regular Random Forests? How can\nthis extra randomness help? Are Extra-Trees slower or faster than regular Ran\u2010\ndom Forests? 6. If your AdaBoost ensemble underfits the training data, which hyperparameters\nshould you tweak and how? Exercises | 7. If your Gradient Boosting ensemble overfits the training set, should you increase\nor decrease the learning rate? 8."
  },
  {
    "id": 137,
    "content": "Load the MNIST data (introduced in Chapter 3), and split it into a training set, a\nvalidation set, and a test set (e.g., use 50,000 instances for training, 10,000 for val\u2010\nidation, and 10,000 for testing). Then train various classifiers, such as a Random\nForest classifier, an Extra-Trees classifier, and an SVM classifier. Next, try to com\u2010\nbine them into an ensemble that outperforms each individual classifier on the\nvalidation set, using soft or hard voting. Once you have found one, try it on the\ntest set. How much better does it perform compared to the individual classifiers? 9. Run the individual classifiers from the previous exercise to make predictions on\nthe validation set, and create a new training set with the resulting predictions:\neach training instance is a vector containing the set of predictions from all your\nclassifiers for an image, and the target is the image\u2019s class. Train a classifier on\nthis new training set. Congratulations, you have just trained a blender, and\ntogether with the classifiers it forms a stacking ensemble! Now evaluate the\nensemble on the test set. For each image in the test set, make predictions with all\nyour classifiers, then feed the predictions to the blender to get the ensemble\u2019s pre\u2010\ndictions. How does it compare to the voting classifier you trained earlier? Solutions to these exercises are available in Appendix A. | Chapter 7: Ensemble Learning and Random Forests\nCHAPTER 8\nDimensionality Reduction\nMany Machine Learning problems involve thousands or even millions of features for\neach training instance. Not only do all these features make training extremely slow,\nbut they can also make it much harder to find a good solution, as we will see. This\nproblem is often referred to as the curse of dimensionality. Fortunately, in real-world problems, it is often possible to reduce the number of fea\u2010\ntures considerably, turning an intractable problem into a tractable one. For example,\nconsider the MNIST images (introduced in Chapter 3): the pixels on the image bor\u2010\nders are almost always white, so you could completely drop these pixels from the\ntraining set without losing much information. Figure 7-6 confirms that these pixels\nare utterly unimportant for the classification task. Additionally, two neighboring pix\u2010\nels are often highly correlated: if you merge them into a single pixel (e.g., by taking\nthe mean of the two pixel intensities), you will not lose much information. Reducing dimensionality does cause some information loss (just\nlike compressing an image to JPEG can degrade its quality), so\neven though it will speed up training, it may make your system\nperform slightly worse. It also makes your pipelines a bit more\ncomplex and thus harder to maintain. So, if training is too slow,\nyou should first try to train your system with the original data\nbefore considering using dimensionality reduction."
  },
  {
    "id": 138,
    "content": "In some cases,\nreducing the dimensionality of the training data may filter out\nsome noise and unnecessary details and thus result in higher per\u2010\nformance, but in general it won\u2019t; it will just speed up training. Apart from speeding up training, dimensionality reduction is also extremely useful\nfor data visualization (or DataViz). Reducing the number of dimensions down to two\n(or three) makes it possible to plot a condensed view of a high-dimensional training 1 Well, four dimensions if you count time, and a few more if you are a string theorist. 2 Watch a rotating tesseract projected into 3D space at  Image by Wikipedia user Nerd\u2010\nBoy1392 (Creative Commons BY-SA 3.0). Reproduced from \n3 Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much sugar they put\nin their coffee), if you consider enough dimensions. set on a graph and often gain some important insights by visually detecting patterns,\nsuch as clusters. Moreover, DataViz is essential to communicate your conclusions to\npeople who are not data scientists\u2014in particular, decision makers who will use your\nresults. In this chapter we will discuss the curse of dimensionality and get a sense of what\ngoes on in high-dimensional space. Then, we will consider the two main approaches\nto dimensionality reduction (projection and Manifold Learning), and we will go\nthrough three of the most popular dimensionality reduction techniques: PCA, Kernel\nPCA, and LLE. The Curse of Dimensionality\nWe are so used to living in three dimensions1 that our intuition fails us when we try\nto imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to\npicture in our minds (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a\n1,000-dimensional space. Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2\nIt turns out that many things behave very differently in high-dimensional space. For\nexample, if you pick a random point in a unit square (a 1 \u00d7 1 square), it will have only\nabout a 0.4% chance of being located less than 0.001 from a border (in other words, it\nis very unlikely that a random point will be \u201cextreme\u201d along any dimension). But in a\n10,000-dimensional unit hypercube, this probability is greater than 99.999999%. Most\npoints in a high-dimensional hypercube are very close to the border.3 | Chapter 8: Dimensionality Reduction\nHere is a more troublesome difference: if you pick two points randomly in a unit\nsquare, the distance between these two points will be, on average, roughly 0.52. If you\npick two random points in a unit 3D cube, the average distance will be roughly 0.66. But what about two points picked randomly in a 1,000,000-dimensional hypercube? The average distance, believe it or not, will be about 408.25 (roughly 1, 000, 000/6)! This is counterintuitive: how can two points be so far apart when they both lie within\nthe same unit hypercube? Well, there\u2019s just plenty of space in high dimensions."
  },
  {
    "id": 139,
    "content": "As a\nresult, high-dimensional datasets are at risk of being very sparse: most training\ninstances are likely to be far away from each other. This also means that a new\ninstance will likely be far away from any training instance, making predictions much\nless reliable than in lower dimensions, since they will be based on much larger extrap\u2010\nolations. In short, the more dimensions the training set has, the greater the risk of\noverfitting it. In theory, one solution to the curse of dimensionality could be to increase the size of\nthe training set to reach a sufficient density of training instances. Unfortunately, in\npractice, the number of training instances required to reach a given density grows\nexponentially with the number of dimensions. With just 100 features (significantly\nfewer than in the MNIST problem), you would need more training instances than\natoms in the observable universe in order for training instances to be within 0.1 of\neach other on average, assuming they were spread out uniformly across all dimen\u2010\nsions. Main Approaches for Dimensionality Reduction\nBefore we dive into specific dimensionality reduction algorithms, let\u2019s take a look at\nthe two main approaches to reducing dimensionality: projection and Manifold\nLearning. Projection\nIn most real-world problems, training instances are not spread out uniformly across\nall dimensions. Many features are almost constant, while others are highly correlated\n(as discussed earlier for MNIST). As a result, all training instances lie within (or close\nto) a much lower-dimensional subspace of the high-dimensional space. This sounds\nvery abstract, so let\u2019s look at an example. In Figure 8-2 you can see a 3D dataset repre\u2010\nsented by circles. Main Approaches for Dimensionality Reduction | Figure 8-2. A 3D dataset lying close to a 2D subspace\nNotice that all training instances lie close to a plane: this is a lower-dimensional (2D)\nsubspace of the high-dimensional (3D) space. If we project every training instance\nperpendicularly onto this subspace (as represented by the short lines connecting the\ninstances to the plane), we get the new 2D dataset shown in Figure 8-3. Ta-da! We\nhave just reduced the dataset\u2019s dimensionality from 3D to 2D. Note that the axes cor\u2010\nrespond to new features z1 and z2 (the coordinates of the projections on the plane). Figure 8-3. The new 2D dataset after projection | Chapter 8: Dimensionality Reduction\nHowever, projection is not always the best approach to dimensionality reduction. In\nmany cases the subspace may twist and turn, such as in the famous Swiss roll toy data\u2010\nset represented in Figure 8-4. Figure 8-4. Swiss roll dataset\nSimply projecting onto a plane (e.g., by dropping x3) would squash different layers of\nthe Swiss roll together, as shown on the left side of Figure 8-5. What you really want is\nto unroll the Swiss roll to obtain the 2D dataset on the right side of Figure 8-5. Figure 8-5."
  },
  {
    "id": 140,
    "content": "Squashing by projecting onto a plane (left) versus unrolling the Swiss roll\n(right)\nMain Approaches for Dimensionality Reduction | Manifold Learning\nThe Swiss roll is an example of a 2D manifold. Put simply, a 2D manifold is a 2D\nshape that can be bent and twisted in a higher-dimensional space. More generally, a\nd-dimensional manifold is a part of an n-dimensional space (where d < n) that locally\nresembles a d-dimensional hyperplane. In the case of the Swiss roll, d = 2 and n = 3: it\nlocally resembles a 2D plane, but it is rolled in the third dimension. Many dimensionality reduction algorithms work by modeling the manifold on which\nthe training instances lie; this is called Manifold Learning. It relies on the manifold\nassumption, also called the manifold hypothesis, which holds that most real-world\nhigh-dimensional datasets lie close to a much lower-dimensional manifold. This\nassumption is very often empirically observed. Once again, think about the MNIST dataset: all handwritten digit images have some\nsimilarities. They are made of connected lines, the borders are white, and they are\nmore or less centered. If you randomly generated images, only a ridiculously tiny\nfraction of them would look like handwritten digits. In other words, the degrees of\nfreedom available to you if you try to create a digit image are dramatically lower than\nthe degrees of freedom you would have if you were allowed to generate any image\nyou wanted. These constraints tend to squeeze the dataset into a lower-dimensional\nmanifold. The manifold assumption is often accompanied by another implicit assumption: that\nthe task at hand (e.g., classification or regression) will be simpler if expressed in the\nlower-dimensional space of the manifold. For example, in the top row of Figure 8-6\nthe Swiss roll is split into two classes: in the 3D space (on the left), the decision\nboundary would be fairly complex, but in the 2D unrolled manifold space (on the\nright), the decision boundary is a straight line. However, this implicit assumption does not always hold. For example, in the bottom\nrow of Figure 8-6, the decision boundary is located at x1 = 5. This decision boundary\nlooks very simple in the original 3D space (a vertical plane), but it looks more com\u2010\nplex in the unrolled manifold (a collection of four independent line segments). In short, reducing the dimensionality of your training set before training a model will\nusually speed up training, but it may not always lead to a better or simpler solution; it\nall depends on the dataset. Hopefully you now have a good sense of what the curse of dimensionality is and how\ndimensionality reduction algorithms can fight it, especially when the manifold\nassumption holds. The rest of this chapter will go through some of the most popular\nalgorithms. | Chapter 8: Dimensionality Reduction\nFigure 8-6. The decision boundary may not always be simpler with lower dimensions\nPCA\nPrincipal Component Analysis (PCA) is by far the most popular dimensionality reduc\u2010\ntion algorithm."
  },
  {
    "id": 141,
    "content": "First it identifies the hyperplane that lies closest to the data, and then\nit projects the data onto it, just like in Figure 8-2. Preserving the Variance\nBefore you can project the training set onto a lower-dimensional hyperplane, you\nfirst need to choose the right hyperplane. For example, a simple 2D dataset is repre\u2010\nsented on the left in Figure 8-7, along with three different axes (i.e., 1D hyperplanes). On the right is the result of the projection of the dataset onto each of these axes. As\nyou can see, the projection onto the solid line preserves the maximum variance, while\nthe projection onto the dotted line preserves very little variance and the projection\nonto the dashed line preserves an intermediate amount of variance. PCA | 4 Karl Pearson, \u201cOn Lines and Planes of Closest Fit to Systems of Points in Space,\u201d The London, Edinburgh, and\nDublin Philosophical Magazine and Journal of Science 2, no. 11 (1901): 559-572, \nFigure 8-7. Selecting the subspace to project on\nIt seems reasonable to select the axis that preserves the maximum amount of var\u2010\niance, as it will most likely lose less information than the other projections. Another\nway to justify this choice is that it is the axis that minimizes the mean squared dis\u2010\ntance between the original dataset and its projection onto that axis. This is the rather\nsimple idea behind PCA.4\nPrincipal Components\nPCA identifies the axis that accounts for the largest amount of variance in the train\u2010\ning set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the\nfirst one, that accounts for the largest amount of remaining variance. In this 2D\nexample there is no choice: it is the dotted line. If it were a higher-dimensional data\u2010\nset, PCA would also find a third axis, orthogonal to both previous axes, and a fourth,\na fifth, and so on\u2014as many axes as the number of dimensions in the dataset. The ith axis is called the ith principal component (PC) of the data. In Figure 8-7, the\nfirst PC is the axis on which vector c1 lies, and the second PC is the axis on which\nvector c2 lies. In Figure 8-2 the first two PCs are the orthogonal axes on which the\ntwo arrows lie, on the plane, and the third PC is the axis orthogonal to that plane. | Chapter 8: Dimensionality Reduction\nFor each principal component, PCA finds a zero-centered unit vec\u2010\ntor pointing in the direction of the PC. Since two opposing unit\nvectors lie on the same axis, the direction of the unit vectors\nreturned by PCA is not stable: if you perturb the training set\nslightly and run PCA again, the unit vectors may point in the oppo\u2010\nsite direction as the original vectors. However, they will generally\nstill lie on the same axes."
  },
  {
    "id": 142,
    "content": "In some cases, a pair of unit vectors may\neven rotate or swap (if the variances along these two axes are close),\nbut the plane they define will generally remain the same. So how can you find the principal components of a training set? Luckily, there is a\nstandard matrix factorization technique called Singular Value Decomposition (SVD)\nthat can decompose the training set matrix X into the matrix multiplication of three\nmatrices U \u03a3 V\u22ba, where V contains the unit vectors that define all the principal com\u2010\nponents that we are looking for, as shown in Equation 8-1. Equation 8-1. Principal components matrix\nV =\n\u2223\n\u2223\n\u2223\nc1 c2 \u22efcn\n\u2223\n\u2223\n\u2223\nThe following Python code uses NumPy\u2019s svd() function to obtain all the principal\ncomponents of the training set, then extracts the two unit vectors that define the first\ntwo PCs:\nX_centered = X - X.mean(axis=0)\nU, s, Vt = np.linalg.svd(X_centered)\nc1 = Vt.T[:, 0]\nc2 = Vt.T[:, 1]\nPCA assumes that the dataset is centered around the origin. As we\nwill see, Scikit-Learn\u2019s PCA classes take care of centering the data\nfor you. If you implement PCA yourself (as in the preceding exam\u2010\nple), or if you use other libraries, don\u2019t forget to center the data\nfirst. Projecting Down to d Dimensions\nOnce you have identified all the principal components, you can reduce the dimen\u2010\nsionality of the dataset down to d dimensions by projecting it onto the hyperplane\ndefined by the first d principal components. Selecting this hyperplane ensures that the\nprojection will preserve as much variance as possible. For example, in Figure 8-2 the\n3D dataset is projected down to the 2D plane defined by the first two principal\nPCA | components, preserving a large part of the dataset\u2019s variance. As a result, the 2D pro\u2010\njection looks very much like the original 3D dataset. To project the training set onto the hyperplane and obtain a reduced dataset Xd-proj of\ndimensionality d, compute the matrix multiplication of the training set matrix X by\nthe matrix Wd, defined as the matrix containing the first d columns of V, as shown in\nEquation 8-2. Equation 8-2. Projecting the training set down to d dimensions\nXd\u2010proj = XWd\nThe following Python code projects the training set onto the plane defined by the first\ntwo principal components:\nW2 = Vt.T[:, :2]\nX2D = X_centered.dot(W2)\nThere you have it! You now know how to reduce the dimensionality of any dataset\ndown to any number of dimensions, while preserving as much variance as possible. Using Scikit-Learn\nScikit-Learn\u2019s PCA class uses SVD decomposition to implement PCA, just like we did\nearlier in this chapter."
  },
  {
    "id": 143,
    "content": "The following code applies PCA to reduce the dimensionality\nof the dataset down to two dimensions (note that it automatically takes care of center\u2010\ning the data):\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\nX2D = pca.fit_transform(X)\nAfter fitting the PCA transformer to the dataset, its components_ attribute holds the\ntranspose of Wd (e.g., the unit vector that defines the first principal component is\nequal to pca.components_.T[:, 0]). Explained Variance Ratio\nAnother useful piece of information is the explained variance ratio of each principal\ncomponent, available via the explained_variance_ratio_ variable. The ratio indi\u2010\ncates the proportion of the dataset\u2019s variance that lies along each principal compo\u2010\nnent. For example, let\u2019s look at the explained variance ratios of the first two\ncomponents of the 3D dataset represented in Figure 8-2:\n>>> pca.explained_variance_ratio_\narray([0.84248607, 0.14631839]) | Chapter 8: Dimensionality Reduction\nThis output tells you that 84.2% of the dataset\u2019s variance lies along the first PC, and\n14.6% lies along the second PC. This leaves less than 1.2% for the third PC, so it is\nreasonable to assume that the third PC probably carries little information. Choosing the Right Number of Dimensions\nInstead of arbitrarily choosing the number of dimensions to reduce down to, it is\nsimpler to choose the number of dimensions that add up to a sufficiently large por\u2010\ntion of the variance (e.g., 95%). Unless, of course, you are reducing dimensionality for\ndata visualization\u2014in that case you will want to reduce the dimensionality down to 2\nor 3. The following code performs PCA without reducing dimensionality, then computes\nthe minimum number of dimensions required to preserve 95% of the training set\u2019s\nvariance:\npca = PCA()\npca.fit(X_train)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd = np.argmax(cumsum >= 0.95) + 1\nYou could then set n_components=d and run PCA again. But there is a much better\noption: instead of specifying the number of principal components you want to pre\u2010\nserve, you can set n_components to be a float between 0.0 and 1.0, indicating the ratio\nof variance you wish to preserve:\npca = PCA(n_components=0.95)\nX_reduced = pca.fit_transform(X_train)\nYet another option is to plot the explained variance as a function of the number of\ndimensions (simply plot cumsum; see Figure 8-8). There will usually be an elbow in the\ncurve, where the explained variance stops growing fast. In this case, you can see that\nreducing the dimensionality down to about 100 dimensions wouldn\u2019t lose too much\nexplained variance. PCA | Figure 8-8. Explained variance as a function of the number of dimensions\nPCA for Compression\nAfter dimensionality reduction, the training set takes up much less space. As an\nexample, try applying PCA to the MNIST dataset while preserving 95% of its var\u2010\niance. You should find that each instance will have just over 150 features, instead of\nthe original 784 features. So, while most of the variance is preserved, the dataset is\nnow less than 20% of its original size!"
  },
  {
    "id": 144,
    "content": "This is a reasonable compression ratio, and you\ncan see how this size reduction can speed up a classification algorithm (such as an\nSVM classifier) tremendously. It is also possible to decompress the reduced dataset back to 784 dimensions by\napplying the inverse transformation of the PCA projection. This won\u2019t give you back\nthe original data, since the projection lost a bit of information (within the 5% var\u2010\niance that was dropped), but it will likely be close to the original data. The mean\nsquared distance between the original data and the reconstructed data (compressed\nand then decompressed) is called the reconstruction error. The following code compresses the MNIST dataset down to 154 dimensions, then\nuses the inverse_transform() method to decompress it back to 784 dimensions:\npca = PCA(n_components = 154)\nX_reduced = pca.fit_transform(X_train)\nX_recovered = pca.inverse_transform(X_reduced)\nFigure 8-9 shows a few digits from the original training set (on the left), and the cor\u2010\nresponding digits after compression and decompression. You can see that there is a\nslight image quality loss, but the digits are still mostly intact. | Chapter 8: Dimensionality Reduction\nFigure 8-9. MNIST compression that preserves 95% of the variance\nThe equation of the inverse transformation is shown in Equation 8-3. Equation 8-3. PCA inverse transformation, back to the original number of\ndimensions\nXrecovered = Xd\u2010projWd\n\u22ba\nRandomized PCA\nIf you set the svd_solver hyperparameter to \"randomized\", Scikit-Learn uses a sto\u2010\nchastic algorithm called Randomized PCA that quickly finds an approximation of the\nfirst d principal components. Its computational complexity is O(m \u00d7 d2) + O(d3),\ninstead of O(m \u00d7 n2) + O(n3) for the full SVD approach, so it is dramatically faster\nthan full SVD when d is much smaller than n:\nrnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\nX_reduced = rnd_pca.fit_transform(X_train)\nBy default, svd_solver is actually set to \"auto\": Scikit-Learn automatically uses the\nrandomized PCA algorithm if m or n is greater than 500 and d is less than 80% of m\nor n, or else it uses the full SVD approach. If you want to force Scikit-Learn to use full\nSVD, you can set the svd_solver hyperparameter to \"full\". Incremental PCA\nOne problem with the preceding implementations of PCA is that they require the\nwhole training set to fit in memory in order for the algorithm to run. Fortunately,\nIncremental PCA (IPCA) algorithms have been developed. They allow you to split the\ntraining set into mini-batches and feed an IPCA algorithm one mini-batch at a time. PCA | 5 Scikit-Learn uses the algorithm described in David A. Ross et al., \u201cIncremental Learning for Robust Visual\nTracking,\u201d International Journal of Computer Vision 77, no. 1\u20133 (2008): 125\u2013141. This is useful for large training sets and for applying PCA online (i.e., on the fly, as\nnew instances arrive). The following code splits the MNIST dataset into 100 mini-batches (using NumPy\u2019s\narray_split() function) and feeds them to Scikit-Learn\u2019s IncrementalPCA class5 to\nreduce the dimensionality of the MNIST dataset down to 154 dimensions (just like\nbefore)."
  },
  {
    "id": 145,
    "content": "Note that you must call the partial_fit() method with each mini-batch,\nrather than the fit() method with the whole training set:\nfrom sklearn.decomposition import IncrementalPCA\nn_batches = 100\ninc_pca = IncrementalPCA(n_components=154)\nfor X_batch in np.array_split(X_train, n_batches): inc_pca.partial_fit(X_batch)\nX_reduced = inc_pca.transform(X_train)\nAlternatively, you can use NumPy\u2019s memmap class, which allows you to manipulate a\nlarge array stored in a binary file on disk as if it were entirely in memory; the class\nloads only the data it needs in memory, when it needs it. Since the IncrementalPCA\nclass uses only a small part of the array at any given time, the memory usage remains\nunder control. This makes it possible to call the usual fit() method, as you can see\nin the following code:\nX_mm = np.memmap(filename, dtype=\"float32\", mode=\"readonly\", shape=(m, n))\nbatch_size = m // n_batches\ninc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\ninc_pca.fit(X_mm)\nKernel PCA\nIn Chapter 5 we discussed the kernel trick, a mathematical technique that implicitly\nmaps instances into a very high-dimensional space (called the feature space), enabling\nnonlinear classification and regression with Support Vector Machines. Recall that a\nlinear decision boundary in the high-dimensional feature space corresponds to a\ncomplex nonlinear decision boundary in the original space. It turns out that the same trick can be applied to PCA, making it possible to perform\ncomplex nonlinear projections for dimensionality reduction. This is called Kernel | Chapter 8: Dimensionality Reduction\n6 Bernhard Sch\u00f6lkopf et al., \u201cKernel Principal Component Analysis,\u201d in Lecture Notes in Computer Science 1327\n(Berlin: Springer, 1997): 583\u2013588. PCA (kPCA).6 It is often good at preserving clusters of instances after projection, or\nsometimes even unrolling datasets that lie close to a twisted manifold. The following code uses Scikit-Learn\u2019s KernelPCA class to perform kPCA with an RBF\nkernel (see Chapter 5 for more details about the RBF kernel and other kernels):\nfrom sklearn.decomposition import KernelPCA\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\nX_reduced = rbf_pca.fit_transform(X)\nFigure 8-10 shows the Swiss roll, reduced to two dimensions using a linear kernel\n(equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel. Figure 8-10. Swiss roll reduced to 2D using kPCA with various kernels\nSelecting a Kernel and Tuning Hyperparameters\nAs kPCA is an unsupervised learning algorithm, there is no obvious performance\nmeasure to help you select the best kernel and hyperparameter values. That said,\ndimensionality reduction is often a preparation step for a supervised learning task\n(e.g., classification), so you can use grid search to select the kernel and hyperparame\u2010\nters that lead to the best performance on that task. The following code creates a two-\nstep pipeline, first reducing dimensionality to two dimensions using kPCA, then\napplying Logistic Regression for classification."
  },
  {
    "id": 146,
    "content": "Then it uses GridSearchCV to find the\nbest kernel and gamma value for kPCA in order to get the best classification accuracy\nat the end of the pipeline:\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nKernel PCA | clf = Pipeline([ (\"kpca\", KernelPCA(n_components=2)), (\"log_reg\", LogisticRegression()) ])\nparam_grid = [{ \"kpca__gamma\": np.linspace(0.03, 0.05, 10), \"kpca__kernel\": [\"rbf\", \"sigmoid\"] }]\ngrid_search = GridSearchCV(clf, param_grid, cv=3)\ngrid_search.fit(X, y)\nThe best kernel and hyperparameters are then available through the best_params_\nvariable:\n>>> print(grid_search.best_params_)\n{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}\nAnother approach, this time entirely unsupervised, is to select the kernel and hyper\u2010\nparameters that yield the lowest reconstruction error. Note that reconstruction is not\nas easy as with linear PCA. Here\u2019s why. Figure 8-11 shows the original Swiss roll 3D\ndataset (top left) and the resulting 2D dataset after kPCA is applied using an RBF ker\u2010\nnel (top right). Thanks to the kernel trick, this transformation is mathematically\nequivalent to using the feature map \u03c6 to map the training set to an infinite-\ndimensional feature space (bottom right), then projecting the transformed training\nset down to 2D using linear PCA. Notice that if we could invert the linear PCA step for a given instance in the reduced\nspace, the reconstructed point would lie in feature space, not in the original space\n(e.g., like the one represented by an X in the diagram). Since the feature space is\ninfinite-dimensional, we cannot compute the reconstructed point, and therefore we\ncannot compute the true reconstruction error. Fortunately, it is possible to find a\npoint in the original space that would map close to the reconstructed point. This\npoint is called the reconstruction pre-image. Once you have this pre-image, you can\nmeasure its squared distance to the original instance. You can then select the kernel\nand hyperparameters that minimize this reconstruction pre-image error. | Chapter 8: Dimensionality Reduction\n7 If you set fit_inverse_transform=True, Scikit-Learn will use the algorithm (based on Kernel Ridge Regres\u2010\nsion) described in Gokhan H. Bak\u0131r et al., \u201cLearning to Find Pre-Images\u201d, Proceedings of the 16th International\nConference on Neural Information Processing Systems (2004): 449\u2013456. Figure 8-11. Kernel PCA and the reconstruction pre-image error\nYou may be wondering how to perform this reconstruction. One solution is to train a\nsupervised regression model, with the projected instances as the training set and the\noriginal instances as the targets. Scikit-Learn will do this automatically if you set\nfit_inverse_transform=True, as shown in the following code:7\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\nX_reduced = rbf_pca.fit_transform(X)\nX_preimage = rbf_pca.inverse_transform(X_reduced)\nBy default, fit_inverse_transform=False and KernelPCA has no\ninverse_transform() method. This method only gets created\nwhen you set fit_inverse_transform=True. Kernel PCA | 8 Sam T. Roweis and Lawrence K. Saul, \u201cNonlinear Dimensionality Reduction by Locally Linear Embedding,\u201d\nScience 290, no. 5500 (2000): 2323\u20132326."
  },
  {
    "id": 147,
    "content": "You can then compute the reconstruction pre-image error:\n>>> from sklearn.metrics import mean_squared_error\n>>> mean_squared_error(X, X_preimage)\n32.786308795766132\nNow you can use grid search with cross-validation to find the kernel and hyperpara\u2010\nmeters that minimize this error. LLE\nLocally Linear Embedding (LLE)8 is another powerful nonlinear dimensionality reduc\u2010\ntion (NLDR) technique. It is a Manifold Learning technique that does not rely on\nprojections, like the previous algorithms do. In a nutshell, LLE works by first measur\u2010\ning how each training instance linearly relates to its closest neighbors (c.n. ), and then\nlooking for a low-dimensional representation of the training set where these local\nrelationships are best preserved (more details shortly). This approach makes it partic\u2010\nularly good at unrolling twisted manifolds, especially when there is not too much\nnoise. The following code uses Scikit-Learn\u2019s LocallyLinearEmbedding class to unroll the\nSwiss roll:\nfrom sklearn.manifold import LocallyLinearEmbedding\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\nX_reduced = lle.fit_transform(X)\nThe resulting 2D dataset is shown in Figure 8-12. As you can see, the Swiss roll is\ncompletely unrolled, and the distances between instances are locally well preserved. However, distances are not preserved on a larger scale: the left part of the unrolled\nSwiss roll is stretched, while the right part is squeezed. Nevertheless, LLE did a pretty\ngood job at modeling the manifold. | Chapter 8: Dimensionality Reduction\nFigure 8-12. Unrolled Swiss roll using LLE\nHere\u2019s how LLE works: for each training instance x(i), the algorithm identifies its k\nclosest neighbors (in the preceding code k = 10), then tries to reconstruct x(i) as a lin\u2010\near function of these neighbors. More specifically, it finds the weights wi,j such that\nthe squared distance between x(i) and \u2211j = 1\nm\nwi, jx j is as small as possible, assuming wi,j\n= 0 if x(j) is not one of the k closest neighbors of x(i). Thus the first step of LLE is the\nconstrained optimization problem described in Equation 8-4, where W is the weight\nmatrix containing all the weights wi,j. The second constraint simply normalizes the\nweights for each training instance x(i). Equation 8-4. LLE step one: linearly modeling local relationships\nW = argmin\nW\n\u2211\ni = 1\nm\nx i \u2212\u2211\nj = 1\nm\nwi, jx j subject to\nwi, j = 0\nif x j is not one of the k c.n. of x i\n\u2211\nj = 1\nm\nwi, j = 1 for i = 1, 2, \u22ef, m\nAfter this step, the weight matrix W (containing the weights wi, j) encodes the local\nlinear relationships between the training instances. The second step is to map the\ntraining instances into a d-dimensional space (where d < n) while preserving these\nlocal relationships as much as possible. If z(i) is the image of x(i) in this d-dimensional\nLLE | space, then we want the squared distance between z(i) and \u2211j = 1\nm\nwi, jz j to be as small\nas possible."
  },
  {
    "id": 148,
    "content": "This idea leads to the unconstrained optimization problem described in\nEquation 8-5. It looks very similar to the first step, but instead of keeping the instan\u2010\nces fixed and finding the optimal weights, we are doing the reverse: keeping the\nweights fixed and finding the optimal position of the instances\u2019 images in the low-\ndimensional space. Note that Z is the matrix containing all z(i). Equation 8-5. LLE step two: reducing dimensionality while preserving relationships\nZ = argmin\nZ\n\u2211\ni = 1\nm\nz i \u2212\u2211\nj = 1\nm\nwi, jz j Scikit-Learn\u2019s LLE implementation has the following computational complexity:\nO(m log(m)n log(k)) for finding the k nearest neighbors, O(mnk3) for optimizing the\nweights, and O(dm2) for constructing the low-dimensional representations. Unfortu\u2010\nnately, the m2 in the last term makes this algorithm scale poorly to very large datasets. Other Dimensionality Reduction Techniques\nThere are many other dimensionality reduction techniques, several of which are\navailable in Scikit-Learn. Here are some of the most popular ones:\nRandom Projections\nAs its name suggests, projects the data to a lower-dimensional space using a ran\u2010\ndom linear projection. This may sound crazy, but it turns out that such a random\nprojection is actually very likely to preserve distances well, as was demonstrated\nmathematically by William B. Johnson and Joram Lindenstrauss in a famous\nlemma. The quality of the dimensionality reduction depends on the number of\ninstances and the target dimensionality, but surprisingly not on the initial dimen\u2010\nsionality. Check out the documentation for the sklearn.random_projection\npackage for more details. Multidimensional Scaling (MDS)\nReduces dimensionality while trying to preserve the distances between the\ninstances. | Chapter 8: Dimensionality Reduction\n9 The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between\nthese nodes. Isomap\nCreates a graph by connecting each instance to its nearest neighbors, then\nreduces dimensionality while trying to preserve the geodesic distances9 between\nthe instances. t-Distributed Stochastic Neighbor Embedding (t-SNE)\nReduces dimensionality while trying to keep similar instances close and dissimi\u2010\nlar instances apart. It is mostly used for visualization, in particular to visualize\nclusters of instances in high-dimensional space (e.g., to visualize the MNIST\nimages in 2D). Linear Discriminant Analysis (LDA)\nIs a classification algorithm, but during training it learns the most discriminative\naxes between the classes, and these axes can then be used to define a hyperplane\nonto which to project the data. The benefit of this approach is that the projection\nwill keep classes as far apart as possible, so LDA is a good technique to reduce\ndimensionality before running another classification algorithm such as an SVM\nclassifier. Figure 8-13 shows the results of a few of these techniques. Figure 8-13. Using various techniques to reduce the Swill roll to 2D\nExercises\n1. What are the main motivations for reducing a dataset\u2019s dimensionality? What are\nthe main drawbacks? 2. What is the curse of dimensionality? Exercises | 3."
  },
  {
    "id": 149,
    "content": "Once a dataset\u2019s dimensionality has been reduced, is it possible to reverse the\noperation? If so, how? If not, why? 4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset? 5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\nvariance ratio to 95%. How many dimensions will the resulting dataset have? 6. In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA,\nor Kernel PCA? 7. How can you evaluate the performance of a dimensionality reduction algorithm\non your dataset? 8. Does it make any sense to chain two different dimensionality reduction algo\u2010\nrithms? 9. Load the MNIST dataset (introduced in Chapter 3) and split it into a training set\nand a test set (take the first 60,000 instances for training, and the remaining\n10,000 for testing). Train a Random Forest classifier on the dataset and time how\nlong it takes, then evaluate the resulting model on the test set. Next, use PCA to\nreduce the dataset\u2019s dimensionality, with an explained variance ratio of 95%. Train a new Random Forest classifier on the reduced dataset and see how long it\ntakes. Was training much faster? Next, evaluate the classifier on the test set. How\ndoes it compare to the previous classifier? 10. Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the\nresult using Matplotlib. You can use a scatterplot using 10 different colors to rep\u2010\nresent each image\u2019s target class. Alternatively, you can replace each dot in the\nscatterplot with the corresponding instance\u2019s class (a digit from 0 to 9), or even\nplot scaled-down versions of the digit images themselves (if you plot all digits,\nthe visualization will be too cluttered, so you should either draw a random sam\u2010\nple or plot an instance only if no other instance has already been plotted at a\nclose distance). You should get a nice visualization with well-separated clusters of\ndigits. Try using other dimensionality reduction algorithms such as PCA, LLE, or\nMDS and compare the resulting visualizations. Solutions to these exercises are available in Appendix A. | Chapter 8: Dimensionality Reduction\nCHAPTER 9\nUnsupervised Learning Techniques\nAlthough most of the applications of Machine Learning today are based on super\u2010\nvised learning (and as a result, this is where most of the investments go to), the vast\nmajority of the available data is unlabeled: we have the input features X, but we do\nnot have the labels y. The computer scientist Yann LeCun famously said that \u201cif intel\u2010\nligence was a cake, unsupervised learning would be the cake, supervised learning\nwould be the icing on the cake, and reinforcement learning would be the cherry on\nthe cake.\u201d In other words, there is a huge potential in unsupervised learning that we\nhave only barely started to sink our teeth into. Say you want to create a system that will take a few pictures of each item on a manu\u2010\nfacturing production line and detect which items are defective."
  },
  {
    "id": 150,
    "content": "You can fairly easily\ncreate a system that will take pictures automatically, and this might give you thou\u2010\nsands of pictures every day. You can then build a reasonably large dataset in just a few\nweeks. But wait, there are no labels! If you want to train a regular binary classifier that\nwill predict whether an item is defective or not, you will need to label every single\npicture as \u201cdefective\u201d or \u201cnormal.\u201d This will generally require human experts to sit\ndown and manually go through all the pictures. This is a long, costly, and tedious\ntask, so it will usually only be done on a small subset of the available pictures. As a\nresult, the labeled dataset will be quite small, and the classifier\u2019s performance will be\ndisappointing. Moreover, every time the company makes any change to its products,\nthe whole process will need to be started over from scratch. Wouldn\u2019t it be great if the\nalgorithm could just exploit the unlabeled data without needing humans to label\nevery picture? Enter unsupervised learning. In Chapter 8 we looked at the most common unsupervised learning task: dimension\u2010\nality reduction. In this chapter we will look at a few more unsupervised learning tasks\nand algorithms: Clustering\nThe goal is to group similar instances together into clusters. Clustering is a great\ntool for data analysis, customer segmentation, recommender systems, search\nengines, image segmentation, semi-supervised learning, dimensionality reduc\u2010\ntion, and more. Anomaly detection\nThe objective is to learn what \u201cnormal\u201d data looks like, and then use that to\ndetect abnormal instances, such as defective items on a production line or a new\ntrend in a time series. Density estimation\nThis is the task of estimating the probability density function (PDF) of the random\nprocess that generated the dataset. Density estimation is commonly used for\nanomaly detection: instances located in very low-density regions are likely to be\nanomalies. It is also useful for data analysis and visualization. Ready for some cake? We will start with clustering, using K-Means and DBSCAN,\nand then we will discuss Gaussian mixture models and see how they can be used for\ndensity estimation, clustering, and anomaly detection. Clustering\nAs you enjoy a hike in the mountains, you stumble upon a plant you have never seen\nbefore. You look around and you notice a few more. They are not identical, yet they\nare sufficiently similar for you to know that they most likely belong to the same spe\u2010\ncies (or at least the same genus). You may need a botanist to tell you what species that\nis, but you certainly don\u2019t need an expert to identify groups of similar-looking objects. This is called clustering: it is the task of identifying similar instances and assigning\nthem to clusters, or groups of similar instances. Just like in classification, each instance gets assigned to a group. However, unlike clas\u2010\nsification, clustering is an unsupervised task."
  },
  {
    "id": 151,
    "content": "Consider Figure 9-1: on the left is the\niris dataset (introduced in Chapter 4), where each instance\u2019s species (i.e., its class) is\nrepresented with a different marker. It is a labeled dataset, for which classification\nalgorithms such as Logistic Regression, SVMs, or Random Forest classifiers are well\nsuited. On the right is the same dataset, but without the labels, so you cannot use a\nclassification algorithm anymore. This is where clustering algorithms step in: many of\nthem can easily detect the lower-left cluster. It is also quite easy to see with our own\neyes, but it is not so obvious that the upper-right cluster is composed of two distinct\nsub-clusters. That said, the dataset has two additional features (sepal length and\nwidth), not represented here, and clustering algorithms can make good use of all fea\u2010\ntures, so in fact they identify the three clusters fairly well (e.g., using a Gaussian mix\u2010\nture model, only 5 instances out of 150 are assigned to the wrong cluster). | Chapter 9: Unsupervised Learning Techniques\nFigure 9-1. Classification (left) versus clustering (right)\nClustering is used in a wide variety of applications, including these:\nFor customer segmentation\nYou can cluster your customers based on their purchases and their activity on\nyour website. This is useful to understand who your customers are and what they\nneed, so you can adapt your products and marketing campaigns to each segment. For example, customer segmentation can be useful in recommender systems to\nsuggest content that other users in the same cluster enjoyed. For data analysis\nWhen you analyze a new dataset, it can be helpful to run a clustering algorithm,\nand then analyze each cluster separately. As a dimensionality reduction technique\nOnce a dataset has been clustered, it is usually possible to measure each instance\u2019s\naffinity with each cluster (affinity is any measure of how well an instance fits into\na cluster). Each instance\u2019s feature vector x can then be replaced with the vector of\nits cluster affinities. If there are k clusters, then this vector is k-dimensional. This\nvector is typically much lower-dimensional than the original feature vector, but it\ncan preserve enough information for further processing. For anomaly detection (also called outlier detection)\nAny instance that has a low affinity to all the clusters is likely to be an anomaly. For example, if you have clustered the users of your website based on their\nbehavior, you can detect users with unusual behavior, such as an unusual number\nof requests per second. Anomaly detection is particularly useful in detecting\ndefects in manufacturing, or for fraud detection. For semi-supervised learning\nIf you only have a few labels, you could perform clustering and propagate the\nlabels to all the instances in the same cluster. This technique can greatly increase\nClustering | 1 Stuart P. Lloyd, \u201cLeast Squares Quantization in PCM,\u201d IEEE Transactions on Information Theory 28, no. 2\n(1982): 129\u2013137. the number of labels available for a subsequent supervised learning algorithm,\nand thus improve its performance."
  },
  {
    "id": 152,
    "content": "For search engines\nSome search engines let you search for images that are similar to a reference\nimage. To build such a system, you would first apply a clustering algorithm to all\nthe images in your database; similar images would end up in the same cluster. Then when a user provides a reference image, all you need to do is use the\ntrained clustering model to find this image\u2019s cluster, and you can then simply\nreturn all the images from this cluster. To segment an image\nBy clustering pixels according to their color, then replacing each pixel\u2019s color\nwith the mean color of its cluster, it is possible to considerably reduce the num\u2010\nber of different colors in the image. Image segmentation is used in many object\ndetection and tracking systems, as it makes it easier to detect the contour of each\nobject. There is no universal definition of what a cluster is: it really depends on the context,\nand different algorithms will capture different kinds of clusters. Some algorithms\nlook for instances centered around a particular point, called a centroid. Others look\nfor continuous regions of densely packed instances: these clusters can take on any\nshape. Some algorithms are hierarchical, looking for clusters of clusters. And the list\ngoes on. In this section, we will look at two popular clustering algorithms, K-Means and\nDBSCAN, and explore some of their applications, such as nonlinear dimensionality\nreduction, semi-supervised learning, and anomaly detection. K-Means\nConsider the unlabeled dataset represented in Figure 9-2: you can clearly see five\nblobs of instances. The K-Means algorithm is a simple algorithm capable of clustering\nthis kind of dataset very quickly and efficiently, often in just a few iterations. It was\nproposed by Stuart Lloyd at Bell Labs in 1957 as a technique for pulse-code modula\u2010\ntion, but it was only published outside of the company in 1982.1 In 1965, Edward W.\nForgy had published virtually the same algorithm, so K-Means is sometimes referred\nto as Lloyd\u2013Forgy. | Chapter 9: Unsupervised Learning Techniques\nFigure 9-2. An unlabeled dataset composed of five blobs of instances\nLet\u2019s train a K-Means clusterer on this dataset. It will try to find each blob\u2019s center and\nassign each instance to the closest blob:\nfrom sklearn.cluster import KMeans\nk = 5\nkmeans = KMeans(n_clusters=k)\ny_pred = kmeans.fit_predict(X)\nNote that you have to specify the number of clusters k that the algorithm must find. In this example, it is pretty obvious from looking at the data that k should be set to 5,\nbut in general it is not that easy. We will discuss this shortly. Each instance was assigned to one of the five clusters. In the context of clustering, an\ninstance\u2019s label is the index of the cluster that this instance gets assigned to by the\nalgorithm: this is not to be confused with the class labels in classification (remember\nthat clustering is an unsupervised learning task)."
  },
  {
    "id": 153,
    "content": "The KMeans instance preserves a\ncopy of the labels of the instances it was trained on, available via the labels_ instance\nvariable:\n>>> y_pred\narray([4, 0, 1, ..., 2, 1, 0], dtype=int32)\n>>> y_pred is kmeans.labels_\nTrue\nWe can also take a look at the five centroids that the algorithm found:\n>>> kmeans.cluster_centers_\narray([[-2.80389616, 1.80117999], [ 0.20876306, 2.25551336], [-2.79290307, 2.79641063], [-1.46679593, 2.28585348], [-2.80037642, 1.30082566]])\nClustering | You can easily assign new instances to the cluster whose centroid is closest:\n>>> X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n>>> kmeans.predict(X_new)\narray([1, 1, 2, 2], dtype=int32)\nIf you plot the cluster\u2019s decision boundaries, you get a Voronoi tessellation (see\nFigure 9-3, where each centroid is represented with an X). Figure 9-3. K-Means decision boundaries (Voronoi tessellation)\nThe vast majority of the instances were clearly assigned to the appropriate cluster, but\na few instances were probably mislabeled (especially near the boundary between the\ntop-left cluster and the central cluster). Indeed, the K-Means algorithm does not\nbehave very well when the blobs have very different diameters because all it cares\nabout when assigning an instance to a cluster is the distance to the centroid. Instead of assigning each instance to a single cluster, which is called hard clustering, it\ncan be useful to give each instance a score per cluster, which is called soft clustering. The score can be the distance between the instance and the centroid; conversely, it\ncan be a similarity score (or affinity), such as the Gaussian Radial Basis Function\n(introduced in Chapter 5). In the KMeans class, the transform() method measures\nthe distance from each instance to every centroid:\n>>> kmeans.transform(X_new)\narray([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901], [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351], [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031], [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])\nIn this example, the first instance in X_new is located at a distance of 2.81 from the\nfirst centroid, 0.33 from the second centroid, 2.90 from the third centroid, 1.49 from\nthe fourth centroid, and 2.89 from the fifth centroid. If you have a high-dimensional\ndataset and you transform it this way, you end up with a k-dimensional dataset: this\ntransformation can be a very efficient nonlinear dimensionality reduction technique. | Chapter 9: Unsupervised Learning Techniques\n2 That\u2019s because the mean squared distance between the instances and their closest centroid can only go down\nat each step. The K-Means algorithm\nSo, how does the algorithm work? Well, suppose you were given the centroids. You\ncould easily label all the instances in the dataset by assigning each of them to the clus\u2010\nter whose centroid is closest. Conversely, if you were given all the instance labels, you\ncould easily locate all the centroids by computing the mean of the instances for each\ncluster. But you are given neither the labels nor the centroids, so how can you pro\u2010\nceed? Well, just start by placing the centroids randomly (e.g., by picking k instances at\nrandom and using their locations as centroids)."
  },
  {
    "id": 154,
    "content": "Then label the instances, update the\ncentroids, label the instances, update the centroids, and so on until the centroids stop\nmoving. The algorithm is guaranteed to converge in a finite number of steps (usually\nquite small); it will not oscillate forever.2\nYou can see the algorithm in action in Figure 9-4: the centroids are initialized ran\u2010\ndomly (top left), then the instances are labeled (top right), then the centroids are\nupdated (center left), the instances are relabeled (center right), and so on. As you can\nsee, in just three iterations, the algorithm has reached a clustering that seems close to\noptimal. The computational complexity of the algorithm is generally linear\nwith regard to the number of instances m, the number of clusters k,\nand the number of dimensions n. However, this is only true when\nthe data has a clustering structure. If it does not, then in the worst-\ncase scenario the complexity can increase exponentially with the\nnumber of instances. In practice, this rarely happens, and K-Means\nis generally one of the fastest clustering algorithms. Clustering | Figure 9-4. The K-Means algorithm\nAlthough the algorithm is guaranteed to converge, it may not converge to the right\nsolution (i.e., it may converge to a local optimum): whether it does or not depends on\nthe centroid initialization. Figure 9-5 shows two suboptimal solutions that the algo\u2010\nrithm can converge to if you are not lucky with the random initialization step. Figure 9-5. Suboptimal solutions due to unlucky centroid initializations\nLet\u2019s look at a few ways you can mitigate this risk by improving the centroid\ninitialization. | Chapter 9: Unsupervised Learning Techniques\n3 David Arthur and Sergei Vassilvitskii, \u201ck-Means++: The Advantages of Careful Seeding,\u201d Proceedings of the\n18th Annual ACM-SIAM Symposium on Discrete Algorithms (2007): 1027\u20131035. Centroid initialization methods\nIf you happen to know approximately where the centroids should be (e.g., if you ran\nanother clustering algorithm earlier), then you can set the init hyperparameter to a\nNumPy array containing the list of centroids, and set n_init to 1:\ngood_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\nkmeans = KMeans(n_clusters=5, init=good_init, n_init=1)\nAnother solution is to run the algorithm multiple times with different random initial\u2010\nizations and keep the best solution. The number of random initializations is con\u2010\ntrolled by the n_init hyperparameter: by default, it is equal to 10, which means that\nthe whole algorithm described earlier runs 10 times when you call fit(), and Scikit-\nLearn keeps the best solution. But how exactly does it know which solution is the\nbest? It uses a performance metric! That metric is called the model\u2019s inertia, which is\nthe mean squared distance between each instance and its closest centroid. It is\nroughly equal to 223.3 for the model on the left in Figure 9-5, 237.5 for the model on\nthe right in Figure 9-5, and 211.6 for the model in Figure 9-3. The KMeans class runs\nthe algorithm n_init times and keeps the model with the lowest inertia."
  },
  {
    "id": 155,
    "content": "In this\nexample, the model in Figure 9-3 will be selected (unless we are very unlucky with\nn_init consecutive random initializations). If you are curious, a model\u2019s inertia is\naccessible via the inertia_ instance variable:\n>>> kmeans.inertia_\n211.59853725816856\nThe score() method returns the negative inertia. Why negative? Because a predic\u2010\ntor\u2019s score() method must always respect Scikit-Learn\u2019s \u201cgreater is better\u201d rule: if a\npredictor is better than another, its score() method should return a greater score. >>> kmeans.score(X)\n-211.59853725816856\nAn important improvement to the K-Means algorithm, K-Means++, was proposed in\na 2006 paper by David Arthur and Sergei Vassilvitskii.3 They introduced a smarter\ninitialization step that tends to select centroids that are distant from one another, and\nthis improvement makes the K-Means algorithm much less likely to converge to a\nsuboptimal solution. They showed that the additional computation required for the\nsmarter initialization step is well worth it because it makes it possible to drastically\nreduce the number of times the algorithm needs to be run to find the optimal solu\u2010\ntion. Here is the K-Means++ initialization algorithm:\n1. Take one centroid c(1), chosen uniformly at random from the dataset. Clustering | 4 Charles Elkan, \u201cUsing the Triangle Inequality to Accelerate k-Means,\u201d Proceedings of the 20th International\nConference on Machine Learning (2003): 147\u2013153. 5 The triangle inequality is AC \u2264 AB + BC where A, B and C are three points and AB, AC, and BC are the\ndistances between these points. 6 David Sculley, \u201cWeb-Scale K-Means Clustering,\u201d Proceedings of the 19th International Conference on World\nWide Web (2010): 1177\u20131178. 2. Take a new centroid c(i), choosing an instance x(i) with probability D \ufffdi\n2 /\n\u2211j = 1\nm\nD \ufffdj 2, where D(x(i)) is the distance between the instance x(i) and the clos\u2010\nest centroid that was already chosen. This probability distribution ensures that\ninstances farther away from already chosen centroids are much more likely be\nselected as centroids. 3. Repeat the previous step until all k centroids have been chosen. The KMeans class uses this initialization method by default. If you want to force it to\nuse the original method (i.e., picking k instances randomly to define the initial cent\u2010\nroids), then you can set the init hyperparameter to \"random\". You will rarely need to\ndo this. Accelerated K-Means and mini-batch K-Means\nAnother important improvement to the K-Means algorithm was proposed in a 2003\npaper by Charles Elkan.4 It considerably accelerates the algorithm by avoiding many\nunnecessary distance calculations. Elkan achieved this by exploiting the triangle\ninequality (i.e., that a straight line is always the shortest distance between two points5)\nand by keeping track of lower and upper bounds for distances between instances and\ncentroids. This is the algorithm the KMeans class uses by default (you can force it to\nuse the original algorithm by setting the algorithm hyperparameter to \"full\",\nalthough you probably will never need to)."
  },
  {
    "id": 156,
    "content": "Yet another important variant of the K-Means algorithm was proposed in a 2010\npaper by David Sculley.6 Instead of using the full dataset at each iteration, the algo\u2010\nrithm is capable of using mini-batches, moving the centroids just slightly at each iter\u2010\nation. This speeds up the algorithm typically by a factor of three or four and makes it\npossible to cluster huge datasets that do not fit in memory. Scikit-Learn implements\nthis algorithm in the MiniBatchKMeans class. You can just use this class like the\nKMeans class:\nfrom sklearn.cluster import MiniBatchKMeans\nminibatch_kmeans = MiniBatchKMeans(n_clusters=5)\nminibatch_kmeans.fit(X) | Chapter 9: Unsupervised Learning Techniques\nIf the dataset does not fit in memory, the simplest option is to use the memmap class, as\nwe did for incremental PCA in Chapter 8. Alternatively, you can pass one mini-batch\nat a time to the partial_fit() method, but this will require much more work, since\nyou will need to perform multiple initializations and select the best one yourself (see\nthe mini-batch K-Means section of the notebook for an example). Although the Mini-batch K-Means algorithm is much faster than the regular K-\nMeans algorithm, its inertia is generally slightly worse, especially as the number of\nclusters increases. You can see this in Figure 9-6: the plot on the left compares the\ninertias of Mini-batch K-Means and regular K-Means models trained on the previous\ndataset using various numbers of clusters k. The difference between the two curves\nremains fairly constant, but this difference becomes more and more significant as k\nincreases, since the inertia becomes smaller and smaller. In the plot on the right, you\ncan see that Mini-batch K-Means is much faster than regular K-Means, and this dif\u2010\nference increases with k.\nFigure 9-6. Mini-batch K-Means has a higher inertia than K-Means (left) but it is much\nfaster (right), especially as k increases\nFinding the optimal number of clusters\nSo far, we have set the number of clusters k to 5 because it was obvious by looking at\nthe data that this was the correct number of clusters. But in general, it will not be so\neasy to know how to set k, and the result might be quite bad if you set it to the wrong\nvalue. As you can see in Figure 9-7, setting k to 3 or 8 results in fairly bad models. Clustering | Figure 9-7. Bad choices for the number of clusters: when k is too small, separate clusters\nget merged (left), and when k is too large, some clusters get chopped into multiple pieces\n(right)\nYou might be thinking that we could just pick the model with the lowest inertia,\nright? Unfortunately, it is not that simple. The inertia for k=3 is 653.2, which is much\nhigher than for k=5 (which was 211.6). But with k=8, the inertia is just 119.1."
  },
  {
    "id": 157,
    "content": "The\ninertia is not a good performance metric when trying to choose k because it keeps\ngetting lower as we increase k. Indeed, the more clusters there are, the closer each\ninstance will be to its closest centroid, and therefore the lower the inertia will be. Let\u2019s\nplot the inertia as a function of k (see Figure 9-8). Figure 9-8. When plotting the inertia as a function of the number of clusters k, the curve\noften contains an inflexion point called the \u201celbow\u201d\nAs you can see, the inertia drops very quickly as we increase k up to 4, but then it\ndecreases much more slowly as we keep increasing k. This curve has roughly the\nshape of an arm, and there is an \u201celbow\u201d at k = 4. So, if we did not know better, 4\nwould be a good choice: any lower value would be dramatic, while any higher value\nwould not help much, and we might just be splitting perfectly good clusters in half for\nno good reason. This technique for choosing the best value for the number of clusters is rather coarse. A more precise approach (but also more computationally expensive) is to use the\nsilhouette score, which is the mean silhouette coefficient over all the instances. An | Chapter 9: Unsupervised Learning Techniques\ninstance\u2019s silhouette coefficient is equal to (b \u2013 a) / max(a, b), where a is the mean\ndistance to the other instances in the same cluster (i.e., the mean intra-cluster dis\u2010\ntance) and b is the mean nearest-cluster distance (i.e., the mean distance to the\ninstances of the next closest cluster, defined as the one that minimizes b, excluding\nthe instance\u2019s own cluster). The silhouette coefficient can vary between \u20131 and +1. A\ncoefficient close to +1 means that the instance is well inside its own cluster and far\nfrom other clusters, while a coefficient close to 0 means that it is close to a cluster\nboundary, and finally a coefficient close to \u20131 means that the instance may have been\nassigned to the wrong cluster. To compute the silhouette score, you can use Scikit-Learn\u2019s silhouette_score()\nfunction, giving it all the instances in the dataset and the labels they were assigned:\n>>> from sklearn.metrics import silhouette_score\n>>> silhouette_score(X, kmeans.labels_)\n0.655517642572828\nLet\u2019s compare the silhouette scores for different numbers of clusters (see Figure 9-9). Figure 9-9. Selecting the number of clusters k using the silhouette score\nAs you can see, this visualization is much richer than the previous one: although it\nconfirms that k = 4 is a very good choice, it also underlines the fact that k = 5 is quite\ngood as well, and much better than k = 6 or 7. This was not visible when comparing\ninertias. An even more informative visualization is obtained when you plot every instance\u2019s\nsilhouette coefficient, sorted by the cluster they are assigned to and by the value of the\ncoefficient. This is called a silhouette diagram (see Figure 9-10)."
  },
  {
    "id": 158,
    "content": "Each diagram con\u2010\ntains one knife shape per cluster. The shape\u2019s height indicates the number of instances\nthe cluster contains, and its width represents the sorted silhouette coefficients of the\ninstances in the cluster (wider is better). The dashed line indicates the mean silhou\u2010\nette coefficient. Clustering | Figure 9-10. Analyzing the silhouette diagrams for various values of k\nThe vertical dashed lines represent the silhouette score for each number of clusters. When most of the instances in a cluster have a lower coefficient than this score (i.e., if\nmany of the instances stop short of the dashed line, ending to the left of it), then the\ncluster is rather bad since this means its instances are much too close to other clus\u2010\nters. We can see that when k = 3 and when k = 6, we get bad clusters. But when k = 4\nor k = 5, the clusters look pretty good: most instances extend beyond the dashed line,\nto the right and closer to 1.0. When k = 4, the cluster at index 1 (the third from the\ntop) is rather big. When k = 5, all clusters have similar sizes. So, even though the\noverall silhouette score from k = 4 is slightly greater than for k = 5, it seems like a\ngood idea to use k = 5 to get clusters of similar sizes. Limits of K-Means\nDespite its many merits, most notably being fast and scalable, K-Means is not perfect. As we saw, it is necessary to run the algorithm several times to avoid suboptimal solu\u2010\ntions, plus you need to specify the number of clusters, which can be quite a hassle. Moreover, K-Means does not behave very well when the clusters have varying sizes, | Chapter 9: Unsupervised Learning Techniques\ndifferent densities, or nonspherical shapes. For example, Figure 9-11 shows how K-\nMeans clusters a dataset containing three ellipsoidal clusters of different dimensions,\ndensities, and orientations. Figure 9-11. K-Means fails to cluster these ellipsoidal blobs properly\nAs you can see, neither of these solutions is any good. The solution on the left is bet\u2010\nter, but it still chops off 25% of the middle cluster and assigns it to the cluster on the\nright. The solution on the right is just terrible, even though its inertia is lower. So,\ndepending on the data, different clustering algorithms may perform better. On these\ntypes of elliptical clusters, Gaussian mixture models work great. It is important to scale the input features before you run K-Means,\nor the clusters may be very stretched and K-Means will perform\npoorly. Scaling the features does not guarantee that all the clusters\nwill be nice and spherical, but it generally improves things. Now let\u2019s look at a few ways we can benefit from clustering. We will use K-Means, but\nfeel free to experiment with other clustering algorithms. Using Clustering for Image Segmentation\nImage segmentation is the task of partitioning an image into multiple segments."
  },
  {
    "id": 159,
    "content": "In\nsemantic segmentation, all pixels that are part of the same object type get assigned to\nthe same segment. For example, in a self-driving car\u2019s vision system, all pixels that are\npart of a pedestrian\u2019s image might be assigned to the \u201cpedestrian\u201d segment (there\nwould be one segment containing all the pedestrians). In instance segmentation, all\npixels that are part of the same individual object are assigned to the same segment. In\nthis case there would be a different segment for each pedestrian. The state of the art\nin semantic or instance segmentation today is achieved using complex architectures\nbased on convolutional neural networks (see Chapter 14). Here, we are going to do\nsomething much simpler: color segmentation. We will simply assign pixels to the same\nsegment if they have a similar color. In some applications, this may be sufficient. For\nClustering | example, if you want to analyze satellite images to measure how much total forest\narea there is in a region, color segmentation may be just fine. First, use Matplotlib\u2019s imread() function to load the image (see the upper-left image\nin Figure 9-12):\n>>> from matplotlib.image import imread # or `from imageio import imread`\n>>> image = imread(os.path.join(\"images\",\"unsupervised_learning\",\"ladybug.png\"))\n>>> image.shape\n(533, 800, 3)\nThe image is represented as a 3D array. The first dimension\u2019s size is the height; the\nsecond is the width; and the third is the number of color channels, in this case red,\ngreen, and blue (RGB). In other words, for each pixel there is a 3D vector containing\nthe intensities of red, green, and blue, each between 0.0 and 1.0 (or between 0 and\n255, if you use imageio.imread()). Some images may have fewer channels, such as\ngrayscale images (one channel). And some images may have more channels, such as\nimages with an additional alpha channel for transparency or satellite images, which\noften contain channels for many light frequencies (e.g., infrared). The following code\nreshapes the array to get a long list of RGB colors, then it clusters these colors using\nK-Means:\nX = image.reshape(-1, 3)\nkmeans = KMeans(n_clusters=8).fit(X)\nsegmented_img = kmeans.cluster_centers_[kmeans.labels_]\nsegmented_img = segmented_img.reshape(image.shape)\nFor example, it may identify a color cluster for all shades of green. Next, for each\ncolor (e.g., dark green), it looks for the mean color of the pixel\u2019s color cluster. For\nexample, all shades of green may be replaced with the same light green color (assum\u2010\ning the mean color of the green cluster is light green). Finally, it reshapes this long list\nof colors to get the same shape as the original image. And we\u2019re done! This outputs the image shown in the upper right of Figure 9-12. You can experiment\nwith various numbers of clusters, as shown in the figure. When you use fewer than\neight clusters, notice that the ladybug\u2019s flashy red color fails to get a cluster of its own:\nit gets merged with colors from the environment. This is because K-Means prefers\nclusters of similar sizes."
  },
  {
    "id": 160,
    "content": "The ladybug is small\u2014much smaller than the rest of the\nimage\u2014so even though its color is flashy, K-Means fails to dedicate a cluster to it. | Chapter 9: Unsupervised Learning Techniques\nFigure 9-12. Image segmentation using K-Means with various numbers of color clusters\nThat wasn\u2019t too hard, was it? Now let\u2019s look at another application of clustering: pre\u2010\nprocessing. Using Clustering for Preprocessing\nClustering can be an efficient approach to dimensionality reduction, in particular as a\npreprocessing step before a supervised learning algorithm. As an example of using\nclustering for dimensionality reduction, let\u2019s tackle the digits dataset, which is a sim\u2010\nple MNIST-like dataset containing 1,797 grayscale 8 \u00d7 8 images representing the dig\u2010\nits 0 to 9. First, load the dataset:\nfrom sklearn.datasets import load_digits\nX_digits, y_digits = load_digits(return_X_y=True)\nNow, split it into a training set and a test set:\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_digits, y_digits)\nNext, fit a Logistic Regression model:\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train, y_train)\nLet\u2019s evaluate its accuracy on the test set:\n>>> log_reg.score(X_test, y_test)\n0.9688888888888889\nClustering | OK, that\u2019s our baseline: 96.9% accuracy. Let\u2019s see if we can do better by using K-Means\nas a preprocessing step. We will create a pipeline that will first cluster the training set\ninto 50 clusters and replace the images with their distances to these 50 clusters, then\napply a Logistic Regression model:\nfrom sklearn.pipeline import Pipeline\npipeline = Pipeline([ (\"kmeans\", KMeans(n_clusters=50)), (\"log_reg\", LogisticRegression()),\n])\npipeline.fit(X_train, y_train)\nSince there are 10 different digits, it is tempting to set the number\nof clusters to 10. However, each digit can be written several differ\u2010\nent ways, so it is preferable to use a larger number of clusters, such\nas 50. Now let\u2019s evaluate this classification pipeline:\n>>> pipeline.score(X_test, y_test)\n0.9777777777777777\nHow about that? We reduced the error rate by almost 30% (from about 3.1% to about\n2.2%)! But we chose the number of clusters k arbitrarily; we can surely do better. Since K-\nMeans is just a preprocessing step in a classification pipeline, finding a good value for\nk is much simpler than earlier. There\u2019s no need to perform silhouette analysis or mini\u2010\nmize the inertia; the best value of k is simply the one that results in the best classifica\u2010\ntion performance during cross-validation. We can use GridSearchCV to find the\noptimal number of clusters:\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = dict(kmeans__n_clusters=range(2, 100))\ngrid_clf = GridSearchCV(pipeline, param_grid, cv=3, verbose=2)\ngrid_clf.fit(X_train, y_train)\nLet\u2019s look at the best value for k and the performance of the resulting pipeline:\n>>> grid_clf.best_params_\n{'kmeans__n_clusters': 99}\n>>> grid_clf.score(X_test, y_test)\n0.9822222222222222\nWith k = 99 clusters, we get a significant accuracy boost, reaching 98.22% accuracy\non the test set. Cool! You may want to keep exploring higher values for k, since 99\nwas the largest value in the range we explored."
  },
  {
    "id": 161,
    "content": "| Chapter 9: Unsupervised Learning Techniques\nUsing Clustering for Semi-Supervised Learning\nAnother use case for clustering is in semi-supervised learning, when we have plenty\nof unlabeled instances and very few labeled instances. Let\u2019s train a Logistic Regression\nmodel on a sample of 50 labeled instances from the digits dataset:\nn_labeled = 50\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train[:n_labeled], y_train[:n_labeled])\nWhat is the performance of this model on the test set? >>> log_reg.score(X_test, y_test)\n0.8333333333333334\nThe accuracy is just 83.3%. It should come as no surprise that this is much lower than\nearlier, when we trained the model on the full training set. Let\u2019s see how we can do\nbetter. First, let\u2019s cluster the training set into 50 clusters. Then for each cluster, let\u2019s\nfind the image closest to the centroid. We will call these images the representative\nimages:\nk = 50\nkmeans = KMeans(n_clusters=k)\nX_digits_dist = kmeans.fit_transform(X_train)\nrepresentative_digit_idx = np.argmin(X_digits_dist, axis=0)\nX_representative_digits = X_train[representative_digit_idx]\nFigure 9-13 shows these 50 representative images. Figure 9-13. Fifty representative digit images (one per cluster)\nLet\u2019s look at each image and manually label it:\ny_representative_digits = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])\nNow we have a dataset with just 50 labeled instances, but instead of being random\ninstances, each of them is a representative image of its cluster. Let\u2019s see if the perfor\u2010\nmance is any better:\n>>> log_reg = LogisticRegression()\n>>> log_reg.fit(X_representative_digits, y_representative_digits)\n>>> log_reg.score(X_test, y_test)\n0.9222222222222223\nClustering | Wow! We jumped from 83.3% accuracy to 92.2%, although we are still only training\nthe model on 50 instances. Since it is often costly and painful to label instances, espe\u2010\ncially when it has to be done manually by experts, it is a good idea to label representa\u2010\ntive instances rather than just random instances. But perhaps we can go one step further: what if we propagated the labels to all the\nother instances in the same cluster? This is called label propagation:\ny_train_propagated = np.empty(len(X_train), dtype=np.int32)\nfor i in range(k): y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]\nNow let\u2019s train the model again and look at its performance:\n>>> log_reg = LogisticRegression()\n>>> log_reg.fit(X_train, y_train_propagated)\n>>> log_reg.score(X_test, y_test)\n0.9333333333333333\nWe got a reasonable accuracy boost, but nothing absolutely astounding. The problem\nis that we propagated each representative instance\u2019s label to all the instances in the\nsame cluster, including the instances located close to the cluster boundaries, which\nare more likely to be mislabeled. Let\u2019s see what happens if we only propagate the\nlabels to the 20% of the instances that are closest to the centroids:\npercentile_closest = 20\nX_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\nfor i in range(k): in_cluster = (kmeans.labels_ == i) cluster_dist = X_cluster_dist[in_cluster] cutoff_distance = np.percentile(cluster_dist, percentile_closest) above_cutoff = (X_cluster_dist > cutoff_distance) X_cluster_dist[in_cluster & above_cutoff] = -1\npartially_propagated = (X_cluster_dist != -1)\nX_train_partially_propagated = X_train[partially_propagated]\ny_train_partially_propagated = y_train_propagated[partially_propagated]\nNow let\u2019s train the model again on this partially propagated dataset:\n>>> log_reg = LogisticRegression()\n>>> log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)\n>>> log_reg.score(X_test, y_test)\n0.94\nNice!"
  },
  {
    "id": 162,
    "content": "With just 50 labeled instances (only 5 examples per class on average! ), we got\n94.0% accuracy, which is pretty close to the performance of Logistic Regression on\nthe fully labeled digits dataset (which was 96.9%). This good performance is due to\nthe fact that the propagated labels are actually pretty good\u2014their accuracy is very\nclose to 99%, as the following code shows: | Chapter 9: Unsupervised Learning Techniques\n>>> np.mean(y_train_partially_propagated == y_train[partially_propagated])\n0.9896907216494846\nActive Learning\nTo continue improving your model and your training set, the next step could be to do\na few rounds of active learning, which is when a human expert interacts with the\nlearning algorithm, providing labels for specific instances when the algorithm\nrequests them. There are many different strategies for active learning, but one of the\nmost common ones is called uncertainty sampling. Here is how it works:\n1. The model is trained on the labeled instances gathered so far, and this model is\nused to make predictions on all the unlabeled instances. 2. The instances for which the model is most uncertain (i.e., when its estimated\nprobability is lowest) are given to the expert to be labeled. 3. You iterate this process until the performance improvement stops being worth\nthe labeling effort. Other strategies include labeling the instances that would result in the largest model\nchange, or the largest drop in the model\u2019s validation error, or the instances that differ\u2010\nent models disagree on (e.g., an SVM or a Random Forest). Before we move on to Gaussian mixture models, let\u2019s take a look at DBSCAN,\nanother popular clustering algorithm that illustrates a very different approach based\non local density estimation. This approach allows the algorithm to identify clusters of\narbitrary shapes. DBSCAN\nThis algorithm defines clusters as continuous regions of high density. Here is how it\nworks:\n\u2022 For each instance, the algorithm counts how many instances are located within a\nsmall distance \u03b5 (epsilon) from it. This region is called the instance\u2019s \u03b5-\nneighborhood. \u2022 If an instance has at least min_samples instances in its \u03b5-neighborhood (includ\u2010\ning itself), then it is considered a core instance. In other words, core instances are\nthose that are located in dense regions. \u2022 All instances in the neighborhood of a core instance belong to the same cluster. This neighborhood may include other core instances; therefore, a long sequence\nof neighboring core instances forms a single cluster. Clustering | \u2022 Any instance that is not a core instance and does not have one in its neighbor\u2010\nhood is considered an anomaly. This algorithm works well if all the clusters are dense enough and if they are well sep\u2010\narated by low-density regions. The DBSCAN class in Scikit-Learn is as simple to use as\nyou might expect."
  },
  {
    "id": 163,
    "content": "Let\u2019s test it on the moons dataset, introduced in Chapter 5:\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=1000, noise=0.05)\ndbscan = DBSCAN(eps=0.05, min_samples=5)\ndbscan.fit(X)\nThe labels of all the instances are now available in the labels_ instance variable:\n>>> dbscan.labels_\narray([ 0, 2, -1, -1, 1, 0, 0, 0, ..., 3, 2, 3, 3, 4, 2, 6, 3])\nNotice that some instances have a cluster index equal to \u20131, which means that they\nare considered as anomalies by the algorithm. The indices of the core instances are\navailable in the core_sample_indices_ instance variable, and the core instances\nthemselves are available in the components_ instance variable:\n>>> len(dbscan.core_sample_indices_) >>> dbscan.core_sample_indices_\narray([ 0, 4, 5, 6, 7, 8, 10, 11, ..., 992, 993, 995, 997, 998, 999])\n>>> dbscan.components_\narray([[-0.02137124, 0.40618608], [-0.84192557, 0.53058695], ... [-0.94355873, 0.3278936 ], [ 0.79419406, 0.60777171]])\nThis clustering is represented in the lefthand plot of Figure 9-14. As you can see, it\nidentified quite a lot of anomalies, plus seven different clusters. How disappointing! Fortunately, if we widen each instance\u2019s neighborhood by increasing eps to 0.2, we get\nthe clustering on the right, which looks perfect. Let\u2019s continue with this model. | Chapter 9: Unsupervised Learning Techniques\nFigure 9-14. DBSCAN clustering using two different neighborhood radiuses\nSomewhat surprisingly, the DBSCAN class does not have a predict() method, although\nit has a fit_predict() method. In other words, it cannot predict which cluster a new\ninstance belongs to. This implementation decision was made because different classi\u2010\nfication algorithms can be better for different tasks, so the authors decided to let the\nuser choose which one to use. Moreover, it\u2019s not hard to implement. For example, let\u2019s\ntrain a KNeighborsClassifier:\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=50)\nknn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\nNow, given a few new instances, we can predict which cluster they most likely belong\nto and even estimate a probability for each cluster:\n>>> X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\n>>> knn.predict(X_new)\narray([1, 0, 1, 0])\n>>> knn.predict_proba(X_new)\narray([[0.18, 0.82], [1. , 0. ], [0.12, 0.88], [1. , 0. ]]) Note that we only trained the classifier on the core instances, but we could also have\nchosen to train it on all the instances, or all but the anomalies: this choice depends on\nthe final task. The decision boundary is represented in Figure 9-15 (the crosses represent the four\ninstances in X_new). Notice that since there is no anomaly in the training set, the clas\u2010\nsifier always chooses a cluster, even when that cluster is far away. It is fairly straight\u2010\nforward to introduce a maximum distance, in which case the two instances that are\nfar away from both clusters are classified as anomalies. To do this, use the kneigh\nbors() method of the KNeighborsClassifier."
  },
  {
    "id": 164,
    "content": "Given a set of instances, it returns the\nClustering | distances and the indices of the k nearest neighbors in the training set (two matrices,\neach with k columns):\n>>> y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\n>>> y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\n>>> y_pred[y_dist > 0.2] = -1\n>>> y_pred.ravel()\narray([-1, 0, 1, -1])\nFigure 9-15. Decision boundary between two clusters\nIn short, DBSCAN is a very simple yet powerful algorithm capable of identifying any\nnumber of clusters of any shape. It is robust to outliers, and it has just two hyperpara\u2010\nmeters (eps and min_samples). If the density varies significantly across the clusters,\nhowever, it can be impossible for it to capture all the clusters properly. Its computa\u2010\ntional complexity is roughly O(m log m), making it pretty close to linear with regard\nto the number of instances, but Scikit-Learn\u2019s implementation can require up to\nO(m2) memory if eps is large. You may also want to try Hierarchical DBSCAN (HDBSCAN),\nwhich is implemented in the scikit-learn-contrib project. Other Clustering Algorithms\nScikit-Learn implements several more clustering algorithms that you should take a\nlook at. We cannot cover them all in detail here, but here is a brief overview:\nAgglomerative clustering\nA hierarchy of clusters is built from the bottom up. Think of many tiny bubbles\nfloating on water and gradually attaching to each other until there\u2019s one big group\nof bubbles. Similarly, at each iteration, agglomerative clustering connects the\nnearest pair of clusters (starting with individual instances). If you drew a tree | Chapter 9: Unsupervised Learning Techniques\nwith a branch for every pair of clusters that merged, you would get a binary tree\nof clusters, where the leaves are the individual instances. This approach scales\nvery well to large numbers of instances or clusters. It can capture clusters of vari\u2010\nous shapes, it produces a flexible and informative cluster tree instead of forcing\nyou to choose a particular cluster scale, and it can be used with any pairwise dis\u2010\ntance. It can scale nicely to large numbers of instances if you provide a connectiv\u2010\nity matrix, which is a sparse m \u00d7 m matrix that indicates which pairs of instances\nare neighbors (e.g., returned by sklearn.neighbors.kneighbors_graph()). Without a connectivity matrix, the algorithm does not scale well to large datasets. BIRCH\nThe BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)\nalgorithm was designed specifically for very large datasets, and it can be faster\nthan batch K-Means, with similar results, as long as the number of features is not\ntoo large (<20). During training, it builds a tree structure containing just enough\ninformation to quickly assign each new instance to a cluster, without having to\nstore all the instances in the tree: this approach allows it to use limited memory,\nwhile handling huge datasets. Mean-Shift\nThis algorithm starts by placing a circle centered on each instance; then for each\ncircle it computes the mean of all the instances located within it, and it shifts the\ncircle so that it is centered on the mean."
  },
  {
    "id": 165,
    "content": "Next, it iterates this mean-shifting step\nuntil all the circles stop moving (i.e., until each of them is centered on the mean\nof the instances it contains). Mean-Shift shifts the circles in the direction of\nhigher density, until each of them has found a local density maximum. Finally, all\nthe instances whose circles have settled in the same place (or close enough) are\nassigned to the same cluster. Mean-Shift has some of the same features as\nDBSCAN, like how it can find any number of clusters of any shape, it has very\nfew hyperparameters (just one\u2014the radius of the circles, called the bandwidth),\nand it relies on local density estimation. But unlike DBSCAN, Mean-Shift tends\nto chop clusters into pieces when they have internal density variations. Unfortu\u2010\nnately, its computational complexity is O(m2), so it is not suited for large datasets. Affinity propagation\nThis algorithm uses a voting system, where instances vote for similar instances to\nbe their representatives, and once the algorithm converges, each representative\nand its voters form a cluster. Affinity propagation can detect any number of clus\u2010\nters of different sizes. Unfortunately, this algorithm has a computational com\u2010\nplexity of O(m2), so it too is not suited for large datasets. Spectral clustering\nThis algorithm takes a similarity matrix between the instances and creates a low-\ndimensional embedding from it (i.e., it reduces its dimensionality), then it uses\nClustering | 7 Phi (\u03d5 or \u03c6) is the 21st letter of the Greek alphabet. another clustering algorithm in this low-dimensional space (Scikit-Learn\u2019s imple\u2010\nmentation uses K-Means.) Spectral clustering can capture complex cluster struc\u2010\ntures, and it can also be used to cut graphs (e.g., to identify clusters of friends on\na social network). It does not scale well to large numbers of instances, and it does\nnot behave well when the clusters have very different sizes. Now let\u2019s dive into Gaussian mixture models, which can be used for density estima\u2010\ntion, clustering, and anomaly detection. Gaussian Mixtures\nA Gaussian mixture model (GMM) is a probabilistic model that assumes that the\ninstances were generated from a mixture of several Gaussian distributions whose\nparameters are unknown. All the instances generated from a single Gaussian distri\u2010\nbution form a cluster that typically looks like an ellipsoid. Each cluster can have a dif\u2010\nferent ellipsoidal shape, size, density, and orientation, just like in Figure 9-11. When\nyou observe an instance, you know it was generated from one of the Gaussian distri\u2010\nbutions, but you are not told which one, and you do not know what the parameters of\nthese distributions are. There are several GMM variants. In the simplest variant, implemented in the Gaus\nsianMixture class, you must know in advance the number k of Gaussian distribu\u2010\ntions. The dataset X is assumed to have been generated through the following\nprobabilistic process:\n\u2022 For each instance, a cluster is picked randomly from among k clusters."
  },
  {
    "id": 166,
    "content": "The prob\u2010\nability of choosing the jth cluster is defined by the cluster\u2019s weight, \u03d5(j).7 The index\nof the cluster chosen for the ith instance is noted z(i). \u2022 If z(i)=j, meaning the ith instance has been assigned to the jth cluster, the location\nx(i) of this instance is sampled randomly from the Gaussian distribution with\nmean \u03bc(j) and covariance matrix \u03a3(j). This is noted x i \u223c\ufffd\u03bc j , \u03a3 j . This generative process can be represented as a graphical model. Figure 9-16 repre\u2010\nsents the structure of the conditional dependencies between random variables. | Chapter 9: Unsupervised Learning Techniques\n8 Most of these notations are standard, but a few additional notations were taken from the Wikipedia article on\nplate notation. Figure 9-16. A graphical representation of a Gaussian mixture model, including its\nparameters (squares), random variables (circles), and their conditional dependencies\n(solid arrows)\nHere is how to interpret the figure:8\n\u2022 The circles represent random variables. \u2022 The squares represent fixed values (i.e., parameters of the model). \u2022 The large rectangles are called plates. They indicate that their content is repeated\nseveral times. \u2022 The number at the bottom right of each plate indicates how many times its con\u2010\ntent is repeated. So, there are m random variables z(i) (from z(1) to z(m)) and m\nrandom variables x(i). There are also k means \u03bc(j) and k covariance matrices \u03a3(j). Lastly, there is just one weight vector \u03d5 (containing all the weights \u03d5(1) to \u03d5(k)). \u2022 Each variable z(i) is drawn from the categorical distribution with weights \u03d5. Each\nvariable x(i) is drawn from the normal distribution, with the mean and covariance\nmatrix defined by its cluster z(i). \u2022 The solid arrows represent conditional dependencies. For example, the probabil\u2010\nity distribution for each random variable z(i) depends on the weight vector \u03d5. Note that when an arrow crosses a plate boundary, it means that it applies to all\nthe repetitions of that plate. For example, the weight vector \u03d5 conditions the\nprobability distributions of all the random variables x(1) to x(m). \u2022 The squiggly arrow from z(i) to x(i) represents a switch: depending on the value of\nz(i), the instance x(i) will be sampled from a different Gaussian distribution. For\nexample, if z(i)=j, then x i \u223c\ufffd\u03bc j , \u03a3 j . Gaussian Mixtures | \u2022 Shaded nodes indicate that the value is known. So, in this case, only the random\nvariables x(i) have known values: they are called observed variables. The unknown\nrandom variables z(i) are called latent variables. So, what can you do with such a model? Well, given the dataset X, you typically want\nto start by estimating the weights \u03d5 and all the distribution parameters \u03bc(1) to \u03bc(k) and\n\u03a3(1) to \u03a3(k)."
  },
  {
    "id": 167,
    "content": "Scikit-Learn\u2019s GaussianMixture class makes this super easy:\nfrom sklearn.mixture import GaussianMixture\ngm = GaussianMixture(n_components=3, n_init=10)\ngm.fit(X)\nLet\u2019s look at the parameters that the algorithm estimated:\n>>> gm.weights_\narray([0.20965228, 0.4000662 , 0.39028152])\n>>> gm.means_\narray([[ 3.39909717, 1.05933727], [-1.40763984, 1.42710194], [ 0.05135313, 0.07524095]])\n>>> gm.covariances_\narray([[[ 1.14807234, -0.03270354], [-0.03270354, 0.95496237]], [[ 0.63478101, 0.72969804], [ 0.72969804, 1.1609872 ]], [[ 0.68809572, 0.79608475], [ 0.79608475, 1.21234145]]])\nGreat, it worked fine! Indeed, the weights that were used to generate the data were\n0.2, 0.4, and 0.4; and similarly, the means and covariance matrices were very close to\nthose found by the algorithm. But how? This class relies on the Expectation-\nMaximization (EM) algorithm, which has many similarities with the K-Means algo\u2010\nrithm: it also initializes the cluster parameters randomly, then it repeats two steps\nuntil convergence, first assigning instances to clusters (this is called the expectation\nstep) and then updating the clusters (this is called the maximization step). Sounds\nfamiliar, right? In the context of clustering, you can think of EM as a generalization of\nK-Means that not only finds the cluster centers (\u03bc(1) to \u03bc(k)), but also their size, shape,\nand orientation (\u03a3(1) to \u03a3(k)), as well as their relative weights (\u03d5(1) to \u03d5(k)). Unlike K-\nMeans, though, EM uses soft cluster assignments, not hard assignments. For each\ninstance, during the expectation step, the algorithm estimates the probability that it\nbelongs to each cluster (based on the current cluster parameters). Then, during the\nmaximization step, each cluster is updated using all the instances in the dataset, with\neach instance weighted by the estimated probability that it belongs to that cluster. These probabilities are called the responsibilities of the clusters for the instances. | Chapter 9: Unsupervised Learning Techniques\nDuring the maximization step, each cluster\u2019s update will mostly be impacted by the\ninstances it is most responsible for. Unfortunately, just like K-Means, EM can end up converging to\npoor solutions, so it needs to be run several times, keeping only the\nbest solution. This is why we set n_init to 10. Be careful: by default\nn_init is set to 1. You can check whether or not the algorithm converged and how many iterations it\ntook:\n>>> gm.converged_\nTrue\n>>> gm.n_iter_ Now that you have an estimate of the location, size, shape, orientation, and relative\nweight of each cluster, the model can easily assign each instance to the most likely\ncluster (hard clustering) or estimate the probability that it belongs to a particular\ncluster (soft clustering)."
  },
  {
    "id": 168,
    "content": "Just use the predict() method for hard clustering, or the\npredict_proba() method for soft clustering:\n>>> gm.predict(X)\narray([2, 2, 1, ..., 0, 0, 0])\n>>> gm.predict_proba(X)\narray([[2.32389467e-02, 6.77397850e-07, 9.76760376e-01], [1.64685609e-02, 6.75361303e-04, 9.82856078e-01], [2.01535333e-06, 9.99923053e-01, 7.49319577e-05], ..., [9.99999571e-01, 2.13946075e-26, 4.28788333e-07], [1.00000000e+00, 1.46454409e-41, 5.12459171e-16], [1.00000000e+00, 8.02006365e-41, 2.27626238e-15]])\nA Gaussian mixture model is a generative model, meaning you can sample new\ninstances from it (note that they are ordered by cluster index):\n>>> X_new, y_new = gm.sample(6)\n>>> X_new\narray([[ 2.95400315, 2.63680992], [-1.16654575, 1.62792705], [-1.39477712, -1.48511338], [ 0.27221525, 0.690366 ], [ 0.54095936, 0.48591934], [ 0.38064009, -0.56240465]])\n>>> y_new\narray([0, 1, 2, 2, 2, 2])\nIt is also possible to estimate the density of the model at any given location. This is\nachieved using the score_samples() method: for each instance it is given, this\nGaussian Mixtures | method estimates the log of the probability density function (PDF) at that location. The greater the score, the higher the density:\n>>> gm.score_samples(X)\narray([-2.60782346, -3.57106041, -3.33003479, ..., -3.51352783, -4.39802535, -3.80743859])\nIf you compute the exponential of these scores, you get the value of the PDF at the\nlocation of the given instances. These are not probabilities, but probability densities:\nthey can take on any positive value, not just a value between 0 and 1. To estimate the\nprobability that an instance will fall within a particular region, you would have to\nintegrate the PDF over that region (if you do so over the entire space of possible\ninstance locations, the result will be 1). Figure 9-17 shows the cluster means, the decision boundaries (dashed lines), and the\ndensity contours of this model. Figure 9-17. Cluster means, decision boundaries, and density contours of a trained\nGaussian mixture model\nNice! The algorithm clearly found an excellent solution. Of course, we made its task\neasy by generating the data using a set of 2D Gaussian distributions (unfortunately,\nreal-life data is not always so Gaussian and low-dimensional). We also gave the algo\u2010\nrithm the correct number of clusters. When there are many dimensions, or many\nclusters, or few instances, EM can struggle to converge to the optimal solution. You\nmight need to reduce the difficulty of the task by limiting the number of parameters\nthat the algorithm has to learn. One way to do this is to limit the range of shapes and\norientations that the clusters can have. This can be achieved by imposing constraints\non the covariance matrices. To do this, set the covariance_type hyperparameter to\none of the following values: | Chapter 9: Unsupervised Learning Techniques\n\"spherical\"\nAll clusters must be spherical, but they can have different diameters (i.e., differ\u2010\nent variances). \"diag\"\nClusters can take on any ellipsoidal shape of any size, but the ellipsoid\u2019s axes must\nbe parallel to the coordinate axes (i.e., the covariance matrices must be diagonal). \"tied\"\nAll clusters must have the same ellipsoidal shape, size, and orientation (i.e., all\nclusters share the same covariance matrix)."
  },
  {
    "id": 169,
    "content": "By default, covariance_type is equal to \"full\", which means that each cluster can\ntake on any shape, size, and orientation (it has its own unconstrained covariance\nmatrix). Figure 9-18 plots the solutions found by the EM algorithm when cova\nriance_type is set to \"tied\" or \"spherical.\u201d\nFigure 9-18. Gaussian mixtures for tied clusters (left) and spherical clusters (right)\nThe computational complexity of training a GaussianMixture\nmodel depends on the number of instances m, the number of\ndimensions n, the number of clusters k, and the constraints on the\ncovariance matrices. If covariance_type is \"spherical or \"diag\",\nit is O(kmn), assuming the data has a clustering structure. If cova\nriance_type is \"tied\" or \"full\", it is O(kmn2 + kn3), so it will not\nscale to large numbers of features. Gaussian mixture models can also be used for anomaly detection. Let\u2019s see how. Gaussian Mixtures | Anomaly Detection Using Gaussian Mixtures\nAnomaly detection (also called outlier detection) is the task of detecting instances that\ndeviate strongly from the norm. These instances are called anomalies, or outliers,\nwhile the normal instances are called inliers. Anomaly detection is useful in a wide\nvariety of applications, such as fraud detection, detecting defective products in manu\u2010\nfacturing, or removing outliers from a dataset before training another model (which\ncan significantly improve the performance of the resulting model). Using a Gaussian mixture model for anomaly detection is quite simple: any instance\nlocated in a low-density region can be considered an anomaly. You must define what\ndensity threshold you want to use. For example, in a manufacturing company that\ntries to detect defective products, the ratio of defective products is usually well\nknown. Say it is equal to 4%. You then set the density threshold to be the value that\nresults in having 4% of the instances located in areas below that threshold density. If\nyou notice that you get too many false positives (i.e., perfectly good products that are\nflagged as defective), you can lower the threshold. Conversely, if you have too many\nfalse negatives (i.e., defective products that the system does not flag as defective), you\ncan increase the threshold. This is the usual precision/recall trade-off (see Chapter 3). Here is how you would identify the outliers using the fourth percentile lowest density\nas the threshold (i.e., approximately 4% of the instances will be flagged as anomalies):\ndensities = gm.score_samples(X)\ndensity_threshold = np.percentile(densities, 4)\nanomalies = X[densities < density_threshold]\nFigure 9-19 represents these anomalies as stars. Figure 9-19. Anomaly detection using a Gaussian mixture model | Chapter 9: Unsupervised Learning Techniques\nA closely related task is novelty detection: it differs from anomaly detection in that the\nalgorithm is assumed to be trained on a \u201cclean\u201d dataset, uncontaminated by outliers,\nwhereas anomaly detection does not make this assumption. Indeed, outlier detection\nis often used to clean up a dataset."
  },
  {
    "id": 170,
    "content": "Gaussian mixture models try to fit all the data, including the outli\u2010\ners, so if you have too many of them, this will bias the model\u2019s view\nof \u201cnormality,\u201d and some outliers may wrongly be considered as\nnormal. If this happens, you can try to fit the model once, use it to\ndetect and remove the most extreme outliers, then fit the model\nagain on the cleaned-up dataset. Another approach is to use robust\ncovariance estimation methods (see the EllipticEnvelope class). Just like K-Means, the GaussianMixture algorithm requires you to specify the num\u2010\nber of clusters. So, how can you find it? Selecting the Number of Clusters\nWith K-Means, you could use the inertia or the silhouette score to select the appro\u2010\npriate number of clusters. But with Gaussian mixtures, it is not possible to use these\nmetrics because they are not reliable when the clusters are not spherical or have dif\u2010\nferent sizes. Instead, you can try to find the model that minimizes a theoretical infor\u2010\nmation criterion, such as the Bayesian information criterion (BIC) or the Akaike\ninformation criterion (AIC), defined in Equation 9-1. Equation 9-1. Bayesian information criterion (BIC) and Akaike information\ncriterion (AIC)\nBIC =\nlog m p \u22122 log L\nAIC =\n2p \u22122 log L\nIn these equations:\n\u2022 m is the number of instances, as always. \u2022 p is the number of parameters learned by the model. \u2022 L is the maximized value of the likelihood function of the model. Both the BIC and the AIC penalize models that have more parameters to learn (e.g.,\nmore clusters) and reward models that fit the data well. They often end up selecting\nthe same model. When they differ, the model selected by the BIC tends to be simpler\nGaussian Mixtures | (fewer parameters) than the one selected by the AIC, but tends to not fit the data\nquite as well (this is especially true for larger datasets). Likelihood Function\nThe terms \u201cprobability\u201d and \u201clikelihood\u201d are often used interchangeably in the\nEnglish language, but they have very different meanings in statistics. Given a statisti\u2010\ncal model with some parameters \u03b8, the word \u201cprobability\u201d is used to describe how\nplausible a future outcome x is (knowing the parameter values \u03b8), while the word\n\u201clikelihood\u201d is used to describe how plausible a particular set of parameter values \u03b8\nare, after the outcome x is known. Consider a 1D mixture model of two Gaussian distributions centered at \u20134 and +1. For simplicity, this toy model has a single parameter \u03b8 that controls the standard devi\u2010\nations of both distributions. The top-left contour plot in Figure 9-20 shows the entire\nmodel f(x; \u03b8) as a function of both x and \u03b8. To estimate the probability distribution of\na future outcome x, you need to set the model parameter \u03b8. For example, if you set \u03b8\nto 1.3 (the horizontal line), you get the probability density function f(x; \u03b8=1.3) shown\nin the lower-left plot."
  },
  {
    "id": 171,
    "content": "Say you want to estimate the probability that x will fall between\n\u20132 and +2. You must calculate the integral of the PDF on this range (i.e., the surface of\nthe shaded region). But what if you don\u2019t know \u03b8, and instead if you have observed a\nsingle instance x=2.5 (the vertical line in the upper-left plot)? In this case, you get the\nlikelihood function \u2112(\u03b8|x=2.5)=f(x=2.5; \u03b8), represented in the upper-right plot. Figure 9-20. A model\u2019s parametric function (top left), and some derived functions: a PDF\n(lower left), a likelihood function (top right), and a log likelihood function (lower right) | Chapter 9: Unsupervised Learning Techniques\nIn short, the PDF is a function of x (with \u03b8 fixed), while the likelihood function is a\nfunction of \u03b8 (with x fixed). It is important to understand that the likelihood function\nis not a probability distribution: if you integrate a probability distribution over all\npossible values of x, you always get 1; but if you integrate the likelihood function over\nall possible values of \u03b8, the result can be any positive value. Given a dataset X, a common task is to try to estimate the most likely values for the\nmodel parameters. To do this, you must find the values that maximize the likelihood\nfunction, given X. In this example, if you have observed a single instance x=2.5, the\nmaximum likelihood estimate (MLE) of \u03b8 is \u03b8=1.5. If a prior probability distribution g\nover \u03b8 exists, it is possible to take it into account by maximizing \u2112(\u03b8|x)g(\u03b8) rather\nthan just maximizing \u2112(\u03b8|x). This is called maximum a-posteriori (MAP) estimation. Since MAP constrains the parameter values, you can think of it as a regularized ver\u2010\nsion of MLE. Notice that maximizing the likelihood function is equivalent to maximizing its loga\u2010\nrithm (represented in the lower-righthand plot in Figure 9-20). Indeed the logarithm\nis a strictly increasing function, so if \u03b8 maximizes the log likelihood, it also maximizes\nthe likelihood. It turns out that it is generally easier to maximize the log likelihood. For example, if you observed several independent instances x(1) to x(m), you would\nneed to find the value of \u03b8 that maximizes the product of the individual likelihood\nfunctions. But it is equivalent, and much simpler, to maximize the sum (not the prod\u2010\nuct) of the log likelihood functions, thanks to the magic of the logarithm which con\u2010\nverts products into sums: log(ab)=log(a)+log(b). Once you have estimated \u03b8, the value of \u03b8 that maximizes the likelihood function,\nthen you are ready to compute L = \u2112\u03b8, \ufffd, which is the value used to compute the\nAIC and BIC; you can think of it as a measure of how well the model fits the data."
  },
  {
    "id": 172,
    "content": "To compute the BIC and AIC, call the bic() and aic() methods:\n>>> gm.bic(X)\n8189.74345832983\n>>> gm.aic(X)\n8102.518178214792\nFigure 9-21 shows the BIC for different numbers of clusters k. As you can see, both\nthe BIC and the AIC are lowest when k=3, so it is most likely the best choice. Note\nthat we could also search for the best value for the covariance_type hyperparameter. For example, if it is \"spherical\" rather than \"full\", then the model has significantly\nfewer parameters to learn, but it does not fit the data as well. Gaussian Mixtures | Figure 9-21. AIC and BIC for different numbers of clusters k\nBayesian Gaussian Mixture Models\nRather than manually searching for the optimal number of clusters, you can use the\nBayesianGaussianMixture class, which is capable of giving weights equal (or close)\nto zero to unnecessary clusters. Set the number of clusters n_components to a value\nthat you have good reason to believe is greater than the optimal number of clusters\n(this assumes some minimal knowledge about the problem at hand), and the algo\u2010\nrithm will eliminate the unnecessary clusters automatically. For example, let\u2019s set the\nnumber of clusters to 10 and see what happens:\n>>> from sklearn.mixture import BayesianGaussianMixture\n>>> bgm = BayesianGaussianMixture(n_components=10, n_init=10)\n>>> bgm.fit(X)\n>>> np.round(bgm.weights_, 2)\narray([0.4 , 0.21, 0.4 , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]) Perfect: the algorithm automatically detected that only three clusters are needed, and\nthe resulting clusters are almost identical to the ones in Figure 9-17. In this model, the cluster parameters (including the weights, means, and covariance\nmatrices) are not treated as fixed model parameters anymore, but as latent random\nvariables, like the cluster assignments (see Figure 9-22). So z now includes both the\ncluster parameters and the cluster assignments. The Beta distribution is commonly used to model random variables whose values lie\nwithin a fixed range. In this case, the range is from 0 to 1. The Stick-Breaking Process\n(SBP) is best explained through an example: suppose \u03a6=[0.3, 0.6, 0.5,\u2026], then 30% of\nthe instances will be assigned to cluster 0, then 60% of the remaining instances will be\nassigned to cluster 1, then 50% of the remaining instances will be assigned to cluster\n2, and so on. This process is a good model for datasets where new instances are more\nlikely to join large clusters than small clusters (e.g., people are more likely to move to\nlarger cities). If the concentration \u03b1 is high, then \u03a6 values will likely be close to 0, and\nthe SBP generate many clusters. Conversely, if the concentration is low, then \u03a6 values | Chapter 9: Unsupervised Learning Techniques\nwill likely be close to 1, and there will be few clusters. Finally, the Wishart distribution\nis used to sample covariance matrices: the parameters d and V control the distribu\u2010\ntion of cluster shapes. Figure 9-22."
  },
  {
    "id": 173,
    "content": "Bayesian Gaussian mixture model\nPrior knowledge about the latent variables z can be encoded in a probability distribu\u2010\ntion p(z) called the prior. For example, we may have a prior belief that the clusters are\nlikely to be few (low concentration), or conversely, that they are likely to be plentiful\n(high concentration). This prior belief about the number of clusters can be adjusted\nusing the weight_concentration_prior hyperparameter. Setting it to 0.01 or 10,000\ngives very different clusterings (see Figure 9-23). The more data we have, however,\nthe less the priors matter. In fact, to plot diagrams with such large differences, you\nmust use very strong priors and little data. Figure 9-23. Using different concentration priors on the same data results in different\nnumbers of clusters\nGaussian Mixtures | Bayes\u2019 theorem (Equation 9-2) tells us how to update the probability distribution over\nthe latent variables after we observe some data X. It computes the posterior distribu\u2010\ntion p(z|X), which is the conditional probability of z given X. Equation 9-2. Bayes\u2019 theorem\np z X = posterior = likelihood\u00a0\u00d7\u00a0prior\nevidence\n= p X z p z\np X\nUnfortunately, in a Gaussian mixture model (and many other problems), the denomi\u2010\nnator p(x) is intractable, as it requires integrating over all the possible values of z\n(Equation 9-3), which would require considering all possible combinations of cluster\nparameters and cluster assignments. Equation 9-3. The evidence p(X) is often intractable\np X =\u222bp X z p z dz\nThis intractability is one of the central problems in Bayesian statistics, and there are\nseveral approaches to solving it. One of them is variational inference, which picks a\nfamily of distributions q(z; \u03bb) with its own variational parameters \u03bb (lambda), then\noptimizes these parameters to make q(z) a good approximation of p(z|X). This is\nachieved by finding the value of \u03bb that minimizes the KL divergence from q(z) to\np(z|X), noted DKL(q\u2016p). The KL divergence equation is shown in Equation 9-4, and it\ncan be rewritten as the log of the evidence (log p(X)) minus the evidence lower bound\n(ELBO). Since the log of the evidence does not depend on q, it is a constant term, so\nminimizing the KL divergence just requires maximizing the ELBO. Equation 9-4. KL divergence from q(z) to p(z|X)\nDKL q \u2225p = \ufffdq log\nq z\np z\nX\n= \ufffdq log q z \u2212log p z\nX\n= \ufffdq log q z \u2212log p z, X\np X\n= \ufffdq log q z \u2212log p z, X + log p X\n= \ufffdq log q z\n\u2212\ufffdq log p z, X\n+ \ufffdq log p X\n= \ufffdq log p X\n\u2212\ufffdq log p z, X\n\u2212\ufffdq log q z\n=\nlog p X \u2212ELBO\nwhere ELBO = \ufffdq log p z, X\n\u2212\ufffdq log q z | Chapter 9: Unsupervised Learning Techniques\nIn practice, there are different techniques to maximize the ELBO."
  },
  {
    "id": 174,
    "content": "In mean field varia\u2010\ntional inference, it is necessary to pick the family of distributions q(z; \u03bb) and the prior\np(z) very carefully to ensure that the equation for the ELBO simplifies to a form that\ncan be computed. Unfortunately, there is no general way to do this. Picking the right\nfamily of distributions and the right prior depends on the task and requires some\nmathematical skills. For example, the distributions and lower-bound equations used\nin Scikit-Learn\u2019s BayesianGaussianMixture class are presented in the documenta\u2010\ntion. From these equations it is possible to derive update equations for the cluster\nparameters and assignment variables: these are then used very much like in the\nExpectation-Maximization algorithm. In fact, the computational complexity of the\nBayesianGaussianMixture class is similar to that of the GaussianMixture class (but\ngenerally significantly slower). A simpler approach to maximizing the ELBO is called\nblack box stochastic variational inference (BBSVI): at each iteration, a few samples are\ndrawn from q, and they are used to estimate the gradients of the ELBO with regard to\nthe variational parameters \u03bb, which are then used in a gradient ascent step. This\napproach makes it possible to use Bayesian inference with any kind of model (pro\u2010\nvided it is differentiable), even deep neural networks; using Bayesian inference with\ndeep neural networks is called Bayesian Deep Learning. If you want to dive deeper into Bayesian statistics, check out the\nbook Bayesian Data Analysis by Andrew Gelman et al. (Chapman\n& Hall). Gaussian mixture models work great on clusters with ellipsoidal shapes, but if you try\nto fit a dataset with different shapes, you may have bad surprises. For example, let\u2019s\nsee what happens if we use a Bayesian Gaussian mixture model to cluster the moons\ndataset (see Figure 9-24). Figure 9-24. Fitting a Gaussian mixture to nonellipsoidal clusters\nGaussian Mixtures | Oops! The algorithm desperately searched for ellipsoids, so it found eight different\nclusters instead of two. The density estimation is not too bad, so this model could\nperhaps be used for anomaly detection, but it failed to identify the two moons. Let\u2019s\nnow look at a few clustering algorithms capable of dealing with arbitrarily shaped\nclusters. Other Algorithms for Anomaly and Novelty Detection\nScikit-Learn implements other algorithms dedicated to anomaly detection or novelty\ndetection:\nPCA (and other dimensionality reduction techniques with an inverse_transform()\nmethod)\nIf you compare the reconstruction error of a normal instance with the recon\u2010\nstruction error of an anomaly, the latter will usually be much larger. This is a sim\u2010\nple and often quite efficient anomaly detection approach (see this chapter\u2019s\nexercises for an application of this approach). Fast-MCD (minimum covariance determinant)\nImplemented by the EllipticEnvelope class, this algorithm is useful for outlier\ndetection, in particular to clean up a dataset. It assumes that the normal instances\n(inliers) are generated from a single Gaussian distribution (not a mixture). It also\nassumes that the dataset is contaminated with outliers that were not generated\nfrom this Gaussian distribution."
  },
  {
    "id": 175,
    "content": "When the algorithm estimates the parameters of\nthe Gaussian distribution (i.e., the shape of the elliptic envelope around the inli\u2010\ners), it is careful to ignore the instances that are most likely outliers. This techni\u2010\nque gives a better estimation of the elliptic envelope and thus makes the\nalgorithm better at identifying the outliers. Isolation Forest\nThis is an efficient algorithm for outlier detection, especially in high-dimensional\ndatasets. The algorithm builds a Random Forest in which each Decision Tree is\ngrown randomly: at each node, it picks a feature randomly, then it picks a ran\u2010\ndom threshold value (between the min and max values) to split the dataset in\ntwo. The dataset gradually gets chopped into pieces this way, until all instances\nend up isolated from the other instances. Anomalies are usually far from other\ninstances, so on average (across all the Decision Trees) they tend to get isolated in\nfewer steps than normal instances. Local Outlier Factor (LOF)\nThis algorithm is also good for outlier detection. It compares the density of\ninstances around a given instance to the density around its neighbors. An anom\u2010\naly is often more isolated than its k nearest neighbors. | Chapter 9: Unsupervised Learning Techniques\nOne-class SVM\nThis algorithm is better suited for novelty detection. Recall that a kernelized\nSVM classifier separates two classes by first (implicitly) mapping all the instances\nto a high-dimensional space, then separating the two classes using a linear SVM\nclassifier within this high-dimensional space (see Chapter 5). Since we just have\none class of instances, the one-class SVM algorithm instead tries to separate the\ninstances in high-dimensional space from the origin. In the original space, this\nwill correspond to finding a small region that encompasses all the instances. If a\nnew instance does not fall within this region, it is an anomaly. There are a few\nhyperparameters to tweak: the usual ones for a kernelized SVM, plus a margin\nhyperparameter that corresponds to the probability of a new instance being mis\u2010\ntakenly considered as novel when it is in fact normal. It works great, especially\nwith high-dimensional datasets, but like all SVMs it does not scale to large\ndatasets. Exercises\n1. How would you define clustering? Can you name a few clustering algorithms? 2. What are some of the main applications of clustering algorithms? 3. Describe two techniques to select the right number of clusters when using\nK-Means. 4. What is label propagation? Why would you implement it, and how? 5. Can you name two clustering algorithms that can scale to large datasets? And\ntwo that look for regions of high density? 6. Can you think of a use case where active learning would be useful? How would\nyou implement it? 7. What is the difference between anomaly detection and novelty detection? 8. What is a Gaussian mixture? What tasks can you use it for? 9. Can you name two techniques to find the right number of clusters when using a\nGaussian mixture model? 10."
  },
  {
    "id": 176,
    "content": "The classic Olivetti faces dataset contains 400 grayscale 64 \u00d7 64\u2013pixel images of\nfaces. Each image is flattened to a 1D vector of size 4,096. 40 different people\nwere photographed (10 times each), and the usual task is to train a model that\ncan predict which person is represented in each picture. Load the dataset using\nthe sklearn.datasets.fetch_olivetti_faces() function, then split it into a\ntraining set, a validation set, and a test set (note that the dataset is already scaled\nbetween 0 and 1). Since the dataset is quite small, you probably want to use strati\u2010\nfied sampling to ensure that there are the same number of images per person in\neach set. Next, cluster the images using K-Means, and ensure that you have a\nExercises | good number of clusters (using one of the techniques discussed in this chapter). Visualize the clusters: do you see similar faces in each cluster? 11. Continuing with the Olivetti faces dataset, train a classifier to predict which per\u2010\nson is represented in each picture, and evaluate it on the validation set. Next, use\nK-Means as a dimensionality reduction tool, and train a classifier on the reduced\nset. Search for the number of clusters that allows the classifier to get the best per\u2010\nformance: what performance can you reach? What if you append the features\nfrom the reduced set to the original features (again, searching for the best num\u2010\nber of clusters)? 12. Train a Gaussian mixture model on the Olivetti faces dataset. To speed up the\nalgorithm, you should probably reduce the dataset\u2019s dimensionality (e.g., use\nPCA, preserving 99% of the variance). Use the model to generate some new faces\n(using the sample() method), and visualize them (if you used PCA, you will need\nto use its inverse_transform() method). Try to modify some images (e.g.,\nrotate, flip, darken) and see if the model can detect the anomalies (i.e., compare\nthe output of the score_samples() method for normal images and for anoma\u2010\nlies). 13. Some dimensionality reduction techniques can also be used for anomaly detec\u2010\ntion. For example, take the Olivetti faces dataset and reduce it with PCA, preserv\u2010\ning 99% of the variance. Then compute the reconstruction error for each image. Next, take some of the modified images you built in the previous exercise, and\nlook at their reconstruction error: notice how much larger the reconstruction\nerror is. If you plot a reconstructed image, you will see why: it tries to reconstruct\na normal face. Solutions to these exercises are available in Appendix A. | Chapter 9: Unsupervised Learning Techniques\nPART II\nNeural Networks and Deep Learning\n1 You can get the best of both worlds by being open to biological inspirations without being afraid to create\nbiologically unrealistic models, as long as they work well. CHAPTER 10\nIntroduction to Artificial Neural Networks\nwith Keras\nBirds inspired us to fly, burdock plants inspired Velcro, and nature has inspired\ncountless more inventions."
  },
  {
    "id": 177,
    "content": "It seems only logical, then, to look at the brain\u2019s architec\u2010\nture for inspiration on how to build an intelligent machine. This is the logic that\nsparked artificial neural networks (ANNs): an ANN is a Machine Learning model\ninspired by the networks of biological neurons found in our brains. However,\nalthough planes were inspired by birds, they don\u2019t have to flap their wings. Similarly,\nANNs have gradually become quite different from their biological cousins. Some\nresearchers even argue that we should drop the biological analogy altogether (e.g., by\nsaying \u201cunits\u201d rather than \u201cneurons\u201d), lest we restrict our creativity to biologically\nplausible systems.1\nANNs are at the very core of Deep Learning. They are versatile, powerful, and scala\u2010\nble, making them ideal to tackle large and highly complex Machine Learning tasks\nsuch as classifying billions of images (e.g., Google Images), powering speech recogni\u2010\ntion services (e.g., Apple\u2019s Siri), recommending the best videos to watch to hundreds\nof millions of users every day (e.g., YouTube), or learning to beat the world champion\nat the game of Go (DeepMind\u2019s AlphaGo). The first part of this chapter introduces artificial neural networks, starting with a\nquick tour of the very first ANN architectures and leading up to Multilayer Percep\u2010\ntrons (MLPs), which are heavily used today (other architectures will be explored in\nthe next chapters). In the second part, we will look at how to implement neural net\u2010\nworks using the popular Keras API. This is a beautifully designed and simple high- 2 Warren S. McCulloch and Walter Pitts, \u201cA Logical Calculus of the Ideas Immanent in Nervous Activity,\u201d The\nBulletin of Mathematical Biology 5, no. 4 (1943): 115\u2013113. level API for building, training, evaluating, and running neural networks. But don\u2019t\nbe fooled by its simplicity: it is expressive and flexible enough to let you build a wide\nvariety of neural network architectures. In fact, it will probably be sufficient for most\nof your use cases. And should you ever need extra flexibility, you can always write\ncustom Keras components using its lower-level API, as we will see in Chapter 12. But first, let\u2019s go back in time to see how artificial neural networks came to be! From Biological to Artificial Neurons\nSurprisingly, ANNs have been around for quite a while: they were first introduced\nback in 1943 by the neurophysiologist Warren McCulloch and the mathematician\nWalter Pitts. In their landmark paper2 \u201cA Logical Calculus of Ideas Immanent in\nNervous Activity,\u201d McCulloch and Pitts presented a simplified computational model\nof how biological neurons might work together in animal brains to perform complex\ncomputations using propositional logic. This was the first artificial neural network\narchitecture. Since then many other architectures have been invented, as we will see. The early successes of ANNs led to the widespread belief that we would soon be con\u2010\nversing with truly intelligent machines. When it became clear in the 1960s that this\npromise would go unfulfilled (at least for quite a while), funding flew elsewhere, and\nANNs entered a long winter."
  },
  {
    "id": 178,
    "content": "In the early 1980s, new architectures were invented and\nbetter training techniques were developed, sparking a revival of interest in connec\u2010\ntionism (the study of neural networks). But progress was slow, and by the 1990s other\npowerful Machine Learning techniques were invented, such as Support Vector\nMachines (see Chapter 5). These techniques seemed to offer better results and stron\u2010\nger theoretical foundations than ANNs, so once again the study of neural networks\nwas put on hold. We are now witnessing yet another wave of interest in ANNs. Will this wave die out\nlike the previous ones did? Well, here are a few good reasons to believe that this time\nis different and that the renewed interest in ANNs will have a much more profound\nimpact on our lives:\n\u2022 There is now a huge quantity of data available to train neural networks, and\nANNs frequently outperform other ML techniques on very large and complex\nproblems. \u2022 The tremendous increase in computing power since the 1990s now makes it pos\u2010\nsible to train large neural networks in a reasonable amount of time. This is in\npart due to Moore\u2019s law (the number of components in integrated circuits has | Chapter 10: Introduction to Artificial Neural Networks with Keras\n3 They are not actually attached, just so close that they can very quickly exchange chemical signals. doubled about every 2 years over the last 50 years), but also thanks to the gaming\nindustry, which has stimulated the production of powerful GPU cards by the mil\u2010\nlions. Moreover, cloud platforms have made this power accessible to everyone. \u2022 The training algorithms have been improved. To be fair they are only slightly dif\u2010\nferent from the ones used in the 1990s, but these relatively small tweaks have had\na huge positive impact. \u2022 Some theoretical limitations of ANNs have turned out to be benign in practice. For example, many people thought that ANN training algorithms were doomed\nbecause they were likely to get stuck in local optima, but it turns out that this is\nrather rare in practice (and when it is the case, they are usually fairly close to the\nglobal optimum). \u2022 ANNs seem to have entered a virtuous circle of funding and progress. Amazing\nproducts based on ANNs regularly make the headline news, which pulls more\nand more attention and funding toward them, resulting in more and more pro\u2010\ngress and even more amazing products. Biological Neurons\nBefore we discuss artificial neurons, let\u2019s take a quick look at a biological neuron (rep\u2010\nresented in Figure 10-1). It is an unusual-looking cell mostly found in animal brains. It\u2019s composed of a cell body containing the nucleus and most of the cell\u2019s complex\ncomponents, many branching extensions called dendrites, plus one very long exten\u2010\nsion called the axon. The axon\u2019s length may be just a few times longer than the cell\nbody, or up to tens of thousands of times longer."
  },
  {
    "id": 179,
    "content": "Near its extremity the axon splits off\ninto many branches called telodendria, and at the tip of these branches are minuscule\nstructures called synaptic terminals (or simply synapses), which are connected to the\ndendrites or cell bodies of other neurons.3 Biological neurons produce short electrical\nimpulses called action potentials (APs, or just signals) which travel along the axons\nand make the synapses release chemical signals called neurotransmitters. When a neu\u2010\nron receives a sufficient amount of these neurotransmitters within a few milliseconds,\nit fires its own electrical impulses (actually, it depends on the neurotransmitters, as\nsome of them inhibit the neuron from firing). From Biological to Artificial Neurons | 4 Image by Bruce Blaus (Creative Commons 3.0). Reproduced from \n5 In the context of Machine Learning, the phrase \u201cneural networks\u201d generally refers to ANNs, not BNNs. Figure 10-1. Biological neuron4\nThus, individual biological neurons seem to behave in a rather simple way, but they\nare organized in a vast network of billions, with each neuron typically connected to\nthousands of other neurons. Highly complex computations can be performed by a\nnetwork of fairly simple neurons, much like a complex anthill can emerge from the\ncombined efforts of simple ants. The architecture of biological neural networks\n(BNNs)5 is still the subject of active research, but some parts of the brain have been\nmapped, and it seems that neurons are often organized in consecutive layers, espe\u2010\ncially in the cerebral cortex (i.e., the outer layer of your brain), as shown in\nFigure 10-2. | Chapter 10: Introduction to Artificial Neural Networks with Keras\n6 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from \ndia.org/wiki/Cerebral_cortex. Figure 10-2. Multiple layers in a biological neural network (human cortex)6\nLogical Computations with Neurons\nMcCulloch and Pitts proposed a very simple model of the biological neuron, which\nlater became known as an artificial neuron: it has one or more binary (on/off) inputs\nand one binary output. The artificial neuron activates its output when more than a\ncertain number of its inputs are active. In their paper, they showed that even with\nsuch a simplified model it is possible to build a network of artificial neurons that\ncomputes any logical proposition you want. To see how such a network works, let\u2019s\nbuild a few ANNs that perform various logical computations (see Figure 10-3),\nassuming that a neuron is activated when at least two of its inputs are active. Figure 10-3. ANNs performing simple logical computations\nFrom Biological to Artificial Neurons | Let\u2019s see what these networks do:\n\u2022 The first network on the left is the identity function: if neuron A is activated,\nthen neuron C gets activated as well (since it receives two input signals from neu\u2010\nron A); but if neuron A is off, then neuron C is off as well."
  },
  {
    "id": 180,
    "content": "\u2022 The second network performs a logical AND: neuron C is activated only when\nboth neurons A and B are activated (a single input signal is not enough to acti\u2010\nvate neuron C). \u2022 The third network performs a logical OR: neuron C gets activated if either neu\u2010\nron A or neuron B is activated (or both). \u2022 Finally, if we suppose that an input connection can inhibit the neuron\u2019s activity\n(which is the case with biological neurons), then the fourth network computes a\nslightly more complex logical proposition: neuron C is activated only if neuron A\nis active and neuron B is off. If neuron A is active all the time, then you get a\nlogical NOT: neuron C is active when neuron B is off, and vice versa. You can imagine how these networks can be combined to compute complex logical\nexpressions (see the exercises at the end of the chapter for an example). The Perceptron\nThe Perceptron is one of the simplest ANN architectures, invented in 1957 by Frank\nRosenblatt. It is based on a slightly different artificial neuron (see Figure 10-4) called\na threshold logic unit (TLU), or sometimes a linear threshold unit (LTU). The inputs\nand output are numbers (instead of binary on/off values), and each input connection\nis associated with a weight. The TLU computes a weighted sum of its inputs (z = w1 x1\n+ w2 x2 + \u22ef + wn xn = x\u22ba w), then applies a step function to that sum and outputs the\nresult: hw(x) = step(z), where z = x\u22ba w.\nFigure 10-4. Threshold logic unit: an artificial neuron which computes a weighted sum\nof its inputs then applies a step function | Chapter 10: Introduction to Artificial Neural Networks with Keras\n7 The name Perceptron is sometimes used to mean a tiny network with a single TLU. The most common step function used in Perceptrons is the Heaviside step function\n(see Equation 10-1). Sometimes the sign function is used instead. Equation 10-1. Common step functions used in Perceptrons (assuming threshold =\n0)\nheaviside z = 0 if z < 0\n1 if z \u22650\nsgn z =\n\u22121 if z < 0 if z = 0\n+1 if z > 0\nA single TLU can be used for simple linear binary classification. It computes a linear\ncombination of the inputs, and if the result exceeds a threshold, it outputs the posi\u2010\ntive class. Otherwise it outputs the negative class (just like a Logistic Regression or\nlinear SVM classifier). You could, for example, use a single TLU to classify iris flowers\nbased on petal length and width (also adding an extra bias feature x0 = 1, just like we\ndid in previous chapters). Training a TLU in this case means finding the right values\nfor w0, w1, and w2 (the training algorithm is discussed shortly). A Perceptron is simply composed of a single layer of TLUs,7 with each TLU connected\nto all the inputs."
  },
  {
    "id": 181,
    "content": "When all the neurons in a layer are connected to every neuron in the\nprevious layer (i.e., its input neurons), the layer is called a fully connected layer, or a\ndense layer. The inputs of the Perceptron are fed to special passthrough neurons\ncalled input neurons: they output whatever input they are fed. All the input neurons\nform the input layer. Moreover, an extra bias feature is generally added (x0 = 1): it is\ntypically represented using a special type of neuron called a bias neuron, which out\u2010\nputs 1 all the time. A Perceptron with two inputs and three outputs is represented in\nFigure 10-5. This Perceptron can classify instances simultaneously into three different\nbinary classes, which makes it a multioutput classifier. From Biological to Artificial Neurons | Figure 10-5. Architecture of a Perceptron with two input neurons, one bias neuron, and\nthree output neurons\nThanks to the magic of linear algebra, Equation 10-2 makes it possible to efficiently\ncompute the outputs of a layer of artificial neurons for several instances at once. Equation 10-2. Computing the outputs of a fully connected layer\nhW, b X = \u03d5 XW + b\nIn this equation:\n\u2022 As always, X represents the matrix of input features. It has one row per instance\nand one column per feature. \u2022 The weight matrix W contains all the connection weights except for the ones\nfrom the bias neuron. It has one row per input neuron and one column per artifi\u2010\ncial neuron in the layer. \u2022 The bias vector b contains all the connection weights between the bias neuron\nand the artificial neurons. It has one bias term per artificial neuron. \u2022 The function \u03d5 is called the activation function: when the artificial neurons are\nTLUs, it is a step function (but we will discuss other activation functions shortly). So, how is a Perceptron trained? The Perceptron training algorithm proposed by\nRosenblatt was largely inspired by Hebb\u2019s rule. In his 1949 book The Organization of\nBehavior (Wiley), Donald Hebb suggested that when a biological neuron triggers\nanother neuron often, the connection between these two neurons grows stronger. Sie\u2010\ngrid L\u00f6wel later summarized Hebb\u2019s idea in the catchy phrase, \u201cCells that fire\ntogether, wire together\u201d; that is, the connection weight between two neurons tends to\nincrease when they fire simultaneously. This rule later became known as Hebb\u2019s rule\n(or Hebbian learning). Perceptrons are trained using a variant of this rule that takes\ninto account the error made by the network when it makes a prediction; the | Chapter 10: Introduction to Artificial Neural Networks with Keras\n8 Note that this solution is not unique: when data points are linearly separable, there is an infinity of hyper\u2010\nplanes that can separate them. Perceptron learning rule reinforces connections that help reduce the error. More\nspecifically, the Perceptron is fed one training instance at a time, and for each\ninstance it makes its predictions."
  },
  {
    "id": 182,
    "content": "For every output neuron that produced a wrong\nprediction, it reinforces the connection weights from the inputs that would have con\u2010\ntributed to the correct prediction. The rule is shown in Equation 10-3. Equation 10-3. Perceptron learning rule (weight update)\nwi, j\nnext step = wi, j + \u03b7 yj \u2212y j xi\nIn this equation:\n\u2022 wi, j is the connection weight between the ith input neuron and the jth output\nneuron. \u2022 xi is the ith input value of the current training instance. \u2022 y j is the output of the jth output neuron for the current training instance. \u2022 yj is the target output of the jth output neuron for the current training instance. \u2022 \u03b7 is the learning rate. The decision boundary of each output neuron is linear, so Perceptrons are incapable\nof learning complex patterns (just like Logistic Regression classifiers). However, if the\ntraining instances are linearly separable, Rosenblatt demonstrated that this algorithm\nwould converge to a solution.8 This is called the Perceptron convergence theorem. Scikit-Learn provides a Perceptron class that implements a single-TLU network. It\ncan be used pretty much as you would expect\u2014for example, on the iris dataset (intro\u2010\nduced in Chapter 4):\nimport numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import Perceptron\niris = load_iris()\nX = iris.data[:, (2, 3)] # petal length, petal width\ny = (iris.target == 0).astype(np.int) # Iris setosa? per_clf = Perceptron()\nper_clf.fit(X, y)\ny_pred = per_clf.predict([[2, 0.5]])\nFrom Biological to Artificial Neurons | You may have noticed that the Perceptron learning algorithm strongly resembles Sto\u2010\nchastic Gradient Descent. In fact, Scikit-Learn\u2019s Perceptron class is equivalent to\nusing an SGDClassifier with the following hyperparameters: loss=\"perceptron\",\nlearning_rate=\"constant\", eta0=1 (the learning rate), and penalty=None (no\nregularization). Note that contrary to Logistic Regression classifiers, Perceptrons do not output a class\nprobability; rather, they make predictions based on a hard threshold. This is one rea\u2010\nson to prefer Logistic Regression over Perceptrons. In their 1969 monograph Perceptrons, Marvin Minsky and Seymour Papert highligh\u2010\nted a number of serious weaknesses of Perceptrons\u2014in particular, the fact that they\nare incapable of solving some trivial problems (e.g., the Exclusive OR (XOR) classifi\u2010\ncation problem; see the left side of Figure 10-6). This is true of any other linear classi\u2010\nfication model (such as Logistic Regression classifiers), but researchers had expected\nmuch more from Perceptrons, and some were so disappointed that they dropped\nneural networks altogether in favor of higher-level problems such as logic, problem\nsolving, and search. It turns out that some of the limitations of Perceptrons can be eliminated by stacking\nmultiple Perceptrons. The resulting ANN is called a Multilayer Perceptron (MLP). An\nMLP can solve the XOR problem, as you can verify by computing the output of the\nMLP represented on the right side of Figure 10-6: with inputs (0, 0) or (1, 1), the net\u2010\nwork outputs 0, and with inputs (0, 1) or (1, 0) it outputs 1."
  },
  {
    "id": 183,
    "content": "All connections have a\nweight equal to 1, except the four connections where the weight is shown. Try verify\u2010\ning that this network indeed solves the XOR problem! Figure 10-6. XOR classification problem and an MLP that solves it | Chapter 10: Introduction to Artificial Neural Networks with Keras\n9 In the 1990s, an ANN with more than two hidden layers was considered deep. Nowadays, it is common to see\nANNs with dozens of layers, or even hundreds, so the definition of \u201cdeep\u201d is quite fuzzy. The Multilayer Perceptron and Backpropagation\nAn MLP is composed of one (passthrough) input layer, one or more layers of TLUs,\ncalled hidden layers, and one final layer of TLUs called the output layer (see\nFigure 10-7). The layers close to the input layer are usually called the lower layers, and\nthe ones close to the outputs are usually called the upper layers. Every layer except the\noutput layer includes a bias neuron and is fully connected to the next layer. Figure 10-7. Architecture of a Multilayer Perceptron with two inputs, one hidden layer of\nfour neurons, and three output neurons (the bias neurons are shown here, but usually\nthey are implicit)\nThe signal flows only in one direction (from the inputs to the out\u2010\nputs), so this architecture is an example of a feedforward neural net\u2010\nwork (FNN). When an ANN contains a deep stack of hidden layers,9 it is called a deep neural net\u2010\nwork (DNN). The field of Deep Learning studies DNNs, and more generally models\ncontaining deep stacks of computations. Even so, many people talk about Deep\nLearning whenever neural networks are involved (even shallow ones). For many years researchers struggled to find a way to train MLPs, without success. But in 1986, David Rumelhart, Geoffrey Hinton, and Ronald Williams published a\nFrom Biological to Artificial Neurons | 10 David Rumelhart et al. \u201cLearning Internal Representations by Error Propagation,\u201d (Defense Technical Infor\u2010\nmation Center technical report, September 1985). 11 This technique was actually independently invented several times by various researchers in different fields,\nstarting with Paul Werbos in 1974.\ngroundbreaking paper10 that introduced the backpropagation training algorithm,\nwhich is still used today. In short, it is Gradient Descent (introduced in Chapter 4)\nusing an efficient technique for computing the gradients automatically:11 in just two\npasses through the network (one forward, one backward), the backpropagation algo\u2010\nrithm is able to compute the gradient of the network\u2019s error with regard to every sin\u2010\ngle model parameter. In other words, it can find out how each connection weight and\neach bias term should be tweaked in order to reduce the error. Once it has these gra\u2010\ndients, it just performs a regular Gradient Descent step, and the whole process is\nrepeated until the network converges to the solution. Automatically computing gradients is called automatic differentia\u2010\ntion, or autodiff. There are various autodiff techniques, with differ\u2010\nent pros and cons. The one used by backpropagation is called\nreverse-mode autodiff."
  },
  {
    "id": 184,
    "content": "It is fast and precise, and is well suited when\nthe function to differentiate has many variables (e.g., connection\nweights) and few outputs (e.g., one loss). If you want to learn more\nabout autodiff, check out Appendix D.\nLet\u2019s run through this algorithm in a bit more detail:\n\u2022 It handles one mini-batch at a time (for example, containing 32 instances each),\nand it goes through the full training set multiple times. Each pass is called an\nepoch. \u2022 Each mini-batch is passed to the network\u2019s input layer, which sends it to the first\nhidden layer. The algorithm then computes the output of all the neurons in this\nlayer (for every instance in the mini-batch). The result is passed on to the next\nlayer, its output is computed and passed to the next layer, and so on until we get\nthe output of the last layer, the output layer. This is the forward pass: it is exactly\nlike making predictions, except all intermediate results are preserved since they\nare needed for the backward pass. \u2022 Next, the algorithm measures the network\u2019s output error (i.e., it uses a loss func\u2010\ntion that compares the desired output and the actual output of the network, and\nreturns some measure of the error). \u2022 Then it computes how much each output connection contributed to the error. This is done analytically by applying the chain rule (perhaps the most fundamen\u2010\ntal rule in calculus), which makes this step fast and precise. | Chapter 10: Introduction to Artificial Neural Networks with Keras\n\u2022 The algorithm then measures how much of these error contributions came from\neach connection in the layer below, again using the chain rule, working backward\nuntil the algorithm reaches the input layer. As explained earlier, this reverse pass\nefficiently measures the error gradient across all the connection weights in the\nnetwork by propagating the error gradient backward through the network (hence\nthe name of the algorithm). \u2022 Finally, the algorithm performs a Gradient Descent step to tweak all the connec\u2010\ntion weights in the network, using the error gradients it just computed. This algorithm is so important that it\u2019s worth summarizing it again: for each training\ninstance, the backpropagation algorithm first makes a prediction (forward pass) and\nmeasures the error, then goes through each layer in reverse to measure the error con\u2010\ntribution from each connection (reverse pass), and finally tweaks the connection\nweights to reduce the error (Gradient Descent step). It is important to initialize all the hidden layers\u2019 connection weights\nrandomly, or else training will fail. For example, if you initialize all\nweights and biases to zero, then all neurons in a given layer will be\nperfectly identical, and thus backpropagation will affect them in\nexactly the same way, so they will remain identical. In other words,\ndespite having hundreds of neurons per layer, your model will act\nas if it had only one neuron per layer: it won\u2019t be too smart."
  },
  {
    "id": 185,
    "content": "If\ninstead you randomly initialize the weights, you break the symme\u2010\ntry and allow backpropagation to train a diverse team of neurons. In order for this algorithm to work properly, its authors made a key change to the\nMLP\u2019s architecture: they replaced the step function with the logistic (sigmoid) func\u2010\ntion, \u03c3(z) = 1 / (1 + exp(\u2013z)). This was essential because the step function contains\nonly flat segments, so there is no gradient to work with (Gradient Descent cannot\nmove on a flat surface), while the logistic function has a well-defined nonzero deriva\u2010\ntive everywhere, allowing Gradient Descent to make some progress at every step. In\nfact, the backpropagation algorithm works well with many other activation functions,\nnot just the logistic function. Here are two other popular choices:\nThe hyperbolic tangent function: tanh(z) = 2\u03c3(2z) \u2013 1\nJust like the logistic function, this activation function is S-shaped, continuous,\nand differentiable, but its output value ranges from \u20131 to 1 (instead of 0 to 1 in\nthe case of the logistic function). That range tends to make each layer\u2019s output\nmore or less centered around 0 at the beginning of training, which often helps\nspeed up convergence. From Biological to Artificial Neurons | 12 Biological neurons seem to implement a roughly sigmoid (S-shaped) activation function, so researchers stuck\nto sigmoid functions for a very long time. But it turns out that ReLU generally works better in ANNs. This is\none of the cases where the biological analogy was misleading. The Rectified Linear Unit function: ReLU(z) = max(0, z)\nThe ReLU function is continuous but unfortunately not differentiable at z = 0\n(the slope changes abruptly, which can make Gradient Descent bounce around),\nand its derivative is 0 for z < 0. In practice, however, it works very well and has\nthe advantage of being fast to compute, so it has become the default.12 Most\nimportantly, the fact that it does not have a maximum output value helps reduce\nsome issues during Gradient Descent (we will come back to this in Chapter 11). These popular activation functions and their derivatives are represented in\nFigure 10-8. But wait! Why do we need activation functions in the first place? Well, if\nyou chain several linear transformations, all you get is a linear transformation. For\nexample, if f(x) = 2x + 3 and g(x) = 5x \u2013 1, then chaining these two linear functions\ngives you another linear function: f(g(x)) = 2(5x \u2013 1) + 3 = 10x + 1. So if you don\u2019t\nhave some nonlinearity between layers, then even a deep stack of layers is equivalent\nto a single layer, and you can\u2019t solve very complex problems with that. Conversely, a\nlarge enough DNN with nonlinear activations can theoretically approximate any con\u2010\ntinuous function. Figure 10-8. Activation functions and their derivatives\nOK! You know where neural nets came from, what their architecture is, and how to\ncompute their outputs. You\u2019ve also learned about the backpropagation algorithm."
  },
  {
    "id": 186,
    "content": "But\nwhat exactly can you do with them? Regression MLPs\nFirst, MLPs can be used for regression tasks. If you want to predict a single value (e.g.,\nthe price of a house, given many of its features), then you just need a single output\nneuron: its output is the predicted value. For multivariate regression (i.e., to predict | Chapter 10: Introduction to Artificial Neural Networks with Keras\nmultiple values at once), you need one output neuron per output dimension. For\nexample, to locate the center of an object in an image, you need to predict 2D coordi\u2010\nnates, so you need two output neurons. If you also want to place a bounding box\naround the object, then you need two more numbers: the width and the height of the\nobject. So, you end up with four output neurons. In general, when building an MLP for regression, you do not want to use any activa\u2010\ntion function for the output neurons, so they are free to output any range of values. If\nyou want to guarantee that the output will always be positive, then you can use the\nReLU activation function in the output layer. Alternatively, you can use the softplus\nactivation function, which is a smooth variant of ReLU: softplus(z) = log(1 + exp(z)). It is close to 0 when z is negative, and close to z when z is positive. Finally, if you want\nto guarantee that the predictions will fall within a given range of values, then you can\nuse the logistic function or the hyperbolic tangent, and then scale the labels to the\nappropriate range: 0 to 1 for the logistic function and \u20131 to 1 for the hyperbolic\ntangent. The loss function to use during training is typically the mean squared error, but if you\nhave a lot of outliers in the training set, you may prefer to use the mean absolute\nerror instead. Alternatively, you can use the Huber loss, which is a combination of\nboth. The Huber loss is quadratic when the error is smaller than a thres\u2010\nhold \u03b4 (typically 1) but linear when the error is larger than \u03b4. The\nlinear part makes it less sensitive to outliers than the mean squared\nerror, and the quadratic part allows it to converge faster and be\nmore precise than the mean absolute error. Table 10-1 summarizes the typical architecture of a regression MLP. Table 10-1. Typical regression MLP architecture\nHyperparameter\nTypical value\n# input neurons\nOne per input feature (e.g., 28 x 28 = 784 for MNIST)\n# hidden layers\nDepends on the problem, but typically 1 to 5\n# neurons per hidden layer Depends on the problem, but typically 10 to 100\n# output neurons\n1 per prediction dimension\nHidden activation\nReLU (or SELU, see Chapter 11)\nOutput activation\nNone, or ReLU/softplus (if positive outputs) or logistic/tanh (if bounded outputs)\nLoss function\nMSE or MAE/Huber (if outliers)\nFrom Biological to Artificial Neurons | Classification MLPs\nMLPs can also be used for classification tasks."
  },
  {
    "id": 187,
    "content": "For a binary classification problem,\nyou just need a single output neuron using the logistic activation function: the output\nwill be a number between 0 and 1, which you can interpret as the estimated probabil\u2010\nity of the positive class. The estimated probability of the negative class is equal to one\nminus that number. MLPs can also easily handle multilabel binary classification tasks (see Chapter 3). For\nexample, you could have an email classification system that predicts whether each\nincoming email is ham or spam, and simultaneously predicts whether it is an urgent\nor nonurgent email. In this case, you would need two output neurons, both using the\nlogistic activation function: the first would output the probability that the email is\nspam, and the second would output the probability that it is urgent. More generally,\nyou would dedicate one output neuron for each positive class. Note that the output\nprobabilities do not necessarily add up to 1. This lets the model output any combina\u2010\ntion of labels: you can have nonurgent ham, urgent ham, nonurgent spam, and per\u2010\nhaps even urgent spam (although that would probably be an error). If each instance can belong only to a single class, out of three or more possible classes\n(e.g., classes 0 through 9 for digit image classification), then you need to have one\noutput neuron per class, and you should use the softmax activation function for the\nwhole output layer (see Figure 10-9). The softmax function (introduced in Chapter 4)\nwill ensure that all the estimated probabilities are between 0 and 1 and that they add\nup to 1 (which is required if the classes are exclusive). This is called multiclass\nclassification. Figure 10-9. A modern MLP (including ReLU and softmax) for classification | Chapter 10: Introduction to Artificial Neural Networks with Keras\n13 Project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System). Regarding the loss function, since we are predicting probability distributions, the\ncross-entropy loss (also called the log loss, see Chapter 4) is generally a good choice. Table 10-2 summarizes the typical architecture of a classification MLP. Table 10-2. Typical classification MLP architecture\nHyperparameter\nBinary classification\nMultilabel binary classification Multiclass classification\nInput and hidden layers Same as regression\nSame as regression\nSame as regression\n# output neurons 1 per label\n1 per class\nOutput layer activation\nLogistic\nLogistic\nSoftmax\nLoss function\nCross entropy\nCross entropy\nCross entropy\nBefore we go on, I recommend you go through exercise 1 at the\nend of this chapter. You will play with various neural network\narchitectures and visualize their outputs using the TensorFlow Play\u2010\nground. This will be very useful to better understand MLPs, includ\u2010\ning the effects of all the hyperparameters (number of layers and\nneurons, activation functions, and more). Now you have all the concepts you need to start implementing MLPs with Keras! Implementing MLPs with Keras\nKeras is a high-level Deep Learning API that allows you to easily build, train, evalu\u2010\nate, and execute all sorts of neural networks."
  },
  {
    "id": 188,
    "content": "Its documentation (or specification) is\navailable at  The reference implementation, also called Keras, was\ndeveloped by Fran\u00e7ois Chollet as part of a research project13 and was released as an\nopen source project in March 2015. It quickly gained popularity, owing to its ease of\nuse, flexibility, and beautiful design. To perform the heavy computations required by\nneural networks, this reference implementation relies on a computation backend. At\npresent, you can choose from three popular open source Deep Learning libraries:\nTensorFlow, Microsoft Cognitive Toolkit (CNTK), and Theano. Therefore, to avoid\nany confusion, we will refer to this reference implementation as multibackend Keras. Since late 2016, other implementations have been released. You can now run Keras on\nApache MXNet, Apple\u2019s Core ML, JavaScript or TypeScript (to run Keras code in a\nweb browser), and PlaidML (which can run on all sorts of GPU devices, not just Nvi\u2010\ndia). Moreover, TensorFlow itself now comes bundled with its own Keras implemen\u2010\ntation, tf.keras. It only supports TensorFlow as the backend, but it has the advantage\nof offering some very useful extra features (see Figure 10-10): for example, it supports\nImplementing MLPs with Keras | TensorFlow\u2019s Data API, which makes it easy to load and preprocess data efficiently. For this reason, we will use tf.keras in this book. However, in this chapter we will not\nuse any of the TensorFlow-specific features, so the code should run fine on other\nKeras implementations as well (at least in Python), with only minor modifications,\nsuch as changing the imports. Figure 10-10. Two implementations of the Keras API: multibackend Keras (left) and\ntf.keras (right)\nThe most popular Deep Learning library, after Keras and TensorFlow, is Facebook\u2019s\nPyTorch library. The good news is that its API is quite similar to Keras\u2019s (in part\nbecause both APIs were inspired by Scikit-Learn and Chainer), so once you know\nKeras, it is not difficult to switch to PyTorch, if you ever want to. PyTorch\u2019s popularity\ngrew exponentially in 2018, largely thanks to its simplicity and excellent documenta\u2010\ntion, which were not TensorFlow 1.x\u2019s main strengths. However, TensorFlow 2 is\narguably just as simple as PyTorch, as it has adopted Keras as its official high-level\nAPI and its developers have greatly simplified and cleaned up the rest of the API. The\ndocumentation has also been completely reorganized, and it is much easier to find\nwhat you need now. Similarly, PyTorch\u2019s main weaknesses (e.g., limited portability\nand no computation graph analysis) have been largely addressed in PyTorch 1.0. Healthy competition is beneficial to everyone. All right, it\u2019s time to code! As tf.keras is bundled with TensorFlow, let\u2019s start by instal\u2010\nling TensorFlow. Installing TensorFlow 2\nAssuming you installed Jupyter and Scikit-Learn by following the installation instruc\u2010\ntions in Chapter 2, use pip to install TensorFlow."
  },
  {
    "id": 189,
    "content": "If you created an isolated environ\u2010\nment using virtualenv, you first need to activate it: | Chapter 10: Introduction to Artificial Neural Networks with Keras\n$ cd $ML_PATH # Your ML working directory (e.g., $HOME/ml)\n$ source my_env/bin/activate # on Linux or macOS\n$ .\\my_env\\Scripts\\activate # on Windows\nNext, install TensorFlow 2 (if you are not using a virtualenv, you will need adminis\u2010\ntrator rights, or to add the --user option):\n$ python3 -m pip install -U tensorflow\nFor GPU support, at the time of this writing you need to install\ntensorflow-gpu instead of tensorflow, but the TensorFlow team\nis working on having a single library that will support both CPU-\nonly and GPU-equipped systems. You will still need to install extra\nlibraries for GPU support (see  for\nmore details). We will look at GPUs in more depth in Chapter 19. To test your installation, open a Python shell or a Jupyter notebook, then import\nTensorFlow and tf.keras and print their versions:\n>>> import tensorflow as tf\n>>> from tensorflow import keras\n>>> tf.__version__\n'2.0.0'\n>>> keras.__version__\n'2.2.4-tf'\nThe second version is the version of the Keras API implemented by tf.keras. Note that\nit ends with -tf, highlighting the fact that tf.keras implements the Keras API, plus\nsome extra TensorFlow-specific features. Now let\u2019s use tf.keras! We\u2019ll start by building a simple image classifier. Building an Image Classifier Using the Sequential API\nFirst, we need to load a dataset. In this chapter we will tackle Fashion MNIST, which\nis a drop-in replacement of MNIST (introduced in Chapter 3). It has the exact same\nformat as MNIST (70,000 grayscale images of 28 \u00d7 28 pixels each, with 10 classes),\nbut the images represent fashion items rather than handwritten digits, so each class is\nmore diverse, and the problem turns out to be significantly more challenging than\nMNIST. For example, a simple linear model reaches about 92% accuracy on MNIST,\nbut only about 83% on Fashion MNIST. Using Keras to load the dataset\nKeras provides some utility functions to fetch and load common datasets, including\nMNIST, Fashion MNIST, and the California housing dataset we used in Chapter 2. Let\u2019s load Fashion MNIST:\nImplementing MLPs with Keras | fashion_mnist = keras.datasets.fashion_mnist\n(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\nWhen loading MNIST or Fashion MNIST using Keras rather than Scikit-Learn, one\nimportant difference is that every image is represented as a 28 \u00d7 28 array rather than\na 1D array of size 784. Moreover, the pixel intensities are represented as integers\n(from 0 to 255) rather than floats (from 0.0 to 255.0). Let\u2019s take a look at the shape\nand data type of the training set:\n>>> X_train_full.shape\n(60000, 28, 28)\n>>> X_train_full.dtype\ndtype('uint8')\nNote that the dataset is already split into a training set and a test set, but there is no\nvalidation set, so we\u2019ll create one now. Additionally, since we are going to train the\nneural network using Gradient Descent, we must scale the input features."
  },
  {
    "id": 190,
    "content": "For simplic\u2010\nity, we\u2019ll scale the pixel intensities down to the 0\u20131 range by dividing them by 255.0\n(this also converts them to floats):\nX_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\ny_valid, y_train = y_train_full[:5000], y_train_full[5000:]\nWith MNIST, when the label is equal to 5, it means that the image represents the\nhandwritten digit 5. Easy. For Fashion MNIST, however, we need the list of class\nnames to know what we are dealing with:\nclass_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\nFor example, the first image in the training set represents a coat:\n>>> class_names[y_train[0]]\n'Coat'\nFigure 10-11 shows some samples from the Fashion MNIST dataset. Figure 10-11. Samples from Fashion MNIST | Chapter 10: Introduction to Artificial Neural Networks with Keras\nCreating the model using the Sequential API\nNow let\u2019s build the neural network! Here is a classification MLP with two hidden\nlayers:\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Flatten(input_shape=[28, 28]))\nmodel.add(keras.layers.Dense(300, activation=\"relu\"))\nmodel.add(keras.layers.Dense(100, activation=\"relu\"))\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\nLet\u2019s go through this code line by line:\n\u2022 The first line creates a Sequential model. This is the simplest kind of Keras\nmodel for neural networks that are just composed of a single stack of layers con\u2010\nnected sequentially. This is called the Sequential API. \u2022 Next, we build the first layer and add it to the model. It is a Flatten layer whose\nrole is to convert each input image into a 1D array: if it receives input data X, it\ncomputes X.reshape(-1, 1). This layer does not have any parameters; it is just\nthere to do some simple preprocessing. Since it is the first layer in the model, you\nshould specify the input_shape, which doesn\u2019t include the batch size, only the\nshape of the instances. Alternatively, you could add a keras.layers.InputLayer\nas the first layer, setting input_shape=[28,28]. \u2022 Next we add a Dense hidden layer with 300 neurons. It will use the ReLU activa\u2010\ntion function. Each Dense layer manages its own weight matrix, containing all the\nconnection weights between the neurons and their inputs. It also manages a vec\u2010\ntor of bias terms (one per neuron). When it receives some input data, it computes\nEquation 10-2. \u2022 Then we add a second Dense hidden layer with 100 neurons, also using the ReLU\nactivation function. \u2022 Finally, we add a Dense output layer with 10 neurons (one per class), using the\nsoftmax activation function (because the classes are exclusive). Specifying activation=\"relu\" is equivalent to specifying activa\ntion=keras.activations.relu. Other activation functions are\navailable in the keras.activations package, we will use many of\nthem in this book. See  for the full list. Instead of adding the layers one by one as we just did, you can pass a list of layers\nwhen creating the Sequential model:\nImplementing MLPs with Keras | 14 You can use keras.utils.plot_model() to generate an image of your model."
  },
  {
    "id": 191,
    "content": "model = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(300, activation=\"relu\"), keras.layers.Dense(100, activation=\"relu\"), keras.layers.Dense(10, activation=\"softmax\")\n])\nUsing Code Examples from keras.io\nCode examples documented on keras.io will work fine with tf.keras, but you need to\nchange the imports. For example, consider this keras.io code:\nfrom keras.layers import Dense\noutput_layer = Dense(10)\nYou must change the imports like this:\nfrom tensorflow.keras.layers import Dense\noutput_layer = Dense(10)\nOr simply use full paths, if you prefer:\nfrom tensorflow import keras\noutput_layer = keras.layers.Dense(10)\nThis approach is more verbose, but I use it in this book so you can easily see which\npackages to use, and to avoid confusion between standard classes and custom classes. In production code, I prefer the previous approach. Many people also use from ten\nsorflow.keras import layers followed by layers.Dense(10). The model\u2019s summary() method displays all the model\u2019s layers,14 including each layer\u2019s\nname (which is automatically generated unless you set it when creating the layer), its\noutput shape (None means the batch size can be anything), and its number of parame\u2010\nters. The summary ends with the total number of parameters, including trainable and\nnon-trainable parameters. Here we only have trainable parameters (we will see exam\u2010\nples of non-trainable parameters in Chapter 11):\n>>> model.summary()\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type) Output Shape Param #\n=================================================================\nflatten (Flatten) (None, 784) 0\n_________________________________________________________________\ndense (Dense) (None, 300) 235500\n_________________________________________________________________ | Chapter 10: Introduction to Artificial Neural Networks with Keras\ndense_1 (Dense) (None, 100) 30100\n_________________________________________________________________\ndense_2 (Dense) (None, 10) 1010\n=================================================================\nTotal params: 266,610\nTrainable params: 266,610\nNon-trainable params: 0\n_________________________________________________________________\nNote that Dense layers often have a lot of parameters. For example, the first hidden\nlayer has 784 \u00d7 300 connection weights, plus 300 bias terms, which adds up to\n235,500 parameters! This gives the model quite a lot of flexibility to fit the training\ndata, but it also means that the model runs the risk of overfitting, especially when you\ndo not have a lot of training data. We will come back to this later. You can easily get a model\u2019s list of layers, to fetch a layer by its index, or you can fetch\nit by name:\n>>> model.layers\n[<tensorflow.python.keras.layers.core.Flatten at 0x132414e48>, <tensorflow.python.keras.layers.core.Dense at 0x1324149b0>, <tensorflow.python.keras.layers.core.Dense at 0x1356ba8d0>, <tensorflow.python.keras.layers.core.Dense at 0x13240d240>]\n>>> hidden1 = model.layers[1]\n>>> hidden1.name\n'dense'\n>>> model.get_layer('dense') is hidden1\nTrue\nAll the parameters of a layer can be accessed using its get_weights() and\nset_weights() methods. For a Dense layer, this includes both the connection weights\nand the bias terms:\n>>> weights, biases = hidden1.get_weights()\n>>> weights\narray([[ 0.02448617, -0.00877795, -0.02189048, ..., -0.02766046, 0.03859074, -0.06889391], ..., [-0.06022581, 0.01577859, -0.02585464, ..., -0.00527829, 0.00272203, -0.06793761]], dtype=float32)\n>>> weights.shape\n(784, 300)\n>>> biases\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., ..., 0., 0., 0. ], dtype=float32)\n>>> biases.shape\n(300,)\nNotice that the Dense layer initialized the connection weights randomly (which is\nneeded to break symmetry, as we discussed earlier), and the biases were initialized to\nzeros, which is fine."
  },
  {
    "id": 192,
    "content": "If you ever want to use a different initialization method, you can\nset kernel_initializer (kernel is another name for the matrix of connection\nImplementing MLPs with Keras | weights) or bias_initializer when creating the layer. We will discuss initializers\nfurther in Chapter 11, but if you want the full list, see \nThe shape of the weight matrix depends on the number of inputs. This is why it is recommended to specify the input_shape when\ncreating the first layer in a Sequential model. However, if you do\nnot specify the input shape, it\u2019s OK: Keras will simply wait until it\nknows the input shape before it actually builds the model. This will\nhappen either when you feed it actual data (e.g., during training),\nor when you call its build() method. Until the model is really\nbuilt, the layers will not have any weights, and you will not be able\nto do certain things (such as print the model summary or save the\nmodel). So, if you know the input shape when creating the model,\nit is best to specify it. Compiling the model\nAfter a model is created, you must call its compile() method to specify the loss func\u2010\ntion and the optimizer to use. Optionally, you can specify a list of extra metrics to\ncompute during training and evaluation:\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\nUsing loss=\"sparse_categorical_crossentropy\" is equivalent to\nusing loss=keras.losses.sparse_categorical_crossentropy. Similarly, specifying optimizer=\"sgd\" is equivalent to specifying\noptimizer=keras.optimizers.SGD(), and metrics=[\"accuracy\"]\nis equivalent to metrics=[keras.metrics.sparse_categori\ncal_accuracy] (when using this loss). We will use many other los\u2010\nses, optimizers, and metrics in this book; for the full lists, see\n  and \nkeras.io/metrics. This code requires some explanation. First, we use the \"sparse_categorical_cross\nentropy\" loss because we have sparse labels (i.e., for each instance, there is just a tar\u2010\nget class index, from 0 to 9 in this case), and the classes are exclusive. If instead we\nhad one target probability per class for each instance (such as one-hot vectors, e.g. [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would\nneed to use the \"categorical_crossentropy\" loss instead. If we were doing binary\nclassification (with one or more binary labels), then we would use the \"sigmoid\" (i.e.,\nlogistic) activation function in the output layer instead of the \"softmax\" activation\nfunction, and we would use the \"binary_crossentropy\" loss. | Chapter 10: Introduction to Artificial Neural Networks with Keras\nIf you want to convert sparse labels (i.e., class indices) to one-hot\nvector labels, use the keras.utils.to_categorical() function. To\ngo the other way round, use the np.argmax() function with\naxis=1. Regarding the optimizer, \"sgd\" means that we will train the model using simple Sto\u2010\nchastic Gradient Descent. In other words, Keras will perform the backpropagation\nalgorithm described earlier (i.e., reverse-mode autodiff plus Gradient Descent). We\nwill discuss more efficient optimizers in Chapter 11 (they improve the Gradient\nDescent part, not the autodiff). When using the SGD optimizer, it is important to tune the learning\nrate."
  },
  {
    "id": 193,
    "content": "So, you will generally want to use optimizer=keras.optimiz\ners.SGD(lr=???) to set the learning rate, rather than opti\nmizer=\"sgd\", which defaults to lr=0.01. Finally, since this is a classifier, it\u2019s useful to measure its \"accuracy\" during training\nand evaluation. Training and evaluating the model\nNow the model is ready to be trained. For this we simply need to call its fit()\nmethod:\n>>> history = model.fit(X_train, y_train, epochs=30,\n... validation_data=(X_valid, y_valid))\n... Train on 55000 samples, validate on 5000 samples\nEpoch 1/30\n55000/55000 [======] - 3s 49us/sample - loss: 0.7218 - accuracy: 0.7660 - val_loss: 0.4973 - val_accuracy: 0.8366\nEpoch 2/30\n55000/55000 [======] - 2s 45us/sample - loss: 0.4840 - accuracy: 0.8327 - val_loss: 0.4456 - val_accuracy: 0.8480\n[...]\nEpoch 30/30\n55000/55000 [======] - 3s 53us/sample - loss: 0.2252 - accuracy: 0.9192 - val_loss: 0.2999 - val_accuracy: 0.8926\nWe pass it the input features (X_train) and the target classes (y_train), as well as the\nnumber of epochs to train (or else it would default to just 1, which would definitely\nnot be enough to converge to a good solution). We also pass a validation set (this is\noptional). Keras will measure the loss and the extra metrics on this set at the end of\neach epoch, which is very useful to see how well the model really performs. If the per\u2010\nformance on the training set is much better than on the validation set, your model is\nImplementing MLPs with Keras | 15 If your training or validation data does not match the expected shape, you will get an exception. This is per\u2010\nhaps the most common error, so you should get familiar with the error message. The message is actually quite\nclear: for example, if you try to train this model with an array containing flattened images\n(X_train.reshape(-1, 784)), then you will get the following exception: \u201cValueError: Error when checking\ninput: expected flatten_input to have 3 dimensions, but got array with shape (60000, 784).\u201d\nprobably overfitting the training set (or there is a bug, such as a data mismatch\nbetween the training set and the validation set). And that\u2019s it! The neural network is trained.15 At each epoch during training, Keras\ndisplays the number of instances processed so far (along with a progress bar), the\nmean training time per sample, and the loss and accuracy (or any other extra metrics\nyou asked for) on both the training set and the validation set. You can see that the\ntraining loss went down, which is a good sign, and the validation accuracy reached\n89.26% after 30 epochs. That\u2019s not too far from the training accuracy, so there does\nnot seem to be much overfitting going on. Instead of passing a validation set using the validation_data\nargument, you could set validation_split to the ratio of the\ntraining set that you want Keras to use for validation. For example,\nvalidation_split=0.1 tells Keras to use the last 10% of the data\n(before shuffling) for validation."
  },
  {
    "id": 194,
    "content": "If the training set was very skewed, with some classes being overrepresented and oth\u2010\ners underrepresented, it would be useful to set the class_weight argument when\ncalling the fit() method, which would give a larger weight to underrepresented\nclasses and a lower weight to overrepresented classes. These weights would be used by\nKeras when computing the loss. If you need per-instance weights, set the sam\nple_weight argument (if both class_weight and sample_weight are provided, Keras\nmultiplies them). Per-instance weights could be useful if some instances were labeled\nby experts while others were labeled using a crowdsourcing platform: you might want\nto give more weight to the former. You can also provide sample weights (but not class\nweights) for the validation set by adding them as a third item in the validation_data\ntuple. The fit() method returns a History object containing the training parameters\n(history.params), the list of epochs it went through (history.epoch), and most\nimportantly a dictionary (history.history) containing the loss and extra metrics it\nmeasured at the end of each epoch on the training set and on the validation set (if\nany). If you use this dictionary to create a pandas DataFrame and call its plot()\nmethod, you get the learning curves shown in Figure 10-12: | Chapter 10: Introduction to Artificial Neural Networks with Keras\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\nplt.show()\nFigure 10-12. Learning curves: the mean training loss and accuracy measured over each\nepoch, and the mean validation loss and accuracy measured at the end of each epoch\nYou can see that both the training accuracy and the validation accuracy steadily\nincrease during training, while the training loss and the validation loss decrease. Good! Moreover, the validation curves are close to the training curves, which means\nthat there is not too much overfitting. In this particular case, the model looks like it\nperformed better on the validation set than on the training set at the beginning of\ntraining. But that\u2019s not the case: indeed, the validation error is computed at the end of\neach epoch, while the training error is computed using a running mean during each\nepoch. So the training curve should be shifted by half an epoch to the left. If you do\nthat, you will see that the training and validation curves overlap almost perfectly at\nthe beginning of training. When plotting the training curve, it should be shifted by half an\nepoch to the left. Implementing MLPs with Keras | The training set performance ends up beating the validation performance, as is gen\u2010\nerally the case when you train for long enough. You can tell that the model has not\nquite converged yet, as the validation loss is still going down, so you should probably\ncontinue training. It\u2019s as simple as calling the fit() method again, since Keras just\ncontinues training where it left off (you should be able to reach close to 89% valida\u2010\ntion accuracy)."
  },
  {
    "id": 195,
    "content": "If you are not satisfied with the performance of your model, you should go back and\ntune the hyperparameters. The first one to check is the learning rate. If that doesn\u2019t\nhelp, try another optimizer (and always retune the learning rate after changing any\nhyperparameter). If the performance is still not great, then try tuning model hyper\u2010\nparameters such as the number of layers, the number of neurons per layer, and the\ntypes of activation functions to use for each hidden layer. You can also try tuning\nother hyperparameters, such as the batch size (it can be set in the fit() method using\nthe batch_size argument, which defaults to 32). We will get back to hyperparameter\ntuning at the end of this chapter. Once you are satisfied with your model\u2019s validation\naccuracy, you should evaluate it on the test set to estimate the generalization error\nbefore you deploy the model to production. You can easily do this using the evalu\nate() method (it also supports several other arguments, such as batch_size and\nsample_weight; please check the documentation for more details):\n>>> model.evaluate(X_test, y_test)\n10000/10000 [==========] - 0s 29us/sample - loss: 0.3340 - accuracy: 0.8851\n[0.3339798209667206, 0.8851]\nAs we saw in Chapter 2, it is common to get slightly lower performance on the test set\nthan on the validation set, because the hyperparameters are tuned on the validation\nset, not the test set (however, in this example, we did not do any hyperparameter tun\u2010\ning, so the lower accuracy is just bad luck). Remember to resist the temptation to\ntweak the hyperparameters on the test set, or else your estimate of the generalization\nerror will be too optimistic. Using the model to make predictions\nNext, we can use the model\u2019s predict() method to make predictions on new instan\u2010\nces. Since we don\u2019t have actual new instances, we will just use the first three instances\nof the test set:\n>>> X_new = X_test[:3]\n>>> y_proba = model.predict(X_new)\n>>> y_proba.round(2)\narray([[0. , 0. , 0. , 0. , 0. , 0.03, 0. , 0.01, 0. , 0.96], [0. , 0. , 0.98, 0. , 0.02, 0. , 0. , 0. , 0. , 0. ], [0. , 1. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ]], dtype=float32) | Chapter 10: Introduction to Artificial Neural Networks with Keras\nAs you can see, for each instance the model estimates one probability per class, from\nclass 0 to class 9. For example, for the first image it estimates that the probability of\nclass 9 (ankle boot) is 96%, the probability of class 5 (sandal) is 3%, the probability of\nclass 7 (sneaker) is 1%, and the probabilities of the other classes are negligible. In\nother words, it \u201cbelieves\u201d the first image is footwear, most likely ankle boots but pos\u2010\nsibly sandals or sneakers."
  },
  {
    "id": 196,
    "content": "If you only care about the class with the highest estimated\nprobability (even if that probability is quite low), then you can use the pre\ndict_classes() method instead:\n>>> y_pred = model.predict_classes(X_new)\n>>> y_pred\narray([9, 2, 1])\n>>> np.array(class_names)[y_pred]\narray(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')\nHere, the classifier actually classified all three images correctly (these images are\nshown in Figure 10-13):\n>>> y_new = y_test[:3]\n>>> y_new\narray([9, 2, 1])\nFigure 10-13. Correctly classified Fashion MNIST images\nNow you know how to use the Sequential API to build, train, evaluate, and use a clas\u2010\nsification MLP. But what about regression? Building a Regression MLP Using the Sequential API\nLet\u2019s switch to the California housing problem and tackle it using a regression neural\nnetwork. For simplicity, we will use Scikit-Learn\u2019s fetch_california_housing()\nfunction to load the data. This dataset is simpler than the one we used in Chapter 2,\nsince it contains only numerical features (there is no ocean_proximity feature), and\nthere is no missing value. After loading the data, we split it into a training set, a vali\u2010\ndation set, and a test set, and we scale all the features:\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nImplementing MLPs with Keras | 16 Heng-Tze Cheng et al., \u201cWide & Deep Learning for Recommender Systems,\u201d Proceedings of the First Workshop\non Deep Learning for Recommender Systems (2016): 7\u201310. 17 The short path can also be used to provide manually engineered features to the neural network. housing = fetch_california_housing()\nX_train_full, X_test, y_train_full, y_test = train_test_split( housing.data, housing.target)\nX_train, X_valid, y_train, y_valid = train_test_split( X_train_full, y_train_full)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_valid = scaler.transform(X_valid)\nX_test = scaler.transform(X_test)\nUsing the Sequential API to build, train, evaluate, and use a regression MLP to make\npredictions is quite similar to what we did for classification. The main differences are\nthe fact that the output layer has a single neuron (since we only want to predict a sin\u2010\ngle value) and uses no activation function, and the loss function is the mean squared\nerror. Since the dataset is quite noisy, we just use a single hidden layer with fewer\nneurons than before, to avoid overfitting:\nmodel = keras.models.Sequential([ keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]), keras.layers.Dense(1)\n])\nmodel.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\nhistory = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\nmse_test = model.evaluate(X_test, y_test)\nX_new = X_test[:3] # pretend these are new instances\ny_pred = model.predict(X_new)\nAs you can see, the Sequential API is quite easy to use. However, although Sequen\ntial models are extremely common, it is sometimes useful to build neural networks\nwith more complex topologies, or with multiple inputs or outputs. For this purpose,\nKeras offers the Functional API. Building Complex Models Using the Functional API\nOne example of a nonsequential neural network is a Wide & Deep neural network. This neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng\net al.16 It connects all or part of the inputs directly to the output layer, as shown in\nFigure 10-14."
  },
  {
    "id": 197,
    "content": "This architecture makes it possible for the neural network to learn both\ndeep patterns (using the deep path) and simple rules (through the short path).17 In\ncontrast, a regular MLP forces all the data to flow through the full stack of layers; | Chapter 10: Introduction to Artificial Neural Networks with Keras\n18 The name input_ is used to avoid overshadowing Python\u2019s built-in input() function. thus, simple patterns in the data may end up being distorted by this sequence of\ntransformations. Figure 10-14. Wide & Deep neural network\nLet\u2019s build such a neural network to tackle the California housing problem:\ninput_ = keras.layers.Input(shape=X_train.shape[1:])\nhidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\nhidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\nconcat = keras.layers.Concatenate()([input_, hidden2])\noutput = keras.layers.Dense(1)(concat)\nmodel = keras.Model(inputs=[input_], outputs=[output])\nLet\u2019s go through each line of this code:\n\u2022 First, we need to create an Input object.18 This is a specification of the kind of\ninput the model will get, including its shape and dtype. A model may actually\nhave multiple inputs, as we will see shortly. \u2022 Next, we create a Dense layer with 30 neurons, using the ReLU activation func\u2010\ntion. As soon as it is created, notice that we call it like a function, passing it the\ninput. This is why this is called the Functional API. Note that we are just telling\nKeras how it should connect the layers together; no actual data is being processed\nyet. \u2022 We then create a second hidden layer, and again we use it as a function. Note that\nwe pass it the output of the first hidden layer. Implementing MLPs with Keras | \u2022 Next, we create a Concatenate layer, and once again we immediately use it like a\nfunction, to concatenate the input and the output of the second hidden layer. You\nmay prefer the keras.layers.concatenate() function, which creates a\nConcatenate layer and immediately calls it with the given inputs. \u2022 Then we create the output layer, with a single neuron and no activation function,\nand we call it like a function, passing it the result of the concatenation. \u2022 Lastly, we create a Keras Model, specifying which inputs and outputs to use. Once you have built the Keras model, everything is exactly like earlier, so there\u2019s no\nneed to repeat it here: you must compile the model, train it, evaluate it, and use it to\nmake predictions. But what if you want to send a subset of the features through the wide path and a\ndifferent subset (possibly overlapping) through the deep path (see Figure 10-15)? In\nthis case, one solution is to use multiple inputs. For example, suppose we want to\nsend five features through the wide path (features 0 to 4), and six features through the\ndeep path (features 2 to 7):\ninput_A = keras.layers.Input(shape=[5], name=\"wide_input\")\ninput_B = keras.layers.Input(shape=[6], name=\"deep_input\")\nhidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\nhidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\nconcat = keras.layers.concatenate([input_A, hidden2])\noutput = keras.layers.Dense(1, name=\"output\")(concat)\nmodel = keras.Model(inputs=[input_A, input_B], outputs=[output])\nFigure 10-15."
  },
  {
    "id": 198,
    "content": "Handling multiple inputs | Chapter 10: Introduction to Artificial Neural Networks with Keras\n19 Alternatively, you can pass a dictionary mapping the input names to the input values, like {\"wide_input\":\nX_train_A, \"deep_input\": X_train_B}. This is especially useful when there are many inputs, to avoid get\u2010\nting the order wrong. The code is self-explanatory. You should name at least the most important layers,\nespecially when the model gets a bit complex like this. Note that we specified\ninputs=[input_A, input_B] when creating the model. Now we can compile the\nmodel as usual, but when we call the fit() method, instead of passing a single input\nmatrix X_train, we must pass a pair of matrices (X_train_A, X_train_B): one per\ninput.19 The same is true for X_valid, and also for X_test and X_new when you call\nevaluate() or predict():\nmodel.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\nX_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\nX_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\nX_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\nX_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\nhistory = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))\nmse_test = model.evaluate((X_test_A, X_test_B), y_test)\ny_pred = model.predict((X_new_A, X_new_B))\nThere are many use cases in which you may want to have multiple outputs:\n\u2022 The task may demand it. For instance, you may want to locate and classify the\nmain object in a picture. This is both a regression task (finding the coordinates of\nthe object\u2019s center, as well as its width and height) and a classification task. \u2022 Similarly, you may have multiple independent tasks based on the same data. Sure,\nyou could train one neural network per task, but in many cases you will get better\nresults on all tasks by training a single neural network with one output per task. This is because the neural network can learn features in the data that are useful\nacross tasks. For example, you could perform multitask classification on pictures\nof faces, using one output to classify the person\u2019s facial expression (smiling, sur\u2010\nprised, etc.) and another output to identify whether they are wearing glasses or\nnot. \u2022 Another use case is as a regularization technique (i.e., a training constraint whose\nobjective is to reduce overfitting and thus improve the model\u2019s ability to general\u2010\nize). For example, you may want to add some auxiliary outputs in a neural net\u2010\nwork architecture (see Figure 10-16) to ensure that the underlying part of the\nnetwork learns something useful on its own, without relying on the rest of the\nnetwork. Implementing MLPs with Keras | 20 Alternatively, you can pass a dictionary that maps each output name to the corresponding loss. Just like for\nthe inputs, this is useful when there are multiple outputs, to avoid getting the order wrong. The loss weights\nand metrics (discussed shortly) can also be set using dictionaries. Figure 10-16. Handling multiple outputs, in this example to add an auxiliary output for\nregularization\nAdding extra outputs is quite easy: just connect them to the appropriate layers and\nadd them to your model\u2019s list of outputs."
  },
  {
    "id": 199,
    "content": "For example, the following code builds the\nnetwork represented in Figure 10-16:\n[...] # Same as above, up to the main output layer\noutput = keras.layers.Dense(1, name=\"main_output\")(concat)\naux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\nmodel = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\nEach output will need its own loss function. Therefore, when we compile the model,\nwe should pass a list of losses20 (if we pass a single loss, Keras will assume that the\nsame loss must be used for all outputs). By default, Keras will compute all these losses\nand simply add them up to get the final loss used for training. We care much more\nabout the main output than about the auxiliary output (as it is just used for regulari\u2010\nzation), so we want to give the main output\u2019s loss a much greater weight. Fortunately,\nit is possible to set all the loss weights when compiling the model:\nmodel.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=\"sgd\")\nNow when we train the model, we need to provide labels for each output. In this\nexample, the main output and the auxiliary output should try to predict the same\nthing, so they should use the same labels. So instead of passing y_train, we need to\npass (y_train, y_train) (and the same goes for y_valid and y_test):\nhistory = model.fit( [X_train_A, X_train_B], [y_train, y_train], epochs=20, validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid])) | Chapter 10: Introduction to Artificial Neural Networks with Keras\nWhen we evaluate the model, Keras will return the total loss, as well as all the individ\u2010\nual losses:\ntotal_loss, main_loss, aux_loss = model.evaluate( [X_test_A, X_test_B], [y_test, y_test])\nSimilarly, the predict() method will return predictions for each output:\ny_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])\nAs you can see, you can build any sort of architecture you want quite easily with the\nFunctional API. Let\u2019s look at one last way you can build Keras models. Using the Subclassing API to Build Dynamic Models\nBoth the Sequential API and the Functional API are declarative: you start by declar\u2010\ning which layers you want to use and how they should be connected, and only then\ncan you start feeding the model some data for training or inference. This has many\nadvantages: the model can easily be saved, cloned, and shared; its structure can be\ndisplayed and analyzed; the framework can infer shapes and check types, so errors\ncan be caught early (i.e., before any data ever goes through the model). It\u2019s also fairly\neasy to debug, since the whole model is a static graph of layers. But the flip side is just\nthat: it\u2019s static. Some models involve loops, varying shapes, conditional branching,\nand other dynamic behaviors. For such cases, or simply if you prefer a more impera\u2010\ntive programming style, the Subclassing API is for you. Simply subclass the Model class, create the layers you need in the constructor, and use\nthem to perform the computations you want in the call() method."
  },
  {
    "id": 200,
    "content": "For example, cre\u2010\nating an instance of the following WideAndDeepModel class gives us an equivalent\nmodel to the one we just built with the Functional API. You can then compile it, eval\u2010\nuate it, and use it to make predictions, exactly like we just did:\nclass WideAndDeepModel(keras.Model): def __init__(self, units=30, activation=\"relu\", **kwargs): super().__init__(**kwargs) # handles standard args (e.g., name) self.hidden1 = keras.layers.Dense(units, activation=activation) self.hidden2 = keras.layers.Dense(units, activation=activation) self.main_output = keras.layers.Dense(1) self.aux_output = keras.layers.Dense(1) def call(self, inputs): input_A, input_B = inputs hidden1 = self.hidden1(input_B) hidden2 = self.hidden2(hidden1) concat = keras.layers.concatenate([input_A, hidden2]) main_output = self.main_output(concat) aux_output = self.aux_output(hidden2) return main_output, aux_output\nmodel = WideAndDeepModel()\nImplementing MLPs with Keras | 21 Keras models have an output attribute, so we cannot use that name for the main output layer, which is why\nwe renamed it to main_output. This example looks very much like the Functional API, except we do not need to cre\u2010\nate the inputs; we just use the input argument to the call() method, and we separate\nthe creation of the layers21 in the constructor from their usage in the call() method. The big difference is that you can do pretty much anything you want in the call()\nmethod: for loops, if statements, low-level TensorFlow operations\u2014your imagina\u2010\ntion is the limit (see Chapter 12)! This makes it a great API for researchers experi\u2010\nmenting with new ideas. This extra flexibility does come at a cost: your model\u2019s architecture is hidden within\nthe call() method, so Keras cannot easily inspect it; it cannot save or clone it; and\nwhen you call the summary() method, you only get a list of layers, without any infor\u2010\nmation on how they are connected to each other. Moreover, Keras cannot check types\nand shapes ahead of time, and it is easier to make mistakes. So unless you really need\nthat extra flexibility, you should probably stick to the Sequential API or the Func\u2010\ntional API. Keras models can be used just like regular layers, so you can easily\ncombine them to build complex architectures. Now that you know how to build and train neural nets using Keras, you will want to\nsave them! Saving and Restoring a Model\nWhen using the Sequential API or the Functional API, saving a trained Keras model\nis as simple as it gets:\nmodel = keras.models.Sequential([...]) # or keras.Model([...])\nmodel.compile([...])\nmodel.fit([...])\nmodel.save(\"my_keras_model.h5\")\nKeras will use the HDF5 format to save both the model\u2019s architecture (including every\nlayer\u2019s hyperparameters) and the values of all the model parameters for every layer\n(e.g., connection weights and biases). It also saves the optimizer (including its hyper\u2010\nparameters and any state it may have). In Chapter 19, we will see how to save a\ntf.keras model using TensorFlow\u2019s SavedModel format instead. | Chapter 10: Introduction to Artificial Neural Networks with Keras\nYou will typically have a script that trains a model and saves it, and one or more\nscripts (or web services) that load the model and use it to make predictions."
  },
  {
    "id": 201,
    "content": "Loading\nthe model is just as easy:\nmodel = keras.models.load_model(\"my_keras_model.h5\")\nThis will work when using the Sequential API or the Functional\nAPI, but unfortunately not when using model subclassing. You can\nuse save_weights() and load_weights() to at least save and\nrestore the model parameters, but you will need to save and restore\neverything else yourself. But what if training lasts several hours? This is quite common, especially when train\u2010\ning on large datasets. In this case, you should not only save your model at the end of\ntraining, but also save checkpoints at regular intervals during training, to avoid losing\neverything if your computer crashes. But how can you tell the fit() method to save\ncheckpoints? Use callbacks. Using Callbacks\nThe fit() method accepts a callbacks argument that lets you specify a list of objects\nthat Keras will call at the start and end of training, at the start and end of each epoch,\nand even before and after processing each batch. For example, the ModelCheckpoint\ncallback saves checkpoints of your model at regular intervals during training, by\ndefault at the end of each epoch:\n[...] # build and compile the model\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\nhistory = model.fit(X_train, y_train, epochs=10, callbacks=[checkpoint_cb])\nMoreover, if you use a validation set during training, you can set\nsave_best_only=True when creating the ModelCheckpoint. In this case, it will only\nsave your model when its performance on the validation set is the best so far. This\nway, you do not need to worry about training for too long and overfitting the training\nset: simply restore the last model saved after training, and this will be the best model\non the validation set. The following code is a simple way to implement early stopping\n(introduced in Chapter 4):\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\nhistory = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb])\nmodel = keras.models.load_model(\"my_keras_model.h5\") # roll back to best model\nAnother way to implement early stopping is to simply use the EarlyStopping call\u2010\nback. It will interrupt training when it measures no progress on the validation set for\nImplementing MLPs with Keras | a number of epochs (defined by the patience argument), and it will optionally roll\nback to the best model. You can combine both callbacks to save checkpoints of your\nmodel (in case your computer crashes) and interrupt training early when there is no\nmore progress (to avoid wasting time and resources):\nearly_stopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\nhistory = model.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[checkpoint_cb, early_stopping_cb])\nThe number of epochs can be set to a large value since training will stop automati\u2010\ncally when there is no more progress. In this case, there is no need to restore the best\nmodel saved because the EarlyStopping callback will keep track of the best weights\nand restore them for you at the end of training. There are many other callbacks available in the keras.callbacks\npackage. If you need extra control, you can easily write your own custom callbacks."
  },
  {
    "id": 202,
    "content": "As an\nexample of how to do that, the following custom callback will display the ratio\nbetween the validation loss and the training loss during training (e.g., to detect over\u2010\nfitting):\nclass PrintValTrainRatioCallback(keras.callbacks.Callback): def on_epoch_end(self, epoch, logs): print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"] / logs[\"loss\"]))\nAs you might expect, you can implement on_train_begin(), on_train_end(),\non_epoch_begin(), on_epoch_end(), on_batch_begin(), and on_batch_end(). Call\u2010\nbacks can also be used during evaluation and predictions, should you ever need them\n(e.g., for debugging). For evaluation, you should implement on_test_begin(),\non_test_end(), on_test_batch_begin(), or on_test_batch_end() (called by evalu\nate()), and for prediction you should implement on_predict_begin(), on_pre\ndict_end(), on_predict_batch_begin(), or on_predict_batch_end() (called by\npredict()). Now let\u2019s take a look at one more tool you should definitely have in your toolbox\nwhen using tf.keras: TensorBoard. | Chapter 10: Introduction to Artificial Neural Networks with Keras\nUsing TensorBoard for Visualization\nTensorBoard is a great interactive visualization tool that you can use to view the\nlearning curves during training, compare learning curves between multiple runs, vis\u2010\nualize the computation graph, analyze training statistics, view images generated by\nyour model, visualize complex multidimensional data projected down to 3D and\nautomatically clustered for you, and more! This tool is installed automatically when\nyou install TensorFlow, so you already have it. To use it, you must modify your program so that it outputs the data you want to visu\u2010\nalize to special binary log files called event files. Each binary data record is called a\nsummary. The TensorBoard server will monitor the log directory, and it will automat\u2010\nically pick up the changes and update the visualizations: this allows you to visualize\nlive data (with a short delay), such as the learning curves during training. In general,\nyou want to point the TensorBoard server to a root log directory and configure your\nprogram so that it writes to a different subdirectory every time it runs. This way, the\nsame TensorBoard server instance will allow you to visualize and compare data from\nmultiple runs of your program, without getting everything mixed up. Let\u2019s start by defining the root log directory we will use for our TensorBoard logs,\nplus a small function that will generate a subdirectory path based on the current date\nand time so that it\u2019s different at every run. You may want to include extra information\nin the log directory name, such as hyperparameter values that you are testing, to\nmake it easier to know what you are looking at in TensorBoard:\nimport os\nroot_logdir = os.path.join(os.curdir, \"my_logs\")\ndef get_run_logdir(): import time run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\") return os.path.join(root_logdir, run_id)\nrun_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22'\nThe good news is that Keras provides a nice TensorBoard() callback:\n[...] # Build and compile your model\ntensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\nhistory = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid), callbacks=[tensorboard_cb])\nAnd that\u2019s all there is to it! It could hardly be easier to use."
  },
  {
    "id": 203,
    "content": "If you run this code, the\nTensorBoard() callback will take care of creating the log directory for you (along\nwith its parent directories if needed), and during training it will create event files and\nwrite summaries to them. After running the program a second time (perhaps\nImplementing MLPs with Keras | changing some hyperparameter value), you will end up with a directory structure\nsimilar to this one:\nmy_logs/\n\u251c\u2500\u2500 run_2019_06_07-15_15_22\n\u2502 \u251c\u2500\u2500 train\n\u2502 \u2502 \u251c\u2500\u2500 events.out.tfevents.1559891732.mycomputer.local.38511.694049.v2\n\u2502 \u2502 \u251c\u2500\u2500 events.out.tfevents.1559891732.mycomputer.local.profile-empty\n\u2502 \u2502 \u2514\u2500\u2500 plugins/profile/2019-06-07_15-15-32\n\u2502 \u2502 \u2514\u2500\u2500 local.trace\n\u2502 \u2514\u2500\u2500 validation\n\u2502 \u2514\u2500\u2500 events.out.tfevents.1559891733.mycomputer.local.38511.696430.v2\n\u2514\u2500\u2500 run_2019_06_07-15_15_49 \u2514\u2500\u2500 [...]\nThere\u2019s one directory per run, each containing one subdirectory for training logs and\none for validation logs. Both contain event files, but the training logs also include\nprofiling traces: this allows TensorBoard to show you exactly how much time the\nmodel spent on each part of your model, across all your devices, which is great for\nlocating performance bottlenecks. Next you need to start the TensorBoard server. One way to do this is by running a\ncommand in a terminal. If you installed TensorFlow within a virtualenv, you should\nactivate it. Next, run the following command at the root of the project (or from any\u2010\nwhere else, as long as you point to the appropriate log directory):\n$ tensorboard --logdir=./my_logs --port=6006\nTensorBoard 2.0.0 at  (Press CTRL+C to quit)\nIf your shell cannot find the tensorboard script, then you must update your PATH envi\u2010\nronment variable so that it contains the directory in which the script was installed\n(alternatively, you can just replace tensorboard in the command line with python3\n-m tensorboard.main). Once the server is up, you can open a web browser and go to\n\nAlternatively, you can use TensorBoard directly within Jupyter, by running the fol\u2010\nlowing commands. The first line loads the TensorBoard extension, and the second\nline starts a TensorBoard server on port 6006 (unless it is already started) and con\u2010\nnects to it:\n%load_ext tensorboard\n%tensorboard --logdir=./my_logs --port=6006\nEither way, you should see TensorBoard\u2019s web interface. Click the SCALARS tab to\nview the learning curves (see Figure 10-17). At the bottom left, select the logs you\nwant to visualize (e.g., the training logs from the first and second run), and click the\nepoch_loss scalar. Notice that the training loss went down nicely during both runs,\nbut the second run went down much faster. Indeed, we used a learning rate of 0.05\n(optimizer=keras.optimizers.SGD(lr=0.05)) instead of 0.001. | Chapter 10: Introduction to Artificial Neural Networks with Keras\nFigure 10-17. Visualizing learning curves with TensorBoard\nYou can also visualize the whole graph, the learned weights (projected to 3D), or the\nprofiling traces. The TensorBoard() callback has options to log extra data too, such\nas embeddings (see Chapter 13). Additionally, TensorFlow offers a lower-level API in the tf.summary package."
  },
  {
    "id": 204,
    "content": "The\nfollowing code creates a SummaryWriter using the create_file_writer() function,\nand it uses this writer as a context to log scalars, histograms, images, audio, and text,\nall of which can then be visualized using TensorBoard (give it a try! ):\ntest_logdir = get_run_logdir()\nwriter = tf.summary.create_file_writer(test_logdir)\nwith writer.as_default(): for step in range(1, 1000 + 1): tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step) data = (np.random.randn(100) + 2) * step / 100 # some random data tf.summary.histogram(\"my_hist\", data, buckets=50, step=step) images = np.random.rand(2, 32, 32, 3) # random 32\u00d732 RGB images tf.summary.image(\"my_images\", images * step / 1000, step=step) texts = [\"The step is \" + str(step), \"Its square is \" + str(step**2)] tf.summary.text(\"my_text\", texts, step=step) sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step) audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1]) tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)\nImplementing MLPs with Keras | This is actually a useful visualization tool to have, even beyond TensorFlow or Deep\nLearning. Let\u2019s summarize what you\u2019ve learned so far in this chapter: we saw where neural nets\ncame from, what an MLP is and how you can use it for classification and regression,\nhow to use tf.keras\u2019s Sequential API to build MLPs, and how to use the Functional\nAPI or the Subclassing API to build more complex model architectures. You learned\nhow to save and restore a model and how to use callbacks for checkpointing, early\nstopping, and more. Finally, you learned how to use TensorBoard for visualization. You can already go ahead and use neural networks to tackle many problems! How\u2010\never, you may wonder how to choose the number of hidden layers, the number of\nneurons in the network, and all the other hyperparameters. Let\u2019s look at this now. Fine-Tuning Neural Network Hyperparameters\nThe flexibility of neural networks is also one of their main drawbacks: there are many\nhyperparameters to tweak. Not only can you use any imaginable network architec\u2010\nture, but even in a simple MLP you can change the number of layers, the number of\nneurons per layer, the type of activation function to use in each layer, the weight initi\u2010\nalization logic, and much more. How do you know what combination of hyperpara\u2010\nmeters is the best for your task? One option is to simply try many combinations of hyperparameters and see which\none works best on the validation set (or use K-fold cross-validation). For example, we\ncan use GridSearchCV or RandomizedSearchCV to explore the hyperparameter space,\nas we did in Chapter 2. To do this, we need to wrap our Keras models in objects that\nmimic regular Scikit-Learn regressors."
  },
  {
    "id": 205,
    "content": "The first step is to create a function that will\nbuild and compile a Keras model, given a set of hyperparameters:\ndef build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]): model = keras.models.Sequential() model.add(keras.layers.InputLayer(input_shape=input_shape)) for layer in range(n_hidden): model.add(keras.layers.Dense(n_neurons, activation=\"relu\")) model.add(keras.layers.Dense(1)) optimizer = keras.optimizers.SGD(lr=learning_rate) model.compile(loss=\"mse\", optimizer=optimizer) return model\nThis function creates a simple Sequential model for univariate regression (only one\noutput neuron), with the given input shape and the given number of hidden layers\nand neurons, and it compiles it using an SGD optimizer configured with the specified\nlearning rate. It is good practice to provide reasonable defaults to as many hyperpara\u2010\nmeters as you can, as Scikit-Learn does. Next, let\u2019s create a KerasRegressor based on this build_model() function: | Chapter 10: Introduction to Artificial Neural Networks with Keras\nkeras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\nThe KerasRegressor object is a thin wrapper around the Keras model built using\nbuild_model(). Since we did not specify any hyperparameters when creating it, it\nwill use the default hyperparameters we defined in build_model(). Now we can use\nthis object like a regular Scikit-Learn regressor: we can train it using its fit()\nmethod, then evaluate it using its score() method, and use it to make predictions\nusing its predict() method, as you can see in the following code:\nkeras_reg.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])\nmse_test = keras_reg.score(X_test, y_test)\ny_pred = keras_reg.predict(X_new)\nNote that any extra parameter you pass to the fit() method will get passed to the\nunderlying Keras model. Also note that the score will be the opposite of the MSE\nbecause Scikit-Learn wants scores, not losses (i.e., higher should be better). We don\u2019t want to train and evaluate a single model like this, though we want to train\nhundreds of variants and see which one performs best on the validation set. Since\nthere are many hyperparameters, it is preferable to use a randomized search rather\nthan grid search (as we discussed in Chapter 2). Let\u2019s try to explore the number of\nhidden layers, the number of neurons, and the learning rate:\nfrom scipy.stats import reciprocal\nfrom sklearn.model_selection import RandomizedSearchCV\nparam_distribs = { \"n_hidden\": [0, 1, 2, 3], \"n_neurons\": np.arange(1, 100), \"learning_rate\": reciprocal(3e-4, 3e-2),\n}\nrnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\nrnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[keras.callbacks.EarlyStopping(patience=10)])\nThis is identical to what we did in Chapter 2, except here we pass extra parameters to\nthe fit() method, and they get relayed to the underlying Keras models. Note that\nRandomizedSearchCV uses K-fold cross-validation, so it does not use X_valid and\ny_valid, which are only used for early stopping. The exploration may last many hours, depending on the hardware, the size of the\ndataset, the complexity of the model, and the values of n_iter and cv."
  },
  {
    "id": 206,
    "content": "When it\u2019s over,\nyou can access the best parameters found, the best score, and the trained Keras model\nlike this:\nFine-Tuning Neural Network Hyperparameters | >>> rnd_search_cv.best_params_\n{'learning_rate': 0.0033625641252688094, 'n_hidden': 2, 'n_neurons': 42}\n>>> rnd_search_cv.best_score_\n-0.3189529188278931\n>>> model = rnd_search_cv.best_estimator_.model\nYou can now save this model, evaluate it on the test set, and, if you are satisfied with\nits performance, deploy it to production. Using randomized search is not too hard,\nand it works well for many fairly simple problems. When training is slow, however\n(e.g., for more complex problems with larger datasets), this approach will only\nexplore a tiny portion of the hyperparameter space. You can partially alleviate this\nproblem by assisting the search process manually: first run a quick random search\nusing wide ranges of hyperparameter values, then run another search using smaller\nranges of values centered on the best ones found during the first run, and so on. This\napproach will hopefully zoom in on a good set of hyperparameters. However, it\u2019s very\ntime consuming, and probably not the best use of your time. Fortunately, there are many techniques to explore a search space much more effi\u2010\nciently than randomly. Their core idea is simple: when a region of the space turns out\nto be good, it should be explored more. Such techniques take care of the \u201czooming\u201d\nprocess for you and lead to much better solutions in much less time. Here are some\nPython libraries you can use to optimize hyperparameters:\nHyperopt\nA popular library for optimizing over all sorts of complex search spaces (includ\u2010\ning real values, such as the learning rate, and discrete values, such as the number\nof layers). Hyperas, kopt, or Talos\nUseful libraries for optimizing hyperparameters for Keras models (the first two\nare based on Hyperopt). Keras Tuner\nAn easy-to-use hyperparameter optimization library by Google for Keras models,\nwith a hosted service for visualization and analysis. Scikit-Optimize (skopt)\nA general-purpose optimization library. The BayesSearchCV class performs\nBayesian optimization using an interface similar to GridSearchCV. Spearmint\nA Bayesian optimization library. | Chapter 10: Introduction to Artificial Neural Networks with Keras\n22 Lisha Li et al., \u201cHyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization,\u201d Journal of\nMachine Learning Research 18 (April 2018): 1\u201352. 23 Max Jaderberg et al., \u201cPopulation Based Training of Neural Networks,\u201d arXiv preprint arXiv:1711.09846\n(2017). Hyperband\nA fast hyperparameter tuning library based on the recent Hyperband paper22 by\nLisha Li et al. Sklearn-Deap\nA hyperparameter optimization library based on evolutionary algorithms, with a\nGridSearchCV-like interface. Moreover, many companies offer services for hyperparameter optimization. We\u2019ll dis\u2010\ncuss Google Cloud AI Platform\u2019s hyperparameter tuning service in Chapter 19. Other\noptions include services by Arimo and SigOpt, and CallDesk\u2019s Oscar. Hyperparameter tuning is still an active area of research, and evolutionary algorithms\nare making a comeback. For example, check out DeepMind\u2019s excellent 2017 paper,23\nwhere the authors jointly optimize a population of models and their hyperparame\u2010\nters."
  },
  {
    "id": 207,
    "content": "Google has also used an evolutionary approach, not just to search for hyperpara\u2010\nmeters but also to look for the best neural network architecture for the problem; their\nAutoML suite is already available as a cloud service. Perhaps the days of building neu\u2010\nral networks manually will soon be over? Check out Google\u2019s post on this topic. In\nfact, evolutionary algorithms have been used successfully to train individual neural\nnetworks, replacing the ubiquitous Gradient Descent! For an example, see the 2017\npost by Uber where the authors introduce their Deep Neuroevolution technique. But despite all this exciting progress and all these tools and services, it still helps to\nhave an idea of what values are reasonable for each hyperparameter so that you can\nbuild a quick prototype and restrict the search space. The following sections provide\nguidelines for choosing the number of hidden layers and neurons in an MLP and for\nselecting good values for some of the main hyperparameters. Number of Hidden Layers\nFor many problems, you can begin with a single hidden layer and get reasonable\nresults. An MLP with just one hidden layer can theoretically model even the most\ncomplex functions, provided it has enough neurons. But for complex problems, deep\nnetworks have a much higher parameter efficiency than shallow ones: they can model\ncomplex functions using exponentially fewer neurons than shallow nets, allowing\nthem to reach much better performance with the same amount of training data. To understand why, suppose you are asked to draw a forest using some drawing soft\u2010\nware, but you are forbidden to copy and paste anything. It would take an enormous\nFine-Tuning Neural Network Hyperparameters | amount of time: you would have to draw each tree individually, branch by branch,\nleaf by leaf. If you could instead draw one leaf, copy and paste it to draw a branch,\nthen copy and paste that branch to create a tree, and finally copy and paste this tree to\nmake a forest, you would be finished in no time. Real-world data is often structured\nin such a hierarchical way, and deep neural networks automatically take advantage of\nthis fact: lower hidden layers model low-level structures (e.g., line segments of vari\u2010\nous shapes and orientations), intermediate hidden layers combine these low-level\nstructures to model intermediate-level structures (e.g., squares, circles), and the high\u2010\nest hidden layers and the output layer combine these intermediate structures to\nmodel high-level structures (e.g., faces). Not only does this hierarchical architecture help DNNs converge faster to a good sol\u2010\nution, but it also improves their ability to generalize to new datasets. For example, if\nyou have already trained a model to recognize faces in pictures and you now want to\ntrain a new neural network to recognize hairstyles, you can kickstart the training by\nreusing the lower layers of the first network."
  },
  {
    "id": 208,
    "content": "Instead of randomly initializing the\nweights and biases of the first few layers of the new neural network, you can initialize\nthem to the values of the weights and biases of the lower layers of the first network. This way the network will not have to learn from scratch all the low-level structures\nthat occur in most pictures; it will only have to learn the higher-level structures (e.g.,\nhairstyles). This is called transfer learning. In summary, for many problems you can start with just one or two hidden layers and\nthe neural network will work just fine. For instance, you can easily reach above 97%\naccuracy on the MNIST dataset using just one hidden layer with a few hundred neu\u2010\nrons, and above 98% accuracy using two hidden layers with the same total number of\nneurons, in roughly the same amount of training time. For more complex problems,\nyou can ramp up the number of hidden layers until you start overfitting the training\nset. Very complex tasks, such as large image classification or speech recognition, typi\u2010\ncally require networks with dozens of layers (or even hundreds, but not fully connec\u2010\nted ones, as we will see in Chapter 14), and they need a huge amount of training data. You will rarely have to train such networks from scratch: it is much more common to\nreuse parts of a pretrained state-of-the-art network that performs a similar task. Training will then be a lot faster and require much less data (we will discuss this in\nChapter 11). Number of Neurons per Hidden Layer\nThe number of neurons in the input and output layers is determined by the type of\ninput and output your task requires. For example, the MNIST task requires 28 \u00d7 28 =\n784 input neurons and 10 output neurons. As for the hidden layers, it used to be common to size them to form a pyramid, with\nfewer and fewer neurons at each layer\u2014the rationale being that many low-level fea\u2010 | Chapter 10: Introduction to Artificial Neural Networks with Keras\ntures can coalesce into far fewer high-level features. A typical neural network for\nMNIST might have 3 hidden layers, the first with 300 neurons, the second with 200,\nand the third with 100. However, this practice has been largely abandoned because it\nseems that using the same number of neurons in all hidden layers performs just as\nwell in most cases, or even better; plus, there is only one hyperparameter to tune,\ninstead of one per layer. That said, depending on the dataset, it can sometimes help to\nmake the first hidden layer bigger than the others. Just like the number of layers, you can try increasing the number of neurons gradu\u2010\nally until the network starts overfitting. But in practice, it\u2019s often simpler and more\nefficient to pick a model with more layers and neurons than you actually need, then\nuse early stopping and other regularization techniques to prevent it from overfitting."
  },
  {
    "id": 209,
    "content": "Vincent Vanhoucke, a scientist at Google, has dubbed this the \u201cstretch pants\u201d\napproach: instead of wasting time looking for pants that perfectly match your size,\njust use large stretch pants that will shrink down to the right size. With this approach,\nyou avoid bottleneck layers that could ruin your model. On the flip side, if a layer has\ntoo few neurons, it will not have enough representational power to preserve all the\nuseful information from the inputs (e.g., a layer with two neurons can only output 2D\ndata, so if it processes 3D data, some information will be lost). No matter how big and\npowerful the rest of the network is, that information will never be recovered. In general you will get more bang for your buck by increasing the\nnumber of layers instead of the number of neurons per layer. Learning Rate, Batch Size, and Other Hyperparameters\nThe numbers of hidden layers and neurons are not the only hyperparameters you can\ntweak in an MLP. Here are some of the most important ones, as well as tips on how to\nset them:\nLearning rate\nThe learning rate is arguably the most important hyperparameter. In general, the\noptimal learning rate is about half of the maximum learning rate (i.e., the learn\u2010\ning rate above which the training algorithm diverges, as we saw in Chapter 4). One way to find a good learning rate is to train the model for a few hundred iter\u2010\nations, starting with a very low learning rate (e.g., 10-5) and gradually increasing\nit up to a very large value (e.g., 10). This is done by multiplying the learning rate\nby a constant factor at each iteration (e.g., by exp(log(106)/500) to go from 10-5 to\n10 in 500 iterations). If you plot the loss as a function of the learning rate (using a\nlog scale for the learning rate), you should see it dropping at first. But after a\nwhile, the learning rate will be too large, so the loss will shoot back up: the opti\u2010\nFine-Tuning Neural Network Hyperparameters | 24 Dominic Masters and Carlo Luschi, \u201cRevisiting Small Batch Training for Deep Neural Networks,\u201d arXiv pre\u2010\nprint arXiv:1804.07612 (2018). 25 Elad Hoffer et al., \u201cTrain Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training\nof Neural Networks,\u201d Proceedings of the 31st International Conference on Neural Information Processing Systems\n(2017): 1729\u20131739. 26 Priya Goyal et al., \u201cAccurate, Large Minibatch SGD: Training ImageNet in 1 Hour,\u201d arXiv preprint arXiv:\n1706.02677 (2017). mal learning rate will be a bit lower than the point at which the loss starts to\nclimb (typically about 10 times lower than the turning point). You can then reini\u2010\ntialize your model and train it normally using this good learning rate. We will\nlook at more learning rate techniques in Chapter 11. Optimizer\nChoosing a better optimizer than plain old Mini-batch Gradient Descent (and\ntuning its hyperparameters) is also quite important. We will see several advanced\noptimizers in Chapter 11."
  },
  {
    "id": 210,
    "content": "Batch size\nThe batch size can have a significant impact on your model\u2019s performance and\ntraining time. The main benefit of using large batch sizes is that hardware accel\u2010\nerators like GPUs can process them efficiently (see Chapter 19), so the training\nalgorithm will see more instances per second. Therefore, many researchers and\npractitioners recommend using the largest batch size that can fit in GPU RAM. There\u2019s a catch, though: in practice, large batch sizes often lead to training insta\u2010\nbilities, especially at the beginning of training, and the resulting model may not\ngeneralize as well as a model trained with a small batch size. In April 2018, Yann\nLeCun even tweeted \u201cFriends don\u2019t let friends use mini-batches larger than 32,\u201d\nciting a 2018 paper24 by Dominic Masters and Carlo Luschi which concluded that\nusing small batches (from 2 to 32) was preferable because small batches led to\nbetter models in less training time. Other papers point in the opposite direction,\nhowever; in 2017, papers by Elad Hoffer et al.25 and Priya Goyal et al.26 showed\nthat it was possible to use very large batch sizes (up to 8,192) using various tech\u2010\nniques such as warming up the learning rate (i.e., starting training with a small\nlearning rate, then ramping it up, as we will see in Chapter 11). This led to a very\nshort training time, without any generalization gap. So, one strategy is to try to\nuse a large batch size, using learning rate warmup, and if training is unstable or\nthe final performance is disappointing, then try using a small batch size instead. Activation function\nWe discussed how to choose the activation function earlier in this chapter: in\ngeneral, the ReLU activation function will be a good default for all hidden layers. For the output layer, it really depends on your task. | Chapter 10: Introduction to Artificial Neural Networks with Keras\n27 Leslie N. Smith, \u201cA Disciplined Approach to Neural Network Hyper-Parameters: Part 1\u2014Learning Rate, Batch\nSize, Momentum, and Weight Decay,\u201d arXiv preprint arXiv:1803.09820 (2018). 28 A few extra ANN architectures are presented in Appendix E.\nNumber of iterations\nIn most cases, the number of training iterations does not actually need to be\ntweaked: just use early stopping instead. The optimal learning rate depends on the other hyperparameters\u2014\nespecially the batch size\u2014so if you modify any hyperparameter,\nmake sure to update the learning rate as well. For more best practices regarding tuning neural network hyperparameters, check out\nthe excellent 2018 paper27 by Leslie Smith. This concludes our introduction to artificial neural networks and their implementa\u2010\ntion with Keras. In the next few chapters, we will discuss techniques to train very\ndeep nets. We will also explore how to customize models using TensorFlow\u2019s lower-\nlevel API and how to load and preprocess data efficiently using the Data API."
  },
  {
    "id": 211,
    "content": "And we\nwill dive into other popular neural network architectures: convolutional neural net\u2010\nworks for image processing, recurrent neural networks for sequential data, autoen\u2010\ncoders for representation learning, and generative adversarial networks to model and\ngenerate data.28\nExercises\n1. The TensorFlow Playground is a handy neural network simulator built by the\nTensorFlow team. In this exercise, you will train several binary classifiers in just a\nfew clicks, and tweak the model\u2019s architecture and its hyperparameters to gain\nsome intuition on how neural networks work and what their hyperparameters\ndo. Take some time to explore the following:\na. The patterns learned by a neural net. Try training the default neural network\nby clicking the Run button (top left). Notice how it quickly finds a good solu\u2010\ntion for the classification task. The neurons in the first hidden layer have\nlearned simple patterns, while the neurons in the second hidden layer have\nlearned to combine the simple patterns of the first hidden layer into more\ncomplex patterns. In general, the more layers there are, the more complex the\npatterns can be. b. Activation functions. Try replacing the tanh activation function with a ReLU\nactivation function, and train the network again. Notice that it finds a solution\nExercises | even faster, but this time the boundaries are linear. This is due to the shape of\nthe ReLU function. c. The risk of local minima. Modify the network architecture to have just one\nhidden layer with three neurons. Train it multiple times (to reset the network\nweights, click the Reset button next to the Play button). Notice that the train\u2010\ning time varies a lot, and sometimes it even gets stuck in a local minimum. d. What happens when neural nets are too small. Remove one neuron to keep\njust two. Notice that the neural network is now incapable of finding a good\nsolution, even if you try multiple times. The model has too few parameters\nand systematically underfits the training set. e. What happens when neural nets are large enough. Set the number of neurons\nto eight, and train the network several times. Notice that it is now consistently\nfast and never gets stuck. This highlights an important finding in neural net\u2010\nwork theory: large neural networks almost never get stuck in local minima,\nand even when they do these local optima are almost as good as the global\noptimum. However, they can still get stuck on long plateaus for a long time. f. The risk of vanishing gradients in deep networks. Select the spiral dataset (the\nbottom-right dataset under \u201cDATA\u201d), and change the network architecture to\nhave four hidden layers with eight neurons each. Notice that training takes\nmuch longer and often gets stuck on plateaus for long periods of time. Also\nnotice that the neurons in the highest layers (on the right) tend to evolve\nfaster than the neurons in the lowest layers (on the left)."
  },
  {
    "id": 212,
    "content": "This problem, called\nthe \u201cvanishing gradients\u201d problem, can be alleviated with better weight initial\u2010\nization and other techniques, better optimizers (such as AdaGrad or Adam),\nor Batch Normalization (discussed in Chapter 11). g. Go further. Take an hour or so to play around with other parameters and get a\nfeel for what they do, to build an intuitive understanding about neural\nnetworks. 2. Draw an ANN using the original artificial neurons (like the ones in Figure 10-3)\nthat computes A \u2295 B (where \u2295 represents the XOR operation). Hint: A \u2295 B =\n(A \u2227 \u00ac B \u2228 (\u00ac A \u2227 B). 3. Why is it generally preferable to use a Logistic Regression classifier rather than a\nclassical Perceptron (i.e., a single layer of threshold logic units trained using the\nPerceptron training algorithm)? How can you tweak a Perceptron to make it\nequivalent to a Logistic Regression classifier? 4. Why was the logistic activation function a key ingredient in training the first\nMLPs? 5. Name three popular activation functions. Can you draw them? | Chapter 10: Introduction to Artificial Neural Networks with Keras\n6. Suppose you have an MLP composed of one input layer with 10 passthrough\nneurons, followed by one hidden layer with 50 artificial neurons, and finally one\noutput layer with 3 artificial neurons. All artificial neurons use the ReLU activa\u2010\ntion function. \u2022 What is the shape of the input matrix X? \u2022 What are the shapes of the hidden layer\u2019s weight vector Wh and its bias vector\nbh? \u2022 What are the shapes of the output layer\u2019s weight vector Wo and its bias vector\nbo? \u2022 What is the shape of the network\u2019s output matrix Y? \u2022 Write the equation that computes the network\u2019s output matrix Y as a function\nof X, Wh, bh, Wo, and bo. 7. How many neurons do you need in the output layer if you want to classify email\ninto spam or ham? What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the out\u2010\nput layer, and which activation function should you use? What about for getting\nyour network to predict housing prices, as in Chapter 2? 8. What is backpropagation and how does it work? What is the difference between\nbackpropagation and reverse-mode autodiff? 9. Can you list all the hyperparameters you can tweak in a basic MLP? If the MLP\noverfits the training data, how could you tweak these hyperparameters to try to\nsolve the problem? 10. Train a deep MLP on the MNIST dataset (you can load it using keras.data\nsets.mnist.load_data(). See if you can get over 98% precision. Try searching\nfor the optimal learning rate by using the approach presented in this chapter (i.e.,\nby growing the learning rate exponentially, plotting the loss, and finding the\npoint where the loss shoots up). Try adding all the bells and whistles\u2014save\ncheckpoints, use early stopping, and plot learning curves using TensorBoard."
  },
  {
    "id": 213,
    "content": "Solutions to these exercises are available in Appendix A. Exercises | CHAPTER 11\nTraining Deep Neural Networks\nIn Chapter 10 we introduced artificial neural networks and trained our first deep\nneural networks. But they were shallow nets, with just a few hidden layers. What if\nyou need to tackle a complex problem, such as detecting hundreds of types of objects\nin high-resolution images? You may need to train a much deeper DNN, perhaps with\n10 layers or many more, each containing hundreds of neurons, linked by hundreds of\nthousands of connections. Training a deep DNN isn\u2019t a walk in the park. Here are\nsome of the problems you could run into:\n\u2022 You may be faced with the tricky vanishing gradients problem or the related\nexploding gradients problem. This is when the gradients grow smaller and\nsmaller, or larger and larger, when flowing backward through the DNN during\ntraining. Both of these problems make lower layers very hard to train. \u2022 You might not have enough training data for such a large network, or it might be\ntoo costly to label. \u2022 Training may be extremely slow. \u2022 A model with millions of parameters would severely risk overfitting the training\nset, especially if there are not enough training instances or if they are too noisy. In this chapter we will go through each of these problems and present techniques to\nsolve them. We will start by exploring the vanishing and exploding gradients prob\u2010\nlems and some of their most popular solutions. Next, we will look at transfer learning\nand unsupervised pretraining, which can help you tackle complex tasks even when\nyou have little labeled data. Then we will discuss various optimizers that can speed up\ntraining large models tremendously. Finally, we will go through a few popular regula\u2010\nrization techniques for large neural networks. With these tools, you will be able to train very deep nets. Welcome to Deep Learning! 1 Xavier Glorot and Yoshua Bengio, \u201cUnderstanding the Difficulty of Training Deep Feedforward Neural Net\u2010\nworks,\u201d Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (2010): 249\u2013256. The Vanishing/Exploding Gradients Problems\nAs we discussed in Chapter 10, the backpropagation algorithm works by going from\nthe output layer to the input layer, propagating the error gradient along the way. Once\nthe algorithm has computed the gradient of the cost function with regard to each\nparameter in the network, it uses these gradients to update each parameter with a\nGradient Descent step. Unfortunately, gradients often get smaller and smaller as the algorithm progresses\ndown to the lower layers. As a result, the Gradient Descent update leaves the lower\nlayers\u2019 connection weights virtually unchanged, and training never converges to a\ngood solution. We call this the vanishing gradients problem. In some cases, the oppo\u2010\nsite can happen: the gradients can grow bigger and bigger until layers get insanely\nlarge weight updates and the algorithm diverges. This is the exploding gradients prob\u2010\nlem, which surfaces in recurrent neural networks (see Chapter 15)."
  },
  {
    "id": 214,
    "content": "More generally,\ndeep neural networks suffer from unstable gradients; different layers may learn at\nwidely different speeds. This unfortunate behavior was empirically observed long ago, and it was one of the\nreasons deep neural networks were mostly abandoned in the early 2000s. It wasn\u2019t\nclear what caused the gradients to be so unstable when training a DNN, but some\nlight was shed in a 2010 paper by Xavier Glorot and Yoshua Bengio.1 The authors\nfound a few suspects, including the combination of the popular logistic sigmoid acti\u2010\nvation function and the weight initialization technique that was most popular at the\ntime (i.e., a normal distribution with a mean of 0 and a standard deviation of 1). In\nshort, they showed that with this activation function and this initialization scheme,\nthe variance of the outputs of each layer is much greater than the variance of its\ninputs. Going forward in the network, the variance keeps increasing after each layer\nuntil the activation function saturates at the top layers. This saturation is actually\nmade worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyper\u2010\nbolic tangent function has a mean of 0 and behaves slightly better than the logistic\nfunction in deep networks). Looking at the logistic activation function (see Figure 11-1), you can see that when\ninputs become large (negative or positive), the function saturates at 0 or 1, with a\nderivative extremely close to 0. Thus, when backpropagation kicks in it has virtually\nno gradient to propagate back through the network; and what little gradient exists\nkeeps getting diluted as backpropagation progresses down through the top layers, so\nthere is really nothing left for the lower layers. | Chapter 11: Training Deep Neural Networks\n2 Here\u2019s an analogy: if you set a microphone amplifier\u2019s knob too close to zero, people won\u2019t hear your voice, but\nif you set it too close to the max, your voice will be saturated and people won\u2019t understand what you are say\u2010\ning. Now imagine a chain of such amplifiers: they all need to be set properly in order for your voice to come\nout loud and clear at the end of the chain. Your voice has to come out of each amplifier at the same amplitude\nas it came in. Figure 11-1. Logistic activation function saturation\nGlorot and He Initialization\nIn their paper, Glorot and Bengio propose a way to significantly alleviate the unstable\ngradients problem. They point out that we need the signal to flow properly in both\ndirections: in the forward direction when making predictions, and in the reverse\ndirection when backpropagating gradients. We don\u2019t want the signal to die out, nor\ndo we want it to explode and saturate."
  },
  {
    "id": 215,
    "content": "For the signal to flow properly, the authors\nargue that we need the variance of the outputs of each layer to be equal to the var\u2010\niance of its inputs,2 and we need the gradients to have equal variance before and after\nflowing through a layer in the reverse direction (please check out the paper if you are\ninterested in the mathematical details). It is actually not possible to guarantee both\nunless the layer has an equal number of inputs and neurons (these numbers are called\nthe fan-in and fan-out of the layer), but Glorot and Bengio proposed a good compro\u2010\nmise that has proven to work very well in practice: the connection weights of each\nlayer must be initialized randomly as described in Equation 11-1, where fanavg = (fanin\n+ fanout)/2. This initialization strategy is called Xavier initialization or Glorot initiali\u2010\nzation, after the paper\u2019s first author. The Vanishing/Exploding Gradients Problems | 3 E.g., Kaiming He et al., \u201cDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet\nClassification,\u201d Proceedings of the 2015 IEEE International Conference on Computer Vision (2015): 1026\u20131034. Equation 11-1. Glorot initialization (when using the logistic activation function)\nNormal distribution with mean 0 and variance\u00a0\u03c32 = fanavg\nOr a uniform distribution between\u00a0\u2212r\u00a0and + r, with\u00a0r = fanavg\nIf you replace fanavg with fanin in Equation 11-1, you get an initialization strategy that\nYann LeCun proposed in the 1990s. He called it LeCun initialization. Genevieve Orr\nand Klaus-Robert M\u00fcller even recommended it in their 1998 book Neural Networks:\nTricks of the Trade (Springer). LeCun initialization is equivalent to Glorot initializa\u2010\ntion when fanin = fanout. It took over a decade for researchers to realize how important\nthis trick is. Using Glorot initialization can speed up training considerably, and it is\none of the tricks that led to the success of Deep Learning. Some papers3 have provided similar strategies for different activation functions. These strategies differ only by the scale of the variance and whether they use fanavg or\nfanin, as shown in Table 11-1 (for the uniform distribution, just compute r =\n3\u03c32). The initialization strategy for the ReLU activation function (and its variants, includ\u2010\ning the ELU activation described shortly) is sometimes called He initialization, after\nthe paper\u2019s first author. The SELU activation function will be explained later in this\nchapter. It should be used with LeCun initialization (preferably with a normal distri\u2010\nbution, as we will see). Table 11-1. Initialization parameters for each type of activation function\nInitialization Activation functions\n\u03c3\u00b2 (Normal)\nGlorot\nNone, tanh, logistic, softmax\n1 / fanavg\nHe\nReLU and variants\n2 / fanin\nLeCun\nSELU\n1 / fanin\nBy default, Keras uses Glorot initialization with a uniform distribution."
  },
  {
    "id": 216,
    "content": "When creat\u2010\ning a layer, you can change this to He initialization by setting kernel_initial\nizer=\"he_uniform\" or kernel_initializer=\"he_normal\" like this:\nkeras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\nIf you want He initialization with a uniform distribution but based on fanavg rather\nthan fanin, you can use the VarianceScaling initializer like this: | Chapter 11: Training Deep Neural Networks\n4 Unless it is part of the first hidden layer, a dead neuron may sometimes come back to life: Gradient Descent\nmay indeed tweak neurons in the layers below in such a way that the weighted sum of the dead neuron\u2019s\ninputs is positive again. 5 Bing Xu et al., \u201cEmpirical Evaluation of Rectified Activations in Convolutional Network,\u201d arXiv preprint\narXiv:1505.00853 (2015). he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\nkeras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\nNonsaturating Activation Functions\nOne of the insights in the 2010 paper by Glorot and Bengio was that the problems\nwith unstable gradients were in part due to a poor choice of activation function. Until\nthen most people had assumed that if Mother Nature had chosen to use roughly sig\u2010\nmoid activation functions in biological neurons, they must be an excellent choice. But\nit turns out that other activation functions behave much better in deep neural net\u2010\nworks\u2014in particular, the ReLU activation function, mostly because it does not satu\u2010\nrate for positive values (and because it is fast to compute). Unfortunately, the ReLU activation function is not perfect. It suffers from a problem\nknown as the dying ReLUs: during training, some neurons effectively \u201cdie,\u201d meaning\nthey stop outputting anything other than 0. In some cases, you may find that half of\nyour network\u2019s neurons are dead, especially if you used a large learning rate. A neu\u2010\nron dies when its weights get tweaked in such a way that the weighted sum of its\ninputs are negative for all instances in the training set. When this happens, it just\nkeeps outputting zeros, and Gradient Descent does not affect it anymore because the\ngradient of the ReLU function is zero when its input is negative.4\nTo solve this problem, you may want to use a variant of the ReLU function, such as\nthe leaky ReLU. This function is defined as LeakyReLU\u03b1(z) = max(\u03b1z, z) (see\nFigure 11-2). The hyperparameter \u03b1 defines how much the function \u201cleaks\u201d: it is the\nslope of the function for z < 0 and is typically set to 0.01. This small slope ensures that\nleaky ReLUs never die; they can go into a long coma, but they have a chance to even\u2010\ntually wake up. A 2015 paper5 compared several variants of the ReLU activation func\u2010\ntion, and one of its conclusions was that the leaky variants always outperformed the\nstrict ReLU activation function. In fact, setting \u03b1 = 0.2 (a huge leak) seemed to result\nin better performance than \u03b1 = 0.01 (a small leak). The paper also evaluated the\nrandomized leaky ReLU (RReLU), where \u03b1 is picked randomly in a given range during\ntraining and is fixed to an average value during testing."
  },
  {
    "id": 217,
    "content": "RReLU also performed fairly\nwell and seemed to act as a regularizer (reducing the risk of overfitting the training\nset). Finally, the paper evaluated the parametric leaky ReLU (PReLU), where \u03b1 is\nauthorized to be learned during training (instead of being a hyperparameter, it\nbecomes a parameter that can be modified by backpropagation like any other param\u2010\nThe Vanishing/Exploding Gradients Problems | 6 Djork-Arn\u00e9 Clevert et al., \u201cFast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),\u201d\nProceedings of the International Conference on Learning Representations (2016). eter). PReLU was reported to strongly outperform ReLU on large image datasets, but\non smaller datasets it runs the risk of overfitting the training set. Figure 11-2. Leaky ReLU: like ReLU, but with a small slope for negative values\nLast but not least, a 2015 paper by Djork-Arn\u00e9 Clevert et al.6 proposed a new activa\u2010\ntion function called the exponential linear unit (ELU) that outperformed all the ReLU\nvariants in the authors\u2019 experiments: training time was reduced, and the neural net\u2010\nwork performed better on the test set. Figure 11-3 graphs the function, and Equation\n11-2 shows its definition. Equation 11-2. ELU activation function\nELU\u03b1 z = \u03b1 exp z \u22121 if z < 0\nz\nif z \u22650\nFigure 11-3. ELU activation function | Chapter 11: Training Deep Neural Networks\n7 G\u00fcnter Klambauer et al., \u201cSelf-Normalizing Neural Networks,\u201d Proceedings of the 31st International Conference\non Neural Information Processing Systems (2017): 972\u2013981. The ELU activation function looks a lot like the ReLU function, with a few major\ndifferences:\n\u2022 It takes on negative values when z < 0, which allows the unit to have an average\noutput closer to 0 and helps alleviate the vanishing gradients problem. The\nhyperparameter \u03b1 defines the value that the ELU function approaches when z is a\nlarge negative number. It is usually set to 1, but you can tweak it like any other\nhyperparameter. \u2022 It has a nonzero gradient for z < 0, which avoids the dead neurons problem. \u2022 If \u03b1 is equal to 1 then the function is smooth everywhere, including around z = 0,\nwhich helps speed up Gradient Descent since it does not bounce as much to the\nleft and right of z = 0. The main drawback of the ELU activation function is that it is slower to compute\nthan the ReLU function and its variants (due to the use of the exponential function). Its faster convergence rate during training compensates for that slow computation,\nbut still, at test time an ELU network will be slower than a ReLU network. Then, a 2017 paper7 by G\u00fcnter Klambauer et al. introduced the Scaled ELU (SELU)\nactivation function: as its name suggests, it is a scaled variant of the ELU activation\nfunction."
  },
  {
    "id": 218,
    "content": "The authors showed that if you build a neural network composed exclu\u2010\nsively of a stack of dense layers, and if all hidden layers use the SELU activation func\u2010\ntion, then the network will self-normalize: the output of each layer will tend to\npreserve a mean of 0 and standard deviation of 1 during training, which solves the\nvanishing/exploding gradients problem. As a result, the SELU activation function\noften significantly outperforms other activation functions for such neural nets (espe\u2010\ncially deep ones). There are, however, a few conditions for self-normalization to hap\u2010\npen (see the paper for the mathematical justification):\n\u2022 The input features must be standardized (mean 0 and standard deviation 1). \u2022 Every hidden layer\u2019s weights must be initialized with LeCun normal initialization. In Keras, this means setting kernel_initializer=\"lecun_normal\". \u2022 The network\u2019s architecture must be sequential. Unfortunately, if you try to use\nSELU in nonsequential architectures, such as recurrent networks (see Chap\u2010\nter 15) or networks with skip connections (i.e., connections that skip layers, such\nas in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will\nnot necessarily outperform other activation functions. The Vanishing/Exploding Gradients Problems | \u2022 The paper only guarantees self-normalization if all layers are dense, but some\nresearchers have noted that the SELU activation function can improve perfor\u2010\nmance in convolutional neural nets as well (see Chapter 14). So, which activation function should you use for the hidden layers\nof your deep neural networks? Although your mileage will vary, in\ngeneral SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh\n> logistic. If the network\u2019s architecture prevents it from self-\nnormalizing, then ELU may perform better than SELU (since SELU\nis not smooth at z = 0). If you care a lot about runtime latency, then\nyou may prefer leaky ReLU. If you don\u2019t want to tweak yet another\nhyperparameter, you may use the default \u03b1 values used by Keras\n(e.g., 0.3 for leaky ReLU). If you have spare time and computing\npower, you can use cross-validation to evaluate other activation\nfunctions, such as RReLU if your network is overfitting or PReLU\nif you have a huge training set. That said, because ReLU is the most\nused activation function (by far), many libraries and hardware\naccelerators provide ReLU-specific optimizations; therefore, if\nspeed is your priority, ReLU might still be the best choice. To use the leaky ReLU activation function, create a LeakyReLU layer and add it to your\nmodel just after the layer you want to apply it to:\nmodel = keras.models.Sequential([ [...] keras.layers.Dense(10, kernel_initializer=\"he_normal\"), keras.layers.LeakyReLU(alpha=0.2), [...]\n])\nFor PReLU, replace LeakyRelu(alpha=0.2) with PReLU(). There is currently no offi\u2010\ncial implementation of RReLU in Keras, but you can fairly easily implement your own\n(to learn how to do that, see the exercises at the end of Chapter 12)."
  },
  {
    "id": 219,
    "content": "For SELU activation, set activation=\"selu\" and kernel_initializer=\"lecun_nor\nmal\" when creating a layer:\nlayer = keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")\nBatch Normalization\nAlthough using He initialization along with ELU (or any variant of ReLU) can signifi\u2010\ncantly reduce the danger of the vanishing/exploding gradients problems at the begin\u2010\nning of training, it doesn\u2019t guarantee that they won\u2019t come back during training. | Chapter 11: Training Deep Neural Networks\n8 Sergey Ioffe and Christian Szegedy, \u201cBatch Normalization: Accelerating Deep Network Training by Reducing\nInternal Covariate Shift,\u201d Proceedings of the 32nd International Conference on Machine Learning (2015): 448\u2013\n456. In a 2015 paper,8 Sergey Ioffe and Christian Szegedy proposed a technique called\nBatch Normalization (BN) that addresses these problems. The technique consists of\nadding an operation in the model just before or after the activation function of each\nhidden layer. This operation simply zero-centers and normalizes each input, then\nscales and shifts the result using two new parameter vectors per layer: one for scaling,\nthe other for shifting. In other words, the operation lets the model learn the optimal\nscale and mean of each of the layer\u2019s inputs. In many cases, if you add a BN layer as\nthe very first layer of your neural network, you do not need to standardize your train\u2010\ning set (e.g., using a StandardScaler); the BN layer will do it for you (well, approxi\u2010\nmately, since it only looks at one batch at a time, and it can also rescale and shift each\ninput feature). In order to zero-center and normalize the inputs, the algorithm needs to estimate\neach input\u2019s mean and standard deviation. It does so by evaluating the mean and stan\u2010\ndard deviation of the input over the current mini-batch (hence the name \u201cBatch Nor\u2010\nmalization\u201d). The whole operation is summarized step by step in Equation 11-3. Equation 11-3. Batch Normalization algorithm\n1 . \u03bcB = 1\nmB \u2211\ni = 1\nmB\nx i\n2 . \u03c3B\n2 = 1\nmB \u2211\ni = 1\nmB\nx i \u2212\u03bcB 3 . x i =\nx i \u2212\u03bcB\n\u03c3B\n2 + \u03b5\n4 . z i = \u03b3 \u2297x i + \u03b2\nIn this algorithm:\n\u2022 \u03bcB is the vector of input means, evaluated over the whole mini-batch B (it con\u2010\ntains one mean per input). \u2022 \u03c3B is the vector of input standard deviations, also evaluated over the whole mini-\nbatch (it contains one standard deviation per input). \u2022 mB is the number of instances in the mini-batch. \u2022 x(i) is the vector of zero-centered and normalized inputs for instance i. The Vanishing/Exploding Gradients Problems | \u2022 \u03b3 is the output scale parameter vector for the layer (it contains one scale parame\u2010\nter per input). \u2022 \u2297 represents element-wise multiplication (each input is multiplied by its corre\u2010\nsponding output scale parameter). \u2022 \u03b2 is the output shift (offset) parameter vector for the layer (it contains one offset\nparameter per input). Each input is offset by its corresponding shift parameter."
  },
  {
    "id": 220,
    "content": "\u2022 \u03b5 is a tiny number that avoids division by zero (typically 10\u20135). This is called a\nsmoothing term. \u2022 z(i) is the output of the BN operation. It is a rescaled and shifted version of the\ninputs. So during training, BN standardizes its inputs, then rescales and offsets them. Good! What about at test time? Well, it\u2019s not that simple. Indeed, we may need to make pre\u2010\ndictions for individual instances rather than for batches of instances: in this case, we\nwill have no way to compute each input\u2019s mean and standard deviation. Moreover,\neven if we do have a batch of instances, it may be too small, or the instances may not\nbe independent and identically distributed, so computing statistics over the batch\ninstances would be unreliable. One solution could be to wait until the end of training,\nthen run the whole training set through the neural network and compute the mean\nand standard deviation of each input of the BN layer. These \u201cfinal\u201d input means and\nstandard deviations could then be used instead of the batch input means and stan\u2010\ndard deviations when making predictions. However, most implementations of Batch\nNormalization estimate these final statistics during training by using a moving aver\u2010\nage of the layer\u2019s input means and standard deviations. This is what Keras does auto\u2010\nmatically when you use the BatchNormalization layer. To sum up, four parameter\nvectors are learned in each batch-normalized layer: \u03b3 (the output scale vector) and \u03b2\n(the output offset vector) are learned through regular backpropagation, and \u03bc (the\nfinal input mean vector) and \u03c3 (the final input standard deviation vector) are estima\u2010\nted using an exponential moving average. Note that \u03bc and \u03c3 are estimated during\ntraining, but they are used only after training (to replace the batch input means and\nstandard deviations in Equation 11-3). Ioffe and Szegedy demonstrated that Batch Normalization considerably improved all\nthe deep neural networks they experimented with, leading to a huge improvement in\nthe ImageNet classification task (ImageNet is a large database of images classified into\nmany classes, commonly used to evaluate computer vision systems). The vanishing\ngradients problem was strongly reduced, to the point that they could use saturating\nactivation functions such as the tanh and even the logistic activation function. The\nnetworks were also much less sensitive to the weight initialization. The authors were\nable to use much larger learning rates, significantly speeding up the learning process. Specifically, they note that: | Chapter 11: Training Deep Neural Networks\nApplied to a state-of-the-art image classification model, Batch Normalization achieves\nthe same accuracy with 14 times fewer training steps, and beats the original model by a\nsignificant margin. [\u2026] Using an ensemble of batch-normalized networks, we improve\nupon the best published result on ImageNet classification: reaching 4.9% top-5 valida\u2010\ntion error (and 4.8% test error), exceeding the accuracy of human raters."
  },
  {
    "id": 221,
    "content": "Finally, like a gift that keeps on giving, Batch Normalization acts like a regularizer,\nreducing the need for other regularization techniques (such as dropout, described\nlater in this chapter). Batch Normalization does, however, add some complexity to the model (although it\ncan remove the need for normalizing the input data, as we discussed earlier). More\u2010\nover, there is a runtime penalty: the neural network makes slower predictions due to\nthe extra computations required at each layer. Fortunately, it\u2019s often possible to fuse\nthe BN layer with the previous layer, after training, thereby avoiding the runtime pen\u2010\nalty. This is done by updating the previous layer\u2019s weights and biases so that it directly\nproduces outputs of the appropriate scale and offset. For example, if the previous\nlayer computes XW + b, then the BN layer will compute \u03b3\u2297(XW + b \u2013 \u03bc)/\u03c3 + \u03b2\n(ignoring the smoothing term \u03b5 in the denominator). If we define W\u2032 = \u03b3\u2297W/\u03c3 and b\n\u2032 = \u03b3\u2297(b \u2013 \u03bc)/\u03c3 + \u03b2, the equation simplifies to XW\u2032 + b\u2032. So if we replace the previous\nlayer\u2019s weights and biases (W and b) with the updated weights and biases (W\u2032 and b\u2032),\nwe can get rid of the BN layer (TFLite\u2019s optimizer does this automatically; see Chap\u2010\nter 19). You may find that training is rather slow, because each epoch takes\nmuch more time when you use Batch Normalization. This is usu\u2010\nally counterbalanced by the fact that convergence is much faster\nwith BN, so it will take fewer epochs to reach the same perfor\u2010\nmance. All in all, wall time will usually be shorter (this is the time\nmeasured by the clock on your wall). Implementing Batch Normalization with Keras\nAs with most things with Keras, implementing Batch Normalization is simple and\nintuitive. Just add a BatchNormalization layer before or after each hidden layer\u2019s\nactivation function, and optionally add a BN layer as well as the first layer in your\nmodel. For example, this model applies BN after every hidden layer and as the first\nlayer in the model (after flattening the input images):\nThe Vanishing/Exploding Gradients Problems | 9 However, they are estimated during training, based on the training data, so arguably they are trainable. In\nKeras, \u201cnon-trainable\u201d really means \u201cuntouched by backpropagation.\u201d\nmodel = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.BatchNormalization(), keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"), keras.layers.BatchNormalization(), keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"), keras.layers.BatchNormalization(), keras.layers.Dense(10, activation=\"softmax\")\n])\nThat\u2019s all! In this tiny example with just two hidden layers, it\u2019s unlikely that Batch\nNormalization will have a very positive impact; but for deeper networks it can make a\ntremendous difference."
  },
  {
    "id": 222,
    "content": "Let\u2019s display the model summary:\n>>> model.summary()\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type) Output Shape Param #\n=================================================================\nflatten_3 (Flatten) (None, 784) 0\n_________________________________________________________________\nbatch_normalization_v2 (Batc (None, 784) 3136\n_________________________________________________________________\ndense_50 (Dense) (None, 300) 235500\n_________________________________________________________________\nbatch_normalization_v2_1 (Ba (None, 300) 1200\n_________________________________________________________________\ndense_51 (Dense) (None, 100) 30100\n_________________________________________________________________\nbatch_normalization_v2_2 (Ba (None, 100) 400\n_________________________________________________________________\ndense_52 (Dense) (None, 10) 1010\n=================================================================\nTotal params: 271,346\nTrainable params: 268,978\nNon-trainable params: 2,368\nAs you can see, each BN layer adds four parameters per input: \u03b3, \u03b2, \u03bc, and \u03c3 (for\nexample, the first BN layer adds 3,136 parameters, which is 4 \u00d7 784). The last two\nparameters, \u03bc and \u03c3, are the moving averages; they are not affected by backpropaga\u2010\ntion, so Keras calls them \u201cnon-trainable\u201d9 (if you count the total number of BN\nparameters, 3,136 + 1,200 + 400, and divide by 2, you get 2,368, which is the total\nnumber of non-trainable parameters in this model). | Chapter 11: Training Deep Neural Networks\nLet\u2019s look at the parameters of the first BN layer. Two are trainable (by backpropaga\u2010\ntion), and two are not:\n>>> [(var.name, var.trainable) for var in model.layers[1].variables]\n[('batch_normalization_v2/gamma:0', True), ('batch_normalization_v2/beta:0', True), ('batch_normalization_v2/moving_mean:0', False), ('batch_normalization_v2/moving_variance:0', False)]\nNow when you create a BN layer in Keras, it also creates two operations that will be\ncalled by Keras at each iteration during training. These operations will update the\nmoving averages. Since we are using the TensorFlow backend, these operations are\nTensorFlow operations (we will discuss TF operations in Chapter 12):\n>>> model.layers[1].updates\n[<tf.Operation 'cond_2/Identity' type=Identity>, <tf.Operation 'cond_3/Identity' type=Identity>]\nThe authors of the BN paper argued in favor of adding the BN layers before the acti\u2010\nvation functions, rather than after (as we just did). There is some debate about this, as\nwhich is preferable seems to depend on the task\u2014you can experiment with this too to\nsee which option works best on your dataset. To add the BN layers before the activa\u2010\ntion functions, you must remove the activation function from the hidden layers and\nadd them as separate layers after the BN layers. Moreover, since a Batch Normaliza\u2010\ntion layer includes one offset parameter per input, you can remove the bias term from\nthe previous layer (just pass use_bias=False when creating it):\nmodel = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.BatchNormalization(), keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False), keras.layers.BatchNormalization(), keras.layers.Activation(\"elu\"), keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False), keras.layers.BatchNormalization(), keras.layers.Activation(\"elu\"), keras.layers.Dense(10, activation=\"softmax\")\n])\nThe BatchNormalization class has quite a few hyperparameters you can tweak. The\ndefaults will usually be fine, but you may occasionally need to tweak the momentum. This hyperparameter is used by the BatchNormalization layer when it updates the\nexponential moving averages; given a new value v (i.e., a new vector of input means\nor standard deviations computed over the current batch), the layer updates the run\u2010\nning average \ufffd using the following equation:\nv\nv \u00d7 momentum + v \u00d7 1 \u2212momentum\nThe Vanishing/Exploding Gradients Problems | 10 The Keras API also specifies a keras.backend.learning_phase() function that should return 1 during train\u2010\ning and 0 otherwise."
  },
  {
    "id": 223,
    "content": "11 Hongyi Zhang et al., \u201cFixup Initialization: Residual Learning Without Normalization,\u201d arXiv preprint arXiv:\n1901.09321 (2019). A good momentum value is typically close to 1; for example, 0.9, 0.99, or 0.999 (you\nwant more 9s for larger datasets and smaller mini-batches). Another important hyperparameter is axis: it determines which axis should be nor\u2010\nmalized. It defaults to \u20131, meaning that by default it will normalize the last axis (using\nthe means and standard deviations computed across the other axes). When the input\nbatch is 2D (i.e., the batch shape is [batch size, features]), this means that each input\nfeature will be normalized based on the mean and standard deviation computed\nacross all the instances in the batch. For example, the first BN layer in the previous\ncode example will independently normalize (and rescale and shift) each of the 784\ninput features. If we move the first BN layer before the Flatten layer, then the input\nbatches will be 3D, with shape [batch size, height, width]; therefore, the BN layer will\ncompute 28 means and 28 standard deviations (1 per column of pixels, computed\nacross all instances in the batch and across all rows in the column), and it will nor\u2010\nmalize all pixels in a given column using the same mean and standard deviation. There will also be just 28 scale parameters and 28 shift parameters. If instead you still\nwant to treat each of the 784 pixels independently, then you should set axis=[1, 2]. Notice that the BN layer does not perform the same computation during training and\nafter training: it uses batch statistics during training and the \u201cfinal\u201d statistics after\ntraining (i.e., the final values of the moving averages). Let\u2019s take a peek at the source\ncode of this class to see how this is handled:\nclass BatchNormalization(keras.layers.Layer): [...] def call(self, inputs, training=None): [...]\nThe call() method is the one that performs the computations; as you can see, it has\nan extra training argument, which is set to None by default, but the fit() method\nsets to it to 1 during training. If you ever need to write a custom layer, and it must\nbehave differently during training and testing, add a training argument to the\ncall() method and use this argument in the method to decide what to compute10 (we\nwill discuss custom layers in Chapter 12). BatchNormalization has become one of the most-used layers in deep neural net\u2010\nworks, to the point that it is often omitted in the diagrams, as it is assumed that BN is\nadded after every layer. But a recent paper11 by Hongyi Zhang et al. may change this\nassumption: by using a novel fixed-update (fixup) weight initialization technique, the\nauthors managed to train a very deep neural network (10,000 layers!) without BN, | Chapter 11: Training Deep Neural Networks\n12 Razvan Pascanu et al., \u201cOn the Difficulty of Training Recurrent Neural Networks,\u201d Proceedings of the 30th\nInternational Conference on Machine Learning (2013): 1310\u20131318. achieving state-of-the-art performance on complex image classification tasks."
  },
  {
    "id": 224,
    "content": "As this\nis bleeding-edge research, however, you may want to wait for additional research to\nconfirm this finding before you drop Batch Normalization. Gradient Clipping\nAnother popular technique to mitigate the exploding gradients problem is to clip the\ngradients during backpropagation so that they never exceed some threshold. This is\ncalled Gradient Clipping.12 This technique is most often used in recurrent neural net\u2010\nworks, as Batch Normalization is tricky to use in RNNs, as we will see in Chapter 15. For other types of networks, BN is usually sufficient. In Keras, implementing Gradient Clipping is just a matter of setting the clipvalue or\nclipnorm argument when creating an optimizer, like this:\noptimizer = keras.optimizers.SGD(clipvalue=1.0)\nmodel.compile(loss=\"mse\", optimizer=optimizer)\nThis optimizer will clip every component of the gradient vector to a value between\n\u20131.0 and 1.0. This means that all the partial derivatives of the loss (with regard to each\nand every trainable parameter) will be clipped between \u20131.0 and 1.0. The threshold is\na hyperparameter you can tune. Note that it may change the orientation of the gradi\u2010\nent vector. For instance, if the original gradient vector is [0.9, 100.0], it points mostly\nin the direction of the second axis; but once you clip it by value, you get [0.9, 1.0],\nwhich points roughly in the diagonal between the two axes. In practice, this approach\nworks well. If you want to ensure that Gradient Clipping does not change the direc\u2010\ntion of the gradient vector, you should clip by norm by setting clipnorm instead of\nclipvalue. This will clip the whole gradient if its \u21132 norm is greater than the thres\u2010\nhold you picked. For example, if you set clipnorm=1.0, then the vector [0.9, 100.0]\nwill be clipped to [0.00899964, 0.9999595], preserving its orientation but almost elim\u2010\ninating the first component. If you observe that the gradients explode during training\n(you can track the size of the gradients using TensorBoard), you may want to try both\nclipping by value and clipping by norm, with different thresholds, and see which\noption performs best on the validation set. Reusing Pretrained Layers\nIt is generally not a good idea to train a very large DNN from scratch: instead, you\nshould always try to find an existing neural network that accomplishes a similar task\nto the one you are trying to tackle (we will discuss how to find them in Chapter 14),\nthen reuse the lower layers of this network. This technique is called transfer learning. Reusing Pretrained Layers | It will not only speed up training considerably, but also require significantly less\ntraining data. Suppose you have access to a DNN that was trained to classify pictures into 100 dif\u2010\nferent categories, including animals, plants, vehicles, and everyday objects. You now\nwant to train a DNN to classify specific types of vehicles. These tasks are very similar,\neven partly overlapping, so you should try to reuse parts of the first network (see\nFigure 11-4). Figure 11-4."
  },
  {
    "id": 225,
    "content": "Reusing pretrained layers\nIf the input pictures of your new task don\u2019t have the same size as\nthe ones used in the original task, you will usually have to add a\npreprocessing step to resize them to the size expected by the origi\u2010\nnal model. More generally, transfer learning will work best when\nthe inputs have similar low-level features. The output layer of the original model should usually be replaced because it is most\nlikely not useful at all for the new task, and it may not even have the right number of\noutputs for the new task. Similarly, the upper hidden layers of the original model are less likely to be as useful\nas the lower layers, since the high-level features that are most useful for the new task\nmay differ significantly from the ones that were most useful for the original task. You\nwant to find the right number of layers to reuse. | Chapter 11: Training Deep Neural Networks\nThe more similar the tasks are, the more layers you want to reuse\n(starting with the lower layers). For very similar tasks, try keeping\nall the hidden layers and just replacing the output layer. Try freezing all the reused layers first (i.e., make their weights non-trainable so that\nGradient Descent won\u2019t modify them), then train your model and see how it per\u2010\nforms. Then try unfreezing one or two of the top hidden layers to let backpropaga\u2010\ntion tweak them and see if performance improves. The more training data you have,\nthe more layers you can unfreeze. It is also useful to reduce the learning rate when\nyou unfreeze reused layers: this will avoid wrecking their fine-tuned weights. If you still cannot get good performance, and you have little training data, try drop\u2010\nping the top hidden layer(s) and freezing all the remaining hidden layers again. You\ncan iterate until you find the right number of layers to reuse. If you have plenty of\ntraining data, you may try replacing the top hidden layers instead of dropping them,\nand even adding more hidden layers. Transfer Learning with Keras\nLet\u2019s look at an example. Suppose the Fashion MNIST dataset only contained eight\nclasses\u2014for example, all the classes except for sandal and shirt. Someone built and\ntrained a Keras model on that set and got reasonably good performance (>90% accu\u2010\nracy). Let\u2019s call this model A. You now want to tackle a different task: you have images\nof sandals and shirts, and you want to train a binary classifier (positive=shirt,\nnegative=sandal). Your dataset is quite small; you only have 200 labeled images. When you train a new model for this task (let\u2019s call it model B) with the same archi\u2010\ntecture as model A, it performs reasonably well (97.2% accuracy). But since it\u2019s a\nmuch easier task (there are just two classes), you were hoping for more."
  },
  {
    "id": 226,
    "content": "While drink\u2010\ning your morning coffee, you realize that your task is quite similar to task A, so per\u2010\nhaps transfer learning can help? Let\u2019s find out! First, you need to load model A and create a new model based on that model\u2019s layers. Let\u2019s reuse all the layers except for the output layer:\nmodel_A = keras.models.load_model(\"my_model_A.h5\")\nmodel_B_on_A = keras.models.Sequential(model_A.layers[:-1])\nmodel_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\nNote that model_A and model_B_on_A now share some layers. When you train\nmodel_B_on_A, it will also affect model_A. If you want to avoid that, you need to clone\nmodel_A before you reuse its layers. To do this, you clone model A\u2019s architecture with\nclone_model(), then copy its weights (since clone_model() does not clone the\nweights):\nReusing Pretrained Layers | model_A_clone = keras.models.clone_model(model_A)\nmodel_A_clone.set_weights(model_A.get_weights())\nNow you could train model_B_on_A for task B, but since the new output layer was ini\u2010\ntialized randomly it will make large errors (at least during the first few epochs), so\nthere will be large error gradients that may wreck the reused weights. To avoid this,\none approach is to freeze the reused layers during the first few epochs, giving the new\nlayer some time to learn reasonable weights. To do this, set every layer\u2019s trainable\nattribute to False and compile the model:\nfor layer in model_B_on_A.layers[:-1]: layer.trainable = False\nmodel_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\nYou must always compile your model after you freeze or unfreeze\nlayers. Now you can train the model for a few epochs, then unfreeze the reused layers (which\nrequires compiling the model again) and continue training to fine-tune the reused\nlayers for task B. After unfreezing the reused layers, it is usually a good idea to reduce\nthe learning rate, once again to avoid damaging the reused weights:\nhistory = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, validation_data=(X_valid_B, y_valid_B))\nfor layer in model_B_on_A.layers[:-1]: layer.trainable = True\noptimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-2\nmodel_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nhistory = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, validation_data=(X_valid_B, y_valid_B))\nSo, what\u2019s the final verdict? Well, this model\u2019s test accuracy is 99.25%, which means\nthat transfer learning reduced the error rate from 2.8% down to almost 0.7%! That\u2019s a\nfactor of four! >>> model_B_on_A.evaluate(X_test_B, y_test_B)\n[0.06887910133600235, 0.9925]\nAre you convinced? You shouldn\u2019t be: I cheated! I tried many configurations until I\nfound one that demonstrated a strong improvement. If you try to change the classes\nor the random seed, you will see that the improvement generally drops, or even van\u2010\nishes or reverses. What I did is called \u201ctorturing the data until it confesses.\u201d When a | Chapter 11: Training Deep Neural Networks\npaper just looks too positive, you should be suspicious: perhaps the flashy new tech\u2010\nnique does not actually help much (in fact, it may even degrade performance), but the\nauthors tried many variants and reported only the best results (which may be due to\nsheer luck), without mentioning how many failures they encountered on the way."
  },
  {
    "id": 227,
    "content": "Most of the time, this is not malicious at all, but it is part of the reason so many\nresults in science can never be reproduced. Why did I cheat? It turns out that transfer learning does not work very well with\nsmall dense networks, presumably because small networks learn few patterns, and\ndense networks learn very specific patterns, which are unlikely to be useful in other\ntasks. Transfer learning works best with deep convolutional neural networks, which\ntend to learn feature detectors that are much more general (especially in the lower\nlayers). We will revisit transfer learning in Chapter 14, using the techniques we just\ndiscussed (and this time there will be no cheating, I promise!). Unsupervised Pretraining\nSuppose you want to tackle a complex task for which you don\u2019t have much labeled\ntraining data, but unfortunately you cannot find a model trained on a similar task. Don\u2019t lose hope! First, you should try to gather more labeled training data, but if you\ncan\u2019t, you may still be able to perform unsupervised pretraining (see Figure 11-5). Indeed, it is often cheap to gather unlabeled training examples, but expensive to label\nthem. If you can gather plenty of unlabeled training data, you can try to use it to train\nan unsupervised model, such as an autoencoder or a generative adversarial network\n(see Chapter 17). Then you can reuse the lower layers of the autoencoder or the lower\nlayers of the GAN\u2019s discriminator, add the output layer for your task on top, and fine-\ntune the final network using supervised learning (i.e., with the labeled training\nexamples). It is this technique that Geoffrey Hinton and his team used in 2006 and which led to\nthe revival of neural networks and the success of Deep Learning. Until 2010, unsuper\u2010\nvised pretraining\u2014typically with restricted Boltzmann machines (RBMs; see Appen\u2010\ndix E)\u2014was the norm for deep nets, and only after the vanishing gradients problem\nwas alleviated did it become much more common to train DNNs purely using super\u2010\nvised learning. Unsupervised pretraining (today typically using autoencoders or\nGANs rather than RBMs) is still a good option when you have a complex task to\nsolve, no similar model you can reuse, and little labeled training data but plenty of\nunlabeled training data. Note that in the early days of Deep Learning it was difficult to train deep models, so\npeople would use a technique called greedy layer-wise pretraining (depicted in\nFigure 11-5). They would first train an unsupervised model with a single layer, typi\u2010\ncally an RBM, then they would freeze that layer and add another one on top of it,\nthen train the model again (effectively just training the new layer), then freeze the\nReusing Pretrained Layers | new layer and add another layer on top of it, train the model again, and so on."
  },
  {
    "id": 228,
    "content": "Nowa\u2010\ndays, things are much simpler: people generally train the full unsupervised model in\none shot (i.e., in Figure 11-5, just start directly at step three) and use autoencoders or\nGANs rather than RBMs. Figure 11-5. In unsupervised training, a model is trained on the unlabeled data (or on\nall the data) using an unsupervised learning technique, then it is fine-tuned for the final\ntask on the labeled data using a supervised learning technique; the unsupervised part\nmay train one layer at a time as shown here, or it may train the full model directly\nPretraining on an Auxiliary Task\nIf you do not have much labeled training data, one last option is to train a first neural\nnetwork on an auxiliary task for which you can easily obtain or generate labeled\ntraining data, then reuse the lower layers of that network for your actual task. The\nfirst neural network\u2019s lower layers will learn feature detectors that will likely be reusa\u2010\nble by the second neural network. For example, if you want to build a system to recognize faces, you may only have a\nfew pictures of each individual\u2014clearly not enough to train a good classifier. Gather\u2010\ning hundreds of pictures of each person would not be practical. You could, however,\ngather a lot of pictures of random people on the web and train a first neural network\nto detect whether or not two different pictures feature the same person. Such a | Chapter 11: Training Deep Neural Networks\n13 Boris T. Polyak, \u201cSome Methods of Speeding Up the Convergence of Iteration Methods,\u201d USSR Computational\nMathematics and Mathematical Physics 4, no. 5 (1964): 1\u201317. network would learn good feature detectors for faces, so reusing its lower layers\nwould allow you to train a good face classifier that uses little training data. For natural language processing (NLP) applications, you can download a corpus of\nmillions of text documents and automatically generate labeled data from it. For exam\u2010\nple, you could randomly mask out some words and train a model to predict what the\nmissing words are (e.g., it should predict that the missing word in the sentence \u201cWhat\n___ you saying?\u201d is probably \u201care\u201d or \u201cwere\u201d). If you can train a model to reach good\nperformance on this task, then it will already know quite a lot about language, and\nyou can certainly reuse it for your actual task and fine-tune it on your labeled data\n(we will discuss more pretraining tasks in Chapter 15). Self-supervised learning is when you automatically generate the\nlabels from the data itself, then you train a model on the resulting\n\u201clabeled\u201d dataset using supervised learning techniques. Since this\napproach requires no human labeling whatsoever, it is best classi\u2010\nfied as a form of unsupervised learning. Faster Optimizers\nTraining a very large deep neural network can be painfully slow."
  },
  {
    "id": 229,
    "content": "So far we have seen\nfour ways to speed up training (and reach a better solution): applying a good initiali\u2010\nzation strategy for the connection weights, using a good activation function, using\nBatch Normalization, and reusing parts of a pretrained network (possibly built on an\nauxiliary task or using unsupervised learning). Another huge speed boost comes from\nusing a faster optimizer than the regular Gradient Descent optimizer. In this section\nwe will present the most popular algorithms: momentum optimization, Nesterov\nAccelerated Gradient, AdaGrad, RMSProp, and finally Adam and Nadam\noptimization. Momentum Optimization\nImagine a bowling ball rolling down a gentle slope on a smooth surface: it will start\nout slowly, but it will quickly pick up momentum until it eventually reaches terminal\nvelocity (if there is some friction or air resistance). This is the very simple idea behind\nmomentum optimization, proposed by Boris Polyak in 1964.13 In contrast, regular\nGradient Descent will simply take small, regular steps down the slope, so the algo\u2010\nrithm will take much more time to reach the bottom. Faster Optimizers | Recall that Gradient Descent updates the weights \u03b8 by directly subtracting the gradi\u2010\nent of the cost function J(\u03b8) with regard to the weights (\u2207\u03b8J(\u03b8)) multiplied by the\nlearning rate \u03b7. The equation is: \u03b8 \u2190 \u03b8 \u2013 \u03b7\u2207\u03b8J(\u03b8). It does not care about what the ear\u2010\nlier gradients were. If the local gradient is tiny, it goes very slowly. Momentum optimization cares a great deal about what previous gradients were: at\neach iteration, it subtracts the local gradient from the momentum vector m (multi\u2010\nplied by the learning rate \u03b7), and it updates the weights by adding this momentum\nvector (see Equation 11-4). In other words, the gradient is used for acceleration, not\nfor speed. To simulate some sort of friction mechanism and prevent the momentum\nfrom growing too large, the algorithm introduces a new hyperparameter \u03b2, called the\nmomentum, which must be set between 0 (high friction) and 1 (no friction). A typical\nmomentum value is 0.9. Equation 11-4. Momentum algorithm\n1 . m\n\u03b2m \u2212\u03b7\u2207\u03b8J \u03b8\n2 . \u03b8\n\u03b8 + m\nYou can easily verify that if the gradient remains constant, the terminal velocity (i.e.,\nthe maximum size of the weight updates) is equal to that gradient multiplied by the\nlearning rate \u03b7 multiplied by 1/(1\u2013\u03b2) (ignoring the sign). For example, if \u03b2 = 0.9, then\nthe terminal velocity is equal to 10 times the gradient times the learning rate, so\nmomentum optimization ends up going 10 times faster than Gradient Descent! This\nallows momentum optimization to escape from plateaus much faster than Gradient\nDescent. We saw in Chapter 4 that when the inputs have very different scales, the cost\nfunction will look like an elongated bowl (see Figure 4-7). Gradient Descent goes\ndown the steep slope quite fast, but then it takes a very long time to go down the val\u2010\nley."
  },
  {
    "id": 230,
    "content": "In contrast, momentum optimization will roll down the valley faster and faster\nuntil it reaches the bottom (the optimum). In deep neural networks that don\u2019t use\nBatch Normalization, the upper layers will often end up having inputs with very dif\u2010\nferent scales, so using momentum optimization helps a lot. It can also help roll past\nlocal optima. Due to the momentum, the optimizer may overshoot a bit, then\ncome back, overshoot again, and oscillate like this many times\nbefore stabilizing at the minimum. This is one of the reasons it\u2019s\ngood to have a bit of friction in the system: it gets rid of these oscil\u2010\nlations and thus speeds up convergence. Implementing momentum optimization in Keras is a no-brainer: just use the SGD\noptimizer and set its momentum hyperparameter, then lie back and profit! | Chapter 11: Training Deep Neural Networks\n14 Yurii Nesterov, \u201cA Method for Unconstrained Convex Minimization Problem with the Rate of Convergence\nO(1/k2),\u201d Doklady AN USSR 269 (1983): 543\u2013547. optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)\nThe one drawback of momentum optimization is that it adds yet another hyperpara\u2010\nmeter to tune. However, the momentum value of 0.9 usually works well in practice\nand almost always goes faster than regular Gradient Descent. Nesterov Accelerated Gradient\nOne small variant to momentum optimization, proposed by Yurii Nesterov in 1983,14\nis almost always faster than vanilla momentum optimization. The Nesterov Acceler\u2010\nated Gradient (NAG) method, also known as Nesterov momentum optimization, meas\u2010\nures the gradient of the cost function not at the local position \u03b8 but slightly ahead in\nthe direction of the momentum, at \u03b8 + \u03b2m (see Equation 11-5). Equation 11-5. Nesterov Accelerated Gradient algorithm\n1 . m\n\u03b2m \u2212\u03b7\u2207\u03b8J \u03b8 + \u03b2m\n2 . \u03b8\n\u03b8 + m\nThis small tweak works because in general the momentum vector will be pointing in\nthe right direction (i.e., toward the optimum), so it will be slightly more accurate to\nuse the gradient measured a bit farther in that direction rather than the gradient at\nthe original position, as you can see in Figure 11-6 (where \u22071 represents the gradient\nof the cost function measured at the starting point \u03b8, and \u22072 represents the gradient\nat the point located at \u03b8 + \u03b2m). As you can see, the Nesterov update ends up slightly closer to the optimum. After a\nwhile, these small improvements add up and NAG ends up being significantly faster\nthan regular momentum optimization. Moreover, note that when the momentum\npushes the weights across a valley, \u22071 continues to push farther across the valley,\nwhile \u22072 pushes back toward the bottom of the valley. This helps reduce oscillations\nand thus NAG converges faster. NAG is generally faster than regular momentum optimization. To use it, simply set\nnesterov=True when creating the SGD optimizer:\noptimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)\nFaster Optimizers | 15 John Duchi et al., \u201cAdaptive Subgradient Methods for Online Learning and Stochastic Optimization,\u201d Journal\nof Machine Learning Research 12 (2011): 2121\u20132159. Figure 11-6."
  },
  {
    "id": 231,
    "content": "Regular versus Nesterov momentum optimization: the former applies the\ngradients computed before the momentum step, while the latter applies the gradients\ncomputed after\nAdaGrad\nConsider the elongated bowl problem again: Gradient Descent starts by quickly going\ndown the steepest slope, which does not point straight toward the global optimum,\nthen it very slowly goes down to the bottom of the valley. It would be nice if the algo\u2010\nrithm could correct its direction earlier to point a bit more toward the global opti\u2010\nmum. The AdaGrad algorithm15 achieves this correction by scaling down the gradient\nvector along the steepest dimensions (see Equation 11-6). Equation 11-6. AdaGrad algorithm\n1 . s\ns + \u2207\u03b8J \u03b8 \u2297\u2207\u03b8J \u03b8\n2 . \u03b8\n\u03b8 \u2212\u03b7 \u2207\u03b8J \u03b8 \u2298\ns + \u03b5\nThe first step accumulates the square of the gradients into the vector s (recall that the\n\u2297 symbol represents the element-wise multiplication). This vectorized form is equiv\u2010\nalent to computing si \u2190 si + (\u2202 J(\u03b8) / \u2202 \u03b8i)2 for each element si of the vector s; in other\nwords, each si accumulates the squares of the partial derivative of the cost function\nwith regard to parameter \u03b8i. If the cost function is steep along the ith dimension, then\nsi will get larger and larger at each iteration. The second step is almost identical to Gradient Descent, but with one big difference:\nthe gradient vector is scaled down by a factor of s + \u03b5 (the \u2298 symbol represents the | Chapter 11: Training Deep Neural Networks\n16 This algorithm was created by Geoffrey Hinton and Tijmen Tieleman in 2012 and presented by Geoffrey Hin\u2010\nton in his Coursera class on neural networks (slides:  video:  Amus\u2010\ningly, since the authors did not write a paper to describe the algorithm, researchers often cite \u201cslide 29 in\nlecture 6\u201d in their papers. element-wise division, and \u03b5 is a smoothing term to avoid division by zero, typically\nset to 10\u201310). This vectorized form is equivalent to simultaneously computing\n\u03b8i\n\u03b8i \u2212\u03b7 \u2202J \u03b8 / \u2202\u03b8i/ si + \u03b5 for all parameters \u03b8i. In short, this algorithm decays the learning rate, but it does so faster for steep dimen\u2010\nsions than for dimensions with gentler slopes. This is called an adaptive learning rate. It helps point the resulting updates more directly toward the global optimum (see\nFigure 11-7). One additional benefit is that it requires much less tuning of the learn\u2010\ning rate hyperparameter \u03b7. Figure 11-7. AdaGrad versus Gradient Descent: the former can correct its direction ear\u2010\nlier to point to the optimum\nAdaGrad frequently performs well for simple quadratic problems, but it often stops\ntoo early when training neural networks. The learning rate gets scaled down so much\nthat the algorithm ends up stopping entirely before reaching the global optimum. So\neven though Keras has an Adagrad optimizer, you should not use it to train deep neu\u2010\nral networks (it may be efficient for simpler tasks such as Linear Regression, though)."
  },
  {
    "id": 232,
    "content": "Still, understanding AdaGrad is helpful to grasp the other adaptive learning rate\noptimizers. RMSProp\nAs we\u2019ve seen, AdaGrad runs the risk of slowing down a bit too fast and never con\u2010\nverging to the global optimum. The RMSProp algorithm16 fixes this by accumulating\nonly the gradients from the most recent iterations (as opposed to all the gradients\nFaster Optimizers | 17 Diederik P. Kingma and Jimmy Ba, \u201cAdam: A Method for Stochastic Optimization,\u201d arXiv preprint arXiv:\n1412.6980 (2014). 18 These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the\nfirst moment while the variance is often called the second moment, hence the name of the algorithm. since the beginning of training). It does so by using exponential decay in the first step\n(see Equation 11-7). Equation 11-7. RMSProp algorithm\n1 . s\n\u03b2s + 1 \u2212\u03b2 \u2207\u03b8J \u03b8 \u2297\u2207\u03b8J \u03b8\n2 . \u03b8\n\u03b8 \u2212\u03b7 \u2207\u03b8J \u03b8 \u2298\ns + \u03b5\nThe decay rate \u03b2 is typically set to 0.9. Yes, it is once again a new hyperparameter, but\nthis default value often works well, so you may not need to tune it at all. As you might expect, Keras has an RMSprop optimizer:\noptimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\nNote that the rho argument corresponds to \u03b2 in Equation 11-7. Except on very simple\nproblems, this optimizer almost always performs much better than AdaGrad. In fact,\nit was the preferred optimization algorithm of many researchers until Adam optimi\u2010\nzation came around. Adam and Nadam Optimization\nAdam,17 which stands for adaptive moment estimation, combines the ideas of momen\u2010\ntum optimization and RMSProp: just like momentum optimization, it keeps track of\nan exponentially decaying average of past gradients; and just like RMSProp, it keeps\ntrack of an exponentially decaying average of past squared gradients (see Equation\n11-8).18\nEquation 11-8. Adam algorithm\n1 . m\n\u03b21m \u22121 \u2212\u03b21 \u2207\u03b8J \u03b8\n2 . s\n\u03b22s + 1 \u2212\u03b22 \u2207\u03b8J \u03b8 \u2297\u2207\u03b8J \u03b8\n3 . m\nm\n1 \u2212\u03b21\nt\n4 . s\ns\n1 \u2212\u03b22\nt\n5 . \u03b8\n\u03b8 + \u03b7 m \u2298\ns + \u03b5 | Chapter 11: Training Deep Neural Networks\nIn this equation, t represents the iteration number (starting at 1). If you just look at steps 1, 2, and 5, you will notice Adam\u2019s close similarity to both\nmomentum optimization and RMSProp. The only difference is that step 1 computes\nan exponentially decaying average rather than an exponentially decaying sum, but\nthese are actually equivalent except for a constant factor (the decaying average is just\n1 \u2013 \u03b21 times the decaying sum). Steps 3 and 4 are somewhat of a technical detail: since\nm and s are initialized at 0, they will be biased toward 0 at the beginning of training,\nso these two steps will help boost m and s at the beginning of training. The momentum decay hyperparameter \u03b21 is typically initialized to 0.9, while the scal\u2010\ning decay hyperparameter \u03b22 is often initialized to 0.999."
  },
  {
    "id": 233,
    "content": "As earlier, the smoothing\nterm \u03b5 is usually initialized to a tiny number such as 10\u20137. These are the default values\nfor the Adam class (to be precise, epsilon defaults to None, which tells Keras to use\nkeras.backend.epsilon(), which defaults to 10\u20137; you can change it using\nkeras.backend.set_epsilon()). Here is how to create an Adam optimizer using\nKeras:\noptimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\nSince Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it\nrequires less tuning of the learning rate hyperparameter \u03b7. You can often use the\ndefault value \u03b7 = 0.001, making Adam even easier to use than Gradient Descent. If you are starting to feel overwhelmed by all these different techni\u2010\nques and are wondering how to choose the right ones for your task,\ndon\u2019t worry: some practical guidelines are provided at the end of\nthis chapter. Finally, two variants of Adam are worth mentioning:\nAdaMax\nNotice that in step 2 of Equation 11-8, Adam accumulates the squares of the gra\u2010\ndients in s (with a greater weight for more recent gradients). In step 5, if we\nignore \u03b5 and steps 3 and 4 (which are technical details anyway), Adam scales\ndown the parameter updates by the square root of s. In short, Adam scales down\nthe parameter updates by the \u21132 norm of the time-decayed gradients (recall that\nthe \u21132 norm is the square root of the sum of squares). AdaMax, introduced in the\nsame paper as Adam, replaces the \u21132 norm with the \u2113\u221e norm (a fancy way of say\u2010\ning the max). Specifically, it replaces step 2 in Equation 11-8 with s \u2190 max\n(\u03b22s,\u2207\u03b8J(\u03b8)), it drops step 4, and in step 5 it scales down the gradient updates by a\nfactor of s, which is just the max of the time-decayed gradients. In practice, this\ncan make AdaMax more stable than Adam, but it really depends on the dataset,\nFaster Optimizers | 19 Timothy Dozat, \u201cIncorporating Nesterov Momentum into Adam\u201d (2016). 20 Ashia C. Wilson et al., \u201cThe Marginal Value of Adaptive Gradient Methods in Machine Learning,\u201d Advances in\nNeural Information Processing Systems 30 (2017): 4148\u20134158. and in general Adam performs better. So, this is just one more optimizer you can\ntry if you experience problems with Adam on some task. Nadam\nNadam optimization is Adam optimization plus the Nesterov trick, so it will\noften converge slightly faster than Adam. In his report introducing this techni\u2010\nque,19 the researcher Timothy Dozat compares many different optimizers on vari\u2010\nous tasks and finds that Nadam generally outperforms Adam but is sometimes\noutperformed by RMSProp. Adaptive optimization methods (including RMSProp, Adam, and\nNadam optimization) are often great, converging fast to a good sol\u2010\nution. However, a 2017 paper20 by Ashia C. Wilson et al. showed\nthat they can lead to solutions that generalize poorly on some data\u2010\nsets. So when you are disappointed by your model\u2019s performance,\ntry using plain Nesterov Accelerated Gradient instead: your dataset\nmay just be allergic to adaptive gradients."
  },
  {
    "id": 234,
    "content": "Also check out the latest\nresearch, because it\u2019s moving fast. All the optimization techniques discussed so far only rely on the first-order partial\nderivatives (Jacobians). The optimization literature also contains amazing algorithms\nbased on the second-order partial derivatives (the Hessians, which are the partial\nderivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply\nto deep neural networks because there are n2 Hessians per output (where n is the\nnumber of parameters), as opposed to just n Jacobians per output. Since DNNs typi\u2010\ncally have tens of thousands of parameters, the second-order optimization algorithms\noften don\u2019t even fit in memory, and even when they do, computing the Hessians is\njust too slow. | Chapter 11: Training Deep Neural Networks\nTraining Sparse Models\nAll the optimization algorithms just presented produce dense models, meaning that\nmost parameters will be nonzero. If you need a blazingly fast model at runtime, or if\nyou need it to take up less memory, you may prefer to end up with a sparse model\ninstead. One easy way to achieve this is to train the model as usual, then get rid of the tiny\nweights (set them to zero). Note that this will typically not lead to a very sparse\nmodel, and it may degrade the model\u2019s performance. A better option is to apply strong \u21131 regularization during training (we will see how\nlater in this chapter), as it pushes the optimizer to zero out as many weights as it can\n(as discussed in \u201cLasso Regression\u201d on page 137 in Chapter 4). If these techniques remain insufficient, check out the TensorFlow Model Optimiza\u2010\ntion Toolkit (TF-MOT), which provides a pruning API capable of iteratively remov\u2010\ning connections during training based on their magnitude. Table 11-2 compares all the optimizers we\u2019ve discussed so far (* is bad, ** is average,\nand *** is good). Table 11-2. Optimizer comparison\nClass\nConvergence speed Convergence quality\nSGD\n*\n***\nSGD(momentum=...)\n**\n***\nSGD(momentum=..., nesterov=True)\n**\n***\nAdagrad\n***\n* (stops too early)\nRMSprop\n***\n** or ***\nAdam\n***\n** or ***\nNadam\n***\n** or ***\nAdaMax\n***\n** or ***\nLearning Rate Scheduling\nFinding a good learning rate is very important. If you set it much too high, training\nmay diverge (as we discussed in \u201cGradient Descent\u201d on page 118). If you set it too\nlow, training will eventually converge to the optimum, but it will take a very long\ntime. If you set it slightly too high, it will make progress very quickly at first, but it\nwill end up dancing around the optimum, never really settling down. If you have a\nlimited computing budget, you may have to interrupt training before it has converged\nproperly, yielding a suboptimal solution (see Figure 11-8). Faster Optimizers | Figure 11-8."
  },
  {
    "id": 235,
    "content": "Learning curves for various learning rates \u03b7\nAs we discussed in Chapter 10, you can find a good learning rate by training the\nmodel for a few hundred iterations, exponentially increasing the learning rate from a\nvery small value to a very large value, and then looking at the learning curve and\npicking a learning rate slightly lower than the one at which the learning curve starts\nshooting back up. You can then reinitialize your model and train it with that learning\nrate. But you can do better than a constant learning rate: if you start with a large learning\nrate and then reduce it once training stops making fast progress, you can reach a\ngood solution faster than with the optimal constant learning rate. There are many dif\u2010\nferent strategies to reduce the learning rate during training. It can also be beneficial to\nstart with a low learning rate, increase it, then drop it again. These strategies are\ncalled learning schedules (we briefly introduced this concept in Chapter 4). These are\nthe most commonly used learning schedules:\nPower scheduling\nSet the learning rate to a function of the iteration number t: \u03b7(t) = \u03b70 / (1 + t/s)c.\nThe initial learning rate \u03b70, the power c (typically set to 1), and the steps s are\nhyperparameters. The learning rate drops at each step. After s steps, it is down to\n\u03b70 / 2. After s more steps, it is down to \u03b70 / 3, then it goes down to \u03b70 / 4, then \u03b70 /\n5, and so on. As you can see, this schedule first drops quickly, then more and\nmore slowly. Of course, power scheduling requires tuning \u03b70 and s (and possibly\nc). Exponential scheduling\nSet the learning rate to \u03b7(t) = \u03b70 0.1t/s. The learning rate will gradually drop by a\nfactor of 10 every s steps. While power scheduling reduces the learning rate more\nand more slowly, exponential scheduling keeps slashing it by a factor of 10 every\ns steps. | Chapter 11: Training Deep Neural Networks\n21 Leslie N. Smith, \u201cA Disciplined Approach to Neural Network Hyper-Parameters: Part 1\u2014Learning Rate, Batch\nSize, Momentum, and Weight Decay,\u201d arXiv preprint arXiv:1803.09820 (2018). 22 Andrew Senior et al., \u201cAn Empirical Study of Learning Rates in Deep Neural Networks for Speech Recogni\u2010\ntion,\u201d Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (2013):\n6724\u20136728. Piecewise constant scheduling\nUse a constant learning rate for a number of epochs (e.g., \u03b70 = 0.1 for 5 epochs),\nthen a smaller learning rate for another number of epochs (e.g., \u03b71 = 0.001 for 50\nepochs), and so on. Although this solution can work very well, it requires fid\u2010\ndling around to figure out the right sequence of learning rates and how long to\nuse each of them. Performance scheduling\nMeasure the validation error every N steps (just like for early stopping), and\nreduce the learning rate by a factor of \u03bb when the error stops dropping."
  },
  {
    "id": 236,
    "content": "1cycle scheduling\nContrary to the other approaches, 1cycle (introduced in a 2018 paper21 by Leslie\nSmith) starts by increasing the initial learning rate \u03b70, growing linearly up to \u03b71\nhalfway through training. Then it decreases the learning rate linearly down to \u03b70\nagain during the second half of training, finishing the last few epochs by drop\u2010\nping the rate down by several orders of magnitude (still linearly). The maximum\nlearning rate \u03b71 is chosen using the same approach we used to find the optimal\nlearning rate, and the initial learning rate \u03b70 is chosen to be roughly 10 times\nlower. When using a momentum, we start with a high momentum first (e.g.,\n0.95), then drop it down to a lower momentum during the first half of training\n(e.g., down to 0.85, linearly), and then bring it back up to the maximum value\n(e.g., 0.95) during the second half of training, finishing the last few epochs with\nthat maximum value. Smith did many experiments showing that this approach\nwas often able to speed up training considerably and reach better performance. For example, on the popular CIFAR10 image dataset, this approach reached\n91.9% validation accuracy in just 100 epochs, instead of 90.3% accuracy in 800\nepochs through a standard approach (with the same neural network\narchitecture). A 2013 paper22 by Andrew Senior et al. compared the performance of some of the\nmost popular learning schedules when using momentum optimization to train deep\nneural networks for speech recognition. The authors concluded that, in this setting,\nboth performance scheduling and exponential scheduling performed well. They\nfavored exponential scheduling because it was easy to tune and it converged slightly\nfaster to the optimal solution (they also mentioned that it was easier to implement\nFaster Optimizers | than performance scheduling, but in Keras both options are easy). That said, the\n1cycle approach seems to perform even better. Implementing power scheduling in Keras is the easiest option: just set the decay\nhyperparameter when creating an optimizer:\noptimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)\nThe decay is the inverse of s (the number of steps it takes to divide the learning rate\nby one more unit), and Keras assumes that c is equal to 1. Exponential scheduling and piecewise scheduling are quite simple too. You first need\nto define a function that takes the current epoch and returns the learning rate. For\nexample, let\u2019s implement exponential scheduling:\ndef exponential_decay_fn(epoch): return 0.01 * 0.1**(epoch / 20)\nIf you do not want to hardcode \u03b70 and s, you can create a function that returns a con\u2010\nfigured function:\ndef exponential_decay(lr0, s): def exponential_decay_fn(epoch): return lr0 * 0.1**(epoch / s) return exponential_decay_fn\nexponential_decay_fn = exponential_decay(lr0=0.01, s=20)\nNext, create a LearningRateScheduler callback, giving it the schedule function, and\npass this callback to the fit() method:\nlr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\nhistory = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])\nThe LearningRateScheduler will update the optimizer\u2019s learning_rate attribute at\nthe beginning of each epoch."
  },
  {
    "id": 237,
    "content": "Updating the learning rate once per epoch is usually\nenough, but if you want it to be updated more often, for example at every step, you\ncan always write your own callback (see the \u201cExponential Scheduling\u201d section of the\nnotebook for an example). Updating the learning rate at every step makes sense if\nthere are many steps per epoch. Alternatively, you can use the keras.optimiz\ners.schedules approach, described shortly. The schedule function can optionally take the current learning rate as a second argu\u2010\nment. For example, the following schedule function multiplies the previous learning\nrate by 0.11/20, which results in the same exponential decay (except the decay now\nstarts at the beginning of epoch 0 instead of 1):\ndef exponential_decay_fn(epoch, lr): return lr * 0.1**(1 / 20) | Chapter 11: Training Deep Neural Networks\nThis implementation relies on the optimizer\u2019s initial learning rate (contrary to the\nprevious implementation), so make sure to set it appropriately. When you save a model, the optimizer and its learning rate get saved along with it. This means that with this new schedule function, you could just load a trained model\nand continue training where it left off, no problem. Things are not so simple if your\nschedule function uses the epoch argument, however: the epoch does not get saved,\nand it gets reset to 0 every time you call the fit() method. If you were to continue\ntraining a model where it left off, this could lead to a very large learning rate, which\nwould likely damage your model\u2019s weights. One solution is to manually set the fit()\nmethod\u2019s initial_epoch argument so the epoch starts at the right value. For piecewise constant scheduling, you can use a schedule function like the following\none (as earlier, you can define a more general function if you want; see the \u201cPiecewise\nConstant Scheduling\u201d section of the notebook for an example), then create a Lear\nningRateScheduler callback with this function and pass it to the fit() method, just\nlike we did for exponential scheduling:\ndef piecewise_constant_fn(epoch): if epoch < 5: return 0.01 elif epoch < 15: return 0.005 else: return 0.001\nFor performance scheduling, use the ReduceLROnPlateau callback. For example, if\nyou pass the following callback to the fit() method, it will multiply the learning rate\nby 0.5 whenever the best validation loss does not improve for five consecutive epochs\n(other options are available; please check the documentation for more details):\nlr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\nLastly, tf.keras offers an alternative way to implement learning rate scheduling: define\nthe learning rate using one of the schedules available in keras.optimizers.sched\nules, then pass this learning rate to any optimizer. This approach updates the learn\u2010\ning rate at each step rather than at each epoch."
  },
  {
    "id": 238,
    "content": "For example, here is how to implement\nthe same exponential schedule as the exponential_decay_fn() function we defined\nearlier:\ns = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\nlearning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\noptimizer = keras.optimizers.SGD(learning_rate)\nThis is nice and simple, plus when you save the model, the learning rate and its\nschedule (including its state) get saved as well. This approach, however, is not part of\nthe Keras API; it is specific to tf.keras. Faster Optimizers | As for the 1cycle approach, the implementation poses no particular difficulty: just\ncreate a custom callback that modifies the learning rate at each iteration (you can\nupdate the optimizer\u2019s learning rate by changing self.model.optimizer.lr). See the\n\u201c1Cycle scheduling\u201d section of the notebook for an example. To sum up, exponential decay, performance scheduling, and 1cycle can considerably\nspeed up convergence, so give them a try! Avoiding Overfitting Through Regularization\nWith four parameters I can fit an elephant and with five I can make him wiggle his\ntrunk. \u2014John von Neumann, cited by Enrico Fermi in Nature 427\nWith thousands of parameters, you can fit the whole zoo. Deep neural networks typi\u2010\ncally have tens of thousands of parameters, sometimes even millions. This gives them\nan incredible amount of freedom and means they can fit a huge variety of complex\ndatasets. But this great flexibility also makes the network prone to overfitting the\ntraining set. We need regularization. We already implemented one of the best regularization techniques in Chapter 10:\nearly stopping. Moreover, even though Batch Normalization was designed to solve\nthe unstable gradients problems, it also acts like a pretty good regularizer. In this sec\u2010\ntion we will examine other popular regularization techniques for neural networks: \u21131\nand \u21132 regularization, dropout, and max-norm regularization. \u21131 and \u21132 Regularization\nJust like you did in Chapter 4 for simple linear models, you can use \u21132 regularization\nto constrain a neural network\u2019s connection weights, and/or \u21131 regularization if you\nwant a sparse model (with many weights equal to 0). Here is how to apply \u21132 regulari\u2010\nzation to a Keras layer\u2019s connection weights, using a regularization factor of 0.01:\nlayer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01))\nThe l2() function returns a regularizer that will be called at each step during training\nto compute the regularization loss. This is then added to the final loss. As you might\nexpect, you can just use keras.regularizers.l1() if you want \u21131 regularization; if\nyou want both \u21131 and \u21132 regularization, use keras.regularizers.l1_l2() (specifying\nboth regularization factors). Since you will typically want to apply the same regularizer to all layers in your net\u2010\nwork, as well as using the same activation function and the same initialization strat\u2010\negy in all hidden layers, you may find yourself repeating the same arguments. This | Chapter 11: Training Deep Neural Networks\n23 Geoffrey E. Hinton et al., \u201cImproving Neural Networks by Preventing Co-Adaptation of Feature Detectors,\u201d\narXiv preprint arXiv:1207.0580 (2012)."
  },
  {
    "id": 239,
    "content": "24 Nitish Srivastava et al., \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting,\u201d Journal of\nMachine Learning Research 15 (2014): 1929\u20131958. makes the code ugly and error-prone. To avoid this, you can try refactoring your code\nto use loops. Another option is to use Python\u2019s functools.partial() function,\nwhich lets you create a thin wrapper for any callable, with some default argument\nvalues:\nfrom functools import partial\nRegularizedDense = partial(keras.layers.Dense, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.01))\nmodel = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), RegularizedDense(300), RegularizedDense(100), RegularizedDense(10, activation=\"softmax\", kernel_initializer=\"glorot_uniform\")\n])\nDropout\nDropout is one of the most popular regularization techniques for deep neural net\u2010\nworks. It was proposed in a paper23 by Geoffrey Hinton in 2012 and further detailed\nin a 2014 paper24 by Nitish Srivastava et al., and it has proven to be highly successful:\neven the state-of-the-art neural networks get a 1\u20132% accuracy boost simply by adding\ndropout. This may not sound like a lot, but when a model already has 95% accuracy,\ngetting a 2% accuracy boost means dropping the error rate by almost 40% (going\nfrom 5% error to roughly 3%). It is a fairly simple algorithm: at every training step, every neuron (including the\ninput neurons, but always excluding the output neurons) has a probability p of being\ntemporarily \u201cdropped out,\u201d meaning it will be entirely ignored during this training\nstep, but it may be active during the next step (see Figure 11-9). The hyperparameter\np is called the dropout rate, and it is typically set between 10% and 50%: closer to 20\u2013\n30% in recurrent neural nets (see Chapter 15), and closer to 40\u201350% in convolutional\nneural networks (see Chapter 14). After training, neurons don\u2019t get dropped any\u2010\nmore. And that\u2019s all (except for a technical detail we will discuss momentarily). Avoiding Overfitting Through Regularization | Figure 11-9. With dropout regularization, at each training iteration a random subset of\nall neurons in one or more layers\u2014except the output layer\u2014are \u201cdropped out\u201d; these\nneurons output 0 at this iteration (represented by the dashed arrows)\nIt\u2019s surprising at first that this destructive technique works at all. Would a company\nperform better if its employees were told to toss a coin every morning to decide\nwhether or not to go to work? Well, who knows; perhaps it would! The company\nwould be forced to adapt its organization; it could not rely on any single person to\nwork the coffee machine or perform any other critical tasks, so this expertise would\nhave to be spread across several people. Employees would have to learn to cooperate\nwith many of their coworkers, not just a handful of them. The company would\nbecome much more resilient. If one person quit, it wouldn\u2019t make much of a differ\u2010\nence. It\u2019s unclear whether this idea would actually work for companies, but it certainly\ndoes for neural networks. Neurons trained with dropout cannot co-adapt with their\nneighboring neurons; they have to be as useful as possible on their own."
  },
  {
    "id": 240,
    "content": "They also\ncannot rely excessively on just a few input neurons; they must pay attention to each of\ntheir input neurons. They end up being less sensitive to slight changes in the inputs. In the end, you get a more robust network that generalizes better. Another way to understand the power of dropout is to realize that a unique neural\nnetwork is generated at each training step. Since each neuron can be either present or\nabsent, there are a total of 2N possible networks (where N is the total number of drop\u2010\npable neurons). This is such a huge number that it is virtually impossible for the same\nneural network to be sampled twice. Once you have run 10,000 training steps, you\nhave essentially trained 10,000 different neural networks (each with just one training\ninstance). These neural networks are obviously not independent because they share\nmany of their weights, but they are nevertheless all different. The resulting neural\nnetwork can be seen as an averaging ensemble of all these smaller neural networks. | Chapter 11: Training Deep Neural Networks\nIn practice, you can usually apply dropout only to the neurons in\nthe top one to three layers (excluding the output layer). There is one small but important technical detail. Suppose p = 50%, in which case\nduring testing a neuron would be connected to twice as many input neurons as it\nwould be (on average) during training. To compensate for this fact, we need to multi\u2010\nply each neuron\u2019s input connection weights by 0.5 after training. If we don\u2019t, each\nneuron will get a total input signal roughly twice as large as what the network was\ntrained on and will be unlikely to perform well. More generally, we need to multiply\neach input connection weight by the keep probability (1 \u2013 p) after training. Alterna\u2010\ntively, we can divide each neuron\u2019s output by the keep probability during training\n(these alternatives are not perfectly equivalent, but they work equally well). To implement dropout using Keras, you can use the keras.layers.Dropout layer. During training, it randomly drops some inputs (setting them to 0) and divides the\nremaining inputs by the keep probability. After training, it does nothing at all; it just\npasses the inputs to the next layer. The following code applies dropout regularization\nbefore every Dense layer, using a dropout rate of 0.2:\nmodel = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dropout(rate=0.2), keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"), keras.layers.Dropout(rate=0.2), keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"), keras.layers.Dropout(rate=0.2), keras.layers.Dense(10, activation=\"softmax\")\n])\nSince dropout is only active during training, comparing the train\u2010\ning loss and the validation loss can be misleading. In particular, a\nmodel may be overfitting the training set and yet have similar\ntraining and validation losses. So make sure to evaluate the training\nloss without dropout (e.g., after training). If you observe that the model is overfitting, you can increase the dropout rate. Con\u2010\nversely, you should try decreasing the dropout rate if the model underfits the training\nset."
  },
  {
    "id": 241,
    "content": "It can also help to increase the dropout rate for large layers, and reduce it for\nsmall ones. Moreover, many state-of-the-art architectures only use dropout after the\nlast hidden layer, so you may want to try this if full dropout is too strong. Avoiding Overfitting Through Regularization | 25 Yarin Gal and Zoubin Ghahramani, \u201cDropout as a Bayesian Approximation: Representing Model Uncertainty\nin Deep Learning,\u201d Proceedings of the 33rd International Conference on Machine Learning (2016): 1050\u20131059. 26 Specifically, they show that training a dropout network is mathematically equivalent to approximate Bayesian\ninference in a specific type of probabilistic model called a Deep Gaussian Process. Dropout does tend to significantly slow down convergence, but it usually results in a\nmuch better model when tuned properly. So, it is generally well worth the extra time\nand effort. If you want to regularize a self-normalizing network based on the\nSELU activation function (as discussed earlier), you should use\nalpha dropout: this is a variant of dropout that preserves the mean\nand standard deviation of its inputs (it was introduced in the same\npaper as SELU, as regular dropout would break self-normalization). Monte Carlo (MC) Dropout\nIn 2016, a paper25 by Yarin Gal and Zoubin Ghahramani added a few more good rea\u2010\nsons to use dropout:\n\u2022 First, the paper established a profound connection between dropout networks\n(i.e., neural networks containing a Dropout layer before every weight layer) and\napproximate Bayesian inference,26 giving dropout a solid mathematical justifica\u2010\ntion. \u2022 Second, the authors introduced a powerful technique called MC Dropout, which\ncan boost the performance of any trained dropout model without having to\nretrain it or even modify it at all, provides a much better measure of the model\u2019s\nuncertainty, and is also amazingly simple to implement. If this all sounds like a \u201cone weird trick\u201d advertisement, then take a look at the follow\u2010\ning code. It is the full implementation of MC Dropout, boosting the dropout model\nwe trained earlier without retraining it:\ny_probas = np.stack([model(X_test_scaled, training=True) for sample in range(100)])\ny_proba = y_probas.mean(axis=0)\nWe just make 100 predictions over the test set, setting training=True to ensure that\nthe Dropout layer is active, and stack the predictions. Since dropout is active, all the\npredictions will be different. Recall that predict() returns a matrix with one row per\ninstance and one column per class. Because there are 10,000 instances in the test set\nand 10 classes, this is a matrix of shape [10000, 10]. We stack 100 such matrices, so\ny_probas is an array of shape [100, 10000, 10]. Once we average over the first | Chapter 11: Training Deep Neural Networks\ndimension (axis=0), we get y_proba, an array of shape [10000, 10], like we would get\nwith a single prediction. That\u2019s all! Averaging over multiple predictions with dropout\non gives us a Monte Carlo estimate that is generally more reliable than the result of a\nsingle prediction with dropout off."
  },
  {
    "id": 242,
    "content": "For example, let\u2019s look at the model\u2019s prediction\nfor the first instance in the Fashion MNIST test set, with dropout off:\n>>> np.round(model.predict(X_test_scaled[:1]), 2)\narray([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.01, 0. , 0.99]], dtype=float32)\nThe model seems almost certain that this image belongs to class 9 (ankle boot). Should you trust it? Is there really so little room for doubt? Compare this with the\npredictions made when dropout is activated:\n>>> np.round(y_probas[:, :1], 2)\narray([[[0. , 0. , 0. , 0. , 0. , 0.14, 0. , 0.17, 0. , 0.68]], [[0. , 0. , 0. , 0. , 0. , 0.16, 0. , 0.2 , 0. , 0.64]], [[0. , 0. , 0. , 0. , 0. , 0.02, 0. , 0.01, 0. , 0.97]], [...]\nThis tells a very different story: apparently, when we activate dropout, the model is\nnot sure anymore. It still seems to prefer class 9, but sometimes it hesitates with\nclasses 5 (sandal) and 7 (sneaker), which makes sense given they\u2019re all footwear. Once\nwe average over the first dimension, we get the following MC Dropout predictions:\n>>> np.round(y_proba[:1], 2)\narray([[0. , 0. , 0. , 0. , 0. , 0.22, 0. , 0.16, 0. , 0.62]], dtype=float32)\nThe model still thinks this image belongs to class 9, but only with a 62% confidence,\nwhich seems much more reasonable than 99%. Plus it\u2019s useful to know exactly which\nother classes it thinks are likely. And you can also take a look at the standard devia\u2010\ntion of the probability estimates:\n>>> y_std = y_probas.std(axis=0)\n>>> np.round(y_std[:1], 2)\narray([[0. , 0. , 0. , 0. , 0. , 0.28, 0. , 0.21, 0.02, 0.32]], dtype=float32)\nApparently there\u2019s quite a lot of variance in the probability estimates: if you were\nbuilding a risk-sensitive system (e.g., a medical or financial system), you should prob\u2010\nably treat such an uncertain prediction with extreme caution. You definitely would\nnot treat it like a 99% confident prediction. Moreover, the model\u2019s accuracy got a\nsmall boost from 86.8 to 86.9:\n>>> accuracy = np.sum(y_pred == y_test) / len(y_test)\n>>> accuracy\n0.8694\nAvoiding Overfitting Through Regularization | 27 This MCDropout class will work with all Keras APIs, including the Sequential API. If you only care about the\nFunctional API or the Subclassing API, you do not have to create an MCDropout class; you can create a regular\nDropout layer and call it with training=True. The number of Monte Carlo samples you use (100 in this example)\nis a hyperparameter you can tweak. The higher it is, the more accu\u2010\nrate the predictions and their uncertainty estimates will be. How\u2010\never, if you double it, inference time will also be doubled. Moreover, above a certain number of samples, you will notice little\nimprovement. So your job is to find the right trade-off between\nlatency and accuracy, depending on your application."
  },
  {
    "id": 243,
    "content": "If your model contains other layers that behave in a special way during training (such\nas BatchNormalization layers), then you should not force training mode like we just\ndid. Instead, you should replace the Dropout layers with the following MCDropout\nclass:27\nclass MCDropout(keras.layers.Dropout): def call(self, inputs): return super().call(inputs, training=True)\nHere, we just subclass the Dropout layer and override the call() method to force its\ntraining argument to True (see Chapter 12). Similarly, you could define an MCAlpha\nDropout class by subclassing AlphaDropout instead. If you are creating a model from\nscratch, it\u2019s just a matter of using MCDropout rather than Dropout. But if you have a\nmodel that was already trained using Dropout, you need to create a new model that\u2019s\nidentical to the existing model except that it replaces the Dropout layers with MCDrop\nout, then copy the existing model\u2019s weights to your new model. In short, MC Dropout is a fantastic technique that boosts dropout models and pro\u2010\nvides better uncertainty estimates. And of course, since it is just regular dropout dur\u2010\ning training, it also acts like a regularizer. Max-Norm Regularization\nAnother regularization technique that is popular for neural networks is called max-\nnorm regularization: for each neuron, it constrains the weights w of the incoming\nconnections such that \u2225 w \u22252 \u2264 r, where r is the max-norm hyperparameter and \u2225 \u00b7 \u22252\nis the \u21132 norm. Max-norm regularization does not add a regularization loss term to the overall loss\nfunction. Instead, it is typically implemented by computing \u2225w\u22252 after each training\nstep and rescaling w if needed (w \u2190 w r/\u2016 w \u20162). | Chapter 11: Training Deep Neural Networks\nReducing r increases the amount of regularization and helps reduce overfitting. Max-\nnorm regularization can also help alleviate the unstable gradients problems (if you\nare not using Batch Normalization). To implement max-norm regularization in Keras, set the kernel_constraint argu\u2010\nment of each hidden layer to a max_norm() constraint with the appropriate max value,\nlike this:\nkeras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_constraint=keras.constraints.max_norm(1.)) After each training iteration, the model\u2019s fit() method will call the object returned\nby max_norm(), passing it the layer\u2019s weights and getting rescaled weights in return,\nwhich then replace the layer\u2019s weights. As you\u2019ll see in Chapter 12, you can define\nyour own custom constraint function if necessary and use it as the kernel_con\nstraint. You can also constrain the bias terms by setting the bias_constraint\nargument. The max_norm() function has an axis argument that defaults to 0. A Dense layer usu\u2010\nally has weights of shape [number of inputs, number of neurons], so using axis=0\nmeans that the max-norm constraint will apply independently to each neuron\u2019s\nweight vector. If you want to use max-norm with convolutional layers (see Chap\u2010\nter 14), make sure to set the max_norm() constraint\u2019s axis argument appropriately\n(usually axis=[0, 1, 2]). Summary and Practical Guidelines\nIn this chapter we have covered a wide range of techniques, and you may be wonder\u2010\ning which ones you should use."
  },
  {
    "id": 244,
    "content": "This depends on the task, and there is no clear con\u2010\nsensus yet, but I have found the configuration in Table 11-3 to work fine in most\ncases, without requiring much hyperparameter tuning. That said, please do not con\u2010\nsider these defaults as hard rules! Table 11-3. Default DNN configuration\nHyperparameter\nDefault value\nKernel initializer\nHe initialization\nActivation function\nELU\nNormalization\nNone if shallow; Batch Norm if deep\nRegularization\nEarly stopping (+\u21132 reg. if needed)\nOptimizer\nMomentum optimization (or RMSProp or Nadam)\nLearning rate schedule 1cycle\nSummary and Practical Guidelines | If the network is a simple stack of dense layers, then it can self-normalize, and you\nshould use the configuration in Table 11-4 instead. Table 11-4. DNN configuration for a self-normalizing net\nHyperparameter\nDefault value\nKernel initializer\nLeCun initialization\nActivation function\nSELU\nNormalization\nNone (self-normalization)\nRegularization\nAlpha dropout if needed\nOptimizer\nMomentum optimization (or RMSProp or Nadam)\nLearning rate schedule 1cycle\nDon\u2019t forget to normalize the input features! You should also try to reuse parts of a\npretrained neural network if you can find one that solves a similar problem, or use\nunsupervised pretraining if you have a lot of unlabeled data, or use pretraining on an\nauxiliary task if you have a lot of labeled data for a similar task. While the previous guidelines should cover most cases, here are some exceptions:\n\u2022 If you need a sparse model, you can use \u21131 regularization (and optionally zero out\nthe tiny weights after training). If you need an even sparser model, you can use\nthe TensorFlow Model Optimization Toolkit. This will break self-normalization,\nso you should use the default configuration in this case. \u2022 If you need a low-latency model (one that performs lightning-fast predictions),\nyou may need to use fewer layers, fold the Batch Normalization layers into the\nprevious layers, and possibly use a faster activation function such as leaky ReLU\nor just ReLU. Having a sparse model will also help. Finally, you may want to\nreduce the float precision from 32 bits to 16 or even 8 bits (see \u201cDeploying a\nModel to a Mobile or Embedded Device\u201d on page 685). Again, check out TF-\nMOT. \u2022 If you are building a risk-sensitive application, or inference latency is not very\nimportant in your application, you can use MC Dropout to boost performance\nand get more reliable probability estimates, along with uncertainty estimates. With these guidelines, you are now ready to train very deep nets! I hope you are now\nconvinced that you can go quite a long way using just Keras. There may come a time,\nhowever, when you need to have even more control; for example, to write a custom\nloss function or to tweak the training algorithm. For such cases you will need to use\nTensorFlow\u2019s lower-level API, as you will see in the next chapter. | Chapter 11: Training Deep Neural Networks\nExercises\n1."
  },
  {
    "id": 245,
    "content": "Is it OK to initialize all the weights to the same value as long as that value is\nselected randomly using He initialization? 2. Is it OK to initialize the bias terms to 0? 3. Name three advantages of the SELU activation function over ReLU. 4. In which cases would you want to use each of the following activation functions:\nSELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax? 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g.,\n0.99999) when using an SGD optimizer? 6. Name three ways you can produce a sparse model. 7. Does dropout slow down training? Does it slow down inference (i.e., making\npredictions on new instances)? What about MC Dropout? 8. Practice training a deep neural network on the CIFAR10 image dataset:\na. Build a DNN with 20 hidden layers of 100 neurons each (that\u2019s too many, but\nit\u2019s the point of this exercise). Use He initialization and the ELU activation\nfunction. b. Using Nadam optimization and early stopping, train the network on the\nCIFAR10 dataset. You can load it with keras.datasets.cifar10.load_\ndata(). The dataset is composed of 60,000 32 \u00d7 32\u2013pixel color images (50,000\nfor training, 10,000 for testing) with 10 classes, so you\u2019ll need a softmax out\u2010\nput layer with 10 neurons. Remember to search for the right learning rate each\ntime you change the model\u2019s architecture or hyperparameters. c. Now try adding Batch Normalization and compare the learning curves: Is it\nconverging faster than before? Does it produce a better model? How does it\naffect training speed? d. Try replacing Batch Normalization with SELU, and make the necessary adjust\u2010\nements to ensure the network self-normalizes (i.e., standardize the input fea\u2010\ntures, use LeCun normal initialization, make sure the DNN contains only a\nsequence of dense layers, etc.). e. Try regularizing the model with alpha dropout. Then, without retraining your\nmodel, see if you can achieve better accuracy using MC Dropout. f. Retrain your model using 1cycle scheduling and see if it improves training\nspeed and model accuracy. Solutions to these exercises are available in Appendix A. Exercises | CHAPTER 12\nCustom Models and Training\nwith TensorFlow\nUp until now, we\u2019ve used only TensorFlow\u2019s high-level API, tf.keras, but it already got\nus pretty far: we built various neural network architectures, including regression and\nclassification nets, Wide & Deep nets, and self-normalizing nets, using all sorts of\ntechniques, such as Batch Normalization, dropout, and learning rate schedules. In\nfact, 95% of the use cases you will encounter will not require anything other than\ntf.keras (and tf.data; see Chapter 13). But now it\u2019s time to dive deeper into TensorFlow\nand take a look at its lower-level Python API. This will be useful when you need extra\ncontrol to write custom loss functions, custom metrics, layers, models, initializers,\nregularizers, weight constraints, and more."
  },
  {
    "id": 246,
    "content": "You may even need to fully control the\ntraining loop itself, for example to apply special transformations or constraints to the\ngradients (beyond just clipping them) or to use multiple optimizers for different parts\nof the network. We will cover all these cases in this chapter, and we will also look at\nhow you can boost your custom models and training algorithms using TensorFlow\u2019s\nautomatic graph generation feature. But first, let\u2019s take a quick tour of TensorFlow. TensorFlow 2.0 (beta) was released in June 2019, making Tensor\u2010\nFlow much easier to use. The first edition of this book used TF 1,\nwhile this edition uses TF 2. Custom Models and Training with TensorFlow | 1 TensorFlow includes another Deep Learning API called the Estimators API, but the TensorFlow team recom\u2010\nmends using tf.keras instead. A Quick Tour of TensorFlow\nAs you know, TensorFlow is a powerful library for numerical computation, particu\u2010\nlarly well suited and fine-tuned for large-scale Machine Learning (but you could use\nit for anything else that requires heavy computations). It was developed by the Google\nBrain team and it powers many of Google\u2019s large-scale services, such as Google Cloud\nSpeech, Google Photos, and Google Search. It was open sourced in November 2015,\nand it is now the most popular Deep Learning library (in terms of citations in papers,\nadoption in companies, stars on GitHub, etc.). Countless projects use TensorFlow for\nall sorts of Machine Learning tasks, such as image classification, natural language\nprocessing, recommender systems, and time series forecasting. So what does TensorFlow offer? Here\u2019s a summary:\n\u2022 Its core is very similar to NumPy, but with GPU support. \u2022 It supports distributed computing (across multiple devices and servers). \u2022 It includes a kind of just-in-time (JIT) compiler that allows it to optimize compu\u2010\ntations for speed and memory usage. It works by extracting the computation\ngraph from a Python function, then optimizing it (e.g., by pruning unused\nnodes), and finally running it efficiently (e.g., by automatically running inde\u2010\npendent operations in parallel). \u2022 Computation graphs can be exported to a portable format, so you can train a\nTensorFlow model in one environment (e.g., using Python on Linux) and run it\nin another (e.g., using Java on an Android device). \u2022 It implements autodiff (see Chapter 10 and Appendix D) and provides some\nexcellent optimizers, such as RMSProp and Nadam (see Chapter 11), so you can\neasily minimize all sorts of loss functions. TensorFlow offers many more features built on top of these core features: the most\nimportant is of course tf.keras,1 but it also has data loading and preprocessing ops\n(tf.data, tf.io, etc. ), image processing ops (tf.image), signal processing ops\n(tf.signal), and more (see Figure 12-1 for an overview of TensorFlow\u2019s Python\nAPI). | Chapter 12: Custom Models and Training with TensorFlow\n2 If you ever need to (but you probably won\u2019t), you can write your own operations using the C++ API."
  },
  {
    "id": 247,
    "content": "3 To learn more about TPUs and how they work, check out \nWe will cover many of the packages and functions of the Tensor\u2010\nFlow API, but it\u2019s impossible to cover them all, so you should really\ntake some time to browse through the API; you will find that it is\nquite rich and well documented. Figure 12-1. TensorFlow\u2019s Python API\nAt the lowest level, each TensorFlow operation (op for short) is implemented using\nhighly efficient C++ code.2 Many operations have multiple implementations called\nkernels: each kernel is dedicated to a specific device type, such as CPUs, GPUs, or\neven TPUs (tensor processing units). As you may know, GPUs can dramatically speed\nup computations by splitting them into many smaller chunks and running them in\nparallel across many GPU threads. TPUs are even faster: they are custom ASIC chips\nbuilt specifically for Deep Learning operations3 (we will discuss how to use Tensor\u2010\nFlow with GPUs or TPUs in Chapter 19). TensorFlow\u2019s architecture is shown in Figure 12-2. Most of the time your code will\nuse the high-level APIs (especially tf.keras and tf.data); but when you need more flex\u2010\nibility, you will use the lower-level Python API, handling tensors directly. Note that\nA Quick Tour of TensorFlow | APIs for other languages are also available. In any case, TensorFlow\u2019s execution\nengine will take care of running the operations efficiently, even across multiple devi\u2010\nces and machines if you tell it to. Figure 12-2. TensorFlow\u2019s architecture\nTensorFlow runs not only on Windows, Linux, and macOS, but also on mobile devi\u2010\nces (using TensorFlow Lite), including both iOS and Android (see Chapter 19). If you\ndo not want to use the Python API, there are C++, Java, Go, and Swift APIs. There is\neven a JavaScript implementation called TensorFlow.js that makes it possible to run\nyour models directly in your browser. There\u2019s more to TensorFlow than the library. TensorFlow is at the center of an exten\u2010\nsive ecosystem of libraries. First, there\u2019s TensorBoard for visualization (see Chap\u2010\nter 10). Next, there\u2019s TensorFlow Extended (TFX), which is a set of libraries built by\nGoogle to productionize TensorFlow projects: it includes tools for data validation,\npreprocessing, model analysis, and serving (with TF Serving; see Chapter 19). Goo\u2010\ngle\u2019s TensorFlow Hub provides a way to easily download and reuse pretrained neural\nnetworks. You can also get many neural network architectures, some of them pre\u2010\ntrained, in TensorFlow\u2019s model garden. Check out the TensorFlow Resources and\n for more TensorFlow-based projects. You\nwill find hundreds of TensorFlow projects on GitHub, so it is often easy to find exist\u2010\ning code for whatever you are trying to do. More and more ML papers are released along with their implemen\u2010\ntations, and sometimes even with pretrained models. Check out\n to easily find them. | Chapter 12: Custom Models and Training with TensorFlow\nLast but not least, TensorFlow has a dedicated team of passionate and helpful devel\u2010\nopers, as well as a large community contributing to improving it."
  },
  {
    "id": 248,
    "content": "To ask technical\nquestions, you should use  and tag your question with ten\u2010\nsorflow and python. You can file bugs and feature requests through GitHub. For gen\u2010\neral discussions, join the Google group. OK, it\u2019s time to start coding! Using TensorFlow like NumPy\nTensorFlow\u2019s API revolves around tensors, which flow from operation to operation\u2014\nhence the name TensorFlow. A tensor is very similar to a NumPy ndarray: it is usu\u2010\nally a multidimensional array, but it can also hold a scalar (a simple value, such as 42). These tensors will be important when we create custom cost functions, custom met\u2010\nrics, custom layers, and more, so let\u2019s see how to create and manipulate them. Tensors and Operations\nYou can create a tensor with tf.constant(). For example, here is a tensor represent\u2010\ning a matrix with two rows and three columns of floats:\n>>> tf.constant([[1., 2., 3. ], [4., 5., 6.]]) # matrix\n<tf.Tensor: id=0, shape=(2, 3), dtype=float32, numpy=\narray([[1., 2., 3. ], [4., 5., 6. ]], dtype=float32)>\n>>> tf.constant(42) # scalar\n<tf.Tensor: id=1, shape=(), dtype=int32, numpy=42>\nJust like an ndarray, a tf.Tensor has a shape and a data type (dtype):\n>>> t = tf.constant([[1., 2., 3. ], [4., 5., 6.]]) >>> t.shape\nTensorShape([2, 3])\n>>> t.dtype\ntf.float32\nIndexing works much like in NumPy:\n>>> t[:, 1:]\n<tf.Tensor: id=5, shape=(2, 2), dtype=float32, numpy=\narray([[2., 3. ], [5., 6. ]], dtype=float32)>\n>>> t[..., 1, tf.newaxis]\n<tf.Tensor: id=15, shape=(2, 1), dtype=float32, numpy=\narray([[2. ], [5. ]], dtype=float32)>\nMost importantly, all sorts of tensor operations are available:\n>>> t + 10\n<tf.Tensor: id=18, shape=(2, 3), dtype=float32, numpy=\nUsing TensorFlow like NumPy | 4 A notable exception is tf.math.log(), which is commonly used but doesn\u2019t have a tf.log() alias (as it might\nbe confused with logging). array([[11., 12., 13. ], [14., 15., 16. ]], dtype=float32)>\n>>> tf.square(t)\n<tf.Tensor: id=20, shape=(2, 3), dtype=float32, numpy=\narray([[ 1., 4., 9. ], [16., 25., 36. ]], dtype=float32)>\n>>> t @ tf.transpose(t)\n<tf.Tensor: id=24, shape=(2, 2), dtype=float32, numpy=\narray([[14., 32. ], [32., 77. ]], dtype=float32)>\nNote that writing t + 10 is equivalent to calling tf.add(t, 10) (indeed, Python calls\nthe magic method t.__add__(10), which just calls tf.add(t, 10)). Other operators\nlike - and * are also supported. The @ operator was added in Python 3.5, for matrix\nmultiplication: it is equivalent to calling the tf.matmul() function. You will find all the basic math operations you need (tf.add(), tf.multiply(),\ntf.square(), tf.exp(), tf.sqrt(), etc.) and most operations that you can find in\nNumPy (e.g., tf.reshape(), tf.squeeze(), tf.tile()). Some functions have a dif\u2010\nferent name than in NumPy; for instance, tf.reduce_mean(), tf.reduce_sum(),\ntf.reduce_max(), and tf.math.log() are the equivalent of np.mean(), np.sum(),\nnp.max() and np.log(). When the name differs, there is often a good reason for it. For example, in TensorFlow you must write tf.transpose(t); you cannot just write\nt.T like in NumPy."
  },
  {
    "id": 249,
    "content": "The reason is that the tf.transpose() function does not do\nexactly the same thing as NumPy\u2019s T attribute: in TensorFlow, a new tensor is created\nwith its own copy of the transposed data, while in NumPy, t.T is just a transposed\nview on the same data. Similarly, the tf.reduce_sum() operation is named this way\nbecause its GPU kernel (i.e., GPU implementation) uses a reduce algorithm that does\nnot guarantee the order in which the elements are added: because 32-bit floats have\nlimited precision, the result may change ever so slightly every time you call this oper\u2010\nation. The same is true of tf.reduce_mean() (but of course tf.reduce_max() is\ndeterministic). Many functions and classes have aliases. For example, tf.add()\nand tf.math.add() are the same function. This allows TensorFlow\nto have concise names for the most common operations4 while pre\u2010\nserving well-organized packages. | Chapter 12: Custom Models and Training with TensorFlow\nKeras\u2019 Low-Level API\nThe Keras API has its own low-level API, located in keras.backend. It includes func\u2010\ntions like square(), exp(), and sqrt(). In tf.keras, these functions generally just call\nthe corresponding TensorFlow operations. If you want to write code that will be\nportable to other Keras implementations, you should use these Keras functions. How\u2010\never, they only cover a subset of all functions available in TensorFlow, so in this book\nwe will use the TensorFlow operations directly. Here is as simple example using\nkeras.backend, which is commonly named K for short:\n>>> from tensorflow import keras\n>>> K = keras.backend\n>>> K.square(K.transpose(t)) + 10\n<tf.Tensor: id=39, shape=(3, 2), dtype=float32, numpy=\narray([[11., 26. ], [14., 35. ], [19., 46. ]], dtype=float32)>\nTensors and NumPy\nTensors play nice with NumPy: you can create a tensor from a NumPy array, and vice\nversa. You can even apply TensorFlow operations to NumPy arrays and NumPy oper\u2010\nations to tensors:\n>>> a = np.array([2., 4., 5.]) >>> tf.constant(a)\n<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5. ])>\n>>> t.numpy() # or np.array(t)\narray([[1., 2., 3. ], [4., 5., 6. ]], dtype=float32)\n>>> tf.square(a)\n<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25. ])>\n>>> np.square(t)\narray([[ 1., 4., 9. ], [16., 25., 36. ]], dtype=float32)\nNotice that NumPy uses 64-bit precision by default, while Tensor\u2010\nFlow uses 32-bit. This is because 32-bit precision is generally more\nthan enough for neural networks, plus it runs faster and uses less\nRAM. So when you create a tensor from a NumPy array, make sure\nto set dtype=tf.float32. Type Conversions\nType conversions can significantly hurt performance, and they can easily go unno\u2010\nticed when they are done automatically. To avoid this, TensorFlow does not perform\nUsing TensorFlow like NumPy | any type conversions automatically: it just raises an exception if you try to execute an\noperation on tensors with incompatible types. For example, you cannot add a float\ntensor and an integer tensor, and you cannot even add a 32-bit float and a 64-bit float:\n>>> tf.constant(2.) + tf.constant(40)\nTraceback[...]InvalidArgumentError[...]expected to be a float[...]\n>>> tf.constant(2.)"
  },
  {
    "id": 250,
    "content": "+ tf.constant(40., dtype=tf.float64)\nTraceback[...]InvalidArgumentError[...]expected to be a double[...]\nThis may be a bit annoying at first, but remember that it\u2019s for a good cause! And of\ncourse you can use tf.cast() when you really need to convert types:\n>>> t2 = tf.constant(40., dtype=tf.float64)\n>>> tf.constant(2.0) + tf.cast(t2, tf.float32)\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\nVariables\nThe tf.Tensor values we\u2019ve seen so far are immutable: you cannot modify them. This\nmeans that we cannot use regular tensors to implement weights in a neural network,\nsince they need to be tweaked by backpropagation. Plus, other parameters may also\nneed to change over time (e.g., a momentum optimizer keeps track of past gradients). What we need is a tf.Variable:\n>>> v = tf.Variable([[1., 2., 3. ], [4., 5., 6.]]) >>> v\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\narray([[1., 2., 3. ], [4., 5., 6. ]], dtype=float32)>\nA tf.Variable acts much like a tf.Tensor: you can perform the same operations\nwith it, it plays nicely with NumPy as well, and it is just as picky with types. But it can\nalso be modified in place using the assign() method (or assign_add() or\nassign_sub(), which increment or decrement the variable by the given value). You\ncan also modify individual cells (or slices), by using the cell\u2019s (or slice\u2019s) assign()\nmethod (direct item assignment will not work) or by using the scatter_update() or\nscatter_nd_update() methods:\nv.assign(2 * v) # => [[2., 4., 6. ], [8., 10., 12.]] v[0, 1].assign(42) # => [[2., 42., 6. ], [8., 10., 12.]] v[:, 2].assign([0., 1.]) # => [[2., 42., 0. ], [8., 10., 1.]] v.scatter_nd_update(indices=[[0, 0], [1, 2]], updates=[100., 200.]) # => [[100., 42., 0. ], [8., 10., 200.]] | Chapter 12: Custom Models and Training with TensorFlow\nIn practice you will rarely have to create variables manually, since\nKeras provides an add_weight() method that will take care of it for\nyou, as we will see. Moreover, model parameters will generally be\nupdated directly by the optimizers, so you will rarely need to\nupdate variables manually. Other Data Structures\nTensorFlow supports several other data structures, including the following (please see\nthe \u201cTensors and Operations\u201d section in the notebook or Appendix F for more\ndetails):\nSparse tensors (tf.SparseTensor)\nEfficiently represent tensors containing mostly zeros. The tf.sparse package\ncontains operations for sparse tensors. Tensor arrays (tf.TensorArray)\nAre lists of tensors. They have a fixed size by default but can optionally be made\ndynamic. All tensors they contain must have the same shape and data type. Ragged tensors (tf.RaggedTensor)\nRepresent static lists of lists of tensors, where every tensor has the same shape\nand data type. The tf.ragged package contains operations for ragged tensors. String tensors\nAre regular tensors of type tf.string. These represent byte strings, not Unicode\nstrings, so if you create a string tensor using a Unicode string (e.g., a regular\nPython 3 string like \"caf\u00e9\"), then it will get encoded to UTF-8 automatically\n(e.g., b\"caf\\xc3\\xa9\")."
  },
  {
    "id": 251,
    "content": "Alternatively, you can represent Unicode strings using\ntensors of type tf.int32, where each item represents a Unicode code point (e.g.,\n[99, 97, 102, 233]). The tf.strings package (with an s) contains ops for byte\nstrings and Unicode strings (and to convert one into the other). It\u2019s important to\nnote that a tf.string is atomic, meaning that its length does not appear in the\ntensor\u2019s shape. Once you convert it to a Unicode tensor (i.e., a tensor of type\ntf.int32 holding Unicode code points), the length appears in the shape. Sets\nAre represented as regular tensors (or sparse tensors). For example, tf.con\nstant([[1, 2], [3, 4]]) represents the two sets {1, 2} and {3, 4}. More gener\u2010\nally, each set is represented by a vector in the tensor\u2019s last axis. You can\nmanipulate sets using operations from the tf.sets package. Queues\nStore tensors across multiple steps. TensorFlow offers various kinds of queues:\nsimple First In, First Out (FIFO) queues (FIFOQueue), queues that can prioritize\nUsing TensorFlow like NumPy | some items (PriorityQueue), shuffle their items (RandomShuffleQueue), and\nbatch items of different shapes by padding (PaddingFIFOQueue). These classes are\nall in the tf.queue package. With tensors, operations, variables, and various data structures at your disposal, you\nare now ready to customize your models and training algorithms! Customizing Models and Training Algorithms\nLet\u2019s start by creating a custom loss function, which is a simple and common use case. Custom Loss Functions\nSuppose you want to train a regression model, but your training set is a bit noisy. Of\ncourse, you start by trying to clean up your dataset by removing or fixing the outliers,\nbut that turns out to be insufficient; the dataset is still noisy. Which loss function\nshould you use? The mean squared error might penalize large errors too much and\ncause your model to be imprecise. The mean absolute error would not penalize outli\u2010\ners as much, but training might take a while to converge, and the trained model\nmight not be very precise. This is probably a good time to use the Huber loss (intro\u2010\nduced in Chapter 10) instead of the good old MSE. The Huber loss is not currently\npart of the official Keras API, but it is available in tf.keras (just use an instance of the\nkeras.losses.Huber class). But let\u2019s pretend it\u2019s not there: implementing it is easy as\npie! Just create a function that takes the labels and predictions as arguments, and use\nTensorFlow operations to compute every instance\u2019s loss:\ndef huber_fn(y_true, y_pred): error = y_true - y_pred is_small_error = tf.abs(error) < 1 squared_loss = tf.square(error) / 2 linear_loss = tf.abs(error) - 0.5 return tf.where(is_small_error, squared_loss, linear_loss)\nFor better performance, you should use a vectorized implementa\u2010\ntion, as in this example. Moreover, if you want to benefit from Ten\u2010\nsorFlow\u2019s graph features, you should use only TensorFlow\noperations. It is also preferable to return a tensor containing one loss per instance, rather than\nreturning the mean loss."
  },
  {
    "id": 252,
    "content": "This way, Keras can apply class weights or sample weights\nwhen requested (see Chapter 10). | Chapter 12: Custom Models and Training with TensorFlow\nNow you can use this loss when you compile the Keras model, then train your model:\nmodel.compile(loss=huber_fn, optimizer=\"nadam\")\nmodel.fit(X_train, y_train, [...])\nAnd that\u2019s it! For each batch during training, Keras will call the huber_fn() function\nto compute the loss and use it to perform a Gradient Descent step. Moreover, it will\nkeep track of the total loss since the beginning of the epoch, and it will display the\nmean loss. But what happens to this custom loss when you save the model? Saving and Loading Models That Contain Custom Components\nSaving a model containing a custom loss function works fine, as Keras saves the name\nof the function. Whenever you load it, you\u2019ll need to provide a dictionary that maps\nthe function name to the actual function. More generally, when you load a model\ncontaining custom objects, you need to map the names to the objects:\nmodel = keras.models.load_model(\"my_model_with_a_custom_loss.h5\", custom_objects={\"huber_fn\": huber_fn})\nWith the current implementation, any error between \u20131 and 1 is considered \u201csmall.\u201d\nBut what if you want a different threshold? One solution is to create a function that\ncreates a configured loss function:\ndef create_huber(threshold=1.0): def huber_fn(y_true, y_pred): error = y_true - y_pred is_small_error = tf.abs(error) < threshold squared_loss = tf.square(error) / 2 linear_loss = threshold * tf.abs(error) - threshold**2 / 2 return tf.where(is_small_error, squared_loss, linear_loss) return huber_fn\nmodel.compile(loss=create_huber(2.0), optimizer=\"nadam\")\nUnfortunately, when you save the model, the threshold will not be saved. This means\nthat you will have to specify the threshold value when loading the model (note that\nthe name to use is \"huber_fn\", which is the name of the function you gave Keras, not\nthe name of the function that created it):\nmodel = keras.models.load_model(\"my_model_with_a_custom_loss_threshold_2.h5\", custom_objects={\"huber_fn\": create_huber(2.0)})\nYou can solve this by creating a subclass of the keras.losses.Loss class, and then\nimplementing its get_config() method:\nCustomizing Models and Training Algorithms | 5 It would not be a good idea to use a weighted mean: if you did, then two instances with the same weight but in\ndifferent batches would have a different impact on training, depending on the total weight of each batch. class HuberLoss(keras.losses.Loss): def __init__(self, threshold=1.0, **kwargs): self.threshold = threshold super().__init__(**kwargs) def call(self, y_true, y_pred): error = y_true - y_pred is_small_error = tf.abs(error) < self.threshold squared_loss = tf.square(error) / 2 linear_loss = self.threshold * tf.abs(error) - self.threshold**2 / 2 return tf.where(is_small_error, squared_loss, linear_loss) def get_config(self): base_config = super().get_config() return {**base_config, \"threshold\": self.threshold}\nThe Keras API currently only specifies how to use subclassing to\ndefine layers, models, callbacks, and regularizers. If you build other\ncomponents (such as losses, metrics, initializers, or constraints)\nusing subclassing, they may not be portable to other Keras imple\u2010\nmentations. It\u2019s likely that the Keras API will be updated to specify\nsubclassing for all these components as well."
  },
  {
    "id": 253,
    "content": "Let\u2019s walk through this code:\n\u2022 The constructor accepts **kwargs and passes them to the parent constructor,\nwhich handles standard hyperparameters: the name of the loss and the reduction\nalgorithm to use to aggregate the individual instance losses. By default, it is\n\"sum_over_batch_size\", which means that the loss will be the sum of the\ninstance losses, weighted by the sample weights, if any, and divided by the batch\nsize (not by the sum of weights, so this is not the weighted mean).5 Other possible\nvalues are \"sum\" and \"none\". \u2022 The call() method takes the labels and predictions, computes all the instance\nlosses, and returns them. \u2022 The get_config() method returns a dictionary mapping each hyperparameter\nname to its value. It first calls the parent class\u2019s get_config() method, then adds\nthe new hyperparameters to this dictionary (note that the convenient {**x} syn\u2010\ntax was added in Python 3.5). You can then use any instance of this class when you compile the model:\nmodel.compile(loss=HuberLoss(2. ), optimizer=\"nadam\") | Chapter 12: Custom Models and Training with TensorFlow\nWhen you save the model, the threshold will be saved along with it; and when you\nload the model, you just need to map the class name to the class itself:\nmodel = keras.models.load_model(\"my_model_with_a_custom_loss_class.h5\", custom_objects={\"HuberLoss\": HuberLoss})\nWhen you save a model, Keras calls the loss instance\u2019s get_config() method and\nsaves the config as JSON in the HDF5 file. When you load the model, it calls the\nfrom_config() class method on the HuberLoss class: this method is implemented by\nthe base class (Loss) and creates an instance of the class, passing **config to the\nconstructor. That\u2019s it for losses! That wasn\u2019t too hard, was it? Just as simple are custom activation\nfunctions, initializers, regularizers, and constraints. Let\u2019s look at these now. Custom Activation Functions, Initializers, Regularizers, and\nConstraints\nMost Keras functionalities, such as losses, regularizers, constraints, initializers, met\u2010\nrics, activation functions, layers, and even full models, can be customized in very\nmuch the same way. Most of the time, you will just need to write a simple function\nwith the appropriate inputs and outputs. Here are examples of a custom activation\nfunction (equivalent to keras.activations.softplus() or tf.nn.softplus()), a\ncustom Glorot initializer (equivalent to keras.initializers.glorot_normal()), a\ncustom \u21131 regularizer (equivalent to keras.regularizers.l1(0.01)), and a custom\nconstraint that ensures weights are all positive (equivalent to keras.con\nstraints.nonneg() or tf.nn.relu()):\ndef my_softplus(z): # return value is just tf.nn.softplus(z) return tf.math.log(tf.exp(z) + 1.0)\ndef my_glorot_initializer(shape, dtype=tf.float32): stddev = tf.sqrt(2. / (shape[0] + shape[1])) return tf.random.normal(shape, stddev=stddev, dtype=dtype)\ndef my_l1_regularizer(weights): return tf.reduce_sum(tf.abs(0.01 * weights))\ndef my_positive_weights(weights): # return value is just tf.nn.relu(weights) return tf.where(weights < 0., tf.zeros_like(weights), weights)\nAs you can see, the arguments depend on the type of custom function. These custom\nfunctions can then be used normally; for example:\nlayer = keras.layers.Dense(30, activation=my_softplus, kernel_initializer=my_glorot_initializer, kernel_regularizer=my_l1_regularizer, kernel_constraint=my_positive_weights)\nCustomizing Models and Training Algorithms | 6 However, the Huber loss is seldom used as a metric (the MAE or MSE is preferred)."
  },
  {
    "id": 254,
    "content": "The activation function will be applied to the output of this Dense layer, and its result\nwill be passed on to the next layer. The layer\u2019s weights will be initialized using the\nvalue returned by the initializer. At each training step the weights will be passed to the\nregularization function to compute the regularization loss, which will be added to the\nmain loss to get the final loss used for training. Finally, the constraint function will be\ncalled after each training step, and the layer\u2019s weights will be replaced by the con\u2010\nstrained weights. If a function has hyperparameters that need to be saved along with the model, then\nyou will want to subclass the appropriate class, such as keras.regularizers.Regular\nizer, keras.constraints.Constraint, keras.initializers.Initializer, or\nkeras.layers.Layer (for any layer, including activation functions). Much like we did\nfor the custom loss, here is a simple class for \u21131 regularization that saves its factor\nhyperparameter (this time we do not need to call the parent constructor or the\nget_config() method, as they are not defined by the parent class):\nclass MyL1Regularizer(keras.regularizers.Regularizer): def __init__(self, factor): self.factor = factor def __call__(self, weights): return tf.reduce_sum(tf.abs(self.factor * weights)) def get_config(self): return {\"factor\": self.factor}\nNote that you must implement the call() method for losses, layers (including activa\u2010\ntion functions), and models, or the __call__() method for regularizers, initializers,\nand constraints. For metrics, things are a bit different, as we will see now. Custom Metrics\nLosses and metrics are conceptually not the same thing: losses (e.g., cross entropy)\nare used by Gradient Descent to train a model, so they must be differentiable (at least\nwhere they are evaluated), and their gradients should not be 0 everywhere. Plus, it\u2019s\nOK if they are not easily interpretable by humans. In contrast, metrics (e.g., accuracy)\nare used to evaluate a model: they must be more easily interpretable, and they can be\nnon-differentiable or have 0 gradients everywhere. That said, in most cases, defining a custom metric function is exactly the same as\ndefining a custom loss function. In fact, we could even use the Huber loss function we\ncreated earlier as a metric;6 it would work just fine (and persistence would also work\nthe same way, in this case only saving the name of the function, \"huber_fn\"): | Chapter 12: Custom Models and Training with TensorFlow\nmodel.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\nFor each batch during training, Keras will compute this metric and keep track of its\nmean since the beginning of the epoch. Most of the time, this is exactly what you\nwant. But not always! Consider a binary classifier\u2019s precision, for example. As we saw\nin Chapter 3, precision is the number of true positives divided by the number of posi\u2010\ntive predictions (including both true positives and false positives). Suppose the model\nmade five positive predictions in the first batch, four of which were correct: that\u2019s 80%\nprecision. Then suppose the model made three positive predictions in the second\nbatch, but they were all incorrect: that\u2019s 0% precision for the second batch."
  },
  {
    "id": 255,
    "content": "If you just\ncompute the mean of these two precisions, you get 40%. But wait a second\u2014that\u2019s not\nthe model\u2019s precision over these two batches! Indeed, there were a total of four true\npositives (4 + 0) out of eight positive predictions (5 + 3), so the overall precision is\n50%, not 40%. What we need is an object that can keep track of the number of true\npositives and the number of false positives and that can compute their ratio when\nrequested. This is precisely what the keras.metrics.Precision class does:\n>>> precision = keras.metrics.Precision()\n>>> precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])\n<tf.Tensor: id=581729, shape=(), dtype=float32, numpy=0.8>\n>>> precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\n<tf.Tensor: id=581780, shape=(), dtype=float32, numpy=0.5>\nIn this example, we created a Precision object, then we used it like a function, pass\u2010\ning it the labels and predictions for the first batch, then for the second batch (note\nthat we could also have passed sample weights). We used the same number of true\nand false positives as in the example we just discussed. After the first batch, it returns\na precision of 80%; then after the second batch, it returns 50% (which is the overall\nprecision so far, not the second batch\u2019s precision). This is called a streaming metric (or\nstateful metric), as it is gradually updated, batch after batch. At any point, we can call the result() method to get the current value of the metric. We can also look at its variables (tracking the number of true and false positives) by\nusing the variables attribute, and we can reset these variables using the\nreset_states() method:\n>>> precision.result()\n<tf.Tensor: id=581794, shape=(), dtype=float32, numpy=0.5>\n>>> precision.variables\n[<tf.Variable 'true_positives:0' [...] numpy=array([4. ], dtype=float32)>, <tf.Variable 'false_positives:0' [...] numpy=array([4. ], dtype=float32)>]\n>>> precision.reset_states() # both variables get reset to 0.0\nIf you need to create such a streaming metric, create a subclass of the keras.met\nrics.Metric class. Here is a simple example that keeps track of the total Huber loss\nCustomizing Models and Training Algorithms | 7 This class is for illustration purposes only. A simpler and better implementation would just subclass the\nkeras.metrics.Mean class; see the \u201cStreaming metrics\u201d section of the notebook for an example. and the number of instances seen so far."
  },
  {
    "id": 256,
    "content": "When asked for the result, it returns the\nratio, which is simply the mean Huber loss:\nclass HuberMetric(keras.metrics.Metric): def __init__(self, threshold=1.0, **kwargs): super().__init__(**kwargs) # handles base args (e.g., dtype) self.threshold = threshold self.huber_fn = create_huber(threshold) self.total = self.add_weight(\"total\", initializer=\"zeros\") self.count = self.add_weight(\"count\", initializer=\"zeros\") def update_state(self, y_true, y_pred, sample_weight=None): metric = self.huber_fn(y_true, y_pred) self.total.assign_add(tf.reduce_sum(metric)) self.count.assign_add(tf.cast(tf.size(y_true), tf.float32)) def result(self): return self.total / self.count def get_config(self): base_config = super().get_config() return {**base_config, \"threshold\": self.threshold}\nLet\u2019s walk through this code:7\n\u2022 The constructor uses the add_weight() method to create the variables needed to\nkeep track of the metric\u2019s state over multiple batches\u2014in this case, the sum of all\nHuber losses (total) and the number of instances seen so far (count). You could\njust create variables manually if you preferred. Keras tracks any tf.Variable that\nis set as an attribute (and more generally, any \u201ctrackable\u201d object, such as layers or\nmodels). \u2022 The update_state() method is called when you use an instance of this class as a\nfunction (as we did with the Precision object). It updates the variables, given the\nlabels and predictions for one batch (and sample weights, but in this case we\nignore them). \u2022 The result() method computes and returns the final result, in this case the\nmean Huber metric over all instances. When you use the metric as a function, the\nupdate_state() method gets called first, then the result() method is called,\nand its output is returned. \u2022 We also implement the get_config() method to ensure the threshold gets\nsaved along with the model. \u2022 The default implementation of the reset_states() method resets all variables to\n0.0 (but you can override it if needed). | Chapter 12: Custom Models and Training with TensorFlow\nKeras will take care of variable persistence seamlessly; no action is\nrequired. When you define a metric using a simple function, Keras automatically calls it for\neach batch, and it keeps track of the mean during each epoch, just like we did man\u2010\nually. So the only benefit of our HuberMetric class is that the threshold will be saved. But of course, some metrics, like precision, cannot simply be averaged over batches:\nin those cases, there\u2019s no other option than to implement a streaming metric. Now that we have built a streaming metric, building a custom layer will seem like a\nwalk in the park! Custom Layers\nYou may occasionally want to build an architecture that contains an exotic layer for\nwhich TensorFlow does not provide a default implementation. In this case, you will\nneed to create a custom layer. Or you may simply want to build a very repetitive\narchitecture, containing identical blocks of layers repeated many times, and it would\nbe convenient to treat each block of layers as a single layer."
  },
  {
    "id": 257,
    "content": "For example, if the model\nis a sequence of layers A, B, C, A, B, C, A, B, C, then you might want to define a cus\u2010\ntom layer D containing layers A, B, C, so your model would then simply be D, D, D.\nLet\u2019s see how to build custom layers. First, some layers have no weights, such as keras.layers.Flatten or keras.lay\ners.ReLU. If you want to create a custom layer without any weights, the simplest\noption is to write a function and wrap it in a keras.layers.Lambda layer. For exam\u2010\nple, the following layer will apply the exponential function to its inputs:\nexponential_layer = keras.layers.Lambda(lambda x: tf.exp(x))\nThis custom layer can then be used like any other layer, using the Sequential API, the\nFunctional API, or the Subclassing API. You can also use it as an activation function\n(or you could use activation=tf.exp, activation=keras.activations.exponen\ntial, or simply activation=\"exponential\"). The exponential layer is sometimes\nused in the output layer of a regression model when the values to predict have very\ndifferent scales (e.g., 0.001, 10., 1,000.). As you\u2019ve probably guessed by now, to build a custom stateful layer (i.e., a layer with\nweights), you need to create a subclass of the keras.layers.Layer class. For exam\u2010\nple, the following class implements a simplified version of the Dense layer:\nCustomizing Models and Training Algorithms | 8 This function is specific to tf.keras. You could use keras.layers.Activation instead. 9 The Keras API calls this argument input_shape, but since it also includes the batch dimension, I prefer to call\nit batch_input_shape. Same for compute_output_shape(). class MyDense(keras.layers.Layer): def __init__(self, units, activation=None, **kwargs): super().__init__(**kwargs) self.units = units self.activation = keras.activations.get(activation) def build(self, batch_input_shape): self.kernel = self.add_weight( name=\"kernel\", shape=[batch_input_shape[-1], self.units], initializer=\"glorot_normal\") self.bias = self.add_weight( name=\"bias\", shape=[self.units], initializer=\"zeros\") super().build(batch_input_shape) # must be at the end def call(self, X): return self.activation(X @ self.kernel + self.bias) def compute_output_shape(self, batch_input_shape): return tf.TensorShape(batch_input_shape.as_list()[:-1] + [self.units]) def get_config(self): base_config = super().get_config() return {**base_config, \"units\": self.units, \"activation\": keras.activations.serialize(self.activation)}\nLet\u2019s walk through this code:\n\u2022 The constructor takes all the hyperparameters as arguments (in this example,\nunits and activation), and importantly it also takes a **kwargs argument. It\ncalls the parent constructor, passing it the kwargs: this takes care of standard\narguments such as input_shape, trainable, and name. Then it saves the hyper\u2010\nparameters as attributes, converting the activation argument to the appropriate\nactivation function using the keras.activations.get() function (it accepts\nfunctions, standard strings like \"relu\" or \"selu\", or simply None).8\n\u2022 The build() method\u2019s role is to create the layer\u2019s variables by calling the\nadd_weight() method for each weight. The build() method is called the first\ntime the layer is used. At that point, Keras will know the shape of this layer\u2019s\ninputs, and it will pass it to the build() method,9 which is often necessary to cre\u2010\nate some of the weights."
  },
  {
    "id": 258,
    "content": "For example, we need to know the number of neurons in\nthe previous layer in order to create the connection weights matrix (i.e., the\n\"kernel\"): this corresponds to the size of the last dimension of the inputs. At the\nend of the build() method (and only at the end), you must call the parent\u2019s | Chapter 12: Custom Models and Training with TensorFlow\nbuild() method: this tells Keras that the layer is built (it just sets\nself.built=True). \u2022 The call() method performs the desired operations. In this case, we compute\nthe matrix multiplication of the inputs X and the layer\u2019s kernel, we add the bias\nvector, and we apply the activation function to the result, and this gives us the\noutput of the layer. \u2022 The compute_output_shape() method simply returns the shape of this layer\u2019s\noutputs. In this case, it is the same shape as the inputs, except the last dimension\nis replaced with the number of neurons in the layer. Note that in tf.keras, shapes\nare instances of the tf.TensorShape class, which you can convert to Python lists\nusing as_list(). \u2022 The get_config() method is just like in the previous custom classes. Note that\nwe save the activation function\u2019s full configuration by calling keras.activa\ntions.serialize(). You can now use a MyDense layer just like any other layer! You can generally omit the compute_output_shape() method, as\ntf.keras automatically infers the output shape, except when the\nlayer is dynamic (as we will see shortly). In other Keras implemen\u2010\ntations, this method is either required or its default implementation\nassumes the output shape is the same as the input shape. To create a layer with multiple inputs (e.g., Concatenate), the argument to the call()\nmethod should be a tuple containing all the inputs, and similarly the argument to the\ncompute_output_shape() method should be a tuple containing each input\u2019s batch\nshape. To create a layer with multiple outputs, the call() method should return the\nlist of outputs, and compute_output_shape() should return the list of batch output\nshapes (one per output). For example, the following toy layer takes two inputs and\nreturns three outputs:\nclass MyMultiLayer(keras.layers.Layer): def call(self, X): X1, X2 = X return [X1 + X2, X1 * X2, X1 / X2] def compute_output_shape(self, batch_input_shape): b1, b2 = batch_input_shape return [b1, b1, b1] # should probably handle broadcasting rules\nCustomizing Models and Training Algorithms | 10 The name \u201cSubclassing API\u201d usually refers only to the creation of custom models by subclassing, although\nmany other things can be created by subclassing, as we saw in this chapter. This layer may now be used like any other layer, but of course only using the Func\u2010\ntional and Subclassing APIs, not the Sequential API (which only accepts layers with\none input and one output)."
  },
  {
    "id": 259,
    "content": "If your layer needs to have a different behavior during training and during testing\n(e.g., if it uses Dropout or BatchNormalization layers), then you must add a train\ning argument to the call() method and use this argument to decide what to do. For\nexample, let\u2019s create a layer that adds Gaussian noise during training (for regulariza\u2010\ntion) but does nothing during testing (Keras has a layer that does the same thing,\nkeras.layers.GaussianNoise):\nclass MyGaussianNoise(keras.layers.Layer): def __init__(self, stddev, **kwargs): super().__init__(**kwargs) self.stddev = stddev def call(self, X, training=None): if training: noise = tf.random.normal(tf.shape(X), stddev=self.stddev) return X + noise else: return X def compute_output_shape(self, batch_input_shape): return batch_input_shape\nWith that, you can now build any custom layer you need! Now let\u2019s create custom\nmodels. Custom Models\nWe already looked at creating custom model classes in Chapter 10, when we dis\u2010\ncussed the Subclassing API.10 It\u2019s straightforward: subclass the keras.Model class, cre\u2010\nate layers and variables in the constructor, and implement the call() method to do\nwhatever you want the model to do. Suppose you want to build the model repre\u2010\nsented in Figure 12-3. | Chapter 12: Custom Models and Training with TensorFlow\nFigure 12-3. Custom model example: an arbitrary model with a custom ResidualBlock\nlayer containing a skip connection\nThe inputs go through a first dense layer, then through a residual block composed of\ntwo dense layers and an addition operation (as we will see in Chapter 14, a residual\nblock adds its inputs to its outputs), then through this same residual block three more\ntimes, then through a second residual block, and the final result goes through a dense\noutput layer. Note that this model does not make much sense; it\u2019s just an example to\nillustrate the fact that you can easily build any kind of model you want, even one that\ncontains loops and skip connections. To implement this model, it is best to first create\na ResidualBlock layer, since we are going to create a couple of identical blocks (and\nwe might want to reuse it in another model):\nclass ResidualBlock(keras.layers.Layer): def __init__(self, n_layers, n_neurons, **kwargs): super().__init__(**kwargs) self.hidden = [keras.layers.Dense(n_neurons, activation=\"elu\", kernel_initializer=\"he_normal\") for _ in range(n_layers)] def call(self, inputs): Z = inputs for layer in self.hidden: Z = layer(Z) return inputs + Z\nThis layer is a bit special since it contains other layers. This is handled transparently\nby Keras: it automatically detects that the hidden attribute contains trackable objects\n(layers in this case), so their variables are automatically added to this layer\u2019s list of\nCustomizing Models and Training Algorithms | variables. The rest of this class is self-explanatory. Next, let\u2019s use the Subclassing API\nto define the model itself:\nclass ResidualRegressor(keras.Model): def __init__(self, output_dim, **kwargs): super().__init__(**kwargs) self.hidden1 = keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\") self.block1 = ResidualBlock(2, 30) self.block2 = ResidualBlock(2, 30) self.out = keras.layers.Dense(output_dim) def call(self, inputs): Z = self.hidden1(inputs) for _ in range(1 + 3): Z = self.block1(Z) Z = self.block2(Z) return self.out(Z)\nWe create the layers in the constructor and use them in the call() method."
  },
  {
    "id": 260,
    "content": "This\nmodel can then be used like any other model (compile it, fit it, evaluate it, and use it\nto make predictions). If you also want to be able to save the model using the save()\nmethod and load it using the keras.models.load_model() function, you must\nimplement the get_config() method (as we did earlier) in both the ResidualBlock\nclass and the ResidualRegressor class. Alternatively, you can save and load the\nweights using the save_weights() and load_weights() methods. The Model class is a subclass of the Layer class, so models can be defined and used\nexactly like layers. But a model has some extra functionalities, including of course its\ncompile(), fit(), evaluate(), and predict() methods (and a few variants), plus the\nget_layers() method (which can return any of the model\u2019s layers by name or by\nindex) and the save() method (and support for keras.models.load_model() and\nkeras.models.clone_model()). If models provide more functionality than layers, why not just\ndefine every layer as a model? Well, technically you could, but it is\nusually cleaner to distinguish the internal components of your\nmodel (i.e., layers or reusable blocks of layers) from the model itself\n(i.e., the object you will train). The former should subclass the\nLayer class, while the latter should subclass the Model class. With that, you can naturally and concisely build almost any model that you find in a\npaper, using the Sequential API, the Functional API, the Subclassing API, or even a\nmix of these. \u201cAlmost\u201d any model? Yes, there are still a few things that we need to look | Chapter 12: Custom Models and Training with TensorFlow\nat: first, how to define losses or metrics based on model internals, and second, how to\nbuild a custom training loop. Losses and Metrics Based on Model Internals\nThe custom losses and metrics we defined earlier were all based on the labels and the\npredictions (and optionally sample weights). There will be times when you want to\ndefine losses based on other parts of your model, such as the weights or activations of\nits hidden layers. This may be useful for regularization purposes or to monitor some\ninternal aspect of your model. To define a custom loss based on model internals, compute it based on any part of the\nmodel you want, then pass the result to the add_loss() method.For example, let\u2019s\nbuild a custom regression MLP model composed of a stack of five hidden layers plus\nan output layer. This custom model will also have an auxiliary output on top of the\nupper hidden layer. The loss associated to this auxiliary output will be called the\nreconstruction loss (see Chapter 17): it is the mean squared difference between the\nreconstruction and the inputs. By adding this reconstruction loss to the main loss, we\nwill encourage the model to preserve as much information as possible through the\nhidden layers\u2014even information that is not directly useful for the regression task\nitself. In practice, this loss sometimes improves generalization (it is a regularization\nloss)."
  },
  {
    "id": 261,
    "content": "Here is the code for this custom model with a custom reconstruction loss:\nclass ReconstructingRegressor(keras.Model): def __init__(self, output_dim, **kwargs): super().__init__(**kwargs) self.hidden = [keras.layers.Dense(30, activation=\"selu\", kernel_initializer=\"lecun_normal\") for _ in range(5)] self.out = keras.layers.Dense(output_dim) def build(self, batch_input_shape): n_inputs = batch_input_shape[-1] self.reconstruct = keras.layers.Dense(n_inputs) super().build(batch_input_shape) def call(self, inputs): Z = inputs for layer in self.hidden: Z = layer(Z) reconstruction = self.reconstruct(Z) recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs)) self.add_loss(0.05 * recon_loss) return self.out(Z)\nCustomizing Models and Training Algorithms | 11 You can also call add_loss() on any layer inside the model, as the model recursively gathers losses from all of\nits layers. Let\u2019s go through this code:\n\u2022 The constructor creates the DNN with five dense hidden layers and one dense\noutput layer. \u2022 The build() method creates an extra dense layer which will be used to recon\u2010\nstruct the inputs of the model. It must be created here because its number of units\nmust be equal to the number of inputs, and this number is unknown before the\nbuild() method is called. \u2022 The call() method processes the inputs through all five hidden layers, then\npasses the result through the reconstruction layer, which produces the recon\u2010\nstruction. \u2022 Then the call() method computes the reconstruction loss (the mean squared\ndifference between the reconstruction and the inputs), and adds it to the model\u2019s\nlist of losses using the add_loss() method.11 Notice that we scale down the\nreconstruction loss by multiplying it by 0.05 (this is a hyperparameter you can\ntune). This ensures that the reconstruction loss does not dominate the main loss. \u2022 Finally, the call() method passes the output of the hidden layers to the output\nlayer and returns its output. Similarly, you can add a custom metric based on model internals by computing it in\nany way you want, as long as the result is the output of a metric object. For example,\nyou can create a keras.metrics.Mean object in the constructor, then call it in the\ncall() method, passing it the recon_loss, and finally add it to the model by calling\nthe model\u2019s add_metric() method. This way, when you train the model, Keras will\ndisplay both the mean loss over each epoch (the loss is the sum of the main loss plus\n0.05 times the reconstruction loss) and the mean reconstruction error over each\nepoch. Both will go down during training:\nEpoch 1/5\n11610/11610 [=============] [...] loss: 4.3092 - reconstruction_error: 1.7360\nEpoch 2/5\n11610/11610 [=============] [...] loss: 1.1232 - reconstruction_error: 0.8964\n[...]\nIn over 99% of cases, everything we have discussed so far will be sufficient to imple\u2010\nment whatever model you want to build, even with complex architectures, losses, and\nmetrics. However, in some rare cases you may need to customize the training loop | Chapter 12: Custom Models and Training with TensorFlow\nitself. Before we get there, we need to look at how to compute gradients automatically\nin TensorFlow."
  },
  {
    "id": 262,
    "content": "Computing Gradients Using Autodiff\nTo understand how to use autodiff (see Chapter 10 and Appendix D) to compute gra\u2010\ndients automatically, let\u2019s consider a simple toy function:\ndef f(w1, w2): return 3 * w1 ** 2 + 2 * w1 * w2\nIf you know calculus, you can analytically find that the partial derivative of this func\u2010\ntion with regard to w1 is 6 * w1 + 2 * w2. You can also find that its partial derivative\nwith regard to w2 is 2 * w1. For example, at the point (w1, w2) = (5, 3), these par\u2010\ntial derivatives are equal to 36 and 10, respectively, so the gradient vector at this point\nis (36, 10). But if this were a neural network, the function would be much more com\u2010\nplex, typically with tens of thousands of parameters, and finding the partial deriva\u2010\ntives analytically by hand would be an almost impossible task. One solution could be\nto compute an approximation of each partial derivative by measuring how much the\nfunction\u2019s output changes when you tweak the corresponding parameter:\n>>> w1, w2 = 5, 3\n>>> eps = 1e-6\n>>> (f(w1 + eps, w2) - f(w1, w2)) / eps\n36.000003007075065\n>>> (f(w1, w2 + eps) - f(w1, w2)) / eps\n10.000000003174137\nLooks about right! This works rather well and is easy to implement, but it is just an\napproximation, and importantly you need to call f() at least once per parameter (not\ntwice, since we could compute f(w1, w2) just once). Needing to call f() at least once\nper parameter makes this approach intractable for large neural networks. So instead,\nwe should use autodiff. TensorFlow makes this pretty simple:\nw1, w2 = tf.Variable(5. ), tf.Variable(3.) with tf.GradientTape() as tape: z = f(w1, w2)\ngradients = tape.gradient(z, [w1, w2])\nWe first define two variables w1 and w2, then we create a tf.GradientTape context\nthat will automatically record every operation that involves a variable, and finally we\nask this tape to compute the gradients of the result z with regard to both variables\n[w1, w2]. Let\u2019s take a look at the gradients that TensorFlow computed:\n>>> gradients\n[<tf.Tensor: id=828234, shape=(), dtype=float32, numpy=36.0>, <tf.Tensor: id=828229, shape=(), dtype=float32, numpy=10.0>]\nCustomizing Models and Training Algorithms | 12 If the tape goes out of scope, for example when the function that used it returns, Python\u2019s garbage collector\nwill delete it for you. Perfect! Not only is the result accurate (the precision is only limited by the floating-\npoint errors), but the gradient() method only goes through the recorded computa\u2010\ntions once (in reverse order), no matter how many variables there are, so it is\nincredibly efficient. It\u2019s like magic! To save memory, only put the strict minimum inside the tf.Gra\ndientTape() block. Alternatively, pause recording by creating a\nwith tape.stop_recording() block inside the tf.Gradient\nTape() block."
  },
  {
    "id": 263,
    "content": "The tape is automatically erased immediately after you call its gradient() method, so\nyou will get an exception if you try to call gradient() twice:\nwith tf.GradientTape() as tape: z = f(w1, w2)\ndz_dw1 = tape.gradient(z, w1) # => tensor 36.0\ndz_dw2 = tape.gradient(z, w2) # RuntimeError! If you need to call gradient() more than once, you must make the tape persistent\nand delete it each time you are done with it to free resources:12\nwith tf.GradientTape(persistent=True) as tape: z = f(w1, w2)\ndz_dw1 = tape.gradient(z, w1) # => tensor 36.0\ndz_dw2 = tape.gradient(z, w2) # => tensor 10.0, works fine now! del tape\nBy default, the tape will only track operations involving variables, so if you try to\ncompute the gradient of z with regard to anything other than a variable, the result\nwill be None:\nc1, c2 = tf.constant(5. ), tf.constant(3.) with tf.GradientTape() as tape: z = f(c1, c2)\ngradients = tape.gradient(z, [c1, c2]) # returns [None, None]\nHowever, you can force the tape to watch any tensors you like, to record every opera\u2010\ntion that involves them. You can then compute gradients with regard to these tensors,\nas if they were variables: | Chapter 12: Custom Models and Training with TensorFlow\nwith tf.GradientTape() as tape: tape.watch(c1) tape.watch(c2) z = f(c1, c2)\ngradients = tape.gradient(z, [c1, c2]) # returns [tensor 36., tensor 10.] This can be useful in some cases, like if you want to implement a regularization loss\nthat penalizes activations that vary a lot when the inputs vary little: the loss will be\nbased on the gradient of the activations with regard to the inputs. Since the inputs are\nnot variables, you would need to tell the tape to watch them. Most of the time a gradient tape is used to compute the gradients of a single value\n(usually the loss) with regard to a set of values (usually the model parameters). This is\nwhere reverse-mode autodiff shines, as it just needs to do one forward pass and one\nreverse pass to get all the gradients at once. If you try to compute the gradients of a\nvector, for example a vector containing multiple losses, then TensorFlow will com\u2010\npute the gradients of the vector\u2019s sum. So if you ever need to get the individual gradi\u2010\nents (e.g., the gradients of each loss with regard to the model parameters), you must\ncall the tape\u2019s jacobian() method: it will perform reverse-mode autodiff once for\neach loss in the vector (all in parallel by default). It is even possible to compute\nsecond-order partial derivatives (the Hessians, i.e., the partial derivatives of the par\u2010\ntial derivatives), but this is rarely needed in practice (see the \u201cComputing Gradients\nwith Autodiff\u201d section of the notebook for an example). In some cases you may want to stop gradients from backpropagating through some\npart of your neural network. To do this, you must use the tf.stop_gradient() func\u2010\ntion."
  },
  {
    "id": 264,
    "content": "The function returns its inputs during the forward pass (like tf.identity()),\nbut it does not let gradients through during backpropagation (it acts like a constant):\ndef f(w1, w2): return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\nwith tf.GradientTape() as tape: z = f(w1, w2) # same result as without stop_gradient()\ngradients = tape.gradient(z, [w1, w2]) # => returns [tensor 30., None]\nFinally, you may occasionally run into some numerical issues when computing gradi\u2010\nents. For example, if you compute the gradients of the my_softplus() function for\nlarge inputs, the result will be NaN:\n>>> x = tf.Variable([100.]) >>> with tf.GradientTape() as tape:\n... z = my_softplus(x)\n...\n>>> tape.gradient(z, [x])\n<tf.Tensor: [...] numpy=array([nan], dtype=float32)>\nCustomizing Models and Training Algorithms | This is because computing the gradients of this function using autodiff leads to some\nnumerical difficulties: due to floating-point precision errors, autodiff ends up com\u2010\nputing infinity divided by infinity (which returns NaN). Fortunately, we can analyti\u2010\ncally find that the derivative of the softplus function is just 1 / (1 + 1 / exp(x)), which\nis numerically stable. Next, we can tell TensorFlow to use this stable function when\ncomputing the gradients of the my_softplus() function by decorating it with\n@tf.custom_gradient and making it return both its normal output and the function\nthat computes the derivatives (note that it will receive as input the gradients that were\nbackpropagated so far, down to the softplus function; and according to the chain rule,\nwe should multiply them with this function\u2019s gradients):\n@tf.custom_gradient\ndef my_better_softplus(z): exp = tf.exp(z) def my_softplus_gradients(grad): return grad / (1 + 1 / exp) return tf.math.log(exp + 1), my_softplus_gradients\nNow when we compute the gradients of the my_better_softplus() function, we get\nthe proper result, even for large input values (however, the main output still explodes\nbecause of the exponential; one workaround is to use tf.where() to return the inputs\nwhen they are large). Congratulations! You can now compute the gradients of any function (provided it is\ndifferentiable at the point where you compute it), even blocking backpropagation\nwhen needed, and write your own gradient functions! This is probably more flexibil\u2010\nity than you will ever need, even if you build your own custom training loops, as we\nwill see now. Custom Training Loops\nIn some rare cases, the fit() method may not be flexible enough for what you need\nto do. For example, the Wide & Deep paper we discussed in Chapter 10 uses two dif\u2010\nferent optimizers: one for the wide path and the other for the deep path. Since the\nfit() method only uses one optimizer (the one that we specify when compiling the\nmodel), implementing this paper requires writing your own custom loop. You may also like to write custom training loops simply to feel more confident that\nthey do precisely what you intend them to do (perhaps you are unsure about some\ndetails of the fit() method). It can sometimes feel safer to make everything explicit."
  },
  {
    "id": 265,
    "content": "However, remember that writing a custom training loop will make your code longer,\nmore error-prone, and harder to maintain. | Chapter 12: Custom Models and Training with TensorFlow\nUnless you really need the extra flexibility, you should prefer using\nthe fit() method rather than implementing your own training\nloop, especially if you work in a team. First, let\u2019s build a simple model. No need to compile it, since we will handle the train\u2010\ning loop manually:\nl2_reg = keras.regularizers.l2(0.05)\nmodel = keras.models.Sequential([ keras.layers.Dense(30, activation=\"elu\", kernel_initializer=\"he_normal\", kernel_regularizer=l2_reg), keras.layers.Dense(1, kernel_regularizer=l2_reg)\n])\nNext, let\u2019s create a tiny function that will randomly sample a batch of instances from\nthe training set (in Chapter 13 we will discuss the Data API, which offers a much bet\u2010\nter alternative):\ndef random_batch(X, y, batch_size=32): idx = np.random.randint(len(X), size=batch_size) return X[idx], y[idx]\nLet\u2019s also define a function that will display the training status, including the number\nof steps, the total number of steps, the mean loss since the start of the epoch (i.e., we\nwill use the Mean metric to compute it), and other metrics:\ndef print_status_bar(iteration, total, loss, metrics=None): metrics = \" - \".join([\"{}: {:.4f}\".format(m.name, m.result()) for m in [loss] + (metrics or [])]) end = \"\" if iteration < total else \"\\n\" print(\"\\r{}/{} - \".format(iteration, total) + metrics, end=end)\nThis code is self-explanatory, unless you are unfamiliar with Python string format\u2010\nting: {:.4f} will format a float with four digits after the decimal point, and using \\r\n(carriage return) along with end=\"\" ensures that the status bar always gets printed on\nthe same line. In the notebook, the print_status_bar() function includes a progress\nbar, but you could use the handy tqdm library instead. With that, let\u2019s get down to business! First, we need to define some hyperparameters\nand choose the optimizer, the loss function, and the metrics (just the MAE in this\nexample):\nn_epochs = 5\nbatch_size = 32\nn_steps = len(X_train) // batch_size\noptimizer = keras.optimizers.Nadam(lr=0.01)\nloss_fn = keras.losses.mean_squared_error\nCustomizing Models and Training Algorithms | mean_loss = keras.metrics.Mean()\nmetrics = [keras.metrics.MeanAbsoluteError()]\nAnd now we are ready to build the custom loop! for epoch in range(1, n_epochs + 1): print(\"Epoch {}/{}\".format(epoch, n_epochs)) for step in range(1, n_steps + 1): X_batch, y_batch = random_batch(X_train_scaled, y_train) with tf.GradientTape() as tape: y_pred = model(X_batch, training=True) main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred)) loss = tf.add_n([main_loss] + model.losses) gradients = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(gradients, model.trainable_variables)) mean_loss(loss) for metric in metrics: metric(y_batch, y_pred) print_status_bar(step * batch_size, len(y_train), mean_loss, metrics) print_status_bar(len(y_train), len(y_train), mean_loss, metrics) for metric in [mean_loss] + metrics: metric.reset_states()\nThere\u2019s a lot going on in this code, so let\u2019s walk through it:\n\u2022 We create two nested loops: one for the epochs, the other for the batches within\nan epoch. \u2022 Then we sample a random batch from the training set."
  },
  {
    "id": 266,
    "content": "\u2022 Inside the tf.GradientTape() block, we make a prediction for one batch (using\nthe model as a function), and we compute the loss: it is equal to the main loss\nplus the other losses (in this model, there is one regularization loss per layer). Since the mean_squared_error() function returns one loss per instance, we\ncompute the mean over the batch using tf.reduce_mean() (if you wanted to\napply different weights to each instance, this is where you would do it). The regu\u2010\nlarization losses are already reduced to a single scalar each, so we just need to\nsum them (using tf.add_n(), which sums multiple tensors of the same shape\nand data type). \u2022 Next, we ask the tape to compute the gradient of the loss with regard to each\ntrainable variable (not all variables! ), and we apply them to the optimizer to per\u2010\nform a Gradient Descent step. \u2022 Then we update the mean loss and the metrics (over the current epoch), and we\ndisplay the status bar. | Chapter 12: Custom Models and Training with TensorFlow\n13 The truth is we did not process every single instance in the training set, because we sampled instances ran\u2010\ndomly: some were processed more than once, while others were not processed at all. Likewise, if the training\nset size is not a multiple of the batch size, we will miss a few instances. In practice that\u2019s fine. 14 With the exception of optimizers, as very few people ever customize these; see the \u201cCustom Optimizers\u201d sec\u2010\ntion in the notebook for an example. \u2022 At the end of each epoch, we display the status bar again to make it look com\u2010\nplete13 and to print a line feed, and we reset the states of the mean loss and the\nmetrics. If you set the optimizer\u2019s clipnorm or clipvalue hyperparameter, it will take care of\nthis for you. If you want to apply any other transformation to the gradients, simply do\nso before calling the apply_gradients() method. If you add weight constraints to your model (e.g., by setting kernel_constraint or\nbias_constraint when creating a layer), you should update the training loop to\napply these constraints just after apply_gradients():\nfor variable in model.variables: if variable.constraint is not None: variable.assign(variable.constraint(variable))\nMost importantly, this training loop does not handle layers that behave differently\nduring training and testing (e.g., BatchNormalization or Dropout). To handle these,\nyou need to call the model with training=True and make sure it propagates this to\nevery layer that needs it. As you can see, there are quite a lot of things you need to get right, and it\u2019s easy to\nmake a mistake. But on the bright side, you get full control, so it\u2019s your call."
  },
  {
    "id": 267,
    "content": "Now that you know how to customize any part of your models14 and training algo\u2010\nrithms, let\u2019s see how you can use TensorFlow\u2019s automatic graph generation feature: it\ncan speed up your custom code considerably, and it will also make it portable to any\nplatform supported by TensorFlow (see Chapter 19). TensorFlow Functions and Graphs\nIn TensorFlow 1, graphs were unavoidable (as were the complexities that came with\nthem) because they were a central part of TensorFlow\u2019s API. In TensorFlow 2, they are\nstill there, but not as central, and they\u2019re much (much!) simpler to use. To show just\nhow simple, let\u2019s start with a trivial function that computes the cube of its input:\ndef cube(x): return x ** 3\nTensorFlow Functions and Graphs | 15 However, in this trivial example, the computation graph is so small that there is nothing at all to optimize, so\ntf_cube() actually runs much slower than cube(). We can obviously call this function with a Python value, such as an int or a float, or\nwe can call it with a tensor:\n>>> cube(2) >>> cube(tf.constant(2.0))\n<tf.Tensor: id=18634148, shape=(), dtype=float32, numpy=8.0>\nNow, let\u2019s use tf.function() to convert this Python function to a TensorFlow\nFunction:\n>>> tf_cube = tf.function(cube)\n>>> tf_cube\n<tensorflow.python.eager.def_function.Function at 0x1546fc080>\nThis TF Function can then be used exactly like the original Python function, and it\nwill return the same result (but as tensors):\n>>> tf_cube(2)\n<tf.Tensor: id=18634201, shape=(), dtype=int32, numpy=8>\n>>> tf_cube(tf.constant(2.0))\n<tf.Tensor: id=18634211, shape=(), dtype=float32, numpy=8.0>\nUnder the hood, tf.function() analyzed the computations performed by the cube()\nfunction and generated an equivalent computation graph! As you can see, it was\nrather painless (we will see how this works shortly). Alternatively, we could have used\ntf.function as a decorator; this is actually more common:\n@tf.function\ndef tf_cube(x): return x ** 3\nThe original Python function is still available via the TF Function\u2019s python_function\nattribute, in case you ever need it:\n>>> tf_cube.python_function(2) TensorFlow optimizes the computation graph, pruning unused nodes, simplifying\nexpressions (e.g., 1 + 2 would get replaced with 3), and more. Once the optimized\ngraph is ready, the TF Function efficiently executes the operations in the graph, in the\nappropriate order (and in parallel when it can). As a result, a TF Function will usually\nrun much faster than the original Python function, especially if it performs complex\ncomputations.15 Most of the time you will not really need to know more than that:\nwhen you want to boost a Python function, just transform it into a TF Function. That\u2019s all! | Chapter 12: Custom Models and Training with TensorFlow\nMoreover, when you write a custom loss function, a custom metric, a custom layer, or\nany other custom function and you use it in a Keras model (as we did throughout this\nchapter), Keras automatically converts your function into a TF Function\u2014no need to\nuse tf.function(). So most of the time, all this magic is 100% transparent."
  },
  {
    "id": 268,
    "content": "You can tell Keras not to convert your Python functions to TF\nFunctions by setting dynamic=True when creating a custom layer\nor a custom model. Alternatively, you can set run_eagerly=True\nwhen calling the model\u2019s compile() method. By default, a TF Function generates a new graph for every unique set of input shapes\nand data types and caches it for subsequent calls. For example, if you call\ntf_cube(tf.constant(10)), a graph will be generated for int32 tensors of shape []. Then if you call tf_cube(tf.constant(20)), the same graph will be reused. But if\nyou then call tf_cube(tf.constant([10, 20])), a new graph will be generated for\nint32 tensors of shape [2]. This is how TF Functions handle polymorphism (i.e., vary\u2010\ning argument types and shapes). However, this is only true for tensor arguments: if\nyou pass numerical Python values to a TF Function, a new graph will be generated for\nevery distinct value: for example, calling tf_cube(10) and tf_cube(20) will generate\ntwo graphs. If you call a TF Function many times with different numerical\nPython values, then many graphs will be generated, slowing down\nyour program and using up a lot of RAM (you must delete the TF\nFunction to release it). Python values should be reserved for argu\u2010\nments that will have few unique values, such as hyperparameters\nlike the number of neurons per layer. This allows TensorFlow to\nbetter optimize each variant of your model. AutoGraph and Tracing\nSo how does TensorFlow generate graphs? It starts by analyzing the Python function\u2019s\nsource code to capture all the control flow statements, such as for loops, while loops,\nand if statements, as well as break, continue, and return statements. This first step\nis called AutoGraph. The reason TensorFlow has to analyze the source code is that\nPython does not provide any other way to capture control flow statements: it offers\nmagic methods like __add__() and __mul__() to capture operators like + and *, but\nthere are no __while__() or __if__() magic methods. After analyzing the function\u2019s\ncode, AutoGraph outputs an upgraded version of that function in which all the con\u2010\ntrol flow statements are replaced by the appropriate TensorFlow operations, such as\ntf.while_loop() for loops and tf.cond() for if statements. For example, in\nFigure 12-4, AutoGraph analyzes the source code of the sum_squares() Python\nTensorFlow Functions and Graphs | function, and it generates the tf__sum_squares() function. In this function, the for\nloop is replaced by the definition of the loop_body() function (containing the body\nof the original for loop), followed by a call to the for_stmt() function. This call will\nbuild the appropriate tf.while_loop() operation in the computation graph. Figure 12-4. How TensorFlow generates graphs using AutoGraph and tracing\nNext, TensorFlow calls this \u201cupgraded\u201d function, but instead of passing the argument,\nit passes a symbolic tensor\u2014a tensor without any actual value, only a name, a data\ntype, and a shape. For example, if you call sum_squares(tf.constant(10)), then the\ntf__sum_squares() function will be called with a symbolic tensor of type int32 and\nshape []."
  },
  {
    "id": 269,
    "content": "The function will run in graph mode, meaning that each TensorFlow opera\u2010\ntion will add a node in the graph to represent itself and its output tensor(s) (as\nopposed to the regular mode, called eager execution, or eager mode). In graph mode,\nTF operations do not perform any computations. This should feel familiar if you\nknow TensorFlow 1, as graph mode was the default mode. In Figure 12-4, you can see\nthe tf__sum_squares() function being called with a symbolic tensor as its argument\n(in this case, an int32 tensor of shape []) and the final graph being generated during\ntracing. The nodes represent operations, and the arrows represent tensors (both the\ngenerated function and the graph are simplified). | Chapter 12: Custom Models and Training with TensorFlow\nTo view the generated function\u2019s source code, you can call tf.auto\ngraph.to_code(sum_squares.python_function). The code is not\nmeant to be pretty, but it can sometimes help for debugging. TF Function Rules\nMost of the time, converting a Python function that performs TensorFlow operations\ninto a TF Function is trivial: decorate it with @tf.function or let Keras take care of it\nfor you. However, there are a few rules to respect:\n\u2022 If you call any external library, including NumPy or even the standard library,\nthis call will run only during tracing; it will not be part of the graph. Indeed, a\nTensorFlow graph can only include TensorFlow constructs (tensors, operations,\nvariables, datasets, and so on). So, make sure you use tf.reduce_sum() instead\nof np.sum(), tf.sort() instead of the built-in sorted() function, and so on\n(unless you really want the code to run only during tracing). This has a few addi\u2010\ntional implications:\n\u2014 If you define a TF Function f(x) that just returns np.random.rand(), a ran\u2010\ndom number will only be generated when the function is traced, so f(tf.con\nstant(2.)) and f(tf.constant(3.)) will return the same random number,\nbut f(tf.constant([2., 3.])) will return a different one. If you replace\nnp.random.rand() with tf.random.uniform([]), then a new random num\u2010\nber will be generated upon every call, since the operation will be part of the\ngraph. \u2014 If your non-TensorFlow code has side effects (such as logging something or\nupdating a Python counter), then you should not expect those side effects to\noccur every time you call the TF Function, as they will only occur when the\nfunction is traced. \u2014 You can wrap arbitrary Python code in a tf.py_function() operation, but\ndoing so will hinder performance, as TensorFlow will not be able to do any\ngraph optimization on this code. It will also reduce portability, as the graph\nwill only run on platforms where Python is available (and where the right\nlibraries are installed). \u2022 You can call other Python functions or TF Functions, but they should follow the\nsame rules, as TensorFlow will capture their operations in the computation\ngraph. Note that these other functions do not need to be decorated with\n@tf.function."
  },
  {
    "id": 270,
    "content": "\u2022 If the function creates a TensorFlow variable (or any other stateful TensorFlow\nobject, such as a dataset or a queue), it must do so upon the very first call, and\nonly then, or else you will get an exception. It is usually preferable to create\nTensorFlow Functions and Graphs | variables outside of the TF Function (e.g., in the build() method of a custom\nlayer). If you want to assign a new value to the variable, make sure you call its\nassign() method, instead of using the = operator. \u2022 The source code of your Python function should be available to TensorFlow. If\nthe source code is unavailable (for example, if you define your function in the\nPython shell, which does not give access to the source code, or if you deploy only\nthe compiled *.pyc Python files to production), then the graph generation process\nwill fail or have limited functionality. \u2022 TensorFlow will only capture for loops that iterate over a tensor or a dataset. So\nmake sure you use for i in tf.range(x) rather than for i in range(x), or\nelse the loop will not be captured in the graph. Instead, it will run during tracing. (This may be what you want if the for loop is meant to build the graph, for\nexample to create each layer in a neural network.) \u2022 As always, for performance reasons, you should prefer a vectorized implementa\u2010\ntion whenever you can, rather than using loops. It\u2019s time to sum up! In this chapter we started with a brief overview of TensorFlow,\nthen we looked at TensorFlow\u2019s low-level API, including tensors, operations, vari\u2010\nables, and special data structures. We then used these tools to customize almost every\ncomponent in tf.keras. Finally, we looked at how TF Functions can boost perfor\u2010\nmance, how graphs are generated using AutoGraph and tracing, and what rules to\nfollow when you write TF Functions (if you would like to open the black box a bit\nfurther, for example to explore the generated graphs, you will find technical details in\nAppendix G). In the next chapter, we will look at how to efficiently load and preprocess data with\nTensorFlow. Exercises\n1. How would you describe TensorFlow in a short sentence? What are its main fea\u2010\ntures? Can you name other popular Deep Learning libraries? 2. Is TensorFlow a drop-in replacement for NumPy? What are the main differences\nbetween the two? 3. Do you get the same result with tf.range(10) and tf.constant(np.ara\nnge(10))? 4. Can you name six other data structures available in TensorFlow, beyond regular\ntensors? | Chapter 12: Custom Models and Training with TensorFlow\n5. A custom loss function can be defined by writing a function or by subclassing the\nkeras.losses.Loss class. When would you use each option? 6. Similarly, a custom metric can be defined in a function or a subclass of\nkeras.metrics.Metric. When would you use each option? 7. When should you create a custom layer versus a custom model? 8."
  },
  {
    "id": 271,
    "content": "What are some use cases that require writing your own custom training loop? 9. Can custom Keras components contain arbitrary Python code, or must they be\nconvertible to TF Functions? 10. What are the main rules to respect if you want a function to be convertible to a\nTF Function? 11. When would you need to create a dynamic Keras model? How do you do that? Why not make all your models dynamic? 12. Implement a custom layer that performs Layer Normalization (we will use this\ntype of layer in Chapter 15):\na. The build() method should define two trainable weights \u03b1 and \u03b2, both of\nshape input_shape[-1:] and data type tf.float32. \u03b1 should be initialized\nwith 1s, and \u03b2 with 0s. b. The call() method should compute the mean \u03bc and standard deviation \u03c3 of\neach instance\u2019s features. For this, you can use tf.nn.moments(inputs,\naxes=-1, keepdims=True), which returns the mean \u03bc and the variance \u03c32 of\nall instances (compute the square root of the variance to get the standard\ndeviation). Then the function should compute and return \u03b1\u2297(X - \u03bc)/(\u03c3 + \u03b5) +\n\u03b2, where \u2297 represents itemwise multiplication (*) and \u03b5 is a smoothing term\n(small constant to avoid division by zero, e.g., 0.001). c. Ensure that your custom layer produces the same (or very nearly the same)\noutput as the keras.layers.LayerNormalization layer. 13. Train a model using a custom training loop to tackle the Fashion MNIST dataset\n(see Chapter 10). a. Display the epoch, iteration, mean training loss, and mean accuracy over each\nepoch (updated at each iteration), as well as the validation loss and accuracy at\nthe end of each epoch. b. Try using a different optimizer with a different learning rate for the upper lay\u2010\ners and the lower layers. Solutions to these exercises are available in Appendix A. Exercises | CHAPTER 13\nLoading and Preprocessing Data\nwith TensorFlow\nSo far we have used only datasets that fit in memory, but Deep Learning systems are\noften trained on very large datasets that will not fit in RAM. Ingesting a large dataset\nand preprocessing it efficiently can be tricky to implement with other Deep Learning\nlibraries, but TensorFlow makes it easy thanks to the Data API: you just create a data\u2010\nset object, and tell it where to get the data and how to transform it. TensorFlow takes\ncare of all the implementation details, such as multithreading, queuing, batching, and\nprefetching. Moreover, the Data API works seamlessly with tf.keras! Off the shelf, the Data API can read from text files (such as CSV files), binary files\nwith fixed-size records, and binary files that use TensorFlow\u2019s TFRecord format,\nwhich supports records of varying sizes. TFRecord is a flexible and efficient binary\nformat usually containing protocol buffers (an open source binary format). The Data\nAPI also has support for reading from SQL databases. Moreover, many open source\nextensions are available to read from all sorts of data sources, such as Google\u2019s Big\u2010\nQuery service."
  },
  {
    "id": 272,
    "content": "Reading huge datasets efficiently is not the only difficulty: the data also needs to be\npreprocessed, usually normalized. Moreover, it is not always composed strictly of\nconvenient numerical fields: there may be text features, categorical features, and so\non. These need to be encoded, for example using one-hot encoding, bag-of-words\nencoding, or embeddings (as we will see, an embedding is a trainable dense vector that\nrepresents a category or token). One option to handle all this preprocessing is to\nwrite your own custom preprocessing layers. Another is to use the standard prepro\u2010\ncessing layers provided by Keras. Loading and Preprocessing Data with TensorFlow | In this chapter, we will cover the Data API, the TFRecord format, and how to create\ncustom preprocessing layers and use the standard Keras ones. We will also take a\nquick look at a few related projects from TensorFlow\u2019s ecosystem:\nTF Transform (tf.Transform)\nMakes it possible to write a single preprocessing function that can be run in\nbatch mode on your full training set, before training (to speed it up), and then\nexported to a TF Function and incorporated into your trained model so that once\nit is deployed in production it can take care of preprocessing new instances on\nthe fly. TF Datasets (TFDS)\nProvides a convenient function to download many common datasets of all kinds,\nincluding large ones like ImageNet, as well as convenient dataset objects to\nmanipulate them using the Data API. So let\u2019s get started! The Data API\nThe whole Data API revolves around the concept of a dataset: as you might suspect,\nthis represents a sequence of data items. Usually you will use datasets that gradually\nread data from disk, but for simplicity let\u2019s create a dataset entirely in RAM using\ntf.data.Dataset.from_tensor_slices():\n>>> X = tf.range(10) # any data tensor\n>>> dataset = tf.data.Dataset.from_tensor_slices(X)\n>>> dataset\n<TensorSliceDataset shapes: (), types: tf.int32>\nThe from_tensor_slices() function takes a tensor and creates a tf.data.Dataset\nwhose elements are all the slices of X (along the first dimension), so this dataset con\u2010\ntains 10 items: tensors 0, 1, 2, \u2026, 9. In this case we would have obtained the same\ndataset if we had used tf.data.Dataset.range(10). You can simply iterate over a dataset\u2019s items like this:\n>>> for item in dataset:\n... print(item)\n...\ntf.Tensor(0, shape=(), dtype=int32)\ntf.Tensor(1, shape=(), dtype=int32)\ntf.Tensor(2, shape=(), dtype=int32)\n[...]\ntf.Tensor(9, shape=(), dtype=int32) | Chapter 13: Loading and Preprocessing Data with TensorFlow\nChaining Transformations\nOnce you have a dataset, you can apply all sorts of transformations to it by calling its\ntransformation methods. Each method returns a new dataset, so you can chain trans\u2010\nformations like this (this chain is illustrated in Figure 13-1):\n>>> dataset = dataset.repeat(3).batch(7)\n>>> for item in dataset:\n... print(item)\n...\ntf.Tensor([0 1 2 3 4 5 6], shape=(7,), dtype=int32)\ntf.Tensor([7 8 9 0 1 2 3], shape=(7,), dtype=int32)\ntf.Tensor([4 5 6 7 8 9 0], shape=(7,), dtype=int32)\ntf.Tensor([1 2 3 4 5 6 7], shape=(7,), dtype=int32)\ntf.Tensor([8 9], shape=(2,), dtype=int32)\nFigure 13-1."
  },
  {
    "id": 273,
    "content": "Chaining dataset transformations\nIn this example, we first call the repeat() method on the original dataset, and it\nreturns a new dataset that will repeat the items of the original dataset three times. Of\ncourse, this will not copy all the data in memory three times! (If you call this method\nwith no arguments, the new dataset will repeat the source dataset forever, so the code\nthat iterates over the dataset will have to decide when to stop.) Then we call the\nbatch() method on this new dataset, and again this creates a new dataset. This one\nwill group the items of the previous dataset in batches of seven items. Finally, we iter\u2010\nate over the items of this final dataset. As you can see, the batch() method had to\noutput a final batch of size two instead of seven, but you can call it with drop_remain\nder=True if you want it to drop this final batch so that all batches have the exact same\nsize. The Data API | The dataset methods do not modify datasets, they create new ones,\nso make sure to keep a reference to these new datasets (e.g., with\ndataset = ...), or else nothing will happen. You can also transform the items by calling the map() method. For example, this cre\u2010\nates a new dataset with all items doubled:\n>>> dataset = dataset.map(lambda x: x * 2) # Items: [0,2,4,6,8,10,12]\nThis function is the one you will call to apply any preprocessing you want to your\ndata. Sometimes this will include computations that can be quite intensive, such as\nreshaping or rotating an image, so you will usually want to spawn multiple threads to\nspeed things up: it\u2019s as simple as setting the num_parallel_calls argument. Note that\nthe function you pass to the map() method must be convertible to a TF Function (see\nChapter 12). While the map() method applies a transformation to each item, the apply() method\napplies a transformation to the dataset as a whole. For example, the following code\napplies the unbatch() function to the dataset (this function is currently experimental,\nbut it will most likely move to the core API in a future release). Each item in the new\ndataset will be a single-integer tensor instead of a batch of seven integers:\n>>> dataset = dataset.apply(tf.data.experimental.unbatch()) # Items: 0,2,4,... It is also possible to simply filter the dataset using the filter() method:\n>>> dataset = dataset.filter(lambda x: x < 10) # Items: 0 2 4 6 8 0 2 4 6...\nYou will often want to look at just a few items from a dataset. You can use the take()\nmethod for that:\n>>> for item in dataset.take(3):\n... print(item)\n...\ntf.Tensor(0, shape=(), dtype=int64)\ntf.Tensor(2, shape=(), dtype=int64)\ntf.Tensor(4, shape=(), dtype=int64)\nShuffling the Data\nAs you know, Gradient Descent works best when the instances in the training set are\nindependent and identically distributed (see Chapter 4). A simple way to ensure this\nis to shuffle the instances, using the shuffle() method."
  },
  {
    "id": 274,
    "content": "It will create a new dataset\nthat will start by filling up a buffer with the first items of the source dataset. Then,\nwhenever it is asked for an item, it will pull one out randomly from the buffer and\nreplace it with a fresh one from the source dataset, until it has iterated entirely\nthrough the source dataset. At this point it continues to pull out items randomly from | Chapter 13: Loading and Preprocessing Data with TensorFlow\n1 Imagine a sorted deck of cards on your left: suppose you just take the top three cards and shuffle them, then\npick one randomly and put it to your right, keeping the other two in your hands. Take another card on your\nleft, shuffle the three cards in your hands and pick one of them randomly, and put it on your right. When you\nare done going through all the cards like this, you will have a deck of cards on your right: do you think it will\nbe perfectly shuffled? the buffer until it is empty. You must specify the buffer size, and it is important to\nmake it large enough, or else shuffling will not be very effective.1 Just don\u2019t exceed the\namount of RAM you have, and even if you have plenty of it, there\u2019s no need to go\nbeyond the dataset\u2019s size. You can provide a random seed if you want the same ran\u2010\ndom order every time you run your program. For example, the following code creates\nand displays a dataset containing the integers 0 to 9, repeated 3 times, shuffled using a\nbuffer of size 5 and a random seed of 42, and batched with a batch size of 7:\n>>> dataset = tf.data.Dataset.range(10).repeat(3) # 0 to 9, three times\n>>> dataset = dataset.shuffle(buffer_size=5, seed=42).batch(7)\n>>> for item in dataset:\n... print(item)\n...\ntf.Tensor([0 2 3 6 7 9 4], shape=(7,), dtype=int64)\ntf.Tensor([5 0 1 1 8 6 5], shape=(7,), dtype=int64)\ntf.Tensor([4 8 7 1 2 3 0], shape=(7,), dtype=int64)\ntf.Tensor([5 4 2 7 8 9 9], shape=(7,), dtype=int64)\ntf.Tensor([3 6], shape=(2,), dtype=int64)\nIf you call repeat() on a shuffled dataset, by default it will generate\na new order at every iteration. This is generally a good idea, but if\nyou prefer to reuse the same order at each iteration (e.g., for tests\nor debugging), you can set reshuffle_each_iteration=False. For a large dataset that does not fit in memory, this simple shuffling-buffer approach\nmay not be sufficient, since the buffer will be small compared to the dataset. One sol\u2010\nution is to shuffle the source data itself (for example, on Linux you can shuffle text\nfiles using the shuf command). This will definitely improve shuffling a lot!"
  },
  {
    "id": 275,
    "content": "Even if\nthe source data is shuffled, you will usually want to shuffle it some more, or else the\nsame order will be repeated at each epoch, and the model may end up being biased\n(e.g., due to some spurious patterns present by chance in the source data\u2019s order). To\nshuffle the instances some more, a common approach is to split the source data into\nmultiple files, then read them in a random order during training. However, instances\nlocated in the same file will still end up close to each other. To avoid this you can pick\nmultiple files randomly and read them simultaneously, interleaving their records. Then on top of that you can add a shuffling buffer using the shuffle() method. If all\nThe Data API | this sounds like a lot of work, don\u2019t worry: the Data API makes all this possible in just\na few lines of code. Let\u2019s see how to do this. Interleaving lines from multiple files\nFirst, let\u2019s suppose that you\u2019ve loaded the California housing dataset, shuffled it\n(unless it was already shuffled), and split it into a training set, a validation set, and a\ntest set. Then you split each set into many CSV files that each look like this (each row\ncontains eight input features plus the target median house value):\nMedInc,HouseAge,AveRooms,AveBedrms,Popul,AveOccup,Lat,Long,MedianHouseValue\n3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442\n5.3275,5.0,6.4900,0.9910,3464.0,3.4433,33.69,-117.39,1.687\n3.1,29.0,7.5423,1.5915,1328.0,2.2508,38.44,-122.98,1.621\n[...]\nLet\u2019s also suppose train_filepaths contains the list of training file paths (and you\nalso have valid_filepaths and test_filepaths):\n>>> train_filepaths\n['datasets/housing/my_train_00.csv', 'datasets/housing/my_train_01.csv',...]\nAlternatively, you could use file patterns; for example, train_filepaths = \"data\nsets/housing/my_train_*.csv\". Now let\u2019s create a dataset containing only these file\npaths:\nfilepath_dataset = tf.data.Dataset.list_files(train_filepaths, seed=42)\nBy default, the list_files() function returns a dataset that shuffles the file paths. In\ngeneral this is a good thing, but you can set shuffle=False if you do not want that\nfor some reason. Next, you can call the interleave() method to read from five files at a time and\ninterleave their lines (skipping the first line of each file, which is the header row,\nusing the skip() method):\nn_readers = 5\ndataset = filepath_dataset.interleave( lambda filepath: tf.data.TextLineDataset(filepath).skip(1), cycle_length=n_readers)\nThe interleave() method will create a dataset that will pull five file paths from the\nfilepath_dataset, and for each one it will call the function you gave it (a lambda in\nthis example) to create a new dataset (in this case a TextLineDataset). To be clear, at\nthis stage there will be seven datasets in all: the filepath dataset, the interleave dataset,\nand the five TextLineDatasets created internally by the interleave dataset. When we\niterate over the interleave dataset, it will cycle through these five TextLineDatasets,\nreading one line at a time from each until all datasets are out of items. Then it will get | Chapter 13: Loading and Preprocessing Data with TensorFlow\nthe next five file paths from the filepath_dataset and interleave them the same way,\nand so on until it runs out of file paths."
  },
  {
    "id": 276,
    "content": "For interleaving to work best, it is preferable to have files of identi\u2010\ncal length; otherwise the ends of the longest files will not be inter\u2010\nleaved. By default, interleave() does not use parallelism; it just reads one line at a time\nfrom each file, sequentially. If you want it to actually read files in parallel, you can set\nthe num_parallel_calls argument to the number of threads you want (note that the\nmap() method also has this argument). You can even set it to tf.data.experimen\ntal.AUTOTUNE to make TensorFlow choose the right number of threads dynamically\nbased on the available CPU (however, this is an experimental feature for now). Let\u2019s\nlook at what the dataset contains now:\n>>> for line in dataset.take(5):\n... print(line.numpy())\n...\nb'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782'\nb'4.1812,52.0,5.7013,0.9965,692.0,2.4027,33.73,-118.31,3.215'\nb'3.6875,44.0,4.5244,0.9930,457.0,3.1958,34.04,-118.15,1.625'\nb'3.3456,37.0,4.5140,0.9084,458.0,3.2253,36.67,-121.7,2.526'\nb'3.5214,15.0,3.0499,1.1065,1447.0,1.6059,37.63,-122.43,1.442'\nThese are the first rows (ignoring the header row) of five CSV files, chosen randomly. Looks good! But as you can see, these are just byte strings; we need to parse them and\nscale the data. Preprocessing the Data\nLet\u2019s implement a small function that will perform this preprocessing:\nX_mean, X_std = [...] # mean and scale of each feature in the training set\nn_inputs = 8\ndef preprocess(line): defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)] fields = tf.io.decode_csv(line, record_defaults=defs) x = tf.stack(fields[:-1]) y = tf.stack(fields[-1:]) return (x - X_mean) / X_std, y\nLet\u2019s walk through this code:\nThe Data API | \u2022 First, the code assumes that we have precomputed the mean and standard devia\u2010\ntion of each feature in the training set. X_mean and X_std are just 1D tensors (or\nNumPy arrays) containing eight floats, one per input feature. \u2022 The preprocess() function takes one CSV line and starts by parsing it. For this\nit uses the tf.io.decode_csv() function, which takes two arguments: the first is\nthe line to parse, and the second is an array containing the default value for each\ncolumn in the CSV file. This array tells TensorFlow not only the default value for\neach column, but also the number of columns and their types. In this example,\nwe tell it that all feature columns are floats and that missing values should default\nto 0, but we provide an empty array of type tf.float32 as the default value for\nthe last column (the target): the array tells TensorFlow that this column contains\nfloats, but that there is no default value, so it will raise an exception if it encoun\u2010\nters a missing value. \u2022 The decode_csv() function returns a list of scalar tensors (one per column), but\nwe need to return 1D tensor arrays. So we call tf.stack() on all tensors except\nfor the last one (the target): this will stack these tensors into a 1D array. We then\ndo the same for the target value (this makes it a 1D tensor array with a single\nvalue, rather than a scalar tensor)."
  },
  {
    "id": 277,
    "content": "\u2022 Finally, we scale the input features by subtracting the feature means and then\ndividing by the feature standard deviations, and we return a tuple containing the\nscaled features and the target. Let\u2019s test this preprocessing function:\n>>> preprocess(b'4.2083,44.0,5.3232,0.9171,846.0,2.3370,37.47,-122.2,2.782')\n(<tf.Tensor: id=6227, shape=(8,), dtype=float32, numpy= array([ 0.16579159, 1.216324 , -0.05204564, -0.39215982, -0.5277444 , -0.2633488 , 0.8543046 , -1.3072058 ], dtype=float32)>, <tf.Tensor: [...], numpy=array([2.782], dtype=float32)>)\nLooks good! We can now apply the function to the dataset. Putting Everything Together\nTo make the code reusable, let\u2019s put together everything we have discussed so far into\na small helper function: it will create and return a dataset that will efficiently load Cal\u2010\nifornia housing data from multiple CSV files, preprocess it, shuffle it, optionally\nrepeat it, and batch it (see Figure 13-2):\ndef csv_reader_dataset(filepaths, repeat=1, n_readers=5, n_read_threads=None, shuffle_buffer_size=10000, n_parse_threads=5, batch_size=32): dataset = tf.data.Dataset.list_files(filepaths) dataset = dataset.interleave( lambda filepath: tf.data.TextLineDataset(filepath).skip(1), | Chapter 13: Loading and Preprocessing Data with TensorFlow\n2 In general, just prefetching one batch is fine, but in some cases you may need to prefetch a few more. Alterna\u2010\ntively, you can let TensorFlow decide automatically by passing tf.data.experimental.AUTOTUNE (this is an\nexperimental feature for now). cycle_length=n_readers, num_parallel_calls=n_read_threads) dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads) dataset = dataset.shuffle(shuffle_buffer_size).repeat(repeat) return dataset.batch(batch_size).prefetch(1)\nEverything should make sense in this code, except the very last line (prefetch(1)),\nwhich is important for performance. Figure 13-2. Loading and preprocessing data from multiple CSV files\nPrefetching\nBy calling prefetch(1) at the end, we are creating a dataset that will do its best to\nalways be one batch ahead.2 In other words, while our training algorithm is working\non one batch, the dataset will already be working in parallel on getting the next batch\nready (e.g., reading the data from disk and preprocessing it). This can improve per\u2010\nformance dramatically, as is illustrated in Figure 13-3. If we also ensure that loading\nand preprocessing are multithreaded (by setting num_parallel_calls when calling\ninterleave() and map()), we can exploit multiple cores on the CPU and hopefully\nmake preparing one batch of data shorter than running a training step on the GPU:\nThe Data API | 3 But check out the tf.data.experimental.prefetch_to_device() function, which can prefetch data directly\nto the GPU. this way the GPU will be almost 100% utilized (except for the data transfer time from\nthe CPU to the GPU3), and training will run much faster. Figure 13-3. With prefetching, the CPU and the GPU work in parallel: as the GPU works\non one batch, the CPU works on the next\nIf you plan to purchase a GPU card, its processing power and its\nmemory size are of course very important (in particular, a large\namount of RAM is crucial for computer vision). Just as important\nto get good performance is its memory bandwidth; this is the num\u2010\nber of gigabytes of data it can get into or out of its RAM per\nsecond."
  },
  {
    "id": 278,
    "content": "If the dataset is small enough to fit in memory, you can significantly speed up train\u2010\ning by using the dataset\u2019s cache() method to cache its content to RAM. You should\ngenerally do this after loading and preprocessing the data, but before shuffling,\nrepeating, batching, and prefetching. This way, each instance will only be read and | Chapter 13: Loading and Preprocessing Data with TensorFlow\n4 Support for datasets is specific to tf.keras; this will not work in other implementations of the Keras API. 5 The fit() method will take care of repeating the training dataset. Alternatively, you could call repeat() on\nthe training dataset so that it repeats forever and specify the steps_per_epoch argument when calling the\nfit() method. This may be useful in some rare cases, for example if you want to use a shuffle buffer that\ncrosses over epochs. preprocessed once (instead of once per epoch), but the data will still be shuffled dif\u2010\nferently at each epoch, and the next batch will still be prepared in advance. You now know how to build efficient input pipelines to load and preprocess data\nfrom multiple text files. We have discussed the most common dataset methods, but\nthere are a few more you may want to look at: concatenate(), zip(), window(),\nreduce(), shard(), flat_map(), and padded_batch(). There are also a couple more\nclass methods: from_generator() and from_tensors(), which create a new dataset\nfrom a Python generator or a list of tensors, respectively. Please check the API docu\u2010\nmentation for more details. Also note that there are experimental features available in\ntf.data.experimental, many of which will likely make it to the core API in future\nreleases (e.g., check out the CsvDataset class, as well as the make_csv_dataset()\nmethod, which takes care of inferring the type of each column). Using the Dataset with tf.keras\nNow we can use the csv_reader_dataset() function to create a dataset for the train\u2010\ning set. Note that we do not need to repeat it, as this will be taken care of by tf.keras. We also create datasets for the validation set and the test set:\ntrain_set = csv_reader_dataset(train_filepaths)\nvalid_set = csv_reader_dataset(valid_filepaths)\ntest_set = csv_reader_dataset(test_filepaths)\nAnd now we can simply build and train a Keras model using these datasets.4 All we\nneed to do is pass the training and validation datasets to the fit() method, instead of\nX_train, y_train, X_valid, and y_valid:5\nmodel = keras.models.Sequential([...])\nmodel.compile([...])\nmodel.fit(train_set, epochs=10, validation_data=valid_set)\nSimilarly, we can pass a dataset to the evaluate() and predict() methods:\nmodel.evaluate(test_set)\nnew_set = test_set.take(3).map(lambda X, y: X) # pretend we have 3 new instances\nmodel.predict(new_set) # a dataset containing new instances\nUnlike the other sets, the new_set will usually not contain labels (if it does, Keras will\nignore them). Note that in all these cases, you can still use NumPy arrays instead of\nThe Data API | datasets if you want (but of course they need to have been loaded and preprocessed\nfirst)."
  },
  {
    "id": 279,
    "content": "If you want to build your own custom training loop (as in Chapter 12), you can just\niterate over the training set, very naturally:\nfor X_batch, y_batch in train_set: [...] # perform one Gradient Descent step\nIn fact, it is even possible to create a TF Function (see Chapter 12) that performs the\nwhole training loop:\n@tf.function\ndef train(model, optimizer, loss_fn, n_epochs, [...]): train_set = csv_reader_dataset(train_filepaths, repeat=n_epochs, [...]) for X_batch, y_batch in train_set: with tf.GradientTape() as tape: y_pred = model(X_batch) main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred)) loss = tf.add_n([main_loss] + model.losses) grads = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(grads, model.trainable_variables))\nCongratulations, you now know how to build powerful input pipelines using the Data\nAPI! However, so far we have used CSV files, which are common, simple, and conve\u2010\nnient but not really efficient, and do not support large or complex data structures\n(such as images or audio) very well. So let\u2019s see how to use TFRecords instead. If you are happy with CSV files (or whatever other format you are\nusing), you do not have to use TFRecords. As the saying goes, if it\nain\u2019t broke, don\u2019t fix it! TFRecords are useful when the bottleneck\nduring training is loading and parsing the data. The TFRecord Format\nThe TFRecord format is TensorFlow\u2019s preferred format for storing large amounts of\ndata and reading it efficiently. It is a very simple binary format that just contains a\nsequence of binary records of varying sizes (each record is comprised of a length, a\nCRC checksum to check that the length was not corrupted, then the actual data, and\nfinally a CRC checksum for the data). You can easily create a TFRecord file using the\ntf.io.TFRecordWriter class:\nwith tf.io.TFRecordWriter(\"my_data.tfrecord\") as f: f.write(b\"This is the first record\") f.write(b\"And this is the second record\") | Chapter 13: Loading and Preprocessing Data with TensorFlow\nAnd you can then use a tf.data.TFRecordDataset to read one or more TFRecord\nfiles:\nfilepaths = [\"my_data.tfrecord\"]\ndataset = tf.data.TFRecordDataset(filepaths)\nfor item in dataset: print(item)\nThis will output:\ntf.Tensor(b'This is the first record', shape=(), dtype=string)\ntf.Tensor(b'And this is the second record', shape=(), dtype=string)\nBy default, a TFRecordDataset will read files one by one, but you\ncan make it read multiple files in parallel and interleave their\nrecords by setting num_parallel_reads. Alternatively, you could\nobtain the same result by using list_files() and interleave()\nas we did earlier to read multiple CSV files. Compressed TFRecord Files\nIt can sometimes be useful to compress your TFRecord files, especially if they need to\nbe loaded via a network connection. You can create a compressed TFRecord file by\nsetting the options argument:\noptions = tf.io.TFRecordOptions(compression_type=\"GZIP\")\nwith tf.io.TFRecordWriter(\"my_compressed.tfrecord\", options) as f: [...]\nWhen reading a compressed TFRecord file, you need to specify the compression type:\ndataset = tf.data.TFRecordDataset([\"my_compressed.tfrecord\"], compression_type=\"GZIP\")\nA Brief Introduction to Protocol Buffers\nEven though each record can use any binary format you want, TFRecord files usually\ncontain serialized protocol buffers (also called protobufs)."
  },
  {
    "id": 280,
    "content": "This is a portable, extensi\u2010\nble, and efficient binary format developed at Google back in 2001 and made open\nsource in 2008; protobufs are now widely used, in particular in gRPC, Google\u2019s\nremote procedure call system. They are defined using a simple language that looks\nlike this:\nsyntax = \"proto3\";\nmessage Person { string name = 1; int32 id = 2; repeated string email = 3;\n}\nThe TFRecord Format | 6 Since protobuf objects are meant to be serialized and transmitted, they are called messages. 7 This chapter contains the bare minimum you need to know about protobufs to use TFRecords. To learn more\nabout protobufs, please visit \nThis definition says we are using version 3 of the protobuf format, and it specifies\nthat each Person object6 may (optionally) have a name of type string, an id of type\nint32, and zero or more email fields, each of type string. The numbers 1, 2, and 3\nare the field identifiers: they will be used in each record\u2019s binary representation. Once\nyou have a definition in a .proto file, you can compile it. This requires protoc, the\nprotobuf compiler, to generate access classes in Python (or some other language). Note that the protobuf definitions we will use have already been compiled for you,\nand their Python classes are part of TensorFlow, so you will not need to use protoc. All you need to know is how to use protobuf access classes in Python. To illustrate the\nbasics, let\u2019s look at a simple example that uses the access classes generated for the\nPerson protobuf (the code is explained in the comments):\n>>> from person_pb2 import Person # import the generated access class\n>>> person = Person(name=\"Al\", id=123, email=[\"a@b.com\"]) # create a Person\n>>> print(person) # display the Person\nname: \"Al\"\nid: 123\nemail: \"a@b.com\"\n>>> person.name # read a field\n\"Al\"\n>>> person.name = \"Alice\" # modify a field\n>>> person.email[0] # repeated fields can be accessed like arrays\n\"a@b.com\"\n>>> person.email.append(\"c@d.com\") # add an email address\n>>> s = person.SerializeToString() # serialize the object to a byte string\n>>> s\nb'\\n\\x05Alice\\x10{\\x1a\\x07a@b.com\\x1a\\x07c@d.com'\n>>> person2 = Person() # create a new Person\n>>> person2.ParseFromString(s) # parse the byte string (27 bytes long) >>> person == person2 # now they are equal\nTrue\nIn short, we import the Person class generated by protoc, we create an instance and\nplay with it, visualizing it and reading and writing some fields, then we serialize it\nusing the SerializeToString() method. This is the binary data that is ready to be\nsaved or transmitted over the network. When reading or receiving this binary data,\nwe can parse it using the ParseFromString() method, and we get a copy of the object\nthat was serialized.7\nWe could save the serialized Person object to a TFRecord file, then we could load and\nparse it: everything would work fine."
  },
  {
    "id": 281,
    "content": "However, SerializeToString() and ParseFrom | Chapter 13: Loading and Preprocessing Data with TensorFlow\n8 Why was Example even defined, since it contains no more than a Features object? Well, TensorFlow\u2019s devel\u2010\nopers may one day decide to add more fields to it. As long as the new Example definition still contains the\nfeatures field, with the same ID, it will be backward compatible. This extensibility is one of the great features\nof protobufs. String() are not TensorFlow operations (and neither are the other operations in this\ncode), so they cannot be included in a TensorFlow Function (except by wrapping\nthem in a tf.py_function() operation, which would make the code slower and less\nportable, as we saw in Chapter 12). Fortunately, TensorFlow does include special pro\u2010\ntobuf definitions for which it provides parsing operations. TensorFlow Protobufs\nThe main protobuf typically used in a TFRecord file is the Example protobuf, which\nrepresents one instance in a dataset. It contains a list of named features, where each\nfeature can either be a list of byte strings, a list of floats, or a list of integers. Here is\nthe protobuf definition:\nsyntax = \"proto3\";\nmessage BytesList { repeated bytes value = 1; }\nmessage FloatList { repeated float value = 1 [packed = true]; }\nmessage Int64List { repeated int64 value = 1 [packed = true]; }\nmessage Feature { oneof kind { BytesList bytes_list = 1; FloatList float_list = 2; Int64List int64_list = 3; }\n};\nmessage Features { map<string, Feature> feature = 1; };\nmessage Example { Features features = 1; };\nThe definitions of BytesList, FloatList, and Int64List are straightforward\nenough. Note that [packed = true] is used for repeated numerical fields, for a more\nefficient encoding. A Feature contains either a BytesList, a FloatList, or an\nInt64List. A Features (with an s) contains a dictionary that maps a feature name to\nthe corresponding feature value. And finally, an Example contains only a Features\nobject.8 Here is how you could create a tf.train.Example representing the same per\u2010\nson as earlier and write it to a TFRecord file:\nfrom tensorflow.train import BytesList, FloatList, Int64List\nfrom tensorflow.train import Feature, Features, Example\nperson_example = Example( features=Features( feature={ \"name\": Feature(bytes_list=BytesList(value=[b\"Alice\"])),\nThe TFRecord Format | \"id\": Feature(int64_list=Int64List(value=[123])), \"emails\": Feature(bytes_list=BytesList(value=[b\"a@b.com\", b\"c@d.com\"])) }))\nThe code is a bit verbose and repetitive, but it\u2019s rather straightforward (and you could\neasily wrap it inside a small helper function). Now that we have an Example protobuf,\nwe can serialize it by calling its SerializeToString() method, then write the result\u2010\ning data to a TFRecord file:\nwith tf.io.TFRecordWriter(\"my_contacts.tfrecord\") as f: f.write(person_example.SerializeToString())\nNormally you would write much more than one Example! Typically, you would create\na conversion script that reads from your current format (say, CSV files), creates an\nExample protobuf for each instance, serializes them, and saves them to several TFRe\u2010\ncord files, ideally shuffling them in the process. This requires a bit of work, so once\nagain make sure it is really necessary (perhaps your pipeline works fine with CSV\nfiles)."
  },
  {
    "id": 282,
    "content": "Now that we have a nice TFRecord file containing a serialized Example, let\u2019s try to\nload it. Loading and Parsing Examples\nTo load the serialized Example protobufs, we will use a tf.data.TFRecordDataset\nonce again, and we will parse each Example using tf.io.parse_single_example(). This is a TensorFlow operation, so it can be included in a TF Function. It requires at\nleast two arguments: a string scalar tensor containing the serialized data, and a\ndescription of each feature. The description is a dictionary that maps each feature\nname to either a tf.io.FixedLenFeature descriptor indicating the feature\u2019s shape,\ntype, and default value, or a tf.io.VarLenFeature descriptor indicating only the type\n(if the length of the feature\u2019s list may vary, such as for the \"emails\" feature). The following code defines a description dictionary, then it iterates over the TFRecord\nDataset and parses the serialized Example protobuf this dataset contains:\nfeature_description = { \"name\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"), \"id\": tf.io.FixedLenFeature([], tf.int64, default_value=0), \"emails\": tf.io.VarLenFeature(tf.string),\n}\nfor serialized_example in tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]): parsed_example = tf.io.parse_single_example(serialized_example, feature_description) | Chapter 13: Loading and Preprocessing Data with TensorFlow\nThe fixed-length features are parsed as regular tensors, but the variable-length fea\u2010\ntures are parsed as sparse tensors. You can convert a sparse tensor to a dense tensor\nusing tf.sparse.to_dense(), but in this case it is simpler to just access its values:\n>>> tf.sparse.to_dense(parsed_example[\"emails\"], default_value=b\"\")\n<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>\n>>> parsed_example[\"emails\"].values\n<tf.Tensor: [...] dtype=string, numpy=array([b'a@b.com', b'c@d.com'], [...])>\nA BytesList can contain any binary data you want, including any serialized object. For example, you can use tf.io.encode_jpeg() to encode an image using the JPEG\nformat and put this binary data in a BytesList. Later, when your code reads the\nTFRecord, it will start by parsing the Example, then it will need to call\ntf.io.decode_jpeg() to parse the data and get the original image (or you can use\ntf.io.decode_image(), which can decode any BMP, GIF, JPEG, or PNG image). You\ncan also store any tensor you want in a BytesList by serializing the tensor using\ntf.io.serialize_tensor() then putting the resulting byte string in a BytesList\nfeature. Later, when you parse the TFRecord, you can parse this data using\ntf.io.parse_tensor(). Instead of parsing examples one by one using tf.io.parse_single_example(), you\nmay want to parse them batch by batch using tf.io.parse_example():\ndataset = tf.data.TFRecordDataset([\"my_contacts.tfrecord\"]).batch(10)\nfor serialized_examples in dataset: parsed_examples = tf.io.parse_example(serialized_examples, feature_description)\nAs you can see, the Example protobuf will probably be sufficient for most use cases. However, it may be a bit cumbersome to use when you are dealing with lists of lists. For example, suppose you want to classify text documents. Each document may be\nrepresented as a list of sentences, where each sentence is represented as a list of\nwords. And perhaps each document also has a list of comments, where each com\u2010\nment is represented as a list of words. There may be some contextual data too, such as\nthe document\u2019s author, title, and publication date. TensorFlow\u2019s SequenceExample\nprotobuf is designed for such use cases."
  },
  {
    "id": 283,
    "content": "Handling Lists of Lists Using the SequenceExample Protobuf\nHere is the definition of the SequenceExample protobuf:\nmessage FeatureList { repeated Feature feature = 1; };\nmessage FeatureLists { map<string, FeatureList> feature_list = 1; };\nmessage SequenceExample { Features context = 1; FeatureLists feature_lists = 2;\n};\nThe TFRecord Format | A SequenceExample contains a Features object for the contextual data and a Fea\ntureLists object that contains one or more named FeatureList objects (e.g., a Fea\ntureList named \"content\" and another named \"comments\"). Each FeatureList\ncontains a list of Feature objects, each of which may be a list of byte strings, a list of\n64-bit integers, or a list of floats (in this example, each Feature would represent a\nsentence or a comment, perhaps in the form of a list of word identifiers). Building a\nSequenceExample, serializing it, and parsing it is similar to building, serializing, and\nparsing an Example, but you must use tf.io.parse_single_sequence_example() to\nparse a single SequenceExample or tf.io.parse_sequence_example() to parse a\nbatch. Both functions return a tuple containing the context features (as a dictionary)\nand the feature lists (also as a dictionary). If the feature lists contain sequences of\nvarying sizes (as in the preceding example), you may want to convert them to ragged\ntensors, using tf.RaggedTensor.from_sparse() (see the notebook for the full code):\nparsed_context, parsed_feature_lists = tf.io.parse_single_sequence_example( serialized_sequence_example, context_feature_descriptions, sequence_feature_descriptions)\nparsed_content = tf.RaggedTensor.from_sparse(parsed_feature_lists[\"content\"])\nNow that you know how to efficiently store, load, and parse data, the next step is to\nprepare it so that it can be fed to a neural network. Preprocessing the Input Features\nPreparing your data for a neural network requires converting all features into numer\u2010\nical features, generally normalizing them, and more. In particular, if your data con\u2010\ntains categorical features or text features, they need to be converted to numbers. This\ncan be done ahead of time when preparing your data files, using any tool you like\n(e.g., NumPy, pandas, or Scikit-Learn). Alternatively, you can preprocess your data on\nthe fly when loading it with the Data API (e.g., using the dataset\u2019s map() method, as\nwe saw earlier), or you can include a preprocessing layer directly in your model. Let\u2019s\nlook at this last option now. For example, here is how you can implement a standardization layer using a Lambda\nlayer. For each feature, it subtracts the mean and divides by its standard deviation\n(plus a tiny smoothing term to avoid division by zero):\nmeans = np.mean(X_train, axis=0, keepdims=True)\nstds = np.std(X_train, axis=0, keepdims=True)\neps = keras.backend.epsilon()\nmodel = keras.models.Sequential([ keras.layers.Lambda(lambda inputs: (inputs - means) / (stds + eps)), [...] # other layers\n]) | Chapter 13: Loading and Preprocessing Data with TensorFlow\nThat\u2019s not too hard!"
  },
  {
    "id": 284,
    "content": "However, you may prefer to use a nice self-contained custom\nlayer (much like Scikit-Learn\u2019s StandardScaler), rather than having global variables\nlike means and stds dangling around:\nclass Standardization(keras.layers.Layer): def adapt(self, data_sample): self.means_ = np.mean(data_sample, axis=0, keepdims=True) self.stds_ = np.std(data_sample, axis=0, keepdims=True) def call(self, inputs): return (inputs - self.means_) / (self.stds_ + keras.backend.epsilon())\nBefore you can use this standardization layer, you will need to adapt it to your dataset\nby calling the adapt() method and passing it a data sample. This will allow it to use\nthe appropriate mean and standard deviation for each feature:\nstd_layer = Standardization()\nstd_layer.adapt(data_sample)\nThis sample must be large enough to be representative of your dataset, but it does not\nhave to be the full training set: in general, a few hundred randomly selected instances\nwill suffice (however, this depends on your task). Next, you can use this preprocess\u2010\ning layer like a normal layer:\nmodel = keras.Sequential()\nmodel.add(std_layer)\n[...] # create the rest of the model\nmodel.compile([...])\nmodel.fit([...])\nIf you are thinking that Keras should contain a standardization layer like this one,\nhere\u2019s some good news for you: by the time you read this, the keras.layers.Normal\nization layer will probably be available. It will work very much like our custom\nStandardization layer: first, create the layer, then adapt it to your dataset by passing\na data sample to the adapt() method, and finally use the layer normally. Now let\u2019s look at categorical features. We will start by encoding them as one-hot\nvectors. Encoding Categorical Features Using One-Hot Vectors\nConsider the ocean_proximity feature in the California housing dataset we explored\nin Chapter 2: it is a categorical feature with five possible values: \"<1H OCEAN\",\n\"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", and \"ISLAND\". We need to encode this feature\nbefore we feed it to a neural network. Since there are very few categories, we can use\none-hot encoding. For this, we first need to map each category to its index (0 to 4),\nwhich can be done using a lookup table:\nvocab = [\"<1H OCEAN\", \"INLAND\", \"NEAR OCEAN\", \"NEAR BAY\", \"ISLAND\"]\nindices = tf.range(len(vocab), dtype=tf.int64)\nPreprocessing the Input Features | table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\nnum_oov_buckets = 2\ntable = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)\nLet\u2019s go through this code:\n\u2022 We first define the vocabulary: this is the list of all possible categories. \u2022 Then we create a tensor with the corresponding indices (0 to 4). \u2022 Next, we create an initializer for the lookup table, passing it the list of categories\nand their corresponding indices. In this example, we already have this data, so we\nuse a KeyValueTensorInitializer; but if the categories were listed in a text file\n(with one category per line), we would use a TextFileInitializer instead. \u2022 In the last two lines we create the lookup table, giving it the initializer and speci\u2010\nfying the number of out-of-vocabulary (oov) buckets."
  },
  {
    "id": 285,
    "content": "If we look up a category\nthat does not exist in the vocabulary, the lookup table will compute a hash of this\ncategory and use it to assign the unknown category to one of the oov buckets. Their indices start after the known categories, so in this example the indices of\nthe two oov buckets are 5 and 6. Why use oov buckets? Well, if the number of categories is large (e.g., zip codes, cities,\nwords, products, or users) and the dataset is large as well, or it keeps changing, then\ngetting the full list of categories may not be convenient. One solution is to define the\nvocabulary based on a data sample (rather than the whole training set) and add some\noov buckets for the other categories that were not in the data sample. The more\nunknown categories you expect to find during training, the more oov buckets you\nshould use. Indeed, if there are not enough oov buckets, there will be collisions: dif\u2010\nferent categories will end up in the same bucket, so the neural network will not be\nable to distinguish them (at least not based on this feature). Now let\u2019s use the lookup table to encode a small batch of categorical features to one-\nhot vectors:\n>>> categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n>>> cat_indices = table.lookup(categories)\n>>> cat_indices\n<tf.Tensor: id=514, shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>\n>>> cat_one_hot = tf.one_hot(cat_indices, depth=len(vocab) + num_oov_buckets)\n>>> cat_one_hot\n<tf.Tensor: id=524, shape=(4, 7), dtype=float32, numpy=\narray([[0., 0., 0., 1., 0., 0., 0. ], [0., 0., 0., 0., 0., 1., 0. ], [0., 1., 0., 0., 0., 0., 0. ], [0., 1., 0., 0., 0., 0., 0. ]], dtype=float32)>\nAs you can see, \"NEAR BAY\" was mapped to index 3, the unknown category \"DESERT\"\nwas mapped to one of the two oov buckets (at index 5), and \"INLAND\" was mapped to | Chapter 13: Loading and Preprocessing Data with TensorFlow\nindex 1, twice. Then we used tf.one_hot() to one-hot encode these indices. Notice\nthat we have to tell this function the total number of indices, which is equal to the\nvocabulary size plus the number of oov buckets. Now you know how to encode cate\u2010\ngorical features to one-hot vectors using TensorFlow! Just like earlier, it wouldn\u2019t be too difficult to bundle all of this logic into a nice self-\ncontained class. Its adapt() method would take a data sample and extract all the dis\u2010\ntinct categories it contains. It would create a lookup table to map each category to its\nindex (including unknown categories using oov buckets). Then its call() method\nwould use the lookup table to map the input categories to their indices. Well, here\u2019s\nmore good news: by the time you read this, Keras will probably include a layer called\nkeras.layers.TextVectorization, which will be capable of doing exactly that: its\nadapt() method will extract the vocabulary from a data sample, and its call()\nmethod will convert each category to its index in the vocabulary."
  },
  {
    "id": 286,
    "content": "You could add this\nlayer at the beginning of your model, followed by a Lambda layer that would apply the\ntf.one_hot() function, if you want to convert these indices to one-hot vectors. This may not be the best solution, though. The size of each one-hot vector is the\nvocabulary length plus the number of oov buckets. This is fine when there are just a\nfew possible categories, but if the vocabulary is large, it is much more efficient to\nencode them using embeddings instead. As a rule of thumb, if the number of categories is lower than 10,\nthen one-hot encoding is generally the way to go (but your mileage\nmay vary!). If the number of categories is greater than 50 (which is\noften the case when you use hash buckets), then embeddings are\nusually preferable. In between 10 and 50 categories, you may want\nto experiment with both options and see which one works best for\nyour use case. Encoding Categorical Features Using Embeddings\nAn embedding is a trainable dense vector that represents a category. By default,\nembeddings are initialized randomly, so for example the \"NEAR BAY\" category could\nbe represented initially by a random vector such as [0.131, 0.890], while the \"NEAR\nOCEAN\" category might be represented by another random vector such as [0.631,\n0.791]. In this example, we use 2D embeddings, but the number of dimensions is a\nhyperparameter you can tweak. Since these embeddings are trainable, they will grad\u2010\nually improve during training; and as they represent fairly similar categories, Gradi\u2010\nent Descent will certainly end up pushing them closer together, while it will tend to\nmove them away from the \"INLAND\" category\u2019s embedding (see Figure 13-4). Indeed,\nthe better the representation, the easier it will be for the neural network to make\naccurate predictions, so training tends to make embeddings useful representations of\nPreprocessing the Input Features | 9 Tomas Mikolov et al., \u201cDistributed Representations of Words and Phrases and Their Compositionality,\u201d Pro\u2010\nceedings of the 26th International Conference on Neural Information Processing Systems 2 (2013): 3111\u20133119. the categories. This is called representation learning (we will see other types of repre\u2010\nsentation learning in Chapter 17). Figure 13-4. Embeddings will gradually improve during training\nWord Embeddings\nNot only will embeddings generally be useful representations for the task at hand, but\nquite often these same embeddings can be reused successfully for other tasks. The\nmost common example of this is word embeddings (i.e., embeddings of individual\nwords): when you are working on a natural language processing task, you are often\nbetter off reusing pretrained word embeddings than training your own. The idea of using vectors to represent words dates back to the 1960s, and many\nsophisticated techniques have been used to generate useful vectors, including using\nneural networks. But things really took off in 2013, when Tom\u00e1\u0161 Mikolov and other\nGoogle researchers published a paper9 describing an efficient technique to learn word\nembeddings using neural networks, significantly outperforming previous attempts."
  },
  {
    "id": 287,
    "content": "This allowed them to learn embeddings on a very large corpus of text: they trained a\nneural network to predict the words near any given word, and obtained astounding\nword embeddings. For example, synonyms had very close embeddings, and semanti\u2010\ncally related words such as France, Spain, and Italy ended up clustered together. It\u2019s not just about proximity, though: word embeddings were also organized along\nmeaningful axes in the embedding space. Here is a famous example: if you compute\nKing \u2013 Man + Woman (adding and subtracting the embedding vectors of these\nwords), then the result will be very close to the embedding of the word Queen (see\nFigure 13-5). In other words, the word embeddings encode the concept of gender! | Chapter 13: Loading and Preprocessing Data with TensorFlow\n10 Malvina Nissim et al., \u201cFair Is Better Than Sensational: Man Is to Doctor as Woman Is to Doctor,\u201d arXiv pre\u2010\nprint arXiv:1905.09866 (2019). Similarly, you can compute Madrid \u2013 Spain + France, and the result is close to Paris,\nwhich seems to show that the notion of capital city was also encoded in the\nembeddings. Figure 13-5. Word embeddings of similar words tend to be close, and some axes seem to\nencode meaningful concepts\nUnfortunately, word embeddings sometimes capture our worst biases. For example,\nalthough they correctly learn that Man is to King as Woman is to Queen, they also\nseem to learn that Man is to Doctor as Woman is to Nurse: quite a sexist bias! To be\nfair, this particular example is probably exaggerated, as was pointed out in a 2019\npaper10 by Malvina Nissim et al. Nevertheless, ensuring fairness in Deep Learning\nalgorithms is an important and active research topic. Let\u2019s look at how we could implement embeddings manually, to understand how they\nwork (then we will use a simple Keras layer instead). First, we need to create an\nembedding matrix containing each category\u2019s embedding, initialized randomly; it will\nhave one row per category and per oov bucket, and one column per embedding\ndimension:\nembedding_dim = 2\nembed_init = tf.random.uniform([len(vocab) + num_oov_buckets, embedding_dim])\nembedding_matrix = tf.Variable(embed_init)\nPreprocessing the Input Features | In this example we are using 2D embeddings, but as a rule of thumb embeddings typ\u2010\nically have 10 to 300 dimensions, depending on the task and the vocabulary size (you\nwill have to tune this hyperparameter)."
  },
  {
    "id": 288,
    "content": "This embedding matrix is a random 6 \u00d7 2 matrix, stored in a variable (so it can be\ntweaked by Gradient Descent during training):\n>>> embedding_matrix\n<tf.Variable 'Variable:0' shape=(6, 2) dtype=float32, numpy=\narray([[0.6645621 , 0.44100678], [0.3528825 , 0.46448255], [0.03366041, 0.68467236], [0.74011743, 0.8724445 ], [0.22632635, 0.22319686], [0.3103881 , 0.7223358 ]], dtype=float32)>\nNow let\u2019s encode the same batch of categorical features as earlier, but this time using\nthese embeddings:\n>>> categories = tf.constant([\"NEAR BAY\", \"DESERT\", \"INLAND\", \"INLAND\"])\n>>> cat_indices = table.lookup(categories)\n>>> cat_indices\n<tf.Tensor: id=741, shape=(4,), dtype=int64, numpy=array([3, 5, 1, 1])>\n>>> tf.nn.embedding_lookup(embedding_matrix, cat_indices)\n<tf.Tensor: id=864, shape=(4, 2), dtype=float32, numpy=\narray([[0.74011743, 0.8724445 ], [0.3103881 , 0.7223358 ], [0.3528825 , 0.46448255], [0.3528825 , 0.46448255]], dtype=float32)>\nThe tf.nn.embedding_lookup() function looks up the rows in the embedding\nmatrix, at the given indices\u2014that\u2019s all it does. For example, the lookup table says that\nthe \"INLAND\" category is at index 1, so the tf.nn.embedding_lookup() function\nreturns the embedding at row 1 in the embedding matrix (twice): [0.3528825,\n0.46448255]. Keras provides a keras.layers.Embedding layer that handles the embedding matrix\n(trainable, by default); when the layer is created it initializes the embedding matrix\nrandomly, and then when it is called with some category indices it returns the rows at\nthose indices in the embedding matrix:\n>>> embedding = keras.layers.Embedding(input_dim=len(vocab) + num_oov_buckets,\n... output_dim=embedding_dim)\n...\n>>> embedding(cat_indices)\n<tf.Tensor: id=814, shape=(4, 2), dtype=float32, numpy=\narray([[ 0.02401174, 0.03724445], [-0.01896119, 0.02223358], [-0.01471175, -0.00355174], [-0.01471175, -0.00355174]], dtype=float32)> | Chapter 13: Loading and Preprocessing Data with TensorFlow\nPutting everything together, we can now create a Keras model that can process cate\u2010\ngorical features (along with regular numerical features) and learn an embedding for\neach category (as well as for each oov bucket):\nregular_inputs = keras.layers.Input(shape=[8])\ncategories = keras.layers.Input(shape=[], dtype=tf.string)\ncat_indices = keras.layers.Lambda(lambda cats: table.lookup(cats))(categories)\ncat_embed = keras.layers.Embedding(input_dim=6, output_dim=2)(cat_indices)\nencoded_inputs = keras.layers.concatenate([regular_inputs, cat_embed])\noutputs = keras.layers.Dense(1)(encoded_inputs)\nmodel = keras.models.Model(inputs=[regular_inputs, categories], outputs=[outputs])\nThis model takes two inputs: a regular input containing eight numerical features per\ninstance, plus a categorical input (containing one categorical feature per instance). It\nuses a Lambda layer to look up each category\u2019s index, then it looks up the embeddings\nfor these indices. Next, it concatenates the embeddings and the regular inputs in\norder to give the encoded inputs, which are ready to be fed to a neural network. We\ncould add any kind of neural network at this point, but we just add a dense output\nlayer, and we create the Keras model. When the keras.layers.TextVectorization layer is available, you can call its\nadapt() method to make it extract the vocabulary from a data sample (it will take\ncare of creating the lookup table for you). Then you can add it to your model, and it\nwill perform the index lookup (replacing the Lambda layer in the previous code\nexample). One-hot encoding followed by a Dense layer (with no activation\nfunction and no biases) is equivalent to an Embedding layer. How\u2010\never, the Embedding layer uses way fewer computations (the perfor\u2010\nmance difference becomes clear when the size of the embedding\nmatrix grows)."
  },
  {
    "id": 289,
    "content": "The Dense layer\u2019s weight matrix plays the role of the\nembedding matrix. For example, using one-hot vectors of size 20\nand a Dense layer with 10 units is equivalent to using an Embedding\nlayer with input_dim=20 and output_dim=10. As a result, it would\nbe wasteful to use more embedding dimensions than the number\nof units in the layer that follows the Embedding layer. Now let\u2019s look a bit more closely at the Keras preprocessing layers. Keras Preprocessing Layers\nThe TensorFlow team is working on providing a set of standard Keras preprocessing\nlayers. They will probably be available by the time you read this; however, the API\nmay change slightly by then, so please refer to the notebook for this chapter if any\u2010\nthing behaves unexpectedly. This new API will likely supersede the existing Feature\nPreprocessing the Input Features | Columns API, which is harder to use and less intuitive (if you want to learn more\nabout the Feature Columns API anyway, please check out the notebook for this chap\u2010\nter). We already discussed two of these layers: the keras.layers.Normalization layer that\nwill perform feature standardization (it will be equivalent to the Standardization\nlayer we defined earlier), and the TextVectorization layer that will be capable of\nencoding each word in the inputs into its index in the vocabulary. In both cases, you\ncreate the layer, you call its adapt() method with a data sample, and then you use the\nlayer normally in your model. The other preprocessing layers will follow the same\npattern. The API will also include a keras.layers.Discretization layer that will chop con\u2010\ntinuous data into different bins and encode each bin as a one-hot vector. For example,\nyou could use it to discretize prices into three categories, (low, medium, high), which\nwould be encoded as [1, 0, 0], [0, 1, 0], and [0, 0, 1], respectively. Of course this loses a\nlot of information, but in some cases it can help the model detect patterns that would\notherwise not be obvious when just looking at the continuous values. The Discretization layer will not be differentiable, and it should\nonly be used at the start of your model. Indeed, the model\u2019s prepro\u2010\ncessing layers will be frozen during training, so their parameters\nwill not be affected by Gradient Descent, and thus they do not need\nto be differentiable. This also means that you should not use an\nEmbedding layer directly in a custom preprocessing layer, if you\nwant it to be trainable: instead, it should be added separately to\nyour model, as in the previous code example. It will also be possible to chain multiple preprocessing layers using the Preproces\nsingStage class. For example, the following code will create a preprocessing pipeline\nthat will first normalize the inputs, then discretize them (this may remind you of\nScikit-Learn pipelines)."
  },
  {
    "id": 290,
    "content": "After you adapt this pipeline to a data sample, you can use it\nlike a regular layer in your models (but again, only at the start of the model, since it\ncontains a nondifferentiable preprocessing layer):\nnormalization = keras.layers.Normalization()\ndiscretization = keras.layers.Discretization([...])\npipeline = keras.layers.PreprocessingStage([normalization, discretization])\npipeline.adapt(data_sample)\nThe TextVectorization layer will also have an option to output word-count vectors\ninstead of word indices. For example, if the vocabulary contains three words, say\n[\"and\", \"basketball\", \"more\"], then the text \"more and more\" will be mapped to\nthe vector [1, 0, 2]: the word \"and\" appears once, the word \"basketball\" does not\nappear at all, and the word \"more\" appears twice. This text representation is called a | Chapter 13: Loading and Preprocessing Data with TensorFlow\nbag of words, since it completely loses the order of the words. Common words like\n\"and\" will have a large value in most texts, even though they are usually the least\ninteresting (e.g., in the text \"more and more basketball\" the word \"basketball\" is\nclearly the most important, precisely because it is not a very frequent word). So, the\nword counts should be normalized in a way that reduces the importance of frequent\nwords. A common way to do this is to divide each word count by the log of the total\nnumber of training instances in which the word appears. This technique is called\nTerm-Frequency \u00d7 Inverse-Document-Frequency (TF-IDF). For example, let\u2019s imagine\nthat the words \"and\", \"basketball\", and \"more\" appear respectively in 200, 10, and\n100 text instances in the training set: in this case, the final vector will be [1/\nlog(200), 0/log(10), 2/log(100)], which is approximately equal to [0.19, 0.,\n0.43]. The TextVectorization layer will (likely) have an option to perform TF-IDF. If the standard preprocessing layers are insufficient for your task,\nyou will still have the option to create your own custom prepro\u2010\ncessing layer, much like we did earlier with the Standardization\nclass. Create a subclass of the keras.layers.PreprocessingLayer\nclass with an adapt() method, which should take a data_sample\nargument and optionally an extra reset_state argument: if True,\nthen the adapt() method should reset any existing state before\ncomputing the new state; if False, it should try to update the exist\u2010\ning state. As you can see, these Keras preprocessing layers will make preprocessing much eas\u2010\nier! Now, whether you choose to write your own preprocessing layers or use Keras\u2019s\n(or even use the Feature Columns API), all the preprocessing will be done on the fly. During training, however, it may be preferable to perform preprocessing ahead of\ntime. Let\u2019s see why we\u2019d want to do that and how we\u2019d go about it. TF Transform\nIf preprocessing is computationally expensive, then handling it before training rather\nthan on the fly may give you a significant speedup: the data will be preprocessed just\nonce per instance before training, rather than once per instance and per epoch during\ntraining."
  },
  {
    "id": 291,
    "content": "As mentioned earlier, if the dataset is small enough to fit in RAM, you can\nuse its cache() method. But if it is too large, then tools like Apache Beam or Spark\nwill help. They let you run efficient data processing pipelines over large amounts of\ndata, even distributed across multiple servers, so you can use them to preprocess all\nthe training data before training. This works great and indeed can speed up training, but there is one problem: once\nyour model is trained, suppose you want to deploy it to a mobile app. In that case you\nwill need to write some code in your app to take care of preprocessing the data before\nTF Transform | it is fed to the model. And suppose you also want to deploy the model to Tensor\u2010\nFlow.js so that it runs in a web browser? Once again, you will need to write some pre\u2010\nprocessing code. This can become a maintenance nightmare: whenever you want to\nchange the preprocessing logic, you will need to update your Apache Beam code,\nyour mobile app code, and your JavaScript code. This is not only time-consuming,\nbut also error-prone: you may end up with subtle differences between the preprocess\u2010\ning operations performed before training and the ones performed in your app or in\nthe browser. This training/serving skew will lead to bugs or degraded performance. One improvement would be to take the trained model (trained on data that was pre\u2010\nprocessed by your Apache Beam or Spark code) and, before deploying it to your app\nor the browser, add extra preprocessing layers to take care of preprocessing on the fly. That\u2019s definitely better, since now you just have two versions of your preprocessing\ncode: the Apache Beam or Spark code, and the preprocessing layers\u2019 code. But what if you could define your preprocessing operations just once? This is what\nTF Transform was designed for. It is part of TensorFlow Extended (TFX), an end-to-\nend platform for productionizing TensorFlow models. First, to use a TFX component\nsuch as TF Transform, you must install it; it does not come bundled with TensorFlow. You then define your preprocessing function just once (in Python), by using TF\nTransform functions for scaling, bucketizing, and more. You can also use any Tensor\u2010\nFlow operation you need. Here is what this preprocessing function might look like if\nwe just had two features:\nimport tensorflow_transform as tft\ndef preprocess(inputs): # inputs = a batch of input features median_age = inputs[\"housing_median_age\"] ocean_proximity = inputs[\"ocean_proximity\"] standardized_age = tft.scale_to_z_score(median_age) ocean_proximity_id = tft.compute_and_apply_vocabulary(ocean_proximity) return { \"standardized_median_age\": standardized_age, \"ocean_proximity_id\": ocean_proximity_id }\nNext, TF Transform lets you apply this preprocess() function to the whole training\nset using Apache Beam (it provides an AnalyzeAndTransformDataset class that you\ncan use for this purpose in your Apache Beam pipeline)."
  },
  {
    "id": 292,
    "content": "In the process, it will also\ncompute all the necessary statistics over the whole training set: in this example, the\nmean and standard deviation of the housing_median_age feature, and the vocabulary\nfor the ocean_proximity feature. The components that compute these statistics are\ncalled analyzers. Importantly, TF Transform will also generate an equivalent TensorFlow Function that\nyou can plug into the model you deploy. This TF Function includes some constants | Chapter 13: Loading and Preprocessing Data with TensorFlow\nthat correspond to all the all the necessary statistics computed by Apache Beam (the\nmean, standard deviation, and vocabulary). With the Data API, TFRecords, the Keras preprocessing layers, and TF Transform,\nyou can build highly scalable input pipelines for training and benefit from fast and\nportable data preprocessing in production. But what if you just wanted to use a standard dataset? Well in that case, things are\nmuch simpler: just use TFDS! The TensorFlow Datasets (TFDS) Project\nThe TensorFlow Datasets project makes it very easy to download common datasets,\nfrom small ones like MNIST or Fashion MNIST to huge datasets like ImageNet (you\nwill need quite a bit of disk space!). The list includes image datasets, text datasets\n(including translation datasets), and audio and video datasets. You can visit \nhoml.info/tfds to view the full list, along with a description of each dataset. TFDS is not bundled with TensorFlow, so you need to install the tensorflow-\ndatasets library (e.g., using pip). Then call the tfds.load() function, and it will\ndownload the data you want (unless it was already downloaded earlier) and return\nthe data as a dictionary of datasets (typically one for training and one for testing, but\nthis depends on the dataset you choose). For example, let\u2019s download MNIST:\nimport tensorflow_datasets as tfds\ndataset = tfds.load(name=\"mnist\")\nmnist_train, mnist_test = dataset[\"train\"], dataset[\"test\"]\nYou can then apply any transformation you want (typically shuffling, batching, and\nprefetching), and you\u2019re ready to train your model. Here is a simple example:\nmnist_train = mnist_train.shuffle(10000).batch(32).prefetch(1)\nfor item in mnist_train: images = item[\"image\"] labels = item[\"label\"] [...]\nThe load() function shuffles each data shard it downloads (only\nfor the training set). This may not be sufficient, so it\u2019s best to shuf\u2010\nfle the training data some more. Note that each item in the dataset is a dictionary containing both the features and the\nlabels. But Keras expects each item to be a tuple containing two elements (again, the\nfeatures and the labels). You could transform the dataset using the map() method, like\nthis:\nThe TensorFlow Datasets (TFDS) Project | mnist_train = mnist_train.shuffle(10000).batch(32)\nmnist_train = mnist_train.map(lambda items: (items[\"image\"], items[\"label\"]))\nmnist_train = mnist_train.prefetch(1)\nBut it\u2019s simpler to ask the load() function to do this for you by setting as_super\nvised=True (obviously this works only for labeled datasets). You can also specify the\nbatch size if you want."
  },
  {
    "id": 293,
    "content": "Then you can pass the dataset directly to your tf.keras model:\ndataset = tfds.load(name=\"mnist\", batch_size=32, as_supervised=True)\nmnist_train = dataset[\"train\"].prefetch(1)\nmodel = keras.models.Sequential([...])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\")\nmodel.fit(mnist_train, epochs=5)\nThis was quite a technical chapter, and you may feel that it is a bit far from the\nabstract beauty of neural networks, but the fact is Deep Learning often involves large\namounts of data, and knowing how to load, parse, and preprocess it efficiently is a\ncrucial skill to have. In the next chapter, we will look at convolutional neural net\u2010\nworks, which are among the most successful neural net architectures for image pro\u2010\ncessing and many other applications. Exercises\n1. Why would you want to use the Data API? 2. What are the benefits of splitting a large dataset into multiple files? 3. During training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it? 4. Can you save any binary data to a TFRecord file, or only serialized protocol\nbuffers? 5. Why would you go through the hassle of converting all your data to the Example\nprotobuf format? Why not use your own protobuf definition? 6. When using TFRecords, when would you want to activate compression? Why\nnot do it systematically? 7. Data can be preprocessed directly when writing the data files, or within the\ntf.data pipeline, or in preprocessing layers within your model, or using TF Trans\u2010\nform. Can you list a few pros and cons of each option? 8. Name a few common techniques you can use to encode categorical features. What about text? 9. Load the Fashion MNIST dataset (introduced in Chapter 10); split it into a train\u2010\ning set, a validation set, and a test set; shuffle the training set; and save each\ndataset to multiple TFRecord files. Each record should be a serialized Example\nprotobuf with two features: the serialized image (use tf.io.serialize_tensor() | Chapter 13: Loading and Preprocessing Data with TensorFlow\n11 For large images, you could use tf.io.encode_jpeg() instead. This would save a lot of space, but it would\nlose a bit of image quality. to serialize each image), and the label.11 Then use tf.data to create an efficient\ndataset for each set. Finally, use a Keras model to train these datasets, including a\npreprocessing layer to standardize each input feature. Try to make the input\npipeline as efficient as possible, using TensorBoard to visualize profiling data. 10. In this exercise you will download a dataset, split it, create a tf.data.Dataset to\nload it and preprocess it efficiently, then build and train a binary classification\nmodel containing an Embedding layer:\na. Download the Large Movie Review Dataset, which contains 50,000 movies\nreviews from the Internet Movie Database. The data is organized in two direc\u2010\ntories, train and test, each containing a pos subdirectory with 12,500 positive\nreviews and a neg subdirectory with 12,500 negative reviews. Each review is\nstored in a separate text file."
  },
  {
    "id": 294,
    "content": "There are other files and folders (including pre\u2010\nprocessed bag-of-words), but we will ignore them in this exercise. b. Split the test set into a validation set (15,000) and a test set (10,000). c. Use tf.data to create an efficient dataset for each set. d. Create a binary classification model, using a TextVectorization layer to pre\u2010\nprocess each review. If the TextVectorization layer is not yet available (or if\nyou like a challenge), try to create your own custom preprocessing layer: you\ncan use the functions in the tf.strings package, for example lower() to\nmake everything lowercase, regex_replace() to replace punctuation with\nspaces, and split() to split words on spaces. You should use a lookup table to\noutput word indices, which must be prepared in the adapt() method. e. Add an Embedding layer and compute the mean embedding for each review,\nmultiplied by the square root of the number of words (see Chapter 16). This\nrescaled mean embedding can then be passed to the rest of your model. f. Train the model and see what accuracy you get. Try to optimize your pipelines\nto make training as fast as possible. g. Use TFDS to load the same dataset more easily: tfds.load(\"imdb_reviews\"). Solutions to these exercises are available in Appendix A. Exercises | CHAPTER 14\nDeep Computer Vision Using\nConvolutional Neural Networks\nAlthough IBM\u2019s Deep Blue supercomputer beat the chess world champion Garry Kas\u2010\nparov back in 1996, it wasn\u2019t until fairly recently that computers were able to reliably\nperform seemingly trivial tasks such as detecting a puppy in a picture or recognizing\nspoken words. Why are these tasks so effortless to us humans? The answer lies in the\nfact that perception largely takes place outside the realm of our consciousness, within\nspecialized visual, auditory, and other sensory modules in our brains. By the time\nsensory information reaches our consciousness, it is already adorned with high-level\nfeatures; for example, when you look at a picture of a cute puppy, you cannot choose\nnot to see the puppy, not to notice its cuteness. Nor can you explain how you recog\u2010\nnize a cute puppy; it\u2019s just obvious to you. Thus, we cannot trust our subjective expe\u2010\nrience: perception is not trivial at all, and to understand it we must look at how the\nsensory modules work. Convolutional neural networks (CNNs) emerged from the study of the brain\u2019s visual\ncortex, and they have been used in image recognition since the 1980s. In the last few\nyears, thanks to the increase in computational power, the amount of available training\ndata, and the tricks presented in Chapter 11 for training deep nets, CNNs have man\u2010\naged to achieve superhuman performance on some complex visual tasks. They power\nimage search services, self-driving cars, automatic video classification systems, and\nmore. Moreover, CNNs are not restricted to visual perception: they are also successful\nat many other tasks, such as voice recognition and natural language processing. How\u2010\never, we will focus on visual applications for now."
  },
  {
    "id": 295,
    "content": "In this chapter we will explore where CNNs came from, what their building blocks\nlook like, and how to implement them using TensorFlow and Keras. Then we will dis\u2010\ncuss some of the best CNN architectures, as well as other visual tasks, including 1 David H. Hubel, \u201cSingle Unit Activity in Striate Cortex of Unrestrained Cats,\u201d The Journal of Physiology 147\n(1959): 226\u2013238. 2 David H. Hubel and Torsten N. Wiesel, \u201cReceptive Fields of Single Neurons in the Cat\u2019s Striate Cortex,\u201d The\nJournal of Physiology 148 (1959): 574\u2013591. 3 David H. Hubel and Torsten N. Wiesel, \u201cReceptive Fields and Functional Architecture of Monkey Striate Cor\u2010\ntex,\u201d The Journal of Physiology 195 (1968): 215\u2013243. object detection (classifying multiple objects in an image and placing bounding boxes\naround them) and semantic segmentation (classifying each pixel according to the\nclass of the object it belongs to). The Architecture of the Visual Cortex\nDavid H. Hubel and Torsten Wiesel performed a series of experiments on cats in\n19581 and 19592 (and a few years later on monkeys3), giving crucial insights into the\nstructure of the visual cortex (the authors received the Nobel Prize in Physiology or\nMedicine in 1981 for their work). In particular, they showed that many neurons in\nthe visual cortex have a small local receptive field, meaning they react only to visual\nstimuli located in a limited region of the visual field (see Figure 14-1, in which the\nlocal receptive fields of five neurons are represented by dashed circles). The receptive\nfields of different neurons may overlap, and together they tile the whole visual field. Moreover, the authors showed that some neurons react only to images of horizontal\nlines, while others react only to lines with different orientations (two neurons may\nhave the same receptive field but react to different line orientations). They also\nnoticed that some neurons have larger receptive fields, and they react to more com\u2010\nplex patterns that are combinations of the lower-level patterns. These observations\nled to the idea that the higher-level neurons are based on the outputs of neighboring\nlower-level neurons (in Figure 14-1, notice that each neuron is connected only to a\nfew neurons from the previous layer). This powerful architecture is able to detect all\nsorts of complex patterns in any area of the visual field. | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n4 Kunihiko Fukushima, \u201cNeocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern\nRecognition Unaffected by Shift in Position,\u201d Biological Cybernetics 36 (1980): 193\u2013202. 5 Yann LeCun et al., \u201cGradient-Based Learning Applied to Document Recognition,\u201d Proceedings of the IEEE 86,\nno. 11 (1998): 2278\u20132324. Figure 14-1. Biological neurons in the visual cortex respond to specific patterns in small\nregions of the visual field called receptive fields; as the visual signal makes its way\nthrough consecutive brain modules, neurons respond to more complex patterns in larger\nreceptive fields."
  },
  {
    "id": 296,
    "content": "These studies of the visual cortex inspired the neocognitron,4 introduced in 1980,\nwhich gradually evolved into what we now call convolutional neural networks. An\nimportant milestone was a 1998 paper5 by Yann LeCun et al. that introduced the\nfamous LeNet-5 architecture, widely used by banks to recognize handwritten check\nnumbers. This architecture has some building blocks that you already know, such as\nfully connected layers and sigmoid activation functions, but it also introduces two\nnew building blocks: convolutional layers and pooling layers. Let\u2019s look at them now. Why not simply use a deep neural network with fully connected\nlayers for image recognition tasks? Unfortunately, although this\nworks fine for small images (e.g., MNIST), it breaks down for\nlarger images because of the huge number of parameters it\nrequires. For example, a 100 \u00d7 100\u2013pixel image has 10,000 pixels,\nand if the first layer has just 1,000 neurons (which already severely\nrestricts the amount of information transmitted to the next layer),\nthis means a total of 10 million connections. And that\u2019s just the first\nlayer. CNNs solve this problem using partially connected layers and\nweight sharing. The Architecture of the Visual Cortex | 6 A convolution is a mathematical operation that slides one function over another and measures the integral of\ntheir pointwise multiplication. It has deep connections with the Fourier transform and the Laplace transform\nand is heavily used in signal processing. Convolutional layers actually use cross-correlations, which are very\nsimilar to convolutions (see  for more details). Convolutional Layers\nThe most important building block of a CNN is the convolutional layer:6 neurons in\nthe first convolutional layer are not connected to every single pixel in the input image\n(like they were in the layers discussed in previous chapters), but only to pixels in their\nreceptive fields (see Figure 14-2). In turn, each neuron in the second convolutional\nlayer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on small low-level features in the\nfirst hidden layer, then assemble them into larger higher-level features in the next\nhidden layer, and so on. This hierarchical structure is common in real-world images,\nwhich is one of the reasons why CNNs work so well for image recognition. Figure 14-2. CNN layers with rectangular local receptive fields\nAll the multilayer neural networks we\u2019ve looked at so far had layers\ncomposed of a long line of neurons, and we had to flatten input\nimages to 1D before feeding them to the neural network. In a CNN\neach layer is represented in 2D, which makes it easier to match\nneurons with their corresponding inputs."
  },
  {
    "id": 297,
    "content": "| Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nA neuron located in row i, column j of a given layer is connected to the outputs of the\nneurons in the previous layer located in rows i to i + fh \u2013 1, columns j to j + fw \u2013 1,\nwhere fh and fw are the height and width of the receptive field (see Figure 14-3). In\norder for a layer to have the same height and width as the previous layer, it is com\u2010\nmon to add zeros around the inputs, as shown in the diagram. This is called zero\npadding. Figure 14-3. Connections between layers and zero padding\nIt is also possible to connect a large input layer to a much smaller layer by spacing out\nthe receptive fields, as shown in Figure 14-4. This dramatically reduces the model\u2019s\ncomputational complexity. The shift from one receptive field to the next is called the\nstride. In the diagram, a 5 \u00d7 7 input layer (plus zero padding) is connected to a 3 \u00d7 4\nlayer, using 3 \u00d7 3 receptive fields and a stride of 2 (in this example the stride is the\nsame in both directions, but it does not have to be so). A neuron located in row i,\ncolumn j in the upper layer is connected to the outputs of the neurons in the previous\nlayer located in rows i \u00d7 sh to i \u00d7 sh + fh \u2013 1, columns j \u00d7 sw to j \u00d7 sw + fw \u2013 1, where sh\nand sw are the vertical and horizontal strides. Convolutional Layers | Figure 14-4. Reducing dimensionality using a stride of 2\nFilters\nA neuron\u2019s weights can be represented as a small image the size of the receptive field. For example, Figure 14-5 shows two possible sets of weights, called filters (or convolu\u2010\ntion kernels). The first one is represented as a black square with a vertical white line in\nthe middle (it is a 7 \u00d7 7 matrix full of 0s except for the central column, which is full of\n1s); neurons using these weights will ignore everything in their receptive field except\nfor the central vertical line (since all inputs will get multiplied by 0, except for the\nones located in the central vertical line). The second filter is a black square with a\nhorizontal white line in the middle. Once again, neurons using these weights will\nignore everything in their receptive field except for the central horizontal line. Now if all neurons in a layer use the same vertical line filter (and the same bias term),\nand you feed the network the input image shown in Figure 14-5 (the bottom image),\nthe layer will output the top-left image. Notice that the vertical white lines get\nenhanced while the rest gets blurred."
  },
  {
    "id": 298,
    "content": "Similarly, the upper-right image is what you get\nif all neurons use the same horizontal line filter; notice that the horizontal white lines\nget enhanced while the rest is blurred out. Thus, a layer full of neurons using the\nsame filter outputs a feature map, which highlights the areas in an image that activate\nthe filter the most. Of course, you do not have to define the filters manually: instead,\nduring training the convolutional layer will automatically learn the most useful filters\nfor its task, and the layers above will learn to combine them into more complex\npatterns. | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nFigure 14-5. Applying two different filters to get two feature maps\nStacking Multiple Feature Maps\nUp to now, for simplicity, I have represented the output of each convolutional layer as\na 2D layer, but in reality a convolutional layer has multiple filters (you decide how\nmany) and outputs one feature map per filter, so it is more accurately represented in\n3D (see Figure 14-6). It has one neuron per pixel in each feature map, and all neurons\nwithin a given feature map share the same parameters (i.e., the same weights and bias\nterm). Neurons in different feature maps use different parameters. A neuron\u2019s recep\u2010\ntive field is the same as described earlier, but it extends across all the previous layers\u2019\nfeature maps. In short, a convolutional layer simultaneously applies multiple trainable\nfilters to its inputs, making it capable of detecting multiple features anywhere in its\ninputs. The fact that all neurons in a feature map share the same parame\u2010\nters dramatically reduces the number of parameters in the model. Once the CNN has learned to recognize a pattern in one location, it\ncan recognize it in any other location. In contrast, once a regular\nDNN has learned to recognize a pattern in one location, it can rec\u2010\nognize it only in that particular location. Input images are also composed of multiple sublayers: one per color channel. There\nare typically three: red, green, and blue (RGB). Grayscale images have just one\nConvolutional Layers | channel, but some images may have much more\u2014for example, satellite images that\ncapture extra light frequencies (such as infrared). Figure 14-6. Convolutional layers with multiple feature maps, and images with three\ncolor channels\nSpecifically, a neuron located in row i, column j of the feature map k in a given convo\u2010\nlutional layer l is connected to the outputs of the neurons in the previous layer l \u2013 1,\nlocated in rows i \u00d7 sh to i \u00d7 sh + fh \u2013 1 and columns j \u00d7 sw to j \u00d7 sw + fw \u2013 1, across all\nfeature maps (in layer l \u2013 1). Note that all neurons located in the same row i and col\u2010\numn j but in different feature maps are connected to the outputs of the exact same\nneurons in the previous layer."
  },
  {
    "id": 299,
    "content": "Equation 14-1 summarizes the preceding explanations in one big mathematical equa\u2010\ntion: it shows how to compute the output of a given neuron in a convolutional layer. | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nIt is a bit ugly due to all the different indices, but all it does is calculate the weighted\nsum of all the inputs, plus the bias term. Equation 14-1. Computing the output of a neuron in a convolutional layer\nzi, j, k = bk + \u2211\nu = 0\nf h \u22121\n\u2211\nv = 0\nf w \u22121\n\u2211\nk\u2032 = 0\nf n\u2032 \u22121\nxi\u2032, j\u2032, k\u2032 . wu, v, k\u2032, k\nwith\ni\u2032 = i \u00d7 sh + u\nj\u2032 = j \u00d7 sw + v\nIn this equation:\n\u2022 zi, j, k is the output of the neuron located in row i, column j in feature map k of the\nconvolutional layer (layer l). \u2022 As explained earlier, sh and sw are the vertical and horizontal strides, fh and fw are\nthe height and width of the receptive field, and fn\u2032 is the number of feature maps\nin the previous layer (layer l \u2013 1). \u2022 xi\u2032, j\u2032, k\u2032 is the output of the neuron located in layer l \u2013 1, row i\u2032, column j\u2032, feature\nmap k\u2032 (or channel k\u2032 if the previous layer is the input layer). \u2022 bk is the bias term for feature map k (in layer l). You can think of it as a knob that\ntweaks the overall brightness of the feature map k.\n\u2022 wu, v, k\u2032 ,k is the connection weight between any neuron in feature map k of the layer\nl and its input located at row u, column v (relative to the neuron\u2019s receptive field),\nand feature map k\u2032. TensorFlow Implementation\nIn TensorFlow, each input image is typically represented as a 3D tensor of shape\n[height, width, channels]. A mini-batch is represented as a 4D tensor of shape [mini-\nbatch size, height, width, channels]. The weights of a convolutional layer are repre\u2010\nsented as a 4D tensor of shape [fh, fw, fn\u2032, fn]. The bias terms of a convolutional layer\nare simply represented as a 1D tensor of shape [fn]. Let\u2019s look at a simple example. The following code loads two sample images, using\nScikit-Learn\u2019s load_sample_image() (which loads two color images, one of a Chinese\ntemple, and the other of a flower), then it creates two filters and applies them to both\nimages, and finally it displays one of the resulting feature maps. Note that you must\npip install the Pillow package to use load_sample_image()."
  },
  {
    "id": 300,
    "content": "from sklearn.datasets import load_sample_image\n# Load sample images\nchina = load_sample_image(\"china.jpg\") / 255\nflower = load_sample_image(\"flower.jpg\") / 255\nConvolutional Layers | images = np.array([china, flower])\nbatch_size, height, width, channels = images.shape\n# Create 2 filters\nfilters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\nfilters[:, 3, :, 0] = 1 # vertical line\nfilters[3, :, :, 1] = 1 # horizontal line\noutputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\nplt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image's 2nd feature map\nplt.show()\nLet\u2019s go through this code:\n\u2022 The pixel intensity for each color channel is represented as a byte from 0 to 255,\nso we scale these features simply by dividing by 255, to get floats ranging from 0\nto 1. \u2022 Then we create two 7 \u00d7 7 filters (one with a vertical white line in the middle, and\nthe other with a horizontal white line in the middle). \u2022 We apply them to both images using the tf.nn.conv2d() function, which is part\nof TensorFlow\u2019s low-level Deep Learning API. In this example, we use zero pad\u2010\nding (padding=\"SAME\") and a stride of 1. \u2022 Finally, we plot one of the resulting feature maps (similar to the top-right image\nin Figure 14-5). The tf.nn.conv2d() line deserves a bit more explanation:\n\u2022 images is the input mini-batch (a 4D tensor, as explained earlier). \u2022 filters is the set of filters to apply (also a 4D tensor, as explained earlier). \u2022 strides is equal to 1, but it could also be a 1D array with four elements, where\nthe two central elements are the vertical and horizontal strides (sh and sw). The\nfirst and last elements must currently be equal to 1. They may one day be used to\nspecify a batch stride (to skip some instances) and a channel stride (to skip some\nof the previous layer\u2019s feature maps or channels). \u2022 padding must be either \"SAME\" or \"VALID\":\n\u2014 If set to \"SAME\", the convolutional layer uses zero padding if necessary. The\noutput size is set to the number of input neurons divided by the stride, roun\u2010\nded up. For example, if the input size is 13 and the stride is 5 (see Figure 14-7),\nthen the output size is 3 (i.e., 13 / 5 = 2.6, rounded up to 3). Then zeros are\nadded as evenly as possible around the inputs, as needed. When strides=1,\nthe layer\u2019s outputs will have the same spatial dimensions (width and height) as\nits inputs, hence the name same. | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n\u2014 If set to \"VALID\", the convolutional layer does not use zero padding and may\nignore some rows and columns at the bottom and right of the input image,\ndepending on the stride, as shown in Figure 14-7 (for simplicity, only the hor\u2010\nizontal dimension is shown here, but of course the same logic applies to the\nvertical dimension)."
  },
  {
    "id": 301,
    "content": "This means that every neuron\u2019s receptive field lies strictly\nwithin valid positions inside the input (it does not go out of bounds), hence\nthe name valid. Figure 14-7. Padding=\"SAME\u201d or \u201cVALID\u201d (with input width 13, filter width 6, stride\n5)\nIn this example we manually defined the filters, but in a real CNN you would nor\u2010\nmally define filters as trainable variables so the neural net can learn which filters\nwork best, as explained earlier. Instead of manually creating the variables, use the\nkeras.layers.Conv2D layer:\nconv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")\nThis code creates a Conv2D layer with 32 filters, each 3 \u00d7 3, using a stride of 1 (both\nhorizontally and vertically) and \"same\" padding, and applying the ReLU activation\nfunction to its outputs. As you can see, convolutional layers have quite a few hyper\u2010\nparameters: you must choose the number of filters, their height and width, the\nstrides, and the padding type. As always, you can use cross-validation to find the right\nhyperparameter values, but this is very time-consuming. We will discuss common\nCNN architectures later, to give you some idea of which hyperparameter values work\nbest in practice. Convolutional Layers | 7 A fully connected layer with 150 \u00d7 100 neurons, each connected to all 150 \u00d7 100 \u00d7 3 inputs, would have 1502\n\u00d7 1002 \u00d7 3 = 675 million parameters! 8 In the international system of units (SI), 1 MB = 1,000 KB = 1,000 \u00d7 1,000 bytes = 1,000 \u00d7 1,000 \u00d7 8 bits. Memory Requirements\nAnother problem with CNNs is that the convolutional layers require a huge amount\nof RAM. This is especially true during training, because the reverse pass of backpro\u2010\npagation requires all the intermediate values computed during the forward pass. For example, consider a convolutional layer with 5 \u00d7 5 filters, outputting 200 feature\nmaps of size 150 \u00d7 100, with stride 1 and \"same\" padding. If the input is a 150 \u00d7 100\nRGB image (three channels), then the number of parameters is (5 \u00d7 5 \u00d7 3 + 1) \u00d7 200\n= 15,200 (the + 1 corresponds to the bias terms), which is fairly small compared to a\nfully connected layer.7 However, each of the 200 feature maps contains 150 \u00d7 100 neu\u2010\nrons, and each of these neurons needs to compute a weighted sum of its 5 \u00d7 5 \u00d7 3 =\n75 inputs: that\u2019s a total of 225 million float multiplications. Not as bad as a fully con\u2010\nnected layer, but still quite computationally intensive. Moreover, if the feature maps\nare represented using 32-bit floats, then the convolutional layer\u2019s output will occupy\n200 \u00d7 150 \u00d7 100 \u00d7 32 = 96 million bits (12 MB) of RAM.8 And that\u2019s just for one\ninstance\u2014if a training batch contains 100 instances, then this layer will use up 1.2 GB\nof RAM!"
  },
  {
    "id": 302,
    "content": "During inference (i.e., when making a prediction for a new instance) the RAM occu\u2010\npied by one layer can be released as soon as the next layer has been computed, so you\nonly need as much RAM as required by two consecutive layers. But during training\neverything computed during the forward pass needs to be preserved for the reverse\npass, so the amount of RAM needed is (at least) the total amount of RAM required by\nall layers. If training crashes because of an out-of-memory error, you can try\nreducing the mini-batch size. Alternatively, you can try reducing\ndimensionality using a stride, or removing a few layers. Or you can\ntry using 16-bit floats instead of 32-bit floats. Or you could distrib\u2010\nute the CNN across multiple devices. Now let\u2019s look at the second common building block of CNNs: the pooling layer. Pooling Layers\nOnce you understand how convolutional layers work, the pooling layers are quite\neasy to grasp. Their goal is to subsample (i.e., shrink) the input image in order to | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n9 Other kernels we\u2019ve discussed so far had weights, but pooling kernels do not: they are just stateless sliding\nwindows. reduce the computational load, the memory usage, and the number of parameters\n(thereby limiting the risk of overfitting). Just like in convolutional layers, each neuron in a pooling layer is connected to the\noutputs of a limited number of neurons in the previous layer, located within a small\nrectangular receptive field. You must define its size, the stride, and the padding type,\njust like before. However, a pooling neuron has no weights; all it does is aggregate the\ninputs using an aggregation function such as the max or mean. Figure 14-8 shows a\nmax pooling layer, which is the most common type of pooling layer. In this example,\nwe use a 2 \u00d7 2 pooling kernel,9 with a stride of 2 and no padding. Only the max input\nvalue in each receptive field makes it to the next layer, while the other inputs are\ndropped. For example, in the lower-left receptive field in Figure 14-8, the input values\nare 1, 5, 3, 2, so only the max value, 5, is propagated to the next layer. Because of the\nstride of 2, the output image has half the height and half the width of the input image\n(rounded down since we use no padding). Figure 14-8. Max pooling layer (2 \u00d7 2 pooling kernel, stride 2, no padding)\nA pooling layer typically works on every input channel independ\u2010\nently, so the output depth is the same as the input depth. Other than reducing computations, memory usage, and the number of parameters, a\nmax pooling layer also introduces some level of invariance to small translations, as\nshown in Figure 14-9."
  },
  {
    "id": 303,
    "content": "Here we assume that the bright pixels have a lower value than\ndark pixels, and we consider three images (A, B, C) going through a max pooling\nlayer with a 2 \u00d7 2 kernel and stride 2. Images B and C are the same as image A, but\nPooling Layers | shifted by one and two pixels to the right. As you can see, the outputs of the max\npooling layer for images A and B are identical. This is what translation invariance\nmeans. For image C, the output is different: it is shifted one pixel to the right (but\nthere is still 75% invariance). By inserting a max pooling layer every few layers in a\nCNN, it is possible to get some level of translation invariance at a larger scale. More\u2010\nover, max pooling offers a small amount of rotational invariance and a slight scale\ninvariance. Such invariance (even if it is limited) can be useful in cases where the pre\u2010\ndiction should not depend on these details, such as in classification tasks. Figure 14-9. Invariance to small translations\nHowever, max pooling has some downsides too. Firstly, it is obviously very destruc\u2010\ntive: even with a tiny 2 \u00d7 2 kernel and a stride of 2, the output will be two times\nsmaller in both directions (so its area will be four times smaller), simply dropping\n75% of the input values. And in some applications, invariance is not desirable. Take\nsemantic segmentation (the task of classifying each pixel in an image according to the\nobject that pixel belongs to, which we\u2019ll explore later in this chapter): obviously, if the\ninput image is translated by one pixel to the right, the output should also be trans\u2010\nlated by one pixel to the right. The goal in this case is equivariance, not invariance: a\nsmall change to the inputs should lead to a corresponding small change in the output. TensorFlow Implementation\nImplementing a max pooling layer in TensorFlow is quite easy. The following code\ncreates a max pooling layer using a 2 \u00d7 2 kernel. The strides default to the kernel size,\nso this layer will use a stride of 2 (both horizontally and vertically). By default, it uses\n\"valid\" padding (i.e., no padding at all): | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nmax_pool = keras.layers.MaxPool2D(pool_size=2)\nTo create an average pooling layer, just use AvgPool2D instead of MaxPool2D. As you\nmight expect, it works exactly like a max pooling layer, except it computes the mean\nrather than the max. Average pooling layers used to be very popular, but people\nmostly use max pooling layers now, as they generally perform better. This may seem\nsurprising, since computing the mean generally loses less information than comput\u2010\ning the max. But on the other hand, max pooling preserves only the strongest fea\u2010\ntures, getting rid of all the meaningless ones, so the next layers get a cleaner signal to\nwork with."
  },
  {
    "id": 304,
    "content": "Moreover, max pooling offers stronger translation invariance than average\npooling, and it requires slightly less compute. Note that max pooling and average pooling can be performed along the depth dimen\u2010\nsion rather than the spatial dimensions, although this is not as common. This can\nallow the CNN to learn to be invariant to various features. For example, it could learn\nmultiple filters, each detecting a different rotation of the same pattern (such as hand-\nwritten digits; see Figure 14-10), and the depthwise max pooling layer would ensure\nthat the output is the same regardless of the rotation. The CNN could similarly learn\nto be invariant to anything else: thickness, brightness, skew, color, and so on. Figure 14-10. Depthwise max pooling can help the CNN learn any invariance\nPooling Layers | Keras does not include a depthwise max pooling layer, but TensorFlow\u2019s low-level\nDeep Learning API does: just use the tf.nn.max_pool() function, and specify the\nkernel size and strides as 4-tuples (i.e., tuples of size 4). The first three values of each\nshould be 1: this indicates that the kernel size and stride along the batch, height, and\nwidth dimensions should be 1. The last value should be whatever kernel size and\nstride you want along the depth dimension\u2014for example, 3 (this must be a divisor of\nthe input depth; it will not work if the previous layer outputs 20 feature maps, since\n20 is not a multiple of 3):\noutput = tf.nn.max_pool(images, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding=\"valid\")\nIf you want to include this as a layer in your Keras models, wrap it in a Lambda layer\n(or create a custom Keras layer):\ndepth_pool = keras.layers.Lambda( lambda X: tf.nn.max_pool(X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding=\"valid\"))\nOne last type of pooling layer that you will often see in modern architectures is the\nglobal average pooling layer. It works very differently: all it does is compute the mean\nof each entire feature map (it\u2019s like an average pooling layer using a pooling kernel\nwith the same spatial dimensions as the inputs). This means that it just outputs a sin\u2010\ngle number per feature map and per instance. Although this is of course extremely\ndestructive (most of the information in the feature map is lost), it can be useful as the\noutput layer, as we will see later in this chapter. To create such a layer, simply use the\nkeras.layers.GlobalAvgPool2D class:\nglobal_avg_pool = keras.layers.GlobalAvgPool2D()\nIt\u2019s equivalent to this simple Lambda layer, which computes the mean over the spatial\ndimensions (height and width):\nglobal_avg_pool = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))\nNow you know all the building blocks to create convolutional neural networks. Let\u2019s\nsee how to assemble them. CNN Architectures\nTypical CNN architectures stack a few convolutional layers (each one generally fol\u2010\nlowed by a ReLU layer), then a pooling layer, then another few convolutional layers\n(+ReLU), then another pooling layer, and so on."
  },
  {
    "id": 305,
    "content": "The image gets smaller and smaller\nas it progresses through the network, but it also typically gets deeper and deeper (i.e.,\nwith more feature maps), thanks to the convolutional layers (see Figure 14-11). At the\ntop of the stack, a regular feedforward neural network is added, composed of a few | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nfully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a\nsoftmax layer that outputs estimated class probabilities). Figure 14-11. Typical CNN architecture\nA common mistake is to use convolution kernels that are too large. For example, instead of using a convolutional layer with a 5 \u00d7 5\nkernel, stack two layers with 3 \u00d7 3 kernels: it will use fewer parame\u2010\nters and require fewer computations, and it will usually perform\nbetter. One exception is for the first convolutional layer: it can typi\u2010\ncally have a large kernel (e.g., 5 \u00d7 5), usually with a stride of 2 or\nmore: this will reduce the spatial dimension of the image without\nlosing too much information, and since the input image only has\nthree channels in general, it will not be too costly. Here is how you can implement a simple CNN to tackle the Fashion MNIST dataset\n(introduced in Chapter 10):\nmodel = keras.models.Sequential([ keras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\", input_shape=[28, 28, 1]), keras.layers.MaxPooling2D(2), keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"), keras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"), keras.layers.MaxPooling2D(2), keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"), keras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"), keras.layers.MaxPooling2D(2), keras.layers.Flatten(), keras.layers.Dense(128, activation=\"relu\"), keras.layers.Dropout(0.5), keras.layers.Dense(64, activation=\"relu\"), keras.layers.Dropout(0.5), keras.layers.Dense(10, activation=\"softmax\")\n])\nCNN Architectures | Let\u2019s go through this model:\n\u2022 The first layer uses 64 fairly large filters (7 \u00d7 7) but no stride because the input\nimages are not very large. It also sets input_shape=[28, 28, 1], because the\nimages are 28 \u00d7 28 pixels, with a single color channel (i.e., grayscale). \u2022 Next we have a max pooling layer which uses a pool size of 2, so it divides each\nspatial dimension by a factor of 2. \u2022 Then we repeat the same structure twice: two convolutional layers followed by a\nmax pooling layer. For larger images, we could repeat this structure several more\ntimes (the number of repetitions is a hyperparameter you can tune). \u2022 Note that the number of filters grows as we climb up the CNN toward the output\nlayer (it is initially 64, then 128, then 256): it makes sense for it to grow, since the\nnumber of low-level features is often fairly low (e.g., small circles, horizontal\nlines), but there are many different ways to combine them into higher-level fea\u2010\ntures. It is a common practice to double the number of filters after each pooling\nlayer: since a pooling layer divides each spatial dimension by a factor of 2, we can\nafford to double the number of feature maps in the next layer without fear of\nexploding the number of parameters, memory usage, or computational load."
  },
  {
    "id": 306,
    "content": "\u2022 Next is the fully connected network, composed of two hidden dense layers and a\ndense output layer. Note that we must flatten its inputs, since a dense network\nexpects a 1D array of features for each instance. We also add two dropout layers,\nwith a dropout rate of 50% each, to reduce overfitting. This CNN reaches over 92% accuracy on the test set. It\u2019s not state of the art, but it is\npretty good, and clearly much better than what we achieved with dense networks in\nChapter 10. Over the years, variants of this fundamental architecture have been developed, lead\u2010\ning to amazing advances in the field. A good measure of this progress is the error rate\nin competitions such as the ILSVRC ImageNet challenge. In this competition the top-\nfive error rate for image classification fell from over 26% to less than 2.3% in just six\nyears. The top-five error rate is the number of test images for which the system\u2019s top\nfive predictions did not include the correct answer. The images are large (256 pixels\nhigh) and there are 1,000 classes, some of which are really subtle (try distinguishing\n120 dog breeds). Looking at the evolution of the winning entries is a good way to\nunderstand how CNNs work. We will first look at the classical LeNet-5 architecture (1998), then three of the win\u2010\nners of the ILSVRC challenge: AlexNet (2012), GoogLeNet (2014), and ResNet\n(2015). | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n10 Yann LeCun et al., \u201cGradient-Based Learning Applied to Document Recognition,\u201d Proceedings of the IEEE 86,\nno. 11 (1998): 2278\u20132324. LeNet-5\nThe LeNet-5 architecture10 is perhaps the most widely known CNN architecture. As\nmentioned earlier, it was created by Yann LeCun in 1998 and has been widely used\nfor handwritten digit recognition (MNIST). It is composed of the layers shown in\nTable 14-1. Table 14-1. LeNet-5 architecture\nLayer\nType\nMaps Size\nKernel size\nStride Activation\nOut\nFully connected\n\u2013 \u2013\n\u2013\nRBF\nF6\nFully connected\n\u2013 \u2013\n\u2013\ntanh\nC5\nConvolution 1 \u00d7 1\n5 \u00d7 5 tanh\nS4\nAvg pooling 5 \u00d7 5\n2 \u00d7 2 tanh\nC3\nConvolution 10 \u00d7 10 5 \u00d7 5 tanh\nS2\nAvg pooling 14 \u00d7 14 2 \u00d7 2 tanh\nC1\nConvolution 28 \u00d7 28 5 \u00d7 5 tanh\nIn\nInput 32 \u00d7 32 \u2013\n\u2013\n\u2013\nThere are a few extra details to be noted:\n\u2022 MNIST images are 28 \u00d7 28 pixels, but they are zero-padded to 32 \u00d7 32 pixels and\nnormalized before being fed to the network. The rest of the network does not use\nany padding, which is why the size keeps shrinking as the image progresses\nthrough the network. \u2022 The average pooling layers are slightly more complex than usual: each neuron\ncomputes the mean of its inputs, then multiplies the result by a learnable coeffi\u2010\ncient (one per map) and adds a learnable bias term (again, one per map), then\nfinally applies the activation function."
  },
  {
    "id": 307,
    "content": "\u2022 Most neurons in C3 maps are connected to neurons in only three or four S2\nmaps (instead of all six S2 maps). See table 1 (page 8) in the original paper10 for\ndetails. \u2022 The output layer is a bit special: instead of computing the matrix multiplication\nof the inputs and the weight vector, each neuron outputs the square of the Eucli\u2010\ndian distance between its input vector and its weight vector. Each output meas\u2010\nures how much the image belongs to a particular digit class. The cross-entropy\nCNN Architectures | 11 Alex Krizhevsky et al., \u201cImageNet Classification with Deep Convolutional Neural Networks,\u201d _Proceedings of\nthe 25th International Conference on Neural Information Processing Systems 1 (2012): 1097\u20131105. cost function is now preferred, as it penalizes bad predictions much more, pro\u2010\nducing larger gradients and converging faster. Yann LeCun\u2019s website features great demos of LeNet-5 classifying digits. AlexNet\nThe AlexNet CNN architecture11 won the 2012 ImageNet ILSVRC challenge by a\nlarge margin: it achieved a top-five error rate of 17%, while the second best achieved\nonly 26%! It was developed by Alex Krizhevsky (hence the name), Ilya Sutskever, and\nGeoffrey Hinton. It is similar to LeNet-5, only much larger and deeper, and it was the\nfirst to stack convolutional layers directly on top of one another, instead of stacking a\npooling layer on top of each convolutional layer. Table 14-2 presents this architecture. Table 14-2. AlexNet architecture\nLayer\nType\nMaps\nSize\nKernel size Stride Padding Activation\nOut\nFully connected\n\u2013\n1,000\n\u2013\n\u2013\n\u2013\nSoftmax\nF10\nFully connected\n\u2013\n4,096\n\u2013\n\u2013\n\u2013\nReLU\nF9\nFully connected\n\u2013\n4,096\n\u2013\n\u2013\n\u2013\nReLU\nS8\nMax pooling 6 \u00d7 6\n3 \u00d7 3 valid\n\u2013\nC7\nConvolution 13 \u00d7 13\n3 \u00d7 3 same\nReLU\nC6\nConvolution 13 \u00d7 13\n3 \u00d7 3 same\nReLU\nC5\nConvolution 13 \u00d7 13\n3 \u00d7 3 same\nReLU\nS4\nMax pooling 13 \u00d7 13\n3 \u00d7 3 valid\n\u2013\nC3\nConvolution 27 \u00d7 27\n5 \u00d7 5 same\nReLU\nS2\nMax pooling 27 \u00d7 27\n3 \u00d7 3 valid\n\u2013\nC1\nConvolution 55 \u00d7 55\n11 \u00d7 11 valid\nReLU\nIn\nInput\n3 (RGB)\n227 \u00d7 227 \u2013\n\u2013\n\u2013\n\u2013\nTo reduce overfitting, the authors used two regularization techniques. First, they\napplied dropout (introduced in Chapter 11) with a 50% dropout rate during training\nto the outputs of layers F9 and F10. Second, they performed data augmentation by\nrandomly shifting the training images by various offsets, flipping them horizontally,\nand changing the lighting conditions. | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nData Augmentation\nData augmentation artificially increases the size of the training set by generating\nmany realistic variants of each training instance. This reduces overfitting, making this\na regularization technique. The generated instances should be as realistic as possible:\nideally, given an image from the augmented training set, a human should not be able\nto tell whether it was augmented or not."
  },
  {
    "id": 308,
    "content": "Simply adding white noise will not help; the\nmodifications should be learnable (white noise is not). For example, you can slightly shift, rotate, and resize every picture in the training set\nby various amounts and add the resulting pictures to the training set (see\nFigure 14-12). This forces the model to be more tolerant to variations in the position,\norientation, and size of the objects in the pictures. For a model that\u2019s more tolerant of\ndifferent lighting conditions, you can similarly generate many images with various\ncontrasts. In general, you can also flip the pictures horizontally (except for text, and\nother asymmetrical objects). By combining these transformations, you can greatly\nincrease the size of your training set. Figure 14-12. Generating new training instances from existing ones\nAlexNet also uses a competitive normalization step immediately after the ReLU step\nof layers C1 and C3, called local response normalization (LRN): the most strongly acti\u2010\nvated neurons inhibit other neurons located at the same position in neighboring fea\u2010\nture maps (such competitive activation has been observed in biological neurons). This encourages different feature maps to specialize, pushing them apart and forcing\nCNN Architectures | 12 Matthew D. Zeiler and Rob Fergus, \u201cVisualizing and Understanding Convolutional Networks,\u201d Proceedings of\nthe European Conference on Computer Vision (2014): 818-833. 13 Christian Szegedy et al., \u201cGoing Deeper with Convolutions,\u201d Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (2015): 1\u20139. them to explore a wider range of features, ultimately improving generalization. Equa\u2010\ntion 14-2 shows how to apply LRN. Equation 14-2. Local response normalization (LRN)\nbi = ai k + \u03b1 \u2211\nj = jlow\njhigh\naj \u2212\u03b2\nwith\njhigh = min i + r\n2, f n \u22121\njlow = max 0, i \u2212r In this equation:\n\u2022 bi is the normalized output of the neuron located in feature map i, at some row u\nand column v (note that in this equation we consider only neurons located at this\nrow and column, so u and v are not shown). \u2022 ai is the activation of that neuron after the ReLU step, but before normalization. \u2022 k, \u03b1, \u03b2, and r are hyperparameters. k is called the bias, and r is called the depth\nradius. \u2022 fn is the number of feature maps. For example, if r = 2 and a neuron has a strong activation, it will inhibit the activation\nof the neurons located in the feature maps immediately above and below its own. In AlexNet, the hyperparameters are set as follows: r = 2, \u03b1 = 0.00002, \u03b2 = 0.75, and\nk = 1. This step can be implemented using the tf.nn.local_response_normaliza\ntion() function (which you can wrap in a Lambda layer if you want to use it in a\nKeras model). A variant of AlexNet called ZF Net12 was developed by Matthew Zeiler and Rob Fer\u2010\ngus and won the 2013 ILSVRC challenge. It is essentially AlexNet with a few tweaked\nhyperparameters (number of feature maps, kernel size, stride, etc.)."
  },
  {
    "id": 309,
    "content": "GoogLeNet\nThe GoogLeNet architecture was developed by Christian Szegedy et al. from Google\nResearch,13 and it won the ILSVRC 2014 challenge by pushing the top-five error rate\nbelow 7%. This great performance came in large part from the fact that the network\nwas much deeper than previous CNNs (as you\u2019ll see in Figure 14-14). This was made | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n14 In the 2010 movie Inception, the characters keep going deeper and deeper into multiple layers of dreams;\nhence the name of these modules. possible by subnetworks called inception modules,14 which allow GoogLeNet to use\nparameters much more efficiently than previous architectures: GoogLeNet actually\nhas 10 times fewer parameters than AlexNet (roughly 6 million instead of 60 million). Figure 14-13 shows the architecture of an inception module. The notation \u201c3 \u00d7 3 +\n1(S)\u201d means that the layer uses a 3 \u00d7 3 kernel, stride 1, and \"same\" padding. The input\nsignal is first copied and fed to four different layers. All convolutional layers use the\nReLU activation function. Note that the second set of convolutional layers uses differ\u2010\nent kernel sizes (1 \u00d7 1, 3 \u00d7 3, and 5 \u00d7 5), allowing them to capture patterns at different\nscales. Also note that every single layer uses a stride of 1 and \"same\" padding (even\nthe max pooling layer), so their outputs all have the same height and width as their\ninputs. This makes it possible to concatenate all the outputs along the depth dimen\u2010\nsion in the final depth concatenation layer (i.e., stack the feature maps from all four\ntop convolutional layers). This concatenation layer can be implemented in Tensor\u2010\nFlow using the tf.concat() operation, with axis=3 (the axis is the depth). Figure 14-13. Inception module\nYou may wonder why inception modules have convolutional layers with 1 \u00d7 1 ker\u2010\nnels. Surely these layers cannot capture any features because they look at only one\npixel at a time? In fact, the layers serve three purposes:\n\u2022 Although they cannot capture spatial patterns, they can capture patterns along\nthe depth dimension. \u2022 They are configured to output fewer feature maps than their inputs, so they serve\nas bottleneck layers, meaning they reduce dimensionality. This cuts the computa\u2010\nCNN Architectures | tional cost and the number of parameters, speeding up training and improving\ngeneralization. \u2022 Each pair of convolutional layers ([1 \u00d7 1, 3 \u00d7 3] and [1 \u00d7 1, 5 \u00d7 5]) acts like a\nsingle powerful convolutional layer, capable of capturing more complex patterns. Indeed, instead of sweeping a simple linear classifier across the image (as a single\nconvolutional layer does), this pair of convolutional layers sweeps a two-layer\nneural network across the image. In short, you can think of the whole inception module as a convolutional layer on\nsteroids, able to output feature maps that capture complex patterns at various scales. The number of convolutional kernels for each convolutional layer\nis a hyperparameter."
  },
  {
    "id": 310,
    "content": "Unfortunately, this means that you have six\nmore hyperparameters to tweak for every inception layer you add. Now let\u2019s look at the architecture of the GoogLeNet CNN (see Figure 14-14). The\nnumber of feature maps output by each convolutional layer and each pooling layer is\nshown before the kernel size. The architecture is so deep that it has to be represented\nin three columns, but GoogLeNet is actually one tall stack, including nine inception\nmodules (the boxes with the spinning tops). The six numbers in the inception mod\u2010\nules represent the number of feature maps output by each convolutional layer in the\nmodule (in the same order as in Figure 14-13). Note that all the convolutional layers\nuse the ReLU activation function. | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nFigure 14-14. GoogLeNet architecture\nLet\u2019s go through this network:\n\u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided\nby 16), to reduce the computational load. The first layer uses a large kernel size so\nthat much of the information is preserved. \u2022 Then the local response normalization layer ensures that the previous layers learn\na wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As\nexplained earlier, you can think of this pair as a single smarter convolutional\nlayer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010\nture a wide variety of patterns. \u2022 Next, a max pooling layer reduces the image height and width by 2, again to\nspeed up computations. \u2022 Then comes the tall stack of nine inception modules, interleaved with a couple\nmax pooling layers to reduce dimensionality and speed up the net. CNN Architectures | 15 Karen Simonyan and Andrew Zisserman, \u201cVery Deep Convolutional Networks for Large-Scale Image Recog\u2010\nnition,\u201d arXiv preprint arXiv:1409.1556 (2014). \u2022 Next, the global average pooling layer outputs the mean of each feature map: this\ndrops any remaining spatial information, which is fine because there was not\nmuch spatial information left at that point. Indeed, GoogLeNet input images are\ntypically expected to be 224 \u00d7 224 pixels, so after 5 max pooling layers, each\ndividing the height and width by 2, the feature maps are down to 7 \u00d7 7. More\u2010\nover, it is a classification task, not localization, so it does not matter where the\nobject is. Thanks to the dimensionality reduction brought by this layer, there is\nno need to have several fully connected layers at the top of the CNN (like in\nAlexNet), and this considerably reduces the number of parameters in the net\u2010\nwork and limits the risk of overfitting. \u2022 The last layers are self-explanatory: dropout for regularization, then a fully con\u2010\nnected layer with 1,000 units (since there are 1,000 classes) and a softmax activa\u2010\ntion function to output estimated class probabilities."
  },
  {
    "id": 311,
    "content": "This diagram is slightly simplified: the original GoogLeNet architecture also included\ntwo auxiliary classifiers plugged on top of the third and sixth inception modules. They were both composed of one average pooling layer, one convolutional layer, two\nfully connected layers, and a softmax activation layer. During training, their loss\n(scaled down by 70%) was added to the overall loss. The goal was to fight the vanish\u2010\ning gradients problem and regularize the network. However, it was later shown that\ntheir effect was relatively minor. Several variants of the GoogLeNet architecture were later proposed by Google\nresearchers, including Inception-v3 and Inception-v4, using slightly different incep\u2010\ntion modules and reaching even better performance. VGGNet\nThe runner-up in the ILSVRC 2014 challenge was VGGNet,15 developed by Karen\nSimonyan and Andrew Zisserman from the Visual Geometry Group (VGG) research\nlab at Oxford University. It had a very simple and classical architecture, with 2 or 3\nconvolutional layers and a pooling layer, then again 2 or 3 convolutional layers and a\npooling layer, and so on (reaching a total of just 16 or 19 convolutional layers,\ndepending on the VGG variant), plus a final dense network with 2 hidden layers and\nthe output layer. It used only 3 \u00d7 3 filters, but many filters. | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n16 Kaiming He et al., \u201cDeep Residual Learning for Image Recognition,\u201d arXiv preprint arXiv:1512:03385 (2015). ResNet\nKaiming He et al. won the ILSVRC 2015 challenge using a Residual Network (or\nResNet),16 that delivered an astounding top-five error rate under 3.6%. The winning\nvariant used an extremely deep CNN composed of 152 layers (other variants had 34,\n50, and 101 layers). It confirmed the general trend: models are getting deeper and\ndeeper, with fewer and fewer parameters. The key to being able to train such a deep\nnetwork is to use skip connections (also called shortcut connections): the signal feeding\ninto a layer is also added to the output of a layer located a bit higher up the stack. Let\u2019s\nsee why this is useful. When training a neural network, the goal is to make it model a target function h(x). If you add the input x to the output of the network (i.e., you add a skip connection),\nthen the network will be forced to model f(x) = h(x) \u2013 x rather than h(x). This is\ncalled residual learning (see Figure 14-15). Figure 14-15. Residual learning\nWhen you initialize a regular neural network, its weights are close to zero, so the net\u2010\nwork just outputs values close to zero. If you add a skip connection, the resulting net\u2010\nwork just outputs a copy of its inputs; in other words, it initially models the identity\nfunction. If the target function is fairly close to the identity function (which is often\nthe case), this will speed up training considerably."
  },
  {
    "id": 312,
    "content": "Moreover, if you add many skip connections, the network can start making progress\neven if several layers have not started learning yet (see Figure 14-16). Thanks to skip\nconnections, the signal can easily make its way across the whole network. The deep\nresidual network can be seen as a stack of residual units (RUs), where each residual\nunit is a small neural network with a skip connection. CNN Architectures | Figure 14-16. Regular deep neural network (left) and deep residual network (right)\nNow let\u2019s look at ResNet\u2019s architecture (see Figure 14-17). It is surprisingly simple. It\nstarts and ends exactly like GoogLeNet (except without a dropout layer), and in\nbetween is just a very deep stack of simple residual units. Each residual unit is com\u2010\nposed of two convolutional layers (and no pooling layer! ), with Batch Normalization\n(BN) and ReLU activation, using 3 \u00d7 3 kernels and preserving spatial dimensions\n(stride 1, \"same\" padding). | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nFigure 14-17. ResNet architecture\nNote that the number of feature maps is doubled every few residual units, at the same\ntime as their height and width are halved (using a convolutional layer with stride 2). When this happens, the inputs cannot be added directly to the outputs of the residual\nunit because they don\u2019t have the same shape (for example, this problem affects the\nskip connection represented by the dashed arrow in Figure 14-17). To solve this prob\u2010\nlem, the inputs are passed through a 1 \u00d7 1 convolutional layer with stride 2 and the\nright number of output feature maps (see Figure 14-18). Figure 14-18. Skip connection when changing feature map size and depth\nCNN Architectures | 17 It is a common practice when describing a neural network to count only layers with parameters. 18 Christian Szegedy et al., \u201cInception\u2013v4, Inception-ResNet and the Impact of Residual Connections on Learn\u2010\ning,\u201d arXiv preprint arXiv:1602.07261 (2016). 19 Fran\u00e7ois Chollet, \u201cXception: Deep Learning with Depthwise Separable Convolutions,\u201d arXiv preprint arXiv:\n1610.02357 (2016). 20 This name can sometimes be ambiguous, since spatially separable convolutions are often called \u201cseparable\nconvolutions\u201d as well. ResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and\nthe fully connected layer)17 containing 3 residual units that output 64 feature maps, 4\nRUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. We will imple\u2010\nment this architecture later in this chapter. ResNets deeper than that, such as ResNet-152, use slightly different residual units. Instead of two 3 \u00d7 3 convolutional layers with, say, 256 feature maps, they use three\nconvolutional layers: first a 1 \u00d7 1 convolutional layer with just 64 feature maps (4\ntimes less), which acts as a bottleneck layer (as discussed already), then a 3 \u00d7 3 layer\nwith 64 feature maps, and finally another 1 \u00d7 1 convolutional layer with 256 feature\nmaps (4 times 64) that restores the original depth."
  },
  {
    "id": 313,
    "content": "ResNet-152 contains 3 such RUs\nthat output 256 maps, then 8 RUs with 512 maps, a whopping 36 RUs with 1,024\nmaps, and finally 3 RUs with 2,048 maps. Google\u2019s Inception-v418 architecture merged the ideas of GoogLe\u2010\nNet and ResNet and achieved a top-five error rate of close to 3% on\nImageNet classification. Xception\nAnother variant of the GoogLeNet architecture is worth noting: Xception19 (which\nstands for Extreme Inception) was proposed in 2016 by Fran\u00e7ois Chollet (the author\nof Keras), and it significantly outperformed Inception-v3 on a huge vision task (350\nmillion images and 17,000 classes). Just like Inception-v4, it merges the ideas of Goo\u2010\ngLeNet and ResNet, but it replaces the inception modules with a special type of layer\ncalled a depthwise separable convolution layer (or separable convolution layer for\nshort20). These layers had been used before in some CNN architectures, but they were\nnot as central as in the Xception architecture. While a regular convolutional layer\nuses filters that try to simultaneously capture spatial patterns (e.g., an oval) and cross-\nchannel patterns (e.g., mouth + nose + eyes = face), a separable convolutional layer\nmakes the strong assumption that spatial patterns and cross-channel patterns can be\nmodeled separately (see Figure 14-19). Thus, it is composed of two parts: the first part\napplies a single spatial filter for each input feature map, then the second part looks | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nexclusively for cross-channel patterns\u2014it is just a regular convolutional layer with 1 \u00d7\n1 filters. Figure 14-19. Depthwise separable convolutional layer\nSince separable convolutional layers only have one spatial filter per input channel,\nyou should avoid using them after layers that have too few channels, such as the input\nlayer (granted, that\u2019s what Figure 14-19 represents, but it is just for illustration pur\u2010\nposes). For this reason, the Xception architecture starts with 2 regular convolutional\nlayers, but then the rest of the architecture uses only separable convolutions (34 in\nall), plus a few max pooling layers and the usual final layers (a global average pooling\nlayer and a dense output layer). You might wonder why Xception is considered a variant of GoogLeNet, since it con\u2010\ntains no inception module at all. Well, as we discussed earlier, an inception module\ncontains convolutional layers with 1 \u00d7 1 filters: these look exclusively for cross-\nchannel patterns. However, the convolutional layers that sit on top of them are regu\u2010\nlar convolutional layers that look both for spatial and cross-channel patterns. So you\ncan think of an inception module as an intermediate between a regular convolutional\nlayer (which considers spatial patterns and cross-channel patterns jointly) and a sepa\u2010\nrable convolutional layer (which considers them separately). In practice, it seems that\nseparable convolutional layers generally perform better. CNN Architectures | 21 Xingyu Zeng et al., \u201cCrafting GBD-Net for Object Detection,\u201d IEEE Transactions on Pattern Analysis and\nMachine Intelligence 40, no. 9 (2018): 2109\u20132123."
  },
  {
    "id": 314,
    "content": "22 Jie Hu et al., \u201cSqueeze-and-Excitation Networks,\u201d Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (2018): 7132\u20137141. Separable convolutional layers use fewer parameters, less memory,\nand fewer computations than regular convolutional layers, and in\ngeneral they even perform better, so you should consider using\nthem by default (except after layers with few channels). The ILSVRC 2016 challenge was won by the CUImage team from the Chinese Uni\u2010\nversity of Hong Kong. They used an ensemble of many different techniques, includ\u2010\ning a sophisticated object-detection system called GBD-Net,21 to achieve a top-five\nerror rate below 3%. Although this result is unquestionably impressive, the complex\u2010\nity of the solution contrasted with the simplicity of ResNets. Moreover, one year later\nanother fairly simple architecture performed even better, as we will see now. SENet\nThe winning architecture in the ILSVRC 2017 challenge was the Squeeze-and-\nExcitation Network (SENet).22 This architecture extends existing architectures such as\ninception networks and ResNets, and boosts their performance. This allowed SENet\nto win the competition with an astonishing 2.25% top-five error rate! The extended\nversions of inception networks and ResNets are called SE-Inception and SE-ResNet,\nrespectively. The boost comes from the fact that a SENet adds a small neural network,\ncalled an SE block, to every unit in the original architecture (i.e., every inception\nmodule or every residual unit), as shown in Figure 14-20. Figure 14-20. SE-Inception module (left) and SE-ResNet unit (right) | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nAn SE block analyzes the output of the unit it is attached to, focusing exclusively on\nthe depth dimension (it does not look for any spatial pattern), and it learns which fea\u2010\ntures are usually most active together. It then uses this information to recalibrate the\nfeature maps, as shown in Figure 14-21. For example, an SE block may learn that\nmouths, noses, and eyes usually appear together in pictures: if you see a mouth and a\nnose, you should expect to see eyes as well. So if the block sees a strong activation in\nthe mouth and nose feature maps, but only mild activation in the eye feature map, it\nwill boost the eye feature map (more accurately, it will reduce irrelevant feature\nmaps). If the eyes were somewhat confused with something else, this feature map\nrecalibration will help resolve the ambiguity. Figure 14-21. An SE block performs feature map recalibration\nAn SE block is composed of just three layers: a global average pooling layer, a hidden\ndense layer using the ReLU activation function, and a dense output layer using the\nsigmoid activation function (see Figure 14-22). Figure 14-22. SE block architecture\nAs earlier, the global average pooling layer computes the mean activation for each fea\u2010\nture map: for example, if its input contains 256 feature maps, it will output 256\nCNN Architectures | numbers representing the overall level of response for each filter."
  },
  {
    "id": 315,
    "content": "The next layer is\nwhere the \u201csqueeze\u201d happens: this layer has significantly fewer than 256 neurons\u2014\ntypically 16 times fewer than the number of feature maps (e.g., 16 neurons)\u2014so the\n256 numbers get compressed into a small vector (e.g., 16 dimensions). This is a low-\ndimensional vector representation (i.e., an embedding) of the distribution of feature\nresponses. This bottleneck step forces the SE block to learn a general representation\nof the feature combinations (we will see this principle in action again when we dis\u2010\ncuss autoencoders in Chapter 17). Finally, the output layer takes the embedding and\noutputs a recalibration vector containing one number per feature map (e.g., 256),\neach between 0 and 1. The feature maps are then multiplied by this recalibration vec\u2010\ntor, so irrelevant features (with a low recalibration score) get scaled down while rele\u2010\nvant features (with a recalibration score close to 1) are left alone. Implementing a ResNet-34 CNN Using Keras\nMost CNN architectures described so far are fairly straightforward to implement\n(although generally you would load a pretrained network instead, as we will see). To\nillustrate the process, let\u2019s implement a ResNet-34 from scratch using Keras. First, let\u2019s\ncreate a ResidualUnit layer:\nclass ResidualUnit(keras.layers.Layer): def __init__(self, filters, strides=1, activation=\"relu\", **kwargs): super().__init__(**kwargs) self.activation = keras.activations.get(activation) self.main_layers = [ keras.layers.Conv2D(filters, 3, strides=strides, padding=\"same\", use_bias=False), keras.layers.BatchNormalization(), self.activation, keras.layers.Conv2D(filters, 3, strides=1, padding=\"same\", use_bias=False), keras.layers.BatchNormalization()] self.skip_layers = [] if strides > 1: self.skip_layers = [ keras.layers.Conv2D(filters, 1, strides=strides, padding=\"same\", use_bias=False), keras.layers.BatchNormalization()] def call(self, inputs): Z = inputs for layer in self.main_layers: Z = layer(Z) skip_Z = inputs for layer in self.skip_layers: skip_Z = layer(skip_Z) return self.activation(Z + skip_Z) | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nAs you can see, this code matches Figure 14-18 pretty closely. In the constructor, we\ncreate all the layers we will need: the main layers are the ones on the right side of the\ndiagram, and the skip layers are the ones on the left (only needed if the stride is\ngreater than 1). Then in the call() method, we make the inputs go through the main\nlayers and the skip layers (if any), then we add both outputs and apply the activation\nfunction. Next, we can build the ResNet-34 using a Sequential model, since it\u2019s really just a\nlong sequence of layers (we can treat each residual unit as a single layer now that we\nhave the ResidualUnit class):\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Conv2D(64, 7, strides=2, input_shape=[224, 224, 3], padding=\"same\", use_bias=False))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Activation(\"relu\"))\nmodel.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"))\nprev_filters = 64\nfor filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3: strides = 1 if filters == prev_filters else 2 model.add(ResidualUnit(filters, strides=strides)) prev_filters = filters\nmodel.add(keras.layers.GlobalAvgPool2D())\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(10, activation=\"softmax\"))\nThe only slightly tricky part in this code is the loop that adds the ResidualUnit layers\nto the model: as explained earlier, the first 3 RUs have 64 filters, then the next 4 RUs\nhave 128 filters, and so on."
  },
  {
    "id": 316,
    "content": "We then set the stride to 1 when the number of filters is\nthe same as in the previous RU, or else we set it to 2. Then we add the ResidualUnit,\nand finally we update prev_filters. It is amazing that in fewer than 40 lines of code, we can build the model that won the\nILSVRC 2015 challenge! This demonstrates both the elegance of the ResNet model\nand the expressiveness of the Keras API. Implementing the other CNN architectures\nis not much harder. However, Keras comes with several of these architectures built in,\nso why not use them instead? Using Pretrained Models from Keras\nIn general, you won\u2019t have to implement standard models like GoogLeNet or ResNet\nmanually, since pretrained networks are readily available with a single line of code in\nthe keras.applications package. For example, you can load the ResNet-50 model,\npretrained on ImageNet, with the following line of code:\nmodel = keras.applications.resnet50.ResNet50(weights=\"imagenet\")\nUsing Pretrained Models from Keras | 23 In the ImageNet dataset, each image is associated to a word in the WordNet dataset: the class ID is just a\nWordNet ID. That\u2019s all! This will create a ResNet-50 model and download weights pretrained on\nthe ImageNet dataset. To use it, you first need to ensure that the images have the right\nsize. A ResNet-50 model expects 224 \u00d7 224-pixel images (other models may expect\nother sizes, such as 299 \u00d7 299), so let\u2019s use TensorFlow\u2019s tf.image.resize() function\nto resize the images we loaded earlier:\nimages_resized = tf.image.resize(images, [224, 224])\nThe tf.image.resize() will not preserve the aspect ratio. If this is\na problem, try cropping the images to the appropriate aspect ratio\nbefore resizing. Both operations can be done in one shot with\ntf.image.crop_and_resize(). The pretrained models assume that the images are preprocessed in a specific way. In\nsome cases they may expect the inputs to be scaled from 0 to 1, or \u20131 to 1, and so on. Each model provides a preprocess_input() function that you can use to preprocess\nyour images. These functions assume that the pixel values range from 0 to 255, so we\nmust multiply them by 255 (since earlier we scaled them to the 0\u20131 range):\ninputs = keras.applications.resnet50.preprocess_input(images_resized * 255)\nNow we can use the pretrained model to make predictions:\nY_proba = model.predict(inputs)\nAs usual, the output Y_proba is a matrix with one row per image and one column per\nclass (in this case, there are 1,000 classes). If you want to display the top K predic\u2010\ntions, including the class name and the estimated probability of each predicted class,\nuse the decode_predictions() function."
  },
  {
    "id": 317,
    "content": "For each image, it returns an array contain\u2010\ning the top K predictions, where each prediction is represented as an array containing\nthe class identifier,23 its name, and the corresponding confidence score:\ntop_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)\nfor image_index in range(len(images)): print(\"Image #{}\".format(image_index)) for class_id, name, y_proba in top_K[image_index]: print(\" {} - {:12s} {:.2f}%\".format(class_id, name, y_proba * 100)) print()\nThe output looks like this:\nImage #0 n03877845 - palace 42.87% n02825657 - bell_cote 40.57% n03781244 - monastery 14.56% | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nImage #1 n04522168 - vase 46.83% n07930864 - cup 7.78% n11939491 - daisy 4.87%\nThe correct classes (monastery and daisy) appear in the top three results for both\nimages. That\u2019s pretty good, considering that the model had to choose from among\n1,000 classes. As you can see, it is very easy to create a pretty good image classifier using a pre\u2010\ntrained model. Other vision models are available in keras.applications, including\nseveral ResNet variants, GoogLeNet variants like Inception-v3 and Xception,\nVGGNet variants, and MobileNet and MobileNetV2 (lightweight models for use in\nmobile applications). But what if you want to use an image classifier for classes of images that are not part\nof ImageNet? In that case, you may still benefit from the pretrained models to per\u2010\nform transfer learning. Pretrained Models for Transfer Learning\nIf you want to build an image classifier but you do not have enough training data,\nthen it is often a good idea to reuse the lower layers of a pretrained model, as we dis\u2010\ncussed in Chapter 11. For example, let\u2019s train a model to classify pictures of flowers,\nreusing a pretrained Xception model. First, let\u2019s load the dataset using TensorFlow\nDatasets (see Chapter 13):\nimport tensorflow_datasets as tfds\ndataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)\ndataset_size = info.splits[\"train\"].num_examples # 3670\nclass_names = info.features[\"label\"].names # [\"dandelion\", \"daisy\", ...]\nn_classes = info.features[\"label\"].num_classes # 5\nNote that you can get information about the dataset by setting with_info=True. Here,\nwe get the dataset size and the names of the classes. Unfortunately, there is only a\n\"train\" dataset, no test set or validation set, so we need to split the training set. The\nTF Datasets project provides an API for this. For example, let\u2019s take the first 10% of\nthe dataset for testing, the next 15% for validation, and the remaining 75% for\ntraining:\ntest_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75])\ntest_set = tfds.load(\"tf_flowers\", split=test_split, as_supervised=True)\nvalid_set = tfds.load(\"tf_flowers\", split=valid_split, as_supervised=True)\ntrain_set = tfds.load(\"tf_flowers\", split=train_split, as_supervised=True)\nPretrained Models for Transfer Learning | Next we must preprocess the images. The CNN expects 224 \u00d7 224 images, so we need\nto resize them."
  },
  {
    "id": 318,
    "content": "We also need to run the images through Xception\u2019s prepro\ncess_input() function:\ndef preprocess(image, label): resized_image = tf.image.resize(image, [224, 224]) final_image = keras.applications.xception.preprocess_input(resized_image) return final_image, label\nLet\u2019s apply this preprocessing function to all three datasets, shuffle the training set,\nand add batching and prefetching to all the datasets:\nbatch_size = 32\ntrain_set = train_set.shuffle(1000)\ntrain_set = train_set.map(preprocess).batch(batch_size).prefetch(1)\nvalid_set = valid_set.map(preprocess).batch(batch_size).prefetch(1)\ntest_set = test_set.map(preprocess).batch(batch_size).prefetch(1)\nIf you want to perform some data augmentation, change the preprocessing function\nfor the training set, adding some random transformations to the training images. For\nexample, use tf.image.random_crop() to randomly crop the images, use\ntf.image.random_flip_left_right() to randomly flip the images horizontally, and\nso on (see the \u201cPretrained Models for Transfer Learning\u201d section of the notebook for\nan example). The keras.preprocessing.image.ImageDataGenerator class\nmakes it easy to load images from disk and augment them in vari\u2010\nous ways: you can shift each image, rotate it, rescale it, flip it hori\u2010\nzontally or vertically, shear it, or apply any transformation function\nyou want to it. This is very convenient for simple projects. How\u2010\never, building a tf.data pipeline has many advantages: it can read\nthe images efficiently (e.g., in parallel) from any source, not just the\nlocal disk; you can manipulate the Dataset as you wish; and if you\nwrite a preprocessing function based on tf.image operations, this\nfunction can be used both in the tf.data pipeline and in the model\nyou will deploy to production (see Chapter 19). Next let\u2019s load an Xception model, pretrained on ImageNet. We exclude the top of the\nnetwork by setting include_top=False: this excludes the global average pooling layer\nand the dense output layer. We then add our own global average pooling layer, based\non the output of the base model, followed by a dense output layer with one unit per\nclass, using the softmax activation function. Finally, we create the Keras Model:\nbase_model = keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\navg = keras.layers.GlobalAveragePooling2D()(base_model.output)\noutput = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\nmodel = keras.Model(inputs=base_model.input, outputs=output) | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nAs explained in Chapter 11, it\u2019s usually a good idea to freeze the weights of the pre\u2010\ntrained layers, at least at the beginning of training:\nfor layer in base_model.layers: layer.trainable = False\nSince our model uses the base model\u2019s layers directly, rather than\nthe base_model object itself, setting base_model.trainable=False\nwould have no effect. Finally, we can compile the model and start training:\noptimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\nhistory = model.fit(train_set, epochs=5, validation_data=valid_set)\nThis will be very slow, unless you have a GPU. If you do not, then\nyou should run this chapter\u2019s notebook in Colab, using a GPU run\u2010\ntime (it\u2019s free!). See the instructions at \nhandson-ml2. After training the model for a few epochs, its validation accuracy should reach about\n75\u201380% and stop making much progress."
  },
  {
    "id": 319,
    "content": "This means that the top layers are now\npretty well trained, so we are ready to unfreeze all the layers (or you could try\nunfreezing just the top ones) and continue training (don\u2019t forget to compile the\nmodel when you freeze or unfreeze layers). This time we use a much lower learning\nrate to avoid damaging the pretrained weights:\nfor layer in base_model.layers: layer.trainable = True\noptimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.001)\nmodel.compile(...)\nhistory = model.fit(...)\nIt will take a while, but this model should reach around 95% accuracy on the test set. With that, you can start training amazing image classifiers! But there\u2019s more to com\u2010\nputer vision than just classification. For example, what if you also want to know where\nthe flower is in the picture? Let\u2019s look at this now. Classification and Localization\nLocalizing an object in a picture can be expressed as a regression task, as discussed in\nChapter 10: to predict a bounding box around the object, a common approach is to\nClassification and Localization | 24 Adriana Kovashka et al., \u201cCrowdsourcing in Computer Vision,\u201d Foundations and Trends in Computer Graphics\nand Vision 10, no. 3 (2014): 177\u2013243. predict the horizontal and vertical coordinates of the object\u2019s center, as well as its\nheight and width. This means we have four numbers to predict. It does not require\nmuch change to the model; we just need to add a second dense output layer with four\nunits (typically on top of the global average pooling layer), and it can be trained using\nthe MSE loss:\nbase_model = keras.applications.xception.Xception(weights=\"imagenet\", include_top=False)\navg = keras.layers.GlobalAveragePooling2D()(base_model.output)\nclass_output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\nloc_output = keras.layers.Dense(4)(avg)\nmodel = keras.Model(inputs=base_model.input, outputs=[class_output, loc_output])\nmodel.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"], loss_weights=[0.8, 0.2], # depends on what you care most about optimizer=optimizer, metrics=[\"accuracy\"])\nBut now we have a problem: the flowers dataset does not have bounding boxes\naround the flowers. So, we need to add them ourselves. This is often one of the hard\u2010\nest and most costly parts of a Machine Learning project: getting the labels. It\u2019s a good\nidea to spend time looking for the right tools. To annotate images with bounding\nboxes, you may want to use an open source image labeling tool like VGG Image\nAnnotator, LabelImg, OpenLabeler, or ImgLab, or perhaps a commercial tool like\nLabelBox or Supervisely. You may also want to consider crowdsourcing platforms\nsuch as Amazon Mechanical Turk if you have a very large number of images to anno\u2010\ntate. However, it is quite a lot of work to set up a crowdsourcing platform, prepare the\nform to be sent to the workers, supervise them, and ensure that the quality of the\nbounding boxes they produce is good, so make sure it is worth the effort. If there are\njust a few thousand images to label, and you don\u2019t plan to do this frequently, it may be\npreferable to do it yourself. Adriana Kovashka et al. wrote a very practical paper24\nabout crowdsourcing in computer vision."
  },
  {
    "id": 320,
    "content": "I recommend you check it out, even if you\ndo not plan to use crowdsourcing. Let\u2019s suppose you\u2019ve obtained the bounding boxes for every image in the flowers data\u2010\nset (for now we will assume there is a single bounding box per image). You then need\nto create a dataset whose items will be batches of preprocessed images along with\ntheir class labels and their bounding boxes. Each item should be a tuple of the form\n(images, (class_labels, bounding_boxes)). Then you are ready to train your\nmodel! | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nThe bounding boxes should be normalized so that the horizontal\nand vertical coordinates, as well as the height and width, all range\nfrom 0 to 1. Also, it is common to predict the square root of the\nheight and width rather than the height and width directly: this\nway, a 10-pixel error for a large bounding box will not be penalized\nas much as a 10-pixel error for a small bounding box. The MSE often works fairly well as a cost function to train the model, but it is not a\ngreat metric to evaluate how well the model can predict bounding boxes. The most\ncommon metric for this is the Intersection over Union (IoU): the area of overlap\nbetween the predicted bounding box and the target bounding box, divided by the\narea of their union (see Figure 14-23). In tf.keras, it is implemented by the\ntf.keras.metrics.MeanIoU class. Figure 14-23. Intersection over Union (IoU) metric for bounding boxes\nClassifying and localizing a single object is nice, but what if the images contain multi\u2010\nple objects (as is often the case in the flowers dataset)? Object Detection\nThe task of classifying and localizing multiple objects in an image is called object\ndetection. Until a few years ago, a common approach was to take a CNN that was\ntrained to classify and locate a single object, then slide it across the image, as shown\nin Figure 14-24. In this example, the image was chopped into a 6 \u00d7 8 grid, and we\nshow a CNN (the thick black rectangle) sliding across all 3 \u00d7 3 regions. When the\nCNN was looking at the top left of the image, it detected part of the leftmost rose, and\nthen it detected that same rose again when it was first shifted one step to the right. At\nObject Detection | the next step, it started detecting part of the topmost rose, and then it detected it\nagain once it was shifted one more step to the right. You would then continue to slide\nthe CNN through the whole image, looking at all 3 \u00d7 3 regions. Moreover, since\nobjects can have varying sizes, you would also slide the CNN across regions of differ\u2010\nent sizes. For example, once you are done with the 3 \u00d7 3 regions, you might want to\nslide the CNN across all 4 \u00d7 4 regions as well. Figure 14-24."
  },
  {
    "id": 321,
    "content": "Detecting multiple objects by sliding a CNN across the image\nThis technique is fairly straightforward, but as you can see it will detect the same\nobject multiple times, at slightly different positions. Some post-processing will then\nbe needed to get rid of all the unnecessary bounding boxes. A common approach for\nthis is called non-max suppression. Here\u2019s how you do it:\n1. First, you need to add an extra objectness output to your CNN, to estimate the\nprobability that a flower is indeed present in the image (alternatively, you could\nadd a \u201cno-flower\u201d class, but this usually does not work as well). It must use the\nsigmoid activation function, and you can train it using binary cross-entropy loss. Then get rid of all the bounding boxes for which the objectness score is below\nsome threshold: this will drop all the bounding boxes that don\u2019t actually contain a\nflower. 2. Find the bounding box with the highest objectness score, and get rid of all the\nother bounding boxes that overlap a lot with it (e.g., with an IoU greater than\n60%). For example, in Figure 14-24, the bounding box with the max objectness\nscore is the thick bounding box over the topmost rose (the objectness score is\nrepresented by the thickness of the bounding boxes). The other bounding box | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n25 Jonathan Long et al., \u201cFully Convolutional Networks for Semantic Segmentation,\u201d Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (2015): 3431\u20133440. 26 There is one small exception: a convolutional layer using \"valid\" padding will complain if the input size is\nsmaller than the kernel size. over that same rose overlaps a lot with the max bounding box, so we will get rid\nof it. 3. Repeat step two until there are no more bounding boxes to get rid of. This simple approach to object detection works pretty well, but it requires running\nthe CNN many times, so it is quite slow. Fortunately, there is a much faster way to\nslide a CNN across an image: using a fully convolutional network (FCN). Fully Convolutional Networks\nThe idea of FCNs was first introduced in a 2015 paper25 by Jonathan Long et al., for\nsemantic segmentation (the task of classifying every pixel in an image according to\nthe class of the object it belongs to). The authors pointed out that you could replace\nthe dense layers at the top of a CNN by convolutional layers. To understand this, let\u2019s\nlook at an example: suppose a dense layer with 200 neurons sits on top of a convolu\u2010\ntional layer that outputs 100 feature maps, each of size 7 \u00d7 7 (this is the feature map\nsize, not the kernel size). Each neuron will compute a weighted sum of all 100 \u00d7 7 \u00d7 7\nactivations from the convolutional layer (plus a bias term)."
  },
  {
    "id": 322,
    "content": "Now let\u2019s see what hap\u2010\npens if we replace the dense layer with a convolutional layer using 200 filters, each of\nsize 7 \u00d7 7, and with \"valid\" padding. This layer will output 200 feature maps, each 1\n\u00d7 1 (since the kernel is exactly the size of the input feature maps and we are using\n\"valid\" padding). In other words, it will output 200 numbers, just like the dense\nlayer did; and if you look closely at the computations performed by a convolutional\nlayer, you will notice that these numbers will be precisely the same as those the dense\nlayer produced. The only difference is that the dense layer\u2019s output was a tensor of\nshape [batch size, 200], while the convolutional layer will output a tensor of shape\n[batch size, 1, 1, 200]. To convert a dense layer to a convolutional layer, the number of fil\u2010\nters in the convolutional layer must be equal to the number of units\nin the dense layer, the filter size must be equal to the size of the\ninput feature maps, and you must use \"valid\" padding. The stride\nmay be set to 1 or more, as we will see shortly. Why is this important? Well, while a dense layer expects a specific input size (since it\nhas one weight per input feature), a convolutional layer will happily process images of\nany size26 (however, it does expect its inputs to have a specific number of channels,\nObject Detection | 27 This assumes we used only \"same\" padding in the network: indeed, \"valid\" padding would reduce the size of\nthe feature maps. Moreover, 448 can be neatly divided by 2 several times until we reach 7, without any round\u2010\ning error. If any layer uses a different stride than 1 or 2, then there may be some rounding error, so again the\nfeature maps may end up being smaller. since each kernel contains a different set of weights for each input channel). Since an\nFCN contains only convolutional layers (and pooling layers, which have the same\nproperty), it can be trained and executed on images of any size! For example, suppose we\u2019d already trained a CNN for flower classification and locali\u2010\nzation. It was trained on 224 \u00d7 224 images, and it outputs 10 numbers: outputs 0 to 4\nare sent through the softmax activation function, and this gives the class probabilities\n(one per class); output 5 is sent through the logistic activation function, and this gives\nthe objectness score; outputs 6 to 9 do not use any activation function, and they rep\u2010\nresent the bounding box\u2019s center coordinates, as well as its height and width. We can\nnow convert its dense layers to convolutional layers. In fact, we don\u2019t even need to\nretrain it; we can just copy the weights from the dense layers to the convolutional lay\u2010\ners! Alternatively, we could have converted the CNN into an FCN before training."
  },
  {
    "id": 323,
    "content": "Now suppose the last convolutional layer before the output layer (also called the bot\u2010\ntleneck layer) outputs 7 \u00d7 7 feature maps when the network is fed a 224 \u00d7 224 image\n(see the left side of Figure 14-25). If we feed the FCN a 448 \u00d7 448 image (see the right\nside of Figure 14-25), the bottleneck layer will now output 14 \u00d7 14 feature maps.27\nSince the dense output layer was replaced by a convolutional layer using 10 filters of\nsize 7 \u00d7 7, with \"valid\" padding and stride 1, the output will be composed of 10 fea\u2010\ntures maps, each of size 8 \u00d7 8 (since 14 \u2013 7 + 1 = 8). In other words, the FCN will\nprocess the whole image only once, and it will output an 8 \u00d7 8 grid where each cell\ncontains 10 numbers (5 class probabilities, 1 objectness score, and 4 bounding box\ncoordinates). It\u2019s exactly like taking the original CNN and sliding it across the image\nusing 8 steps per row and 8 steps per column. To visualize this, imagine chopping the\noriginal image into a 14 \u00d7 14 grid, then sliding a 7 \u00d7 7 window across this grid; there\nwill be 8 \u00d7 8 = 64 possible locations for the window, hence 8 \u00d7 8 predictions. How\u2010\never, the FCN approach is much more efficient, since the network only looks at the\nimage once. In fact, You Only Look Once (YOLO) is the name of a very popular object\ndetection architecture, which we\u2019ll look at next. | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n28 Joseph Redmon et al., \u201cYou Only Look Once: Unified, Real-Time Object Detection,\u201d Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (2016): 779\u2013788. 29 Joseph Redmon and Ali Farhadi, \u201cYOLO9000: Better, Faster, Stronger,\u201d Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (2017): 6517\u20136525. 30 Joseph Redmon and Ali Farhadi, \u201cYOLOv3: An Incremental Improvement,\u201d arXiv preprint arXiv:1804.02767\n(2018). Figure 14-25. The same fully convolutional network processing a small image (left) and a\nlarge one (right)\nYou Only Look Once (YOLO)\nYOLO is an extremely fast and accurate object detection architecture proposed by\nJoseph Redmon et al. in a 2015 paper,28 and subsequently improved in 201629\n(YOLOv2) and in 201830 (YOLOv3). It is so fast that it can run in real time on a video,\nas seen in Redmon\u2019s demo. YOLOv3\u2019s architecture is quite similar to the one we just discussed, but with a few\nimportant differences:\nObject Detection | \u2022 It outputs five bounding boxes for each grid cell (instead of just one), and each\nbounding box comes with an objectness score. It also outputs 20 class probabili\u2010\nties per grid cell, as it was trained on the PASCAL VOC dataset, which contains\n20 classes. That\u2019s a total of 45 numbers per grid cell: 5 bounding boxes, each with\n4 coordinates, plus 5 objectness scores, plus 20 class probabilities."
  },
  {
    "id": 324,
    "content": "\u2022 Instead of predicting the absolute coordinates of the bounding box centers,\nYOLOv3 predicts an offset relative to the coordinates of the grid cell, where (0, 0)\nmeans the top left of that cell and (1, 1) means the bottom right. For each grid\ncell, YOLOv3 is trained to predict only bounding boxes whose center lies in that\ncell (but the bounding box itself generally extends well beyond the grid cell). YOLOv3 applies the logistic activation function to the bounding box coordinates\nto ensure they remain in the 0 to 1 range. \u2022 Before training the neural net, YOLOv3 finds five representative bounding box\ndimensions, called anchor boxes (or bounding box priors). It does this by applying\nthe K-Means algorithm (see Chapter 9) to the height and width of the training set\nbounding boxes. For example, if the training images contain many pedestrians,\nthen one of the anchor boxes will likely have the dimensions of a typical pedes\u2010\ntrian. Then when the neural net predicts five bounding boxes per grid cell, it\nactually predicts how much to rescale each of the anchor boxes. For example,\nsuppose one anchor box is 100 pixels tall and 50 pixels wide, and the network\npredicts, say, a vertical rescaling factor of 1.5 and a horizontal rescaling of 0.9 (for\none of the grid cells). This will result in a predicted bounding box of size 150 \u00d7 45\npixels. To be more precise, for each grid cell and each anchor box, the network\npredicts the log of the vertical and horizontal rescaling factors. Having these pri\u2010\nors makes the network more likely to predict bounding boxes of the appropriate\ndimensions, and it also speeds up training because it will more quickly learn what\nreasonable bounding boxes look like. \u2022 The network is trained using images of different scales: every few batches during\ntraining, the network randomly chooses a new image dimension (from 330 \u00d7 330\nto 608 \u00d7 608 pixels). This allows the network to learn to detect objects at different\nscales. Moreover, it makes it possible to use YOLOv3 at different scales: the\nsmaller scale will be less accurate but faster than the larger scale, so you can\nchoose the right trade-off for your use case. There are a few more innovations you might be interested in, such as the use of skip\nconnections to recover some of the spatial resolution that is lost in the CNN (we will\ndiscuss this shortly, when we look at semantic segmentation). In the 2016 paper, the\nauthors introduce the YOLO9000 model that uses hierarchical classification: the\nmodel predicts a probability for each node in a visual hierarchy called WordTree. This\nmakes it possible for the network to predict with high confidence that an image rep\u2010\nresents, say, a dog, even though it is unsure what specific type of dog."
  },
  {
    "id": 325,
    "content": "I encourage you | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nto go ahead and read all three papers: they are quite pleasant to read, and they pro\u2010\nvide excellent examples of how Deep Learning systems can be incrementally\nimproved. Mean Average Precision (mAP)\nA very common metric used in object detection tasks is the mean Average Precision\n(mAP). \u201cMean Average\u201d sounds a bit redundant, doesn\u2019t it? To understand this met\u2010\nric, let\u2019s go back to two classification metrics we discussed in Chapter 3: precision and\nrecall. Remember the trade-off: the higher the recall, the lower the precision. You can\nvisualize this in a precision/recall curve (see Figure 3-5). To summarize this curve\ninto a single number, we could compute its area under the curve (AUC). But note that\nthe precision/recall curve may contain a few sections where precision actually goes up\nwhen recall increases, especially at low recall values (you can see this at the top left of\nFigure 3-5). This is one of the motivations for the mAP metric. Suppose the classifier has 90% precision at 10% recall, but 96% precision at 20%\nrecall. There\u2019s really no trade-off here: it simply makes more sense to use the classifier\nat 20% recall rather than at 10% recall, as you will get both higher recall and higher\nprecision. So instead of looking at the precision at 10% recall, we should really be\nlooking at the maximum precision that the classifier can offer with at least 10% recall. It would be 96%, not 90%. Therefore, one way to get a fair idea of the model\u2019s perfor\u2010\nmance is to compute the maximum precision you can get with at least 0% recall, then\n10% recall, 20%, and so on up to 100%, and then calculate the mean of these maxi\u2010\nmum precisions. This is called the Average Precision (AP) metric. Now when there are\nmore than two classes, we can compute the AP for each class, and then compute the\nmean AP (mAP). That\u2019s it! In an object detection system, there is an additional level of complexity: what if the\nsystem detected the correct class, but at the wrong location (i.e., the bounding box is\ncompletely off)? Surely we should not count this as a positive prediction. One\napproach is to define an IOU threshold: for example, we may consider that a predic\u2010\ntion is correct only if the IOU is greater than, say, 0.5, and the predicted class is cor\u2010\nrect. The corresponding mAP is generally noted mAP@0.5 (or mAP@50%, or\nsometimes just AP50). In some competitions (such as the PASCAL VOC challenge),\nthis is what is done. In others (such as the COCO competition), the mAP is computed\nfor different IOU thresholds (0.50, 0.55, 0.60, \u2026, 0.95), and the final metric is the\nmean of all these mAPs (noted AP@[.50:.95] or AP@[.50:0.05:.95]). Yes, that\u2019s a mean\nmean average. Several YOLO implementations built using TensorFlow are available on GitHub. In\nparticular, check out Zihao Zang\u2019s TensorFlow 2 implementation."
  },
  {
    "id": 326,
    "content": "Other object detec\u2010\ntion models are available in the TensorFlow Models project, many with pretrained\nObject Detection | 31 Wei Liu et al., \u201cSSD: Single Shot Multibox Detector,\u201d Proceedings of the 14th European Conference on Computer\nVision 1 (2016): 21\u201337. 32 Shaoqing Ren et al., \u201cFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,\u201d\nProceedings of the 28th International Conference on Neural Information Processing Systems 1 (2015): 91\u201399. weights; and some have even been ported to TF Hub, such as SSD31 and Faster-\nRCNN,32 which are both quite popular. SSD is also a \u201csingle shot\u201d detection model,\nsimilar to YOLO. Faster R-CNN is more complex: the image first goes through a\nCNN, then the output is passed to a Region Proposal Network (RPN) that proposes\nbounding boxes that are most likely to contain an object, and a classifier is run for\neach bounding box, based on the cropped output of the CNN. The choice of detection system depends on many factors: speed, accuracy, available\npretrained models, training time, complexity, etc. The papers contain tables of met\u2010\nrics, but there is quite a lot of variability in the testing environments, and the technol\u2010\nogies evolve so fast that it is difficult to make a fair comparison that will be useful for\nmost people and remain valid for more than a few months. So, we can locate objects by drawing bounding boxes around them. Great! But per\u2010\nhaps you want to be a bit more precise. Let\u2019s see how to go down to the pixel level. Semantic Segmentation\nIn semantic segmentation, each pixel is classified according to the class of the object it\nbelongs to (e.g., road, car, pedestrian, building, etc. ), as shown in Figure 14-26. Note\nthat different objects of the same class are not distinguished. For example, all the bicy\u2010\ncles on the right side of the segmented image end up as one big lump of pixels. The\nmain difficulty in this task is that when images go through a regular CNN, they grad\u2010\nually lose their spatial resolution (due to the layers with strides greater than 1); so, a\nregular CNN may end up knowing that there\u2019s a person somewhere in the bottom left\nof the image, but it will not be much more precise than that. Just like for object detection, there are many different approaches to tackle this prob\u2010\nlem, some quite complex. However, a fairly simple solution was proposed in the 2015\npaper by Jonathan Long et al. we discussed earlier. The authors start by taking a pre\u2010\ntrained CNN and turning it into an FCN. The CNN applies an overall stride of 32 to\nthe input image (i.e., if you add up all the strides greater than 1), meaning the last\nlayer outputs feature maps that are 32 times smaller than the input image. This is\nclearly too coarse, so they add a single upsampling layer that multiplies the resolution\nby 32."
  },
  {
    "id": 327,
    "content": "| Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n33 This type of layer is sometimes referred to as a deconvolution layer, but it does not perform what mathemati\u2010\ncians call a deconvolution, so this name should be avoided. Figure 14-26. Semantic segmentation\nThere are several solutions available for upsampling (increasing the size of an image),\nsuch as bilinear interpolation, but that only works reasonably well up to \u00d74 or \u00d78. Instead, they use a transposed convolutional layer:33 it is equivalent to first stretching\nthe image by inserting empty rows and columns (full of zeros), then performing a\nregular convolution (see Figure 14-27). Alternatively, some people prefer to think of\nit as a regular convolutional layer that uses fractional strides (e.g., 1/2 in\nFigure 14-27). The transposed convolutional layer can be initialized to perform\nsomething close to linear interpolation, but since it is a trainable layer, it will learn to\ndo better during training. In tf.keras, you can use the Conv2DTranspose layer. Figure 14-27. Upsampling using a transposed convolutional layer\nSemantic Segmentation | In a transposed convolutional layer, the stride defines how much\nthe input will be stretched, not the size of the filter steps, so the\nlarger the stride, the larger the output (unlike for convolutional lay\u2010\ners or pooling layers). TensorFlow Convolution Operations\nTensorFlow also offers a few other kinds of convolutional layers:\nkeras.layers.Conv1D\nCreates a convolutional layer for 1D inputs, such as time series or text (sequences\nof letters or words), as we will see in Chapter 15.\nkeras.layers.Conv3D\nCreates a convolutional layer for 3D inputs, such as 3D PET scans. dilation_rate\nSetting the dilation_rate hyperparameter of any convolutional layer to a value\nof 2 or more creates an \u00e0-trous convolutional layer (\u201c\u00e0 trous\u201d is French for \u201cwith\nholes\u201d). This is equivalent to using a regular convolutional layer with a filter dila\u2010\nted by inserting rows and columns of zeros (i.e., holes). For example, a 1 \u00d7 3 filter\nequal to [[1,2,3]] may be dilated with a dilation rate of 4, resulting in a dilated\nfilter of [[1, 0, 0, 0, 2, 0, 0, 0, 3]]. This lets the convolutional layer have\na larger receptive field at no computational price and using no extra parameters. tf.nn.depthwise_conv2d()\nCan be used to create a depthwise convolutional layer (but you need to create the\nvariables yourself). It applies every filter to every individual input channel inde\u2010\npendently. Thus, if there are fn filters and fn\u2032 input channels, then this will output\nfn \u00d7 fn\u2032 feature maps. This solution is OK, but still too imprecise. To do better, the authors added skip con\u2010\nnections from lower layers: for example, they upsampled the output image by a factor\nof 2 (instead of 32), and they added the output of a lower layer that had this double\nresolution. Then they upsampled the result by a factor of 16, leading to a total upsam\u2010\npling factor of 32 (see Figure 14-28)."
  },
  {
    "id": 328,
    "content": "This recovered some of the spatial resolution\nthat was lost in earlier pooling layers. In their best architecture, they used a second\nsimilar skip connection to recover even finer details from an even lower layer. In\nshort, the output of the original CNN goes through the following extra steps: upscale\n\u00d72, add the output of a lower layer (of the appropriate scale), upscale \u00d72, add the out\u2010\nput of an even lower layer, and finally upscale \u00d78. It is even possible to scale up\nbeyond the size of the original image: this can be used to increase the resolution of an\nimage, which is a technique called super-resolution. | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\n34 Kaiming He et al., \u201cMask R-CNN,\u201d arXiv preprint arXiv:1703.06870 (2017). 35 Geoffrey Hinton et al., \u201cMatrix Capsules with EM Routing,\u201d Proceedings of the International Conference on\nLearning Representations (2018). Figure 14-28. Skip layers recover some spatial resolution from lower layers\nOnce again, many GitHub repositories provide TensorFlow implementations of\nsemantic segmentation (TensorFlow 1 for now), and you will even find pretrained\ninstance segmentation models in the TensorFlow Models project. Instance segmenta\u2010\ntion is similar to semantic segmentation, but instead of merging all objects of the\nsame class into one big lump, each object is distinguished from the others (e.g., it\nidentifies each individual bicycle). At present, the instance segmentation models\navailable in the TensorFlow Models project are based on the Mask R-CNN architec\u2010\nture, which was proposed in a 2017 paper:34 it extends the Faster R-CNN model by\nadditionally producing a pixel mask for each bounding box. So not only do you get a\nbounding box around each object, with a set of estimated class probabilities, but you\nalso get a pixel mask that locates pixels in the bounding box that belong to the object. As you can see, the field of Deep Computer Vision is vast and moving fast, with all\nsorts of architectures popping out every year, all based on convolutional neural net\u2010\nworks. The progress made in just a few years has been astounding, and researchers\nare now focusing on harder and harder problems, such as adversarial learning (which\nattempts to make the network more resistant to images designed to fool it), explaina\u2010\nbility (understanding why the network makes a specific classification), realistic image\ngeneration (which we will come back to in Chapter 17), and single-shot learning (a sys\u2010\ntem that can recognize an object after it has seen it just once). Some even explore\ncompletely novel architectures, such as Geoffrey Hinton\u2019s capsule networks35 (I pre\u2010\nsented them in a couple of videos, with the corresponding code in a notebook). Now\non to the next chapter, where we will look at how to process sequential data such as\ntime series using recurrent neural networks and convolutional neural networks. Semantic Segmentation | Exercises\n1. What are the advantages of a CNN over a fully connected DNN for image classi\u2010\nfication? 2."
  },
  {
    "id": 329,
    "content": "Consider a CNN composed of three convolutional layers, each with 3 \u00d7 3 kernels,\na stride of 2, and \"same\" padding. The lowest layer outputs 100 feature maps, the\nmiddle one outputs 200, and the top one outputs 400. The input images are RGB\nimages of 200 \u00d7 300 pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats,\nat least how much RAM will this network require when making a prediction for a\nsingle instance? What about when training on a mini-batch of 50 images? 3. If your GPU runs out of memory while training a CNN, what are five things you\ncould try to solve the problem? 4. Why would you want to add a max pooling layer rather than a convolutional\nlayer with the same stride? 5. When would you want to add a local response normalization layer? 6. Can you name the main innovations in AlexNet, compared to LeNet-5? What\nabout the main innovations in GoogLeNet, ResNet, SENet, and Xception? 7. What is a fully convolutional network? How can you convert a dense layer into a\nconvolutional layer? 8. What is the main technical difficulty of semantic segmentation? 9. Build your own CNN from scratch and try to achieve the highest possible accu\u2010\nracy on MNIST. 10. Use transfer learning for large image classification, going through these steps:\na. Create a training set containing at least 100 images per class. For example, you\ncould classify your own pictures based on the location (beach, mountain, city,\netc. ), or alternatively you can use an existing dataset (e.g., from TensorFlow\nDatasets). b. Split it into a training set, a validation set, and a test set. c. Build the input pipeline, including the appropriate preprocessing operations,\nand optionally add data augmentation. d. Fine-tune a pretrained model on this dataset. 11. Go through TensorFlow\u2019s Style Transfer tutorial. It is a fun way to generate art\nusing Deep Learning. Solutions to these exercises are available in Appendix A. | Chapter 14: Deep Computer Vision Using Convolutional Neural Networks\nCHAPTER 15\nProcessing Sequences Using\nRNNs and CNNs\nThe batter hits the ball. The outfielder immediately starts running, anticipating the\nball\u2019s trajectory. He tracks it, adapts his movements, and finally catches it (under a\nthunder of applause). Predicting the future is something you do all the time, whether\nyou are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In\nthis chapter we will discuss recurrent neural networks (RNNs), a class of nets that can\npredict the future (well, up to a point, of course). They can analyze time series data\nsuch as stock prices, and tell you when to buy or sell. In autonomous driving systems,\nthey can anticipate car trajectories and help avoid accidents. More generally, they can\nwork on sequences of arbitrary lengths, rather than on fixed-sized inputs like all the\nnets we have considered so far."
  },
  {
    "id": 330,
    "content": "For example, they can take sentences, documents, or\naudio samples as input, making them extremely useful for natural language process\u2010\ning applications such as automatic translation or speech-to-text. In this chapter we will first look at the fundamental concepts underlying RNNs and\nhow to train them using backpropagation through time, then we will use them to\nforecast a time series. After that we\u2019ll explore the two main difficulties that RNNs\nface:\n\u2022 Unstable gradients (discussed in Chapter 11), which can be alleviated using vari\u2010\nous techniques, including recurrent dropout and recurrent layer normalization\n\u2022 A (very) limited short-term memory, which can be extended using LSTM and\nGRU cells\nRNNs are not the only types of neural networks capable of handling sequential data:\nfor small sequences, a regular dense network can do the trick; and for very long\nsequences, such as audio samples or text, convolutional neural networks can actually work quite well too. We will discuss both of these possibilities, and we will finish this\nchapter by implementing a WaveNet: this is a CNN architecture capable of handling\nsequences of tens of thousands of time steps. In Chapter 16, we will continue to\nexplore RNNs and see how to use them for natural language processing, along with\nmore recent architectures based on attention mechanisms. Let\u2019s get started! Recurrent Neurons and Layers\nUp to now we have focused on feedforward neural networks, where the activations\nflow only in one direction, from the input layer to the output layer (a few exceptions\nare discussed in Appendix E). A recurrent neural network looks very much like a\nfeedforward neural network, except it also has connections pointing backward. Let\u2019s\nlook at the simplest possible RNN, composed of one neuron receiving inputs, pro\u2010\nducing an output, and sending that output back to itself, as shown in Figure 15-1\n(left). At each time step t (also called a frame), this recurrent neuron receives the inputs\nx(t) as well as its own output from the previous time step, y(t\u20131). Since there is no previ\u2010\nous output at the first time step, it is generally set to 0. We can represent this tiny net\u2010\nwork against the time axis, as shown in Figure 15-1 (right). This is called unrolling the\nnetwork through time (it\u2019s the same recurrent neuron represented once per time step). Figure 15-1. A recurrent neuron (left) unrolled through time (right)\nYou can easily create a layer of recurrent neurons. At each time step t, every neuron\nreceives both the input vector x(t) and the output vector from the previous time step\ny(t\u20131), as shown in Figure 15-2. Note that both the inputs and outputs are vectors now\n(when there was just a single neuron, the output was a scalar). | Chapter 15: Processing Sequences Using RNNs and CNNs\n1 Note that many researchers prefer to use the hyperbolic tangent (tanh) activation function in RNNs rather\nthan the ReLU activation function."
  },
  {
    "id": 331,
    "content": "For example, take a look at Vu Pham et al.\u2019s 2013 paper \u201cDropout Improves\nRecurrent Neural Networks for Handwriting Recognition\u201d. ReLU-based RNNs are also possible, as shown in\nQuoc V. Le et al.\u2019s 2015 paper \u201cA Simple Way to Initialize Recurrent Networks of Rectified Linear Units\u201d. Figure 15-2. A layer of recurrent neurons (left) unrolled through time (right)\nEach recurrent neuron has two sets of weights: one for the inputs x(t) and the other for\nthe outputs of the previous time step, y(t\u20131). Let\u2019s call these weight vectors wx and wy. If\nwe consider the whole recurrent layer instead of just one recurrent neuron, we can\nplace all the weight vectors in two weight matrices, Wx and Wy. The output vector of\nthe whole recurrent layer can then be computed pretty much as you might expect, as\nshown in Equation 15-1 (b is the bias vector and \u03d5(\u00b7) is the activation function (e.g.,\nReLU1). Equation 15-1. Output of a recurrent layer for a single instance\ny t = \u03d5 Wx\n\u22bax t + Wy\n\u22bay t \u22121 + b\nJust as with feedforward neural networks, we can compute a recurrent layer\u2019s output\nin one shot for a whole mini-batch by placing all the inputs at time step t in an input\nmatrix X(t) (see Equation 15-2). Equation 15-2. Outputs of a layer of recurrent neurons for all instances in a mini-\nbatch\nY t = \u03d5 X t Wx + Y t \u22121 Wy + b\n= \u03d5 X t\nY t \u22121 W + b with W =\nWx\nWy\nRecurrent Neurons and Layers | In this equation:\n\u2022 Y(t) is an m \u00d7 nneurons matrix containing the layer\u2019s outputs at time step t for each\ninstance in the mini-batch (m is the number of instances in the mini-batch and\nnneurons is the number of neurons). \u2022 X(t) is an m \u00d7 ninputs matrix containing the inputs for all instances (ninputs is the\nnumber of input features). \u2022 Wx is an ninputs \u00d7 nneurons matrix containing the connection weights for the inputs\nof the current time step. \u2022 Wy is an nneurons \u00d7 nneurons matrix containing the connection weights for the out\u2010\nputs of the previous time step. \u2022 b is a vector of size nneurons containing each neuron\u2019s bias term. \u2022 The weight matrices Wx and Wy are often concatenated vertically into a single\nweight matrix W of shape (ninputs + nneurons) \u00d7 nneurons (see the second line of Equa\u2010\ntion 15-2). \u2022 The notation [X(t) Y(t\u20131)] represents the horizontal concatenation of the matrices\nX(t) and Y(t\u20131). Notice that Y(t) is a function of X(t) and Y(t\u20131), which is a function of X(t\u20131) and Y(t\u20132),\nwhich is a function of X(t\u20132) and Y(t\u20133), and so on. This makes Y(t) a function of all the\ninputs since time t = 0 (that is, X(0), X(1), \u2026, X(t)). At the first time step, t = 0, there are\nno previous outputs, so they are typically assumed to be all zeros."
  },
  {
    "id": 332,
    "content": "Memory Cells\nSince the output of a recurrent neuron at time step t is a function of all the inputs\nfrom previous time steps, you could say it has a form of memory. A part of a neural\nnetwork that preserves some state across time steps is called a memory cell (or simply\na cell). A single recurrent neuron, or a layer of recurrent neurons, is a very basic cell,\ncapable of learning only short patterns (typically about 10 steps long, but this varies\ndepending on the task). Later in this chapter, we will look at some more complex and\npowerful types of cells capable of learning longer patterns (roughly 10 times longer,\nbut again, this depends on the task). In general a cell\u2019s state at time step t, denoted h(t) (the \u201ch\u201d stands for \u201chidden\u201d), is a\nfunction of some inputs at that time step and its state at the previous time step: h(t) =\nf(h(t\u20131), x(t)). Its output at time step t, denoted y(t), is also a function of the previous\nstate and the current inputs. In the case of the basic cells we have discussed so far, the\noutput is simply equal to the state, but in more complex cells this is not always the\ncase, as shown in Figure 15-3. | Chapter 15: Processing Sequences Using RNNs and CNNs\nFigure 15-3. A cell\u2019s hidden state and its output may be different\nInput and Output Sequences\nAn RNN can simultaneously take a sequence of inputs and produce a sequence of\noutputs (see the top-left network in Figure 15-4). This type of sequence-to-sequence\nnetwork is useful for predicting time series such as stock prices: you feed it the prices\nover the last N days, and it must output the prices shifted by one day into the future\n(i.e., from N \u2013 1 days ago to tomorrow). Alternatively, you could feed the network a sequence of inputs and ignore all outputs\nexcept for the last one (see the top-right network in Figure 15-4). In other words, this\nis a sequence-to-vector network. For example, you could feed the network a sequence\nof words corresponding to a movie review, and the network would output a senti\u2010\nment score (e.g., from \u20131 [hate] to +1 [love]). Conversely, you could feed the network the same input vector over and over again at\neach time step and let it output a sequence (see the bottom-left network of\nFigure 15-4). This is a vector-to-sequence network. For example, the input could be an\nimage (or the output of a CNN), and the output could be a caption for that image. Lastly, you could have a sequence-to-vector network, called an encoder, followed by a\nvector-to-sequence network, called a decoder (see the bottom-right network of\nFigure 15-4). For example, this could be used for translating a sentence from one lan\u2010\nguage to another."
  },
  {
    "id": 333,
    "content": "You would feed the network a sentence in one language, the\nencoder would convert this sentence into a single vector representation, and then the\ndecoder would decode this vector into a sentence in another language. This two-step\nmodel, called an Encoder\u2013Decoder, works much better than trying to translate on the\nfly with a single sequence-to-sequence RNN (like the one represented at the top left):\nthe last words of a sentence can affect the first words of the translation, so you need\nto wait until you have seen the whole sentence before translating it. We will see how\nto implement an Encoder\u2013Decoder in Chapter 16 (as we will see, it is a bit more com\u2010\nplex than in Figure 15-4 suggests). Recurrent Neurons and Layers | Figure 15-4. Seq-to-seq (top left), seq-to-vector (top right), vector-to-seq (bottom left),\nand Encoder\u2013Decoder (bottom right) networks\nSounds promising, but how do you train a recurrent neural network? Training RNNs\nTo train an RNN, the trick is to unroll it through time (like we just did) and then\nsimply use regular backpropagation (see Figure 15-5). This strategy is called backpro\u2010\npagation through time (BPTT). Just like in regular backpropagation, there is a first forward pass through the unrolled\nnetwork (represented by the dashed arrows). Then the output sequence is evaluated\nusing a cost function C(Y(0), Y(1), \u2026Y(T)) (where T is the max time step). Note that this\ncost function may ignore some outputs, as shown in Figure 15-5 (for example, in a\nsequence-to-vector RNN, all outputs are ignored except for the very last one). The\ngradients of that cost function are then propagated backward through the unrolled\nnetwork (represented by the solid arrows). Finally the model parameters are updated\nusing the gradients computed during BPTT. Note that the gradients flow backward\nthrough all the outputs used by the cost function, not just through the final output\n(for example, in Figure 15-5 the cost function is computed using the last three out\u2010\nputs of the network, Y(2), Y(3), and Y(4), so gradients flow through these three outputs, | Chapter 15: Processing Sequences Using RNNs and CNNs\nbut not through Y(0) and Y(1)). Moreover, since the same parameters W and b are used\nat each time step, backpropagation will do the right thing and sum over all time steps. Figure 15-5. Backpropagation through time\nFortunately, tf.keras takes care of all of this complexity for you\u2014so let\u2019s start coding! Forecasting a Time Series\nSuppose you are studying the number of active users per hour on your website, or the\ndaily temperature in your city, or your company\u2019s financial health, measured quar\u2010\nterly using multiple metrics. In all these cases, the data will be a sequence of one or\nmore values per time step. This is called a time series."
  },
  {
    "id": 334,
    "content": "In the first two examples there\nis a single value per time step, so these are univariate time series, while in the financial\nexample there are multiple values per time step (e.g., the company\u2019s revenue, debt,\nand so on), so it is a multivariate time series. A typical task is to predict future values,\nwhich is called forecasting. Another common task is to fill in the blanks: to predict (or\nrather \u201cpostdict\u201d) missing values from the past. This is called imputation. For exam\u2010\nple, Figure 15-6 shows 3 univariate time series, each of them 50 time steps long, and\nthe goal here is to forecast the value at the next time step (represented by the X) for\neach of them. Forecasting a Time Series | Figure 15-6. Time series forecasting\nFor simplicity, we are using a time series generated by the generate_time_series()\nfunction, shown here:\ndef generate_time_series(batch_size, n_steps): freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1) time = np.linspace(0, 1, n_steps) series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) # wave 1 series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # + wave 2 series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5) # + noise return series[..., np.newaxis].astype(np.float32)\nThis function creates as many time series as requested (via the batch_size argu\u2010\nment), each of length n_steps, and there is just one value per time step in each series\n(i.e., all series are univariate). The function returns a NumPy array of shape [batch\nsize, time steps, 1], where each series is the sum of two sine waves of fixed amplitudes\nbut random frequencies and phases, plus a bit of noise. When dealing with time series (and other types of sequences such\nas sentences), the input features are generally represented as 3D\narrays of shape [batch size, time steps, dimensionality], where\ndimensionality is 1 for univariate time series and more for multi\u2010\nvariate time series. Now let\u2019s create a training set, a validation set, and a test set using this function:\nn_steps = 50\nseries = generate_time_series(10000, n_steps + 1)\nX_train, y_train = series[:7000, :n_steps], series[:7000, -1]\nX_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\nX_test, y_test = series[9000:, :n_steps], series[9000:, -1]\nX_train contains 7,000 time series (i.e., its shape is [7000, 50, 1]), while X_valid con\u2010\ntains 2,000 (from the 7,000th time series to the 8,999th) and X_test contains 1,000\n(from the 9,000th to the 9,999th). Since we want to forecast a single value for each ser\u2010\nies, the targets are column vectors (e.g., y_train has a shape of [7000, 1]). | Chapter 15: Processing Sequences Using RNNs and CNNs\nBaseline Metrics\nBefore we start using RNNs, it is often a good idea to have a few baseline metrics, or\nelse we may end up thinking our model works great when in fact it is doing worse\nthan basic models. For example, the simplest approach is to predict the last value in\neach series."
  },
  {
    "id": 335,
    "content": "This is called naive forecasting, and it is sometimes surprisingly difficult to\noutperform. In this case, it gives us a mean squared error of about 0.020:\n>>> y_pred = X_valid[:, -1]\n>>> np.mean(keras.losses.mean_squared_error(y_valid, y_pred))\n0.020211367\nAnother simple approach is to use a fully connected network. Since it expects a flat\nlist of features for each input, we need to add a Flatten layer. Let\u2019s just use a simple\nLinear Regression model so that each prediction will be a linear combination of the\nvalues in the time series:\nmodel = keras.models.Sequential([ keras.layers.Flatten(input_shape=[50, 1]), keras.layers.Dense(1)\n])\nIf we compile this model using the MSE loss and the default Adam optimizer, then fit\nit on the training set for 20 epochs and evaluate it on the validation set, we get an\nMSE of about 0.004. That\u2019s much better than the naive approach! Implementing a Simple RNN\nLet\u2019s see if we can beat that with a simple RNN:\nmodel = keras.models.Sequential([ keras.layers.SimpleRNN(1, input_shape=[None, 1])\n])\nThat\u2019s really the simplest RNN you can build. It just contains a single layer, with a sin\u2010\ngle neuron, as we saw in Figure 15-1. We do not need to specify the length of the\ninput sequences (unlike in the previous model), since a recurrent neural network can\nprocess any number of time steps (this is why we set the first input dimension to\nNone). By default, the SimpleRNN layer uses the hyperbolic tangent activation func\u2010\ntion. It works exactly as we saw earlier: the initial state h(init) is set to 0, and it is passed\nto a single recurrent neuron, along with the value of the first time step, x(0). The neu\u2010\nron computes a weighted sum of these values and applies the hyperbolic tangent acti\u2010\nvation function to the result, and this gives the first output, y0. In a simple RNN, this\noutput is also the new state h0. This new state is passed to the same recurrent neuron\nalong with the next input value, x(1), and the process is repeated until the last time\nstep. Then the layer just outputs the last value, y49. All of this is performed simultane\u2010\nously for every time series. Forecasting a Time Series | By default, recurrent layers in Keras only return the final output. To\nmake them return one output per time step, you must set\nreturn_sequences=True, as we will see. If you compile, fit, and evaluate this model (just like earlier, we train for 20 epochs\nusing Adam), you will find that its MSE reaches only 0.014, so it is better than the\nnaive approach but it does not beat a simple linear model. Note that for each neuron,\na linear model has one parameter per input and per time step, plus a bias term (in the\nsimple linear model we used, that\u2019s a total of 51 parameters)."
  },
  {
    "id": 336,
    "content": "In contrast, for each\nrecurrent neuron in a simple RNN, there is just one parameter per input and per hid\u2010\nden state dimension (in a simple RNN, that\u2019s just the number of recurrent neurons in\nthe layer), plus a bias term. In this simple RNN, that\u2019s a total of just three parameters. Trend and Seasonality\nThere are many other models to forecast time series, such as weighted moving average\nmodels or autoregressive integrated moving average (ARIMA) models. Some of them\nrequire you to first remove the trend and seasonality. For example, if you are studying\nthe number of active users on your website, and it is growing by 10% every month,\nyou would have to remove this trend from the time series. Once the model is trained\nand starts making predictions, you would have to add the trend back to get the final\npredictions. Similarly, if you are trying to predict the amount of sunscreen lotion sold\nevery month, you will probably observe strong seasonality: since it sells well every\nsummer, a similar pattern will be repeated every year. You would have to remove this\nseasonality from the time series, for example by computing the difference between the\nvalue at each time step and the value one year earlier (this technique is called differ\u2010\nencing). Again, after the model is trained and makes predictions, you would have to\nadd the seasonal pattern back to get the final predictions. When using RNNs, it is generally not necessary to do all this, but it may improve per\u2010\nformance in some cases, since the model will not have to learn the trend or the\nseasonality. Apparently our simple RNN was too simple to get good performance. So let\u2019s try to\nadd more recurrent layers! Deep RNNs\nIt is quite common to stack multiple layers of cells, as shown in Figure 15-7. This\ngives you a deep RNN. | Chapter 15: Processing Sequences Using RNNs and CNNs\nFigure 15-7. Deep RNN (left) unrolled through time (right)\nImplementing a deep RNN with tf.keras is quite simple: just stack recurrent layers. In\nthis example, we use three SimpleRNN layers (but we could add any other type of\nrecurrent layer, such as an LSTM layer or a GRU layer, which we will discuss shortly):\nmodel = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20, return_sequences=True), keras.layers.SimpleRNN(1)\n])\nMake sure to set return_sequences=True for all recurrent layers\n(except the last one, if you only care about the last output). If you\ndon\u2019t, they will output a 2D array (containing only the output of\nthe last time step) instead of a 3D array (containing outputs for all\ntime steps), and the next recurrent layer will complain that you are\nnot feeding it sequences in the expected 3D format. If you compile, fit, and evaluate this model, you will find that it reaches an MSE of\n0.003. We finally managed to beat the linear model!"
  },
  {
    "id": 337,
    "content": "Note that the last layer is not ideal: it must have a single unit because we want to fore\u2010\ncast a univariate time series, and this means we must have a single output value per\ntime step. However, having a single unit means that the hidden state is just a single\nnumber. That\u2019s really not much, and it\u2019s probably not that useful; presumably, the\nRNN will mostly use the hidden states of the other recurrent layers to carry over all\nthe information it needs from time step to time step, and it will not use the final lay\u2010\ner\u2019s hidden state very much. Moreover, since a SimpleRNN layer uses the tanh activa\u2010\ntion function by default, the predicted values must lie within the range \u20131 to 1. But\nwhat if you want to use another activation function? For both these reasons, it might\nbe preferable to replace the output layer with a Dense layer: it would run slightly\nForecasting a Time Series | faster, the accuracy would be roughly the same, and it would allow us to choose any\noutput activation function we want. If you make this change, also make sure to\nremove return_sequences=True from the second (now last) recurrent layer:\nmodel = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20), keras.layers.Dense(1)\n])\nIf you train this model, you will see that it converges faster and performs just as well. Plus, you could change the output activation function if you wanted. Forecasting Several Time Steps Ahead\nSo far we have only predicted the value at the next time step, but we could just as\neasily have predicted the value several steps ahead by changing the targets appropri\u2010\nately (e.g., to predict 10 steps ahead, just change the targets to be the value 10 steps\nahead instead of 1 step ahead). But what if we want to predict the next 10 values? The first option is to use the model we already trained, make it predict the next value,\nthen add that value to the inputs (acting as if this predicted value had actually occur\u2010\nred), and use the model again to predict the following value, and so on, as in the fol\u2010\nlowing code:\nseries = generate_time_series(1, n_steps + 10)\nX_new, Y_new = series[:, :n_steps], series[:, n_steps:]\nX = X_new\nfor step_ahead in range(10): y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :] X = np.concatenate([X, y_pred_one], axis=1)\nY_pred = X[:, n_steps:]\nAs you might expect, the prediction for the next step will usually be more accurate\nthan the predictions for later time steps, since the errors might accumulate (as you\ncan see in Figure 15-8). If you evaluate this approach on the validation set, you will\nfind an MSE of about 0.029. This is much higher than the previous models, but it\u2019s\nalso a much harder task, so the comparison doesn\u2019t mean much. It\u2019s much more\nmeaningful to compare this performance with naive predictions (just forecasting that\nthe time series will remain constant for 10 time steps) or with a simple linear model."
  },
  {
    "id": 338,
    "content": "The naive approach is terrible (it gives an MSE of about 0.223), but the linear model\ngives an MSE of about 0.0188: it\u2019s much better than using our RNN to forecast the\nfuture one step at a time, and also much faster to train and run. Still, if you only want\nto forecast a few time steps ahead, on more complex tasks, this approach may work\nwell. | Chapter 15: Processing Sequences Using RNNs and CNNs\nFigure 15-8. Forecasting 10 steps ahead, 1 step at a time\nThe second option is to train an RNN to predict all 10 next values at once. We can\nstill use a sequence-to-vector model, but it will output 10 values instead of 1. How\u2010\never, we first need to change the targets to be vectors containing the next 10 values:\nseries = generate_time_series(10000, n_steps + 10)\nX_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\nX_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:, 0]\nX_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]\nNow we just need the output layer to have 10 units instead of 1:\nmodel = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20), keras.layers.Dense(10)\n])\nAfter training this model, you can predict the next 10 values at once very easily:\nY_pred = model.predict(X_new)\nThis model works nicely: the MSE for the next 10 time steps is about 0.008. That\u2019s\nmuch better than the linear model. But we can still do better: indeed, instead of train\u2010\ning the model to forecast the next 10 values only at the very last time step, we can\ntrain it to forecast the next 10 values at each and every time step. In other words, we\ncan turn this sequence-to-vector RNN into a sequence-to-sequence RNN. The advan\u2010\ntage of this technique is that the loss will contain a term for the output of the RNN at\neach and every time step, not just the output at the last time step. This means there\nwill be many more error gradients flowing through the model, and they won\u2019t have to\nflow only through time; they will also flow from the output of each time step. This\nwill both stabilize and speed up training. Forecasting a Time Series | 2 Note that a TimeDistributed(Dense(n)) layer is equivalent to a Conv1D(n, filter_size=1) layer. To be clear, at time step 0 the model will output a vector containing the forecasts for\ntime steps 1 to 10, then at time step 1 the model will forecast time steps 2 to 11, and\nso on. So each target must be a sequence of the same length as the input sequence,\ncontaining a 10-dimensional vector at each step."
  },
  {
    "id": 339,
    "content": "Let\u2019s prepare these target sequences:\nY = np.empty((10000, n_steps, 10)) # each target is a sequence of 10D vectors\nfor step_ahead in range(1, 10 + 1): Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps, 0]\nY_train = Y[:7000]\nY_valid = Y[7000:9000]\nY_test = Y[9000:]\nIt may be surprising that the targets will contain values that appear\nin the inputs (there is a lot of overlap between X_train and\nY_train). Isn\u2019t that cheating? Fortunately, not at all: at each time\nstep, the model only knows about past time steps, so it cannot look\nahead. It is said to be a causal model. To turn the model into a sequence-to-sequence model, we must set return_sequen\nces=True in all recurrent layers (even the last one), and we must apply the output\nDense layer at every time step. Keras offers a TimeDistributed layer for this very pur\u2010\npose: it wraps any layer (e.g., a Dense layer) and applies it at every time step of its\ninput sequence. It does this efficiently, by reshaping the inputs so that each time step\nis treated as a separate instance (i.e., it reshapes the inputs from [batch size, time steps,\ninput dimensions] to [batch size \u00d7 time steps, input dimensions]; in this example, the\nnumber of input dimensions is 20 because the previous SimpleRNN layer has 20 units),\nthen it runs the Dense layer, and finally it reshapes the outputs back to sequences (i.e.,\nit reshapes the outputs from [batch size \u00d7 time steps, output dimensions] to [batch size,\ntime steps, output dimensions]; in this example the number of output dimensions is\n10, since the Dense layer has 10 units).2 Here is the updated model:\nmodel = keras.models.Sequential([ keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]), keras.layers.SimpleRNN(20, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10))\n])\nThe Dense layer actually supports sequences as inputs (and even higher-dimensional\ninputs): it handles them just like TimeDistributed(Dense(\u2026)), meaning it is applied\nto the last input dimension only (independently across all time steps). Thus, we could\nreplace the last layer with just Dense(10). For the sake of clarity, however, we will\nkeep using TimeDistributed(Dense(10)) because it makes it clear that the Dense | Chapter 15: Processing Sequences Using RNNs and CNNs\nlayer is applied independently at each time step and that the model will output a\nsequence, not just a single vector. All outputs are needed during training, but only the output at the last time step is\nuseful for predictions and for evaluation. So although we will rely on the MSE over all\nthe outputs for training, we will use a custom metric for evaluation, to only compute\nthe MSE over the output at the last time step:\ndef last_time_step_mse(Y_true, Y_pred): return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1])\noptimizer = keras.optimizers.Adam(lr=0.01)\nmodel.compile(loss=\"mse\", optimizer=optimizer, metrics=[last_time_step_mse])\nWe get a validation MSE of about 0.006, which is 25% better than the previous model."
  },
  {
    "id": 340,
    "content": "You can combine this approach with the first one: just predict the next 10 values\nusing this RNN, then concatenate these values to the input time series and use the\nmodel again to predict the next 10 values, and repeat the process as many times as\nneeded. With this approach, you can generate arbitrarily long sequences. It may not\nbe very accurate for long-term predictions, but it may be just fine if your goal is to\ngenerate original music or text, as we will see in Chapter 16. When forecasting time series, it is often useful to have some error\nbars along with your predictions. For this, an efficient technique is\nMC Dropout, introduced in Chapter 11: add an MC Dropout layer\nwithin each memory cell, dropping part of the inputs and hidden\nstates. After training, to forecast a new time series, use the model\nmany times and compute the mean and standard deviation of the\npredictions at each time step. Simple RNNs can be quite good at forecasting time series or handling other kinds of\nsequences, but they do not perform as well on long time series or sequences. Let\u2019s dis\u2010\ncuss why and see what we can do about it. Handling Long Sequences\nTo train an RNN on long sequences, we must run it over many time steps, making the\nunrolled RNN a very deep network. Just like any deep neural network it may suffer\nfrom the unstable gradients problem, discussed in Chapter 11: it may take forever to\ntrain, or training may be unstable. Moreover, when an RNN processes a long\nsequence, it will gradually forget the first inputs in the sequence. Let\u2019s look at both\nthese problems, starting with the unstable gradients problem. Handling Long Sequences | 3 C\u00e9sar Laurent et al., \u201cBatch Normalized Recurrent Neural Networks,\u201d Proceedings of the IEEE International\nConference on Acoustics, Speech, and Signal Processing (2016): 2657\u20132661. 4 Jimmy Lei Ba et al., \u201cLayer Normalization,\u201d arXiv preprint arXiv:1607.06450 (2016). Fighting the Unstable Gradients Problem\nMany of the tricks we used in deep nets to alleviate the unstable gradients problem\ncan also be used for RNNs: good parameter initialization, faster optimizers, dropout,\nand so on. However, nonsaturating activation functions (e.g., ReLU) may not help as\nmuch here; in fact, they may actually lead the RNN to be even more unstable during\ntraining. Why? Well, suppose Gradient Descent updates the weights in a way that\nincreases the outputs slightly at the first time step. Because the same weights are used\nat every time step, the outputs at the second time step may also be slightly increased,\nand those at the third, and so on until the outputs explode\u2014and a nonsaturating acti\u2010\nvation function does not prevent that. You can reduce this risk by using a smaller\nlearning rate, but you can also simply use a saturating activation function like the\nhyperbolic tangent (this explains why it is the default). In much the same way, the\ngradients themselves can explode."
  },
  {
    "id": 341,
    "content": "If you notice that training is unstable, you may\nwant to monitor the size of the gradients (e.g., using TensorBoard) and perhaps use\nGradient Clipping. Moreover, Batch Normalization cannot be used as efficiently with RNNs as with deep\nfeedforward nets. In fact, you cannot use it between time steps, only between recur\u2010\nrent layers. To be more precise, it is technically possible to add a BN layer to a mem\u2010\nory cell (as we will see shortly) so that it will be applied at each time step (both on the\ninputs for that time step and on the hidden state from the previous step). However,\nthe same BN layer will be used at each time step, with the same parameters, regardless\nof the actual scale and offset of the inputs and hidden state. In practice, this does not\nyield good results, as was demonstrated by C\u00e9sar Laurent et al. in a 2015 paper:3 the\nauthors found that BN was slightly beneficial only when it was applied to the inputs,\nnot to the hidden states. In other words, it was slightly better than nothing when\napplied between recurrent layers (i.e., vertically in Figure 15-7), but not within recur\u2010\nrent layers (i.e., horizontally). In Keras this can be done simply by adding a Batch\nNormalization layer before each recurrent layer, but don\u2019t expect too much from it. Another form of normalization often works better with RNNs: Layer Normalization. This idea was introduced by Jimmy Lei Ba et al. in a 2016 paper:4 it is very similar to\nBatch Normalization, but instead of normalizing across the batch dimension, it nor\u2010\nmalizes across the features dimension. One advantage is that it can compute the\nrequired statistics on the fly, at each time step, independently for each instance. This\nalso means that it behaves the same way during training and testing (as opposed to\nBN), and it does not need to use exponential moving averages to estimate the feature\nstatistics across all instances in the training set. Like BN, Layer Normalization learns a | Chapter 15: Processing Sequences Using RNNs and CNNs\n5 It would have been simpler to inherit from SimpleRNNCell instead so that we wouldn\u2019t have to create an inter\u2010\nnal SimpleRNNCell or handle the state_size and output_size attributes, but the goal here was to show how\nto create a custom cell from scratch. scale and an offset parameter for each input. In an RNN, it is typically used right after\nthe linear combination of the inputs and the hidden states. Let\u2019s use tf.keras to implement Layer Normalization within a simple memory cell. For\nthis, we need to define a custom memory cell. It is just like a regular layer, except its\ncall() method takes two arguments: the inputs at the current time step and the hid\u2010\nden states from the previous time step. Note that the states argument is a list con\u2010\ntaining one or more tensors."
  },
  {
    "id": 342,
    "content": "In the case of a simple RNN cell it contains a single\ntensor equal to the outputs of the previous time step, but other cells may have multi\u2010\nple state tensors (e.g., an LSTMCell has a long-term state and a short-term state, as we\nwill see shortly). A cell must also have a state_size attribute and an output_size\nattribute. In a simple RNN, both are simply equal to the number of units. The follow\u2010\ning code implements a custom memory cell which will behave like a SimpleRNNCell,\nexcept it will also apply Layer Normalization at each time step:\nclass LNSimpleRNNCell(keras.layers.Layer): def __init__(self, units, activation=\"tanh\", **kwargs): super().__init__(**kwargs) self.state_size = units self.output_size = units self.simple_rnn_cell = keras.layers.SimpleRNNCell(units, activation=None) self.layer_norm = keras.layers.LayerNormalization() self.activation = keras.activations.get(activation) def call(self, inputs, states): outputs, new_states = self.simple_rnn_cell(inputs, states) norm_outputs = self.activation(self.layer_norm(outputs)) return norm_outputs, [norm_outputs]\nThe code is quite straightforward.5 Our LNSimpleRNNCell class inherits from the\nkeras.layers.Layer class, just like any custom layer. The constructor takes the num\u2010\nber of units and the desired activation function, and it sets the state_size and\noutput_size attributes, then creates a SimpleRNNCell with no activation function\n(because we want to perform Layer Normalization after the linear operation but\nbefore the activation function). Then the constructor creates the LayerNormaliza\ntion layer, and finally it fetches the desired activation function. The call() method\nstarts by applying the simple RNN cell, which computes a linear combination of the\ncurrent inputs and the previous hidden states, and it returns the result twice (indeed,\nin a SimpleRNNCell, the outputs are just equal to the hidden states: in other words,\nnew_states[0] is equal to outputs, so we can safely ignore new_states in the rest of\nthe call() method). Next, the call() method applies Layer Normalization, followed\nHandling Long Sequences | 6 A character from the animated movies Finding Nemo and Finding Dory who has short-term memory loss. 7 Sepp Hochreiter and J\u00fcrgen Schmidhuber, \u201cLong Short-Term Memory,\u201d Neural Computation 9, no. 8 (1997):\n1735\u20131780. 8 Ha\u015fim Sak et al., \u201cLong Short-Term Memory Based Recurrent Neural Network Architectures for Large\nVocabulary Speech Recognition,\u201d arXiv preprint arXiv:1402.1128 (2014). 9 Wojciech Zaremba et al., \u201cRecurrent Neural Network Regularization,\u201d arXiv preprint arXiv:1409.2329 (2014). by the activation function. Finally, it returns the outputs twice (once as the outputs,\nand once as the new hidden states). To use this custom cell, all we need to do is create\na keras.layers.RNN layer, passing it a cell instance:\nmodel = keras.models.Sequential([ keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True, input_shape=[None, 1]), keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10))\n])\nSimilarly, you could create a custom cell to apply dropout between each time step. But\nthere\u2019s a simpler way: all recurrent layers (except for keras.layers.RNN) and all cells\nprovided by Keras have a dropout hyperparameter and a recurrent_dropout hyper\u2010\nparameter: the former defines the dropout rate to apply to the inputs (at each time\nstep), and the latter defines the dropout rate for the hidden states (also at each time\nstep). No need to create a custom cell to apply dropout at each time step in an RNN."
  },
  {
    "id": 343,
    "content": "With these techniques, you can alleviate the unstable gradients problem and train an\nRNN much more efficiently. Now let\u2019s look at how to deal with the short-term mem\u2010\nory problem. Tackling the Short-Term Memory Problem\nDue to the transformations that the data goes through when traversing an RNN,\nsome information is lost at each time step. After a while, the RNN\u2019s state contains vir\u2010\ntually no trace of the first inputs. This can be a showstopper. Imagine Dory the fish6\ntrying to translate a long sentence; by the time she\u2019s finished reading it, she has no\nclue how it started. To tackle this problem, various types of cells with long-term\nmemory have been introduced. They have proven so successful that the basic cells are\nnot used much anymore. Let\u2019s first look at the most popular of these long-term mem\u2010\nory cells: the LSTM cell. LSTM cells\nThe Long Short-Term Memory (LSTM) cell was proposed in 19977 by Sepp Hochreiter\nand J\u00fcrgen Schmidhuber and gradually improved over the years by several research\u2010\ners, such as Alex Graves, Ha\u015fim Sak,8 and Wojciech Zaremba.9 If you consider the | Chapter 15: Processing Sequences Using RNNs and CNNs\nLSTM cell as a black box, it can be used very much like a basic cell, except it will per\u2010\nform much better; training will converge faster, and it will detect long-term depen\u2010\ndencies in the data. In Keras, you can simply use the LSTM layer instead of the\nSimpleRNN layer:\nmodel = keras.models.Sequential([ keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]), keras.layers.LSTM(20, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10))\n])\nAlternatively, you could use the general-purpose keras.layers.RNN layer, giving it an\nLSTMCell as an argument:\nmodel = keras.models.Sequential([ keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True, input_shape=[None, 1]), keras.layers.RNN(keras.layers.LSTMCell(20), return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10))\n])\nHowever, the LSTM layer uses an optimized implementation when running on a GPU\n(see Chapter 19), so in general it is preferable to use it (the RNN layer is mostly useful\nwhen you define custom cells, as we did earlier). So how does an LSTM cell work? Its architecture is shown in Figure 15-9. If you don\u2019t look at what\u2019s inside the box, the LSTM cell looks exactly like a regular\ncell, except that its state is split into two vectors: h(t) and c(t) (\u201cc\u201d stands for \u201ccell\u201d). You\ncan think of h(t) as the short-term state and c(t) as the long-term state. Handling Long Sequences | Figure 15-9. LSTM cell\nNow let\u2019s open the box! The key idea is that the network can learn what to store in the\nlong-term state, what to throw away, and what to read from it. As the long-term state\nc(t\u20131) traverses the network from left to right, you can see that it first goes through a\nforget gate, dropping some memories, and then it adds some new memories via the\naddition operation (which adds the memories that were selected by an input gate). The result c(t) is sent straight out, without any further transformation. So, at each time\nstep, some memories are dropped and some memories are added."
  },
  {
    "id": 344,
    "content": "Moreover, after the\naddition operation, the long-term state is copied and passed through the tanh func\u2010\ntion, and then the result is filtered by the output gate. This produces the short-term\nstate h(t) (which is equal to the cell\u2019s output for this time step, y(t)). Now let\u2019s look at\nwhere new memories come from and how the gates work. First, the current input vector x(t) and the previous short-term state h(t\u20131) are fed to\nfour different fully connected layers. They all serve a different purpose:\n\u2022 The main layer is the one that outputs g(t). It has the usual role of analyzing the\ncurrent inputs x(t) and the previous (short-term) state h(t\u20131). In a basic cell, there is\nnothing other than this layer, and its output goes straight out to y(t) and h(t). In\ncontrast, in an LSTM cell this layer\u2019s output does not go straight out, but instead\nits most important parts are stored in the long-term state (and the rest is\ndropped). \u2022 The three other layers are gate controllers. Since they use the logistic activation\nfunction, their outputs range from 0 to 1. As you can see, their outputs are fed to | Chapter 15: Processing Sequences Using RNNs and CNNs\nelement-wise multiplication operations, so if they output 0s they close the gate,\nand if they output 1s they open it. Specifically:\n\u2014 The forget gate (controlled by f(t)) controls which parts of the long-term state\nshould be erased. \u2014 The input gate (controlled by i(t)) controls which parts of g(t) should be added\nto the long-term state. \u2014 Finally, the output gate (controlled by o(t)) controls which parts of the long-\nterm state should be read and output at this time step, both to h(t) and to y(t). In short, an LSTM cell can learn to recognize an important input (that\u2019s the role of the\ninput gate), store it in the long-term state, preserve it for as long as it is needed (that\u2019s\nthe role of the forget gate), and extract it whenever it is needed. This explains why\nthese cells have been amazingly successful at capturing long-term patterns in time\nseries, long texts, audio recordings, and more. Equation 15-3 summarizes how to compute the cell\u2019s long-term state, its short-term\nstate, and its output at each time step for a single instance (the equations for a whole\nmini-batch are very similar). Equation 15-3."
  },
  {
    "id": 345,
    "content": "LSTM computations\ni t = \u03c3 Wxi\n\u22bax t + Whi\n\u22bah t \u22121 + bi\nf t = \u03c3 Wxf\n\u22bax t + Whf\n\u22bah t \u22121 + bf\no t = \u03c3 Wxo\n\u22bax t + Who\n\u22bah t \u22121 + bo\ng t = tanh Wxg\n\u22bax t + Whg\n\u22bah t \u22121 + bg\nc t = f t \u2297c t \u22121 + i t \u2297g t\ny t = h t = o t \u2297tanh c t\nIn this equation:\n\u2022 Wxi, Wxf, Wxo, Wxg are the weight matrices of each of the four layers for their con\u2010\nnection to the input vector x(t). \u2022 Whi, Whf, Who, and Whg are the weight matrices of each of the four layers for their\nconnection to the previous short-term state h(t\u20131). \u2022 bi, bf, bo, and bg are the bias terms for each of the four layers. Note that Tensor\u2010\nFlow initializes bf to a vector full of 1s instead of 0s. This prevents forgetting\neverything at the beginning of training. Handling Long Sequences | 10 F. A. Gers and J. Schmidhuber, \u201cRecurrent Nets That Time and Count,\u201d Proceedings of the IEEE-INNS-ENNS\nInternational Joint Conference on Neural Networks (2000): 189\u2013194. 11 Kyunghyun Cho et al., \u201cLearning Phrase Representations Using RNN Encoder-Decoder for Statistical\nMachine Translation,\u201d Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing\n(2014): 1724\u20131734. Peephole connections\nIn a regular LSTM cell, the gate controllers can look only at the input x(t) and the pre\u2010\nvious short-term state h(t\u20131). It may be a good idea to give them a bit more context by\nletting them peek at the long-term state as well. This idea was proposed by Felix Gers\nand J\u00fcrgen Schmidhuber in 2000.10 They proposed an LSTM variant with extra con\u2010\nnections called peephole connections: the previous long-term state c(t\u20131) is added as an\ninput to the controllers of the forget gate and the input gate, and the current long-\nterm state c(t) is added as input to the controller of the output gate. This often\nimproves performance, but not always, and there is no clear pattern for which tasks\nare better off with or without them: you will have to try it on your task and see if it\nhelps. In Keras, the LSTM layer is based on the keras.layers.LSTMCell cell, which does not\nsupport peepholes. The experimental tf.keras.experimental.PeepholeLSTMCell\ndoes, however, so you can create a keras.layers.RNN layer and pass a PeepholeLSTM\nCell to its constructor. There are many other variants of the LSTM cell. One particularly popular variant is\nthe GRU cell, which we will look at now. GRU cells\nThe Gated Recurrent Unit (GRU) cell (see Figure 15-10) was proposed by Kyunghyun\nCho et al. in a 2014 paper11 that also introduced the Encoder\u2013Decoder network we\ndiscussed earlier."
  },
  {
    "id": 346,
    "content": "| Chapter 15: Processing Sequences Using RNNs and CNNs\n12 A 2015 paper by Klaus Greff et al., \u201cLSTM: A Search Space Odyssey\u201d, seems to show that all LSTM variants\nperform roughly the same. Figure 15-10. GRU cell\nThe GRU cell is a simplified version of the LSTM cell, and it seems to perform just as\nwell12 (which explains its growing popularity). These are the main simplifications:\n\u2022 Both state vectors are merged into a single vector h(t). \u2022 A single gate controller z(t) controls both the forget gate and the input gate. If the\ngate controller outputs a 1, the forget gate is open (= 1) and the input gate is\nclosed (1 \u2013 1 = 0). If it outputs a 0, the opposite happens. In other words, when\u2010\never a memory must be stored, the location where it will be stored is erased first. This is actually a frequent variant to the LSTM cell in and of itself. \u2022 There is no output gate; the full state vector is output at every time step. How\u2010\never, there is a new gate controller r(t) that controls which part of the previous\nstate will be shown to the main layer (g(t)). Handling Long Sequences | Equation 15-4 summarizes how to compute the cell\u2019s state at each time step for a sin\u2010\ngle instance. Equation 15-4. GRU computations\nz t = \u03c3 Wxz\n\u22bax t + Whz\n\u22bah t \u22121 + bz\nr t = \u03c3 Wxr\n\u22bax t + Whr\n\u22bah t \u22121 + br\ng t = tanh Wxg\n\u22bax t + Whg\n\u22bar t \u2297h t \u22121\n+ bg\nh t = z t \u2297h t \u22121 + 1 \u2212z t\n\u2297g t\nKeras provides a keras.layers.GRU layer (based on the keras.layers.GRUCell\nmemory cell); using it is just a matter of replacing SimpleRNN or LSTM with GRU. LSTM and GRU cells are one of the main reasons behind the success of RNNs. Yet\nwhile they can tackle much longer sequences than simple RNNs, they still have a\nfairly limited short-term memory, and they have a hard time learning long-term pat\u2010\nterns in sequences of 100 time steps or more, such as audio samples, long time series,\nor long sentences. One way to solve this is to shorten the input sequences, for exam\u2010\nple using 1D convolutional layers. Using 1D convolutional layers to process sequences\nIn Chapter 14, we saw that a 2D convolutional layer works by sliding several fairly\nsmall kernels (or filters) across an image, producing multiple 2D feature maps (one\nper kernel). Similarly, a 1D convolutional layer slides several kernels across a\nsequence, producing a 1D feature map per kernel. Each kernel will learn to detect a\nsingle very short sequential pattern (no longer than the kernel size). If you use 10 ker\u2010\nnels, then the layer\u2019s output will be composed of 10 1-dimensional sequences (all of\nthe same length), or equivalently you can view this output as a single 10-dimensional\nsequence."
  },
  {
    "id": 347,
    "content": "This means that you can build a neural network composed of a mix of\nrecurrent layers and 1D convolutional layers (or even 1D pooling layers). If you use a\n1D convolutional layer with a stride of 1 and \"same\" padding, then the output\nsequence will have the same length as the input sequence. But if you use \"valid\"\npadding or a stride greater than 1, then the output sequence will be shorter than the\ninput sequence, so make sure you adjust the targets accordingly. For example, the fol\u2010\nlowing model is the same as earlier, except it starts with a 1D convolutional layer that\ndownsamples the input sequence by a factor of 2, using a stride of 2. The kernel size is\nlarger than the stride, so all inputs will be used to compute the layer\u2019s output, and\ntherefore the model can learn to preserve the useful information, dropping only the\nunimportant details. By shortening the sequences, the convolutional layer may help\nthe GRU layers detect longer patterns. Note that we must also crop off the first three | Chapter 15: Processing Sequences Using RNNs and CNNs\n13 Aaron van den Oord et al., \u201cWaveNet: A Generative Model for Raw Audio,\u201d arXiv preprint arXiv:1609.03499\n(2016). time steps in the targets (since the kernel\u2019s size is 4, the first output of the convolu\u2010\ntional layer will be based on the input time steps 0 to 3), and downsample the targets\nby a factor of 2:\nmodel = keras.models.Sequential([ keras.layers.Conv1D(filters=20, kernel_size=4, strides=2, padding=\"valid\", input_shape=[None, 1]), keras.layers.GRU(20, return_sequences=True), keras.layers.GRU(20, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(10))\n])\nmodel.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\nhistory = model.fit(X_train, Y_train[:, 3::2], epochs=20, validation_data=(X_valid, Y_valid[:, 3::2]))\nIf you train and evaluate this model, you will find that it is the best model so far. The\nconvolutional layer really helps. In fact, it is actually possible to use only 1D convolu\u2010\ntional layers and drop the recurrent layers entirely! WaveNet\nIn a 2016 paper,13 Aaron van den Oord and other DeepMind researchers introduced\nan architecture called WaveNet. They stacked 1D convolutional layers, doubling the\ndilation rate (how spread apart each neuron\u2019s inputs are) at every layer: the first con\u2010\nvolutional layer gets a glimpse of just two time steps at a time, while the next one sees\nfour time steps (its receptive field is four time steps long), the next one sees eight time\nsteps, and so on (see Figure 15-11). This way, the lower layers learn short-term pat\u2010\nterns, while the higher layers learn long-term patterns. Thanks to the doubling dila\u2010\ntion rate, the network can process extremely large sequences very efficiently. Handling Long Sequences | 14 The complete WaveNet uses a few more tricks, such as skip connections like in a ResNet, and Gated Activation\nUnits similar to those found in a GRU cell. Please see the notebook for more details. Figure 15-11."
  },
  {
    "id": 348,
    "content": "WaveNet architecture\nIn the WaveNet paper, the authors actually stacked 10 convolutional layers with dila\u2010\ntion rates of 1, 2, 4, 8, \u2026, 256, 512, then they stacked another group of 10 identical\nlayers (also with dilation rates 1, 2, 4, 8, \u2026, 256, 512), then again another identical\ngroup of 10 layers. They justified this architecture by pointing out that a single stack\nof 10 convolutional layers with these dilation rates will act like a super-efficient con\u2010\nvolutional layer with a kernel of size 1,024 (except way faster, more powerful, and\nusing significantly fewer parameters), which is why they stacked 3 such blocks. They\nalso left-padded the input sequences with a number of zeros equal to the dilation rate\nbefore every layer, to preserve the same sequence length throughout the network. Here is how to implement a simplified WaveNet to tackle the same sequences as\nearlier:14\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.InputLayer(input_shape=[None, 1]))\nfor rate in (1, 2, 4, 8) * 2: model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\", activation=\"relu\", dilation_rate=rate))\nmodel.add(keras.layers.Conv1D(filters=10, kernel_size=1))\nmodel.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\nhistory = model.fit(X_train, Y_train, epochs=20, validation_data=(X_valid, Y_valid))\nThis Sequential model starts with an explicit input layer (this is simpler than trying\nto set input_shape only on the first layer), then continues with a 1D convolutional\nlayer using \"causal\" padding: this ensures that the convolutional layer does not peek\ninto the future when making predictions (it is equivalent to padding the inputs with\nthe right amount of zeros on the left and using \"valid\" padding). We then add | Chapter 15: Processing Sequences Using RNNs and CNNs\nsimilar pairs of layers using growing dilation rates: 1, 2, 4, 8, and again 1, 2, 4, 8. Finally, we add the output layer: a convolutional layer with 10 filters of size 1 and\nwithout any activation function. Thanks to the padding layers, every convolutional\nlayer outputs a sequence of the same length as the input sequences, so the targets we\nuse during training can be the full sequences: no need to crop them or downsample\nthem. The last two models offer the best performance so far in forecasting our time series! In the WaveNet paper, the authors achieved state-of-the-art performance on various\naudio tasks (hence the name of the architecture), including text-to-speech tasks, pro\u2010\nducing incredibly realistic voices across several languages. They also used the model\nto generate music, one audio sample at a time. This feat is all the more impressive\nwhen you realize that a single second of audio can contain tens of thousands of time\nsteps\u2014even LSTMs and GRUs cannot handle such long sequences. In Chapter 16, we will continue to explore RNNs, and we will see how they can tackle\nvarious NLP tasks. Exercises\n1. Can you think of a few applications for a sequence-to-sequence RNN? What\nabout a sequence-to-vector RNN, and a vector-to-sequence RNN? 2. How many dimensions must the inputs of an RNN layer have? What does each\ndimension represent? What about its outputs? 3."
  },
  {
    "id": 349,
    "content": "If you want to build a deep sequence-to-sequence RNN, which RNN layers\nshould have return_sequences=True? What about a sequence-to-vector RNN? 4. Suppose you have a daily univariate time series, and you want to forecast the next\nseven days. Which RNN architecture should you use? 5. What are the main difficulties when training RNNs? How can you handle them? 6. Can you sketch the LSTM cell\u2019s architecture? 7. Why would you want to use 1D convolutional layers in an RNN? 8. Which neural network architecture could you use to classify videos? 9. Train a classification model for the SketchRNN dataset, available in TensorFlow\nDatasets. 10. Download the Bach chorales dataset and unzip it. It is composed of 382 chorales\ncomposed by Johann Sebastian Bach. Each chorale is 100 to 640 time steps long,\nand each time step contains 4 integers, where each integer corresponds to a note\u2019s\nindex on a piano (except for the value 0, which means that no note is played). Train a model\u2014recurrent, convolutional, or both\u2014that can predict the next time\nstep (four notes), given a sequence of time steps from a chorale. Then use this\nExercises | model to generate Bach-like music, one note at a time: you can do this by giving\nthe model the start of a chorale and asking it to predict the next time step, then\nappending these time steps to the input sequence and asking the model for the\nnext note, and so on. Also make sure to check out Google\u2019s Coconet model,\nwhich was used for a nice Google doodle about Bach. Solutions to these exercises are available in Appendix A. | Chapter 15: Processing Sequences Using RNNs and CNNs\n1 Alan Turing, \u201cComputing Machinery and Intelligence,\u201d Mind 49 (1950): 433\u2013460. 2 Of course, the word chatbot came much later. Turing called his test the imitation game: machine A and human\nB chat with human interrogator C via text messages; the interrogator asks questions to figure out which one is\nthe machine (A or B). The machine passes the test if it can fool the interrogator, while the human B must try\nto help the interrogator. CHAPTER 16\nNatural Language Processing with\nRNNs and Attention\nWhen Alan Turing imagined his famous Turing test1 in 1950, his objective was to\nevaluate a machine\u2019s ability to match human intelligence. He could have tested for\nmany things, such as the ability to recognize cats in pictures, play chess, compose\nmusic, or escape a maze, but, interestingly, he chose a linguistic task."
  },
  {
    "id": 350,
    "content": "More specifi\u2010\ncally, he devised a chatbot capable of fooling its interlocutor into thinking it was\nhuman.2 This test does have its weaknesses: a set of hardcoded rules can fool unsus\u2010\npecting or naive humans (e.g., the machine could give vague predefined answers in\nresponse to some keywords; it could pretend that it is joking or drunk, to get a pass\non its weirdest answers; or it could escape difficult questions by answering them with\nits own questions), and many aspects of human intelligence are utterly ignored (e.g.,\nthe ability to interpret nonverbal communication such as facial expressions, or to\nlearn a manual task). But the test does highlight the fact that mastering language is\narguably Homo sapiens\u2019s greatest cognitive ability. Can we build a machine that can\nread and write natural language? A common approach for natural language tasks is to use recurrent neural networks. We will therefore continue to explore RNNs (introduced in Chapter 15), starting with\na character RNN, trained to predict the next character in a sentence. This will allow us\nto generate some original text, and in the process we will see how to build a Tensor\u2010\nFlow Dataset on a very long sequence. We will first use a stateless RNN (which learns on random portions of text at each iteration, without any information on the rest of\nthe text), then we will build a stateful RNN (which preserves the hidden state between\ntraining iterations and continues reading where it left off, allowing it to learn longer\npatterns). Next, we will build an RNN to perform sentiment analysis (e.g., reading\nmovie reviews and extracting the rater\u2019s feeling about the movie), this time treating\nsentences as sequences of words, rather than characters. Then we will show how\nRNNs can be used to build an Encoder\u2013Decoder architecture capable of performing\nneural machine translation (NMT). For this, we will use the seq2seq API provided by\nthe TensorFlow Addons project. In the second part of this chapter, we will look at attention mechanisms. As their name\nsuggests, these are neural network components that learn to select the part of the\ninputs that the rest of the model should focus on at each time step. First we will see\nhow to boost the performance of an RNN-based Encoder\u2013Decoder architecture using\nattention, then we will drop RNNs altogether and look at a very successful attention-\nonly architecture called the Transformer. Finally, we will take a look at some of the\nmost important advances in NLP in 2018 and 2019, including incredibly powerful\nlanguage models such as GPT-2 and BERT, both based on Transformers. Let\u2019s start with a simple and fun model that can write like Shakespeare (well, sort of). Generating Shakespearean Text Using a Character RNN\nIn a famous 2015 blog post titled \u201cThe Unreasonable Effectiveness of Recurrent Neu\u2010\nral Networks,\u201d Andrej Karpathy showed how to train an RNN to predict the next\ncharacter in a sentence."
  },
  {
    "id": 351,
    "content": "This Char-RNN can then be used to generate novel text, one\ncharacter at a time. Here is a small sample of the text generated by a Char-RNN\nmodel after it was trained on all of Shakespeare\u2019s work:\nPANDARUS:\nAlas, I think he shall be come approached and the day\nWhen little srain would be attain\u2019d into being never fed,\nAnd who is but a chain and subjects of his death,\nI should not sleep. Not exactly a masterpiece, but it is still impressive that the model was able to learn\nwords, grammar, proper punctuation, and more, just by learning to predict the next\ncharacter in a sentence. Let\u2019s look at how to build a Char-RNN, step by step, starting\nwith the creation of the dataset. | Chapter 16: Natural Language Processing with RNNs and Attention\nCreating the Training Dataset\nFirst, let\u2019s download all of Shakespeare\u2019s work, using Keras\u2019s handy get_file() func\u2010\ntion and downloading the data from Andrej Karpathy\u2019s Char-RNN project:\nshakespeare_url = \" # shortcut URL\nfilepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\nwith open(filepath) as f: shakespeare_text = f.read()\nNext, we must encode every character as an integer. One option is to create a custom\npreprocessing layer, as we did in Chapter 13. But in this case, it will be simpler to use\nKeras\u2019s Tokenizer class. First we need to fit a tokenizer to the text: it will find all the\ncharacters used in the text and map each of them to a different character ID, from 1\nto the number of distinct characters (it does not start at 0, so we can use that value for\nmasking, as we will see later in this chapter):\ntokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\ntokenizer.fit_on_texts([shakespeare_text])\nWe set char_level=True to get character-level encoding rather than the default\nword-level encoding. Note that this tokenizer converts the text to lowercase by\ndefault (but you can set lower=False if you do not want that). Now the tokenizer can\nencode a sentence (or a list of sentences) to a list of character IDs and back, and it\ntells us how many distinct characters there are and the total number of characters in\nthe text:\n>>> tokenizer.texts_to_sequences([\"First\"])\n[[20, 6, 9, 8, 3]]\n>>> tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])\n['f i r s t']\n>>> max_id = len(tokenizer.word_index) # number of distinct characters\n>>> dataset_size = tokenizer.document_count # total number of characters\nLet\u2019s encode the full text so each character is represented by its ID (we subtract 1 to\nget IDs from 0 to 38, rather than from 1 to 39):\n[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\nBefore we continue, we need to split the dataset into a training set, a validation set,\nand a test set. We can\u2019t just shuffle all the characters in the text, so how do you split a\nsequential dataset? How to Split a Sequential Dataset\nIt is very important to avoid any overlap between the training set, the validation set,\nand the test set."
  },
  {
    "id": 352,
    "content": "For example, we can take the first 90% of the text for the training set,\nthen the next 5% for the validation set, and the final 5% for the test set. It would also\nGenerating Shakespearean Text Using a Character RNN | 3 By definition, a stationary time series\u2019s mean, variance, and autocorrelations (i.e., correlations between values\nin the time series separated by a given interval) do not change over time. This is quite restrictive; for example,\nit excludes time series with trends or cyclical patterns. RNNs are more tolerant in that they can learn trends\nand cyclical patterns. be a good idea to leave a gap between these sets to avoid the risk of a paragraph over\u2010\nlapping over two sets. When dealing with time series, you would in general split across time,: for example,\nyou might take the years 2000 to 2012 for the training set, the years 2013 to 2015 for\nthe validation set, and the years 2016 to 2018 for the test set. However, in some cases\nyou may be able to split along other dimensions, which will give you a longer time\nperiod to train on. For example, if you have data about the financial health of 10,000\ncompanies from 2000 to 2018, you might be able to split this data across the different\ncompanies. It\u2019s very likely that many of these companies will be strongly correlated,\nthough (e.g., whole economic sectors may go up or down jointly), and if you have\ncorrelated companies across the training set and the test set your test set will not be as\nuseful, as its measure of the generalization error will be optimistically biased. So, it is often safer to split across time\u2014but this implicitly assumes that the patterns\nthe RNN can learn in the past (in the training set) will still exist in the future. In other\nwords, we assume that the time series is stationary (at least in a wide sense).3 For\nmany time series this assumption is reasonable (e.g., chemical reactions should be\nfine, since the laws of chemistry don\u2019t change every day), but for many others it is not\n(e.g., financial markets are notoriously not stationary since patterns disappear as soon\nas traders spot them and start exploiting them). To make sure the time series is\nindeed sufficiently stationary, you can plot the model\u2019s errors on the validation set\nacross time: if the model performs much better on the first part of the validation set\nthan on the last part, then the time series may not be stationary enough, and you\nmight be better off training the model on a shorter time span. In short, splitting a time series into a training set, a validation set, and a test set is not\na trivial task, and how it\u2019s done will depend strongly on the task at hand. Now back to Shakespeare!"
  },
  {
    "id": 353,
    "content": "Let\u2019s take the first 90% of the text for the training set\n(keeping the rest for the validation set and the test set), and create a tf.data.Dataset\nthat will return each character one by one from this set:\ntrain_size = dataset_size * 90 // 100\ndataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\nChopping the Sequential Dataset into Multiple Windows\nThe training set now consists of a single sequence of over a million characters, so we\ncan\u2019t just train the neural network directly on it: the RNN would be equivalent to a | Chapter 16: Natural Language Processing with RNNs and Attention\ndeep net with over a million layers, and we would have a single (very long) instance\nto train it. Instead, we will use the dataset\u2019s window() method to convert this long\nsequence of characters into many smaller windows of text. Every instance in the data\u2010\nset will be a fairly short substring of the whole text, and the RNN will be unrolled\nonly over the length of these substrings. This is called truncated backpropagation\nthrough time. Let\u2019s call the window() method to create a dataset of short text windows:\nn_steps = 100\nwindow_length = n_steps + 1 # target = input shifted 1 character ahead\ndataset = dataset.window(window_length, shift=1, drop_remainder=True)\nYou can try tuning n_steps: it is easier to train RNNs on shorter\ninput sequences, but of course the RNN will not be able to learn\nany pattern longer than n_steps, so don\u2019t make it too small. By default, the window() method creates nonoverlapping windows, but to get the\nlargest possible training set we use shift=1 so that the first window contains charac\u2010\nters 0 to 100, the second contains characters 1 to 101, and so on. To ensure that all\nwindows are exactly 101 characters long (which will allow us to create batches\nwithout having to do any padding), we set drop_remainder=True (otherwise the last\n100 windows will contain 100 characters, 99 characters, and so on down to 1\ncharacter). The window() method creates a dataset that contains windows, each of which is also\nrepresented as a dataset. It\u2019s a nested dataset, analogous to a list of lists. This is useful\nwhen you want to transform each window by calling its dataset methods (e.g., to\nshuffle them or batch them). However, we cannot use a nested dataset directly for\ntraining, as our model will expect tensors as input, not datasets. So, we must call the\nflat_map() method: it converts a nested dataset into a flat dataset (one that does not\ncontain datasets). For example, suppose {1, 2, 3} represents a dataset containing the\nsequence of tensors 1, 2, and 3. If you flatten the nested dataset {{1, 2}, {3, 4, 5, 6}},\nyou get back the flat dataset {1, 2, 3, 4, 5, 6}. Moreover, the flat_map() method takes\na function as an argument, which allows you to transform each dataset in the nested\ndataset before flattening."
  },
  {
    "id": 354,
    "content": "For example, if you pass the function lambda ds:\nds.batch(2) to flat_map(), then it will transform the nested dataset {{1, 2}, {3, 4, 5,\n6}} into the flat dataset {[1, 2], [3, 4], [5, 6]}: it\u2019s a dataset of tensors of size 2. With that\nin mind, we are ready to flatten our dataset:\ndataset = dataset.flat_map(lambda window: window.batch(window_length))\nNotice that we call batch(window_length) on each window: since all windows have\nexactly that length, we will get a single tensor for each of them. Now the dataset con\u2010\ntains consecutive windows of 101 characters each. Since Gradient Descent works best\nGenerating Shakespearean Text Using a Character RNN | when the instances in the training set are independent and identically distributed (see\nChapter 4), we need to shuffle these windows. Then we can batch the windows and\nseparate the inputs (the first 100 characters) from the target (the last character):\nbatch_size = 32\ndataset = dataset.shuffle(10000).batch(batch_size)\ndataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\nFigure 16-1 summarizes the dataset preparation steps discussed so far (showing win\u2010\ndows of length 11 rather than 101, and a batch size of 3 instead of 32). Figure 16-1. Preparing a dataset of shuffled windows\nAs discussed in Chapter 13, categorical input features should generally be encoded,\nusually as one-hot vectors or as embeddings. Here, we will encode each character\nusing a one-hot vector because there are fairly few distinct characters (only 39):\ndataset = dataset.map( lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\nFinally, we just need to add prefetching:\ndataset = dataset.prefetch(1)\nThat\u2019s it! Preparing the dataset was the hardest part. Now let\u2019s create the model. Building and Training the Char-RNN Model\nTo predict the next character based on the previous 100 characters, we can use an\nRNN with 2 GRU layers of 128 units each and 20% dropout on both the inputs (drop\nout) and the hidden states (recurrent_dropout). We can tweak these hyperparame\u2010\nters later, if needed. The output layer is a time-distributed Dense layer like we saw in\nChapter 15. This time this layer must have 39 units (max_id) because there are 39 dis\u2010\ntinct characters in the text, and we want to output a probability for each possible\ncharacter (at each time step). The output probabilities should sum up to 1 at each\ntime step, so we apply the softmax activation function to the outputs of the Dense | Chapter 16: Natural Language Processing with RNNs and Attention\nlayer. We can then compile this model, using the \"sparse_categorical_crossen\ntropy\" loss and an Adam optimizer. Finally, we are ready to train the model for sev\u2010\neral epochs (this may take many hours, depending on your hardware):\nmodel = keras.models.Sequential([ keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id], dropout=0.2, recurrent_dropout=0.2), keras.layers.GRU(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\nhistory = model.fit(dataset, epochs=20)\nUsing the Char-RNN Model\nNow we have a model that can predict the next character in text written by Shake\u2010\nspeare."
  },
  {
    "id": 355,
    "content": "To feed it some text, we first need to preprocess it like we did earlier, so let\u2019s\ncreate a little function for this:\ndef preprocess(texts): X = np.array(tokenizer.texts_to_sequences(texts)) - 1 return tf.one_hot(X, max_id)\nNow let\u2019s use the model to predict the next letter in some text:\n>>> X_new = preprocess([\"How are yo\"])\n>>> Y_pred = model.predict_classes(X_new)\n>>> tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char\n'u'\nSuccess! The model guessed right. Now let\u2019s use this model to generate new text. Generating Fake Shakespearean Text\nTo generate new text using the Char-RNN model, we could feed it some text, make\nthe model predict the most likely next letter, add it at the end of the text, then give the\nextended text to the model to guess the next letter, and so on. But in practice this\noften leads to the same words being repeated over and over again. Instead, we can\npick the next character randomly, with a probability equal to the estimated probabil\u2010\nity, using TensorFlow\u2019s tf.random.categorical() function. This will generate more\ndiverse and interesting text. The categorical() function samples random class indi\u2010\nces, given the class log probabilities (logits). To have more control over the diversity\nof the generated text, we can divide the logits by a number called the temperature,\nwhich we can tweak as we wish: a temperature close to 0 will favor the high-\nprobability characters, while a very high temperature will give all characters an equal\nprobability. The following next_char() function uses this approach to pick the next\ncharacter to add to the input text:\nGenerating Shakespearean Text Using a Character RNN | def next_char(text, temperature=1): X_new = preprocess([text]) y_proba = model.predict(X_new)[0, -1:, :] rescaled_logits = tf.math.log(y_proba) / temperature char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1 return tokenizer.sequences_to_texts(char_id.numpy())[0]\nNext, we can write a small function that will repeatedly call next_char() to get the\nnext character and append it to the given text:\ndef complete_text(text, n_chars=50, temperature=1): for _ in range(n_chars): text += next_char(text, temperature) return text\nWe are now ready to generate some text! Let\u2019s try with different temperatures:\n>>> print(complete_text(\"t\", temperature=0.2))\nthe belly the great and who shall be the belly the\n>>> print(complete_text(\"w\", temperature=1))\nthing? or why you gremio. who make which the first\n>>> print(complete_text(\"w\", temperature=2))\nth no cce:\nyeolg-hormer firi. a play asks. fol rusb\nApparently our Shakespeare model works best at a temperature close to 1. To gener\u2010\nate more convincing text, you could try using more GRU layers and more neurons per\nlayer, train for longer, and add some regularization (for example, you could set recur\nrent_dropout=0.3 in the GRU layers). Moreover, the model is currently incapable of\nlearning patterns longer than n_steps, which is just 100 characters. You could try\nmaking this window larger, but it will also make training harder, and even LSTM and\nGRU cells cannot handle very long sequences. Alternatively, you could use a stateful\nRNN."
  },
  {
    "id": 356,
    "content": "Stateful RNN\nUntil now, we have used only stateless RNNs: at each training iteration the model\nstarts with a hidden state full of zeros, then it updates this state at each time step, and\nafter the last time step, it throws it away, as it is not needed anymore. What if we told\nthe RNN to preserve this final state after processing one training batch and use it as\nthe initial state for the next training batch? This way the model can learn long-term\npatterns despite only backpropagating through short sequences. This is called a state\u2010\nful RNN. Let\u2019s see how to build one. First, note that a stateful RNN only makes sense if each input sequence in a batch\nstarts exactly where the corresponding sequence in the previous batch left off. So the\nfirst thing we need to do to build a stateful RNN is to use sequential and nonoverlap\u2010 | Chapter 16: Natural Language Processing with RNNs and Attention\nping input sequences (rather than the shuffled and overlapping sequences we used to\ntrain stateless RNNs). When creating the Dataset, we must therefore use\nshift=n_steps (instead of shift=1) when calling the window() method. Moreover,\nwe must obviously not call the shuffle() method. Unfortunately, batching is much\nharder when preparing a dataset for a stateful RNN than it is for a stateless RNN. Indeed, if we were to call batch(32), then 32 consecutive windows would be put in\nthe same batch, and the following batch would not continue each of these window\nwhere it left off. The first batch would contain windows 1 to 32 and the second batch\nwould contain windows 33 to 64, so if you consider, say, the first window of each\nbatch (i.e., windows 1 and 33), you can see that they are not consecutive. The simplest\nsolution to this problem is to just use \u201cbatches\u201d containing a single window:\ndataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\ndataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\ndataset = dataset.flat_map(lambda window: window.batch(window_length))\ndataset = dataset.batch(1)\ndataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\ndataset = dataset.map( lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\ndataset = dataset.prefetch(1)\nFigure 16-2 summarizes the first steps. Figure 16-2. Preparing a dataset of consecutive sequence fragments for a stateful RNN\nBatching is harder, but it is not impossible. For example, we could chop Shakespeare\u2019s\ntext into 32 texts of equal length, create one dataset of consecutive input sequences\nfor each of them, and finally use tf.train.Dataset.zip(datasets).map(lambda\n*windows: tf.stack(windows)) to create proper consecutive batches, where the nth\ninput sequence in a batch starts off exactly where the nth input sequence ended in the\nprevious batch (see the notebook for the full code). Generating Shakespearean Text Using a Character RNN | Now let\u2019s create the stateful RNN. First, we need to set stateful=True when creating\nevery recurrent layer. Second, the stateful RNN needs to know the batch size (since it\nwill preserve a state for each input sequence in the batch), so we must set the\nbatch_input_shape argument in the first layer."
  },
  {
    "id": 357,
    "content": "Note that we can leave the second\ndimension unspecified, since the inputs could have any length:\nmodel = keras.models.Sequential([ keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2, batch_input_shape=[batch_size, None, max_id]), keras.layers.GRU(128, return_sequences=True, stateful=True, dropout=0.2, recurrent_dropout=0.2), keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n])\nAt the end of each epoch, we need to reset the states before we go back to the begin\u2010\nning of the text. For this, we can use a small callback:\nclass ResetStatesCallback(keras.callbacks.Callback): def on_epoch_begin(self, epoch, logs): self.model.reset_states()\nAnd now we can compile and fit the model (for more epochs, because each epoch is\nmuch shorter than earlier, and there is only one instance per batch):\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\nmodel.fit(dataset, epochs=50, callbacks=[ResetStatesCallback()])\nAfter this model is trained, it will only be possible to use it to make\npredictions for batches of the same size as were used during train\u2010\ning. To avoid this restriction, create an identical stateless model,\nand copy the stateful model\u2019s weights to this model. Now that we have built a character-level model, it\u2019s time to look at word-level models\nand tackle a common natural language processing task: sentiment analysis. In the pro\u2010\ncess we will learn how to handle sequences of variable lengths using masking. Sentiment Analysis\nIf MNIST is the \u201chello world\u201d of computer vision, then the IMDb reviews dataset is\nthe \u201chello world\u201d of natural language processing: it consists of 50,000 movie reviews\nin English (25,000 for training, 25,000 for testing) extracted from the famous Internet\nMovie Database, along with a simple binary target for each review indicating whether\nit is negative (0) or positive (1). Just like MNIST, the IMDb reviews dataset is popular\nfor good reasons: it is simple enough to be tackled on a laptop in a reasonable amount | Chapter 16: Natural Language Processing with RNNs and Attention\n4 Taku Kudo, \u201cSubword Regularization: Improving Neural Network Translation Models with Multiple Subword\nCandidates,\u201d arXiv preprint arXiv:1804.10959 (2018). of time, but challenging enough to be fun and rewarding. Keras provides a simple\nfunction to load it:\n>>> (X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()\n>>> X_train[0][:10]\n[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]\nWhere are the movie reviews? Well, as you can see, the dataset is already prepro\u2010\ncessed for you: X_train consists of a list of reviews, each of which is represented as a\nNumPy array of integers, where each integer represents a word. All punctuation was\nremoved, and then words were converted to lowercase, split by spaces, and finally\nindexed by frequency (so low integers correspond to frequent words). The integers 0,\n1, and 2 are special: they represent the padding token, the start-of-sequence (SSS)\ntoken, and unknown words, respectively."
  },
  {
    "id": 358,
    "content": "If you want to visualize a review, you can\ndecode it like this:\n>>> word_index = keras.datasets.imdb.get_word_index()\n>>> id_to_word = {id_ + 3: word for word, id_ in word_index.items()}\n>>> for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n... id_to_word[id_] = token\n...\n>>> \" \".join([id_to_word[id_] for id_ in X_train[0][:10]])\n'<sos> this film was just brilliant casting location scenery story'\nIn a real project, you will have to preprocess the text yourself. You can do that using\nthe same Tokenizer class we used earlier, but this time setting char_level=False\n(which is the default). When encoding words, it filters out a lot of characters, includ\u2010\ning most punctuation, line breaks, and tabs (but you can change this by setting the\nfilters argument). Most importantly, it uses spaces to identify word boundaries. This is OK for English and many other scripts (written languages) that use spaces\nbetween words, but not all scripts use spaces this way. Chinese does not use spaces\nbetween words, Vietnamese uses spaces even within words, and languages such as\nGerman often attach multiple words together, without spaces. Even in English, spaces\nare not always the best way to tokenize text: think of \u201cSan Francisco\u201d or\n\u201c#ILoveDeepLearning.\u201d\nFortunately, there are better options! The 2018 paper4 by Taku Kudo introduced an\nunsupervised learning technique to tokenize and detokenize text at the subword level\nin a language-independent way, treating spaces like other characters. With this\napproach, even if your model encounters a word it has never seen before, it can still\nreasonably guess what it means. For example, it may never have seen the word\n\u201csmartest\u201d during training, but perhaps it learned the word \u201csmart\u201d and it also\nlearned that the suffix \u201cest\u201d means \u201cthe most,\u201d so it can infer the meaning of\nSentiment Analysis | 5 Taku Kudo and John Richardson, \u201cSentencePiece: A Simple and Language Independent Subword Tokenizer\nand Detokenizer for Neural Text Processing,\u201d arXiv preprint arXiv:1808.06226 (2018). 6 Rico Sennrich et al., \u201cNeural Machine Translation of Rare Words with Subword Units,\u201d Proceedings of the 54th\nAnnual Meeting of the Association for Computational Linguistics 1 (2016): 1715\u20131725. 7 Yonghui Wu et al., \u201cGoogle\u2019s Neural Machine Translation System: Bridging the Gap Between Human and\nMachine Translation,\u201d arXiv preprint arXiv:1609.08144 (2016). \u201csmartest.\u201d Google\u2019s SentencePiece project provides an open source implementation,\ndescribed in a paper5 by Taku Kudo and John Richardson. Another option was proposed in an earlier paper6 by Rico Sennrich et al. that\nexplored other ways of creating subword encodings (e.g., using byte pair encoding). Last but not least, the TensorFlow team released the TF.Text library in June 2019,\nwhich implements various tokenization strategies, including WordPiece7 (a variant of\nbyte pair encoding). If you want to deploy your model to a mobile device or a web browser, and you don\u2019t\nwant to have to write a different preprocessing function every time, then you will\nwant to handle preprocessing using only TensorFlow operations, so it can be included\nin the model itself. Let\u2019s see how."
  },
  {
    "id": 359,
    "content": "First, let\u2019s load the original IMDb reviews, as text\n(byte strings), using TensorFlow Datasets (introduced in Chapter 13):\nimport tensorflow_datasets as tfds\ndatasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\ntrain_size = info.splits[\"train\"].num_examples\nNext, let\u2019s write the preprocessing function:\ndef preprocess(X_batch, y_batch): X_batch = tf.strings.substr(X_batch, 0, 300) X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \") X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \") X_batch = tf.strings.split(X_batch) return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch\nIt starts by truncating the reviews, keeping only the first 300 characters of each: this\nwill speed up training, and it won\u2019t impact performance too much because you can\ngenerally tell whether a review is positive or not in the first sentence or two. Then it\nuses regular expressions to replace <br /> tags with spaces, and to replace any charac\u2010\nters other than letters and quotes with spaces. For example, the text \"Well, I\ncan't<br />\" will become \"Well I can't\". Finally, the preprocess() function\nsplits the reviews by the spaces, which returns a ragged tensor, and it converts this\nragged tensor to a dense tensor, padding all reviews with the padding token \"<pad>\"\nso that they all have the same length. | Chapter 16: Natural Language Processing with RNNs and Attention\nNext, we need to construct the vocabulary. This requires going through the whole\ntraining set once, applying our preprocess() function, and using a Counter to count\nthe number of occurrences of each word:\nfrom collections import Counter\nvocabulary = Counter()\nfor X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess): for review in X_batch: vocabulary.update(list(review.numpy()))\nLet\u2019s look at the three most common words:\n>>> vocabulary.most_common()[:3]\n[(b'<pad>', 215797), (b'the', 61137), (b'a', 38564)]\nGreat! We probably don\u2019t need our model to know all the words in the dictionary to\nget good performance, though, so let\u2019s truncate the vocabulary, keeping only the\n10,000 most common words:\nvocab_size = 10000\ntruncated_vocabulary = [ word for word, count in vocabulary.most_common()[:vocab_size]]\nNow we need to add a preprocessing step to replace each word with its ID (i.e., its\nindex in the vocabulary). Just like we did in Chapter 13, we will create a lookup table\nfor this, using 1,000 out-of-vocabulary (oov) buckets:\nwords = tf.constant(truncated_vocabulary)\nword_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\nvocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\nnum_oov_buckets = 1000\ntable = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\nWe can then use this table to look up the IDs of a few words:\n>>> table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))\n<tf.Tensor: [...], dtype=int64, numpy=array([[ 22, 12, 11, 10054]])>\nNote that the words \u201cthis,\u201d \u201cmovie,\u201d and \u201cwas\u201d were found in the table, so their IDs\nare lower than 10,000, while the word \u201cfaaaaaantastic\u201d was not found, so it was map\u2010\nped to one of the oov buckets, with an ID greater than or equal to 10,000. TF Transform (introduced in Chapter 13) provides some useful\nfunctions to handle such vocabularies. For example, check out the\ntft.compute_and_apply_vocabulary() function: it will go\nthrough the dataset to find all distinct words and build the vocabu\u2010\nlary, and it will generate the TF operations required to encode each\nword using this vocabulary."
  },
  {
    "id": 360,
    "content": "Now we are ready to create the final training set. We batch the reviews, then convert\nthem to short sequences of words using the preprocess() function, then encode\nSentiment Analysis | these words using a simple encode_words() function that uses the table we just built,\nand finally prefetch the next batch:\ndef encode_words(X_batch, y_batch): return table.lookup(X_batch), y_batch\ntrain_set = datasets[\"train\"].batch(32).map(preprocess)\ntrain_set = train_set.map(encode_words).prefetch(1)\nAt last we can create the model and train it:\nembed_size = 128\nmodel = keras.models.Sequential([ keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape=[None]), keras.layers.GRU(128, return_sequences=True), keras.layers.GRU(128), keras.layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nhistory = model.fit(train_set, epochs=5)\nThe first layer is an Embedding layer, which will convert word IDs into embeddings\n(introduced in Chapter 13). The embedding matrix needs to have one row per word\nID (vocab_size + num_oov_buckets) and one column per embedding dimension\n(this example uses 128 dimensions, but this is a hyperparameter you could tune). Whereas the inputs of the model will be 2D tensors of shape [batch size, time steps],\nthe output of the Embedding layer will be a 3D tensor of shape [batch size, time steps,\nembedding size]. The rest of the model is fairly straightforward: it is composed of two GRU layers, with\nthe second one returning only the output of the last time step. The output layer is just\na single neuron using the sigmoid activation function to output the estimated proba\u2010\nbility that the review expresses a positive sentiment regarding the movie. We then\ncompile the model quite simply, and we fit it on the dataset we prepared earlier, for a\nfew epochs. Masking\nAs it stands, the model will need to learn that the padding tokens should be ignored. But we already know that! Why don\u2019t we tell the model to ignore the padding tokens,\nso that it can focus on the data that actually matters? It\u2019s actually trivial: simply add | Chapter 16: Natural Language Processing with RNNs and Attention\n8 Their ID is 0 only because they are the most frequent \u201cwords\u201d in the dataset. It would probably be a good idea\nto ensure that the padding tokens are always encoded as 0, even if they are not the most frequent. mask_zero=True when creating the Embedding layer. This means that padding tokens\n(whose ID is 0)8 will be ignored by all downstream layers. That\u2019s all! The way this works is that the Embedding layer creates a mask tensor equal to\nK.not_equal(inputs, 0) (where K = keras.backend): it is a Boolean tensor with\nthe same shape as the inputs, and it is equal to False anywhere the word IDs are 0, or\nTrue otherwise. This mask tensor is then automatically propagated by the model to\nall subsequent layers, as long as the time dimension is preserved. So in this example,\nboth GRU layers will receive this mask automatically, but since the second GRU layer\ndoes not return sequences (it only returns the output of the last time step), the mask\nwill not be transmitted to the Dense layer."
  },
  {
    "id": 361,
    "content": "Each layer may handle the mask differently,\nbut in general they simply ignore masked time steps (i.e., time steps for which the\nmask is False). For example, when a recurrent layer encounters a masked time step,\nit simply copies the output from the previous time step. If the mask propagates all the\nway to the output (in models that output sequences, which is not the case in this\nexample), then it will be applied to the losses as well, so the masked time steps will\nnot contribute to the loss (their loss will be 0). The LSTM and GRU layers have an optimized implementation for\nGPUs, based on Nvidia\u2019s cuDNN library. However, this implemen\u2010\ntation does not support masking. If your model uses a mask, then\nthese layers will fall back to the (much slower) default implementa\u2010\ntion. Note that the optimized implementation also requires you to\nuse the default values for several hyperparameters: activation,\nrecurrent_activation, recurrent_dropout, unroll, use_bias,\nand reset_after. All layers that receive the mask must support masking (or else an exception will be\nraised). This includes all recurrent layers, as well as the TimeDistributed layer and a\nfew other layers. Any layer that supports masking must have a supports_masking\nattribute equal to True. If you want to implement your own custom layer with mask\u2010\ning support, you should add a mask argument to the call() method (and obviously\nmake the method use the mask somehow). Additionally, you should set\nself.supports_masking = True in the constructor. If your layer does not start with\nan Embedding layer, you may use the keras.layers.Masking layer instead: it sets the\nmask to K.any(K.not_equal(inputs, 0), axis=-1), meaning that time steps where\nthe last dimension is full of zeros will be masked out in subsequent layers (again, as\nlong as the time dimension exists). Sentiment Analysis | Using masking layers and automatic mask propagation works best for simple\nSequential models. It will not always work for more complex models, such as when\nyou need to mix Conv1D layers with recurrent layers. In such cases, you will need to\nexplicitly compute the mask and pass it to the appropriate layers, using either the\nFunctional API or the Subclassing API. For example, the following model is identical\nto the previous model, except it is built using the Functional API and handles mask\u2010\ning manually:\nK = keras.backend\ninputs = keras.layers.Input(shape=[None])\nmask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)\nz = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)\nz = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)\nz = keras.layers.GRU(128)(z, mask=mask)\noutputs = keras.layers.Dense(1, activation=\"sigmoid\")(z)\nmodel = keras.Model(inputs=[inputs], outputs=[outputs])\nAfter training for a few epochs, this model will become quite good at judging whether\na review is positive or not. If you use the TensorBoard() callback, you can visualize\nthe embeddings in TensorBoard as they are being learned: it is fascinating to see\nwords like \u201cawesome\u201d and \u201camazing\u201d gradually cluster on one side of the embedding\nspace, while words like \u201cawful\u201d and \u201cterrible\u201d cluster on the other side."
  },
  {
    "id": 362,
    "content": "Some words\nare not as positive as you might expect (at least with this model), such as the word\n\u201cgood,\u201d presumably because many negative reviews contain the phrase \u201cnot good.\u201d It\u2019s\nimpressive that the model is able to learn useful word embeddings based on just\n25,000 movie reviews. Imagine how good the embeddings would be if we had billions\nof reviews to train on! Unfortunately we don\u2019t, but perhaps we can reuse word\nembeddings trained on some other large text corpus (e.g., Wikipedia articles), even if\nit is not composed of movie reviews? After all, the word \u201camazing\u201d generally has the\nsame meaning whether you use it to talk about movies or anything else. Moreover,\nperhaps embeddings would be useful for sentiment analysis even if they were trained\non another task: since words like \u201cawesome\u201d and \u201camazing\u201d have a similar meaning,\nthey will likely cluster in the embedding space even for other tasks (e.g., predicting\nthe next word in a sentence). If all positive words and all negative words form clus\u2010\nters, then this will be helpful for sentiment analysis. So instead of using so many\nparameters to learn word embeddings, let\u2019s see if we can\u2019t just reuse pretrained\nembeddings. Reusing Pretrained Embeddings\nThe TensorFlow Hub project makes it easy to reuse pretrained model components in\nyour own models. These model components are called modules. Simply browse the\nTF Hub repository, find the one you need, and copy the code example into your\nproject, and the module will be automatically downloaded, along with its pretrained\nweights, and included in your model. Easy! | Chapter 16: Natural Language Processing with RNNs and Attention\n9 To be precise, the sentence embedding is equal to the mean word embedding multiplied by the square root of\nthe number of words in the sentence. This compensates for the fact that the mean of n vectors gets shorter as\nn grows. For example, let\u2019s use the nnlm-en-dim50 sentence embedding module, version 1, in\nour sentiment analysis model:\nimport tensorflow_hub as hub\nmodel = keras.Sequential([ hub.KerasLayer(\" dtype=tf.string, input_shape=[], output_shape=[50]), keras.layers.Dense(128, activation=\"relu\"), keras.layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nThe hub.KerasLayer layer downloads the module from the given URL. This particu\u2010\nlar module is a sentence encoder: it takes strings as input and encodes each one as a\nsingle vector (in this case, a 50-dimensional vector). Internally, it parses the string\n(splitting words on spaces) and embeds each word using an embedding matrix that\nwas pretrained on a huge corpus: the Google News 7B corpus (seven billion words\nlong!). Then it computes the mean of all the word embeddings, and the result is the\nsentence embedding.9 We can then add two simple Dense layers to create a good sen\u2010\ntiment analysis model. By default, a hub.KerasLayer is not trainable, but you can set\ntrainable=True when creating it to change that so that you can fine-tune it for your\ntask. Not all TF Hub modules support TensorFlow 2, so make sure you\nchoose a module that does."
  },
  {
    "id": 363,
    "content": "Next, we can just load the IMDb reviews dataset\u2014no need to preprocess it (except for\nbatching and prefetching)\u2014and directly train the model:\ndatasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\ntrain_size = info.splits[\"train\"].num_examples\nbatch_size = 32\ntrain_set = datasets[\"train\"].batch(batch_size).prefetch(1)\nhistory = model.fit(train_set, epochs=5)\nNote that the last part of the TF Hub module URL specified that we wanted version 1\nof the model. This versioning ensures that if a new module version is released, it will\nnot break our model. Conveniently, if you just enter this URL in a web browser, you\nSentiment Analysis | 10 Ilya Sutskever et al., \u201cSequence to Sequence Learning with Neural Networks,\u201d arXiv preprint arXiv:1409.3215\n(2014). will get the documentation for this module. By default, TF Hub will cache the down\u2010\nloaded files into the local system\u2019s temporary directory. You may prefer to download\nthem into a more permanent directory to avoid having to download them again after\nevery system cleanup. To do that, set the TFHUB_CACHE_DIR environment variable to\nthe directory of your choice (e.g., os.environ[\"TFHUB_CACHE_DIR\"] = \"./\nmy_tfhub_cache\"). So far, we have looked at time series, text generation using Char-RNN, and sentiment\nanalysis using word-level RNN models, training our own word embeddings or reus\u2010\ning pretrained embeddings. Let\u2019s now look at another important NLP task: neural\nmachine translation (NMT), first using a pure Encoder\u2013Decoder model, then improv\u2010\ning it with attention mechanisms, and finally looking the extraordinary Transformer\narchitecture. An Encoder\u2013Decoder Network for Neural Machine\nTranslation\nLet\u2019s take a look at a simple neural machine translation model10 that will translate\nEnglish sentences to French (see Figure 16-3). In short, the English sentences are fed to the encoder, and the decoder outputs the\nFrench translations. Note that the French translations are also used as inputs to the\ndecoder, but shifted back by one step. In other words, the decoder is given as input\nthe word that it should have output at the previous step (regardless of what it actually\noutput). For the very first word, it is given the start-of-sequence (SOS) token. The\ndecoder is expected to end the sentence with an end-of-sequence (EOS) token. Note that the English sentences are reversed before they are fed to the encoder. For\nexample, \u201cI drink milk\u201d is reversed to \u201cmilk drink I.\u201d This ensures that the beginning\nof the English sentence will be fed last to the encoder, which is useful because that\u2019s\ngenerally the first thing that the decoder needs to translate. Each word is initially represented by its ID (e.g., 288 for the word \u201cmilk\u201d). Next, an\nembedding layer returns the word embedding. These word embeddings are what is\nactually fed to the encoder and the decoder. | Chapter 16: Natural Language Processing with RNNs and Attention\nFigure 16-3. A simple machine translation model\nAt each step, the decoder outputs a score for each word in the output vocabulary (i.e.,\nFrench), and then the softmax layer turns these scores into probabilities."
  },
  {
    "id": 364,
    "content": "For exam\u2010\nple, at the first step the word \u201cJe\u201d may have a probability of 20%, \u201cTu\u201d may have a\nprobability of 1%, and so on. The word with the highest probability is output. This is\nvery much like a regular classification task, so you can train the model using the\n\"sparse_categorical_crossentropy\" loss, much like we did in the Char-RNN\nmodel. Note that at inference time (after training), you will not have the target sentence to\nfeed to the decoder. Instead, simply feed the decoder the word that it output at the\nprevious step, as shown in Figure 16-4 (this will require an embedding lookup that is\nnot shown in the diagram). An Encoder\u2013Decoder Network for Neural Machine Translation | Figure 16-4. Feeding the previous output word as input at inference time\nOK, now you have the big picture. Still, there are a few more details to handle if you\nimplement this model:\n\u2022 So far we have assumed that all input sequences (to the encoder and to the\ndecoder) have a constant length. But obviously sentence lengths vary. Since regu\u2010\nlar tensors have fixed shapes, they can only contain sentences of the same length. You can use masking to handle this, as discussed earlier. However, if the senten\u2010\nces have very different lengths, you can\u2019t just crop them like we did for sentiment\nanalysis (because we want full translations, not cropped translations). Instead,\ngroup sentences into buckets of similar lengths (e.g., a bucket for the 1- to 6-\nword sentences, another for the 7- to 12-word sentences, and so on), using pad\u2010\nding for the shorter sequences to ensure all sentences in a bucket have the same\nlength (check out the tf.data.experimental.bucket_by_sequence_length()\nfunction for this). For example, \u201cI drink milk\u201d becomes \u201c<pad> <pad> <pad>\nmilk drink I.\u201d\n\u2022 We want to ignore any output past the EOS token, so these tokens should not\ncontribute to the loss (they must be masked out). For example, if the model out\u2010\nputs \u201cJe bois du lait <eos> oui,\u201d the loss for the last word should be ignored. \u2022 When the output vocabulary is large (which is the case here), outputting a proba\u2010\nbility for each and every possible word would be terribly slow. If the target\nvocabulary contains, say, 50,000 French words, then the decoder would output\n50,000-dimensional vectors, and then computing the softmax function over such\na large vector would be very computationally intensive. To avoid this, one solu\u2010\ntion is to look only at the logits output by the model for the correct word and for\na random sample of incorrect words, then compute an approximation of the loss\nbased only on these logits."
  },
  {
    "id": 365,
    "content": "This sampled softmax technique was introduced in | Chapter 16: Natural Language Processing with RNNs and Attention\n11 S\u00e9bastien Jean et al., \u201cOn Using Very Large Target Vocabulary for Neural Machine Translation,\u201d Proceedings of\nthe 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Con\u2010\nference on Natural Language Processing of the Asian Federation of Natural Language Processing 1 (2015): 1\u201310. 2015 by S\u00e9bastien Jean et al..11 In TensorFlow you can use the tf.nn.sam\npled_softmax_loss() function for this during training and use the normal soft\u2010\nmax function at inference time (sampled softmax cannot be used at inference\ntime because it requires knowing the target). The TensorFlow Addons project includes many sequence-to-sequence tools to let you\neasily build production-ready Encoder\u2013Decoders. For example, the following code\ncreates a basic Encoder\u2013Decoder model, similar to the one represented in\nFigure 16-3:\nimport tensorflow_addons as tfa\nencoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\ndecoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\nsequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\nembeddings = keras.layers.Embedding(vocab_size, embed_size)\nencoder_embeddings = embeddings(encoder_inputs)\ndecoder_embeddings = embeddings(decoder_inputs)\nencoder = keras.layers.LSTM(512, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_embeddings)\nencoder_state = [state_h, state_c]\nsampler = tfa.seq2seq.sampler.TrainingSampler()\ndecoder_cell = keras.layers.LSTMCell(512)\noutput_layer = keras.layers.Dense(vocab_size)\ndecoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler, output_layer=output_layer)\nfinal_outputs, final_state, final_sequence_lengths = decoder( decoder_embeddings, initial_state=encoder_state, sequence_length=sequence_lengths)\nY_proba = tf.nn.softmax(final_outputs.rnn_output)\nmodel = keras.Model(inputs=[encoder_inputs, decoder_inputs, sequence_lengths], outputs=[Y_proba])\nThe code is mostly self-explanatory, but there are a few points to note. First, we set\nreturn_state=True when creating the LSTM layer so that we can get its final hidden\nstate and pass it to the decoder. Since we are using an LSTM cell, it actually returns\ntwo hidden states (short term and long term). The TrainingSampler is one of several\nsamplers available in TensorFlow Addons: their role is to tell the decoder at each step\nwhat it should pretend the previous output was. During inference, this should be the\nAn Encoder\u2013Decoder Network for Neural Machine Translation | 12 Samy Bengio et al., \u201cScheduled Sampling for Sequence Prediction with Recurrent Neural Networks,\u201d arXiv\npreprint arXiv:1506.03099 (2015). embedding of the token that was actually output. During training, it should be the\nembedding of the previous target token: this is why we used the TrainingSampler. In\npractice, it is often a good idea to start training with the embedding of the target of\nthe previous time step and gradually transition to using the embedding of the actual\ntoken that was output at the previous step. This idea was introduced in a 2015 paper12\nby Samy Bengio et al. The ScheduledEmbeddingTrainingSampler will randomly\nchoose between the target or the actual output, with a probability that you can gradu\u2010\nally change during training. Bidirectional RNNs\nA each time step, a regular recurrent layer only looks at past and present inputs\nbefore generating its output. In other words, it is \u201ccausal,\u201d meaning it cannot look into\nthe future."
  },
  {
    "id": 366,
    "content": "This type of RNN makes sense when forecasting time series, but for many\nNLP tasks, such as Neural Machine Translation, it is often preferable to look ahead at\nthe next words before encoding a given word. For example, consider the phrases \u201cthe\nQueen of the United Kingdom,\u201d \u201cthe queen of hearts,\u201d and \u201cthe queen bee\u201d: to prop\u2010\nerly encode the word \u201cqueen,\u201d you need to look ahead. To implement this, run two\nrecurrent layers on the same inputs, one reading the words from left to right and the\nother reading them from right to left. Then simply combine their outputs at each\ntime step, typically by concatenating them. This is called a bidirectional recurrent layer\n(see Figure 16-5). To implement a bidirectional recurrent layer in Keras, wrap a recurrent layer in a\nkeras.layers.Bidirectional layer. For example, the following code creates a bidir\u2010\nectional GRU layer:\nkeras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))\nThe Bidirectional layer will create a clone of the GRU layer (but in\nthe reverse direction), and it will run both and concatenate their\noutputs. So although the GRU layer has 10 units, the Bidirectional\nlayer will output 20 values per time step. | Chapter 16: Natural Language Processing with RNNs and Attention\nFigure 16-5. A bidirectional recurrent layer\nBeam Search\nSuppose you train an Encoder\u2013Decoder model, and use it to translate the French sen\u2010\ntence \u201cComment vas-tu?\u201d to English. You are hoping that it will output the proper\ntranslation (\u201cHow are you?\u201d), but unfortunately it outputs \u201cHow will you?\u201d Looking\nat the training set, you notice many sentences such as \u201cComment vas-tu jouer?\u201d\nwhich translates to \u201cHow will you play?\u201d So it wasn\u2019t absurd for the model to output\n\u201cHow will\u201d after seeing \u201cComment vas.\u201d Unfortunately, in this case it was a mistake,\nand the model could not go back and fix it, so it tried to complete the sentence as best\nit could. By greedily outputting the most likely word at every step, it ended up with a\nsuboptimal translation. How can we give the model a chance to go back and fix mis\u2010\ntakes it made earlier? One of the most common solutions is beam search: it keeps\ntrack of a short list of the k most promising sentences (say, the top three), and at each\ndecoder step it tries to extend them by one word, keeping only the k most likely sen\u2010\ntences. The parameter k is called the beam width. For example, suppose you use the model to translate the sentence \u201cComment vas-tu?\u201d\nusing beam search with a beam width of 3. At the first decoder step, the model will\noutput an estimated probability for each possible word. Suppose the top three words\nare \u201cHow\u201d (75% estimated probability), \u201cWhat\u201d (3%), and \u201cYou\u201d (1%). That\u2019s our\nshort list so far. Next, we create three copies of our model and use them to find the\nnext word for each sentence. Each model will output one estimated probability per\nword in the vocabulary."
  },
  {
    "id": 367,
    "content": "The first model will try to find the next word in the sentence\n\u201cHow,\u201d and perhaps it will output a probability of 36% for the word \u201cwill,\u201d 32% for the\nword \u201care,\u201d 16% for the word \u201cdo,\u201d and so on. Note that these are actually conditional\nprobabilities, given that the sentence starts with \u201cHow.\u201d The second model will try to\ncomplete the sentence \u201cWhat\u201d; it might output a conditional probability of 50% for\nAn Encoder\u2013Decoder Network for Neural Machine Translation | the word \u201care,\u201d and so on. Assuming the vocabulary has 10,000 words, each model\nwill output 10,000 probabilities. Next, we compute the probabilities of each of the 30,000 two-word sentences that\nthese models considered (3 \u00d7 10,000). We do this by multiplying the estimated condi\u2010\ntional probability of each word by the estimated probability of the sentence it com\u2010\npletes. For example, the estimated probability of the sentence \u201cHow\u201d was 75%, while\nthe estimated conditional probability of the word \u201cwill\u201d (given that the first word is\n\u201cHow\u201d) was 36%, so the estimated probability of the sentence \u201cHow will\u201d is 75% \u00d7\n36% = 27%. After computing the probabilities of all 30,000 two-word sentences, we\nkeep only the top 3. Perhaps they all start with the word \u201cHow\u201d: \u201cHow will\u201d (27%),\n\u201cHow are\u201d (24%), and \u201cHow do\u201d (12%). Right now, the sentence \u201cHow will\u201d is win\u2010\nning, but \u201cHow are\u201d has not been eliminated. Then we repeat the same process: we use three models to predict the next word in\neach of these three sentences, and we compute the probabilities of all 30,000 three-\nword sentences we considered. Perhaps the top three are now \u201cHow are you\u201d (10%),\n\u201cHow do you\u201d (8%), and \u201cHow will you\u201d (2%). At the next step we may get \u201cHow do\nyou do\u201d (7%), \u201cHow are you <eos>\u201d (6%), and \u201cHow are you doing\u201d (3%). Notice that\n\u201cHow will\u201d was eliminated, and we now have three perfectly reasonable translations. We boosted our Encoder\u2013Decoder model\u2019s performance without any extra training,\nsimply by using it more wisely. You can implement beam search fairly easily using TensorFlow Addons:\nbeam_width = 10\ndecoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder( cell=decoder_cell, beam_width=beam_width, output_layer=output_layer)\ndecoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch( encoder_state, multiplier=beam_width)\noutputs, _, _ = decoder( embedding_decoder, start_tokens=start_tokens, end_token=end_token, initial_state=decoder_initial_state)\nWe first create a BeamSearchDecoder, which wraps all the decoder clones (in this case\n10 clones). Then we create one copy of the encoder\u2019s final state for each decoder\nclone, and we pass these states to the decoder, along with the start and end tokens. With all this, you can get good translations for fairly short sentences (especially if you\nuse pretrained word embeddings). Unfortunately, this model will be really bad at\ntranslating long sentences. Once again, the problem comes from the limited short-\nterm memory of RNNs. Attention mechanisms are the game-changing innovation that\naddressed this problem."
  },
  {
    "id": 368,
    "content": "| Chapter 16: Natural Language Processing with RNNs and Attention\n13 Dzmitry Bahdanau et al., \u201cNeural Machine Translation by Jointly Learning to Align and Translate,\u201d arXiv pre\u2010\nprint arXiv:1409.0473 (2014). 14 The most common metric used in NMT is the BiLingual Evaluation Understudy (BLEU) score, which com\u2010\npares each translation produced by the model with several good translations produced by humans: it counts\nthe number of n-grams (sequences of n words) that appear in any of the target translations and adjusts the\nscore to take into account the frequency of the produced n-grams in the target translations. Attention Mechanisms\nConsider the path from the word \u201cmilk\u201d to its translation \u201clait\u201d in Figure 16-3: it is\nquite long! This means that a representation of this word (along with all the other\nwords) needs to be carried over many steps before it is actually used. Can\u2019t we make\nthis path shorter? This was the core idea in a groundbreaking 2014 paper13 by Dzmitry Bahdanau et al. They introduced a technique that allowed the decoder to focus on the appropriate\nwords (as encoded by the encoder) at each time step. For example, at the time step\nwhere the decoder needs to output the word \u201clait,\u201d it will focus its attention on the\nword \u201cmilk.\u201d This means that the path from an input word to its translation is now\nmuch shorter, so the short-term memory limitations of RNNs have much less impact. Attention mechanisms revolutionized neural machine translation (and NLP in gen\u2010\neral), allowing a significant improvement in the state of the art, especially for long\nsentences (over 30 words).14\nFigure 16-6 shows this model\u2019s architecture (slightly simplified, as we will see). On the\nleft, you have the encoder and the decoder. Instead of just sending the encoder\u2019s final\nhidden state to the decoder (which is still done, although it is not shown in the fig\u2010\nure), we now send all of its outputs to the decoder. At each time step, the decoder\u2019s\nmemory cell computes a weighted sum of all these encoder outputs: this determines\nwhich words it will focus on at this step. The weight \u03b1(t,i) is the weight of the ith\nencoder output at the tth decoder time step. For example, if the weight \u03b1(3,2) is much\nlarger than the weights \u03b1(3,0) and \u03b1(3,1), then the decoder will pay much more attention\nto word number 2 (\u201cmilk\u201d) than to the other two words, at least at this time step. The\nrest of the decoder works just like earlier: at each time step the memory cell receives\nthe inputs we just discussed, plus the hidden state from the previous time step, and\nfinally (although it is not represented in the diagram) it receives the target word from\nthe previous time step (or at inference time, the output from the previous time step). Attention Mechanisms | 15 Recall that a time-distributed Dense layer is equivalent to a regular Dense layer that you apply independently\nat each time step (only much faster). Figure 16-6."
  },
  {
    "id": 369,
    "content": "Neural machine translation using an Encoder\u2013Decoder network with an\nattention model\nBut where do these \u03b1(t,i) weights come from? It\u2019s actually pretty simple: they are gener\u2010\nated by a type of small neural network called an alignment model (or an attention\nlayer), which is trained jointly with the rest of the Encoder\u2013Decoder model. This\nalignment model is illustrated on the righthand side of Figure 16-6. It starts with a\ntime-distributed Dense layer15 with a single neuron, which receives as input all the\nencoder outputs, concatenated with the decoder\u2019s previous hidden state (e.g., h(2)). This layer outputs a score (or energy) for each encoder output (e.g., e(3, 2)): this score\nmeasures how well each output is aligned with the decoder\u2019s previous hidden state. Finally, all the scores go through a softmax layer to get a final weight for each encoder\noutput (e.g., \u03b1(3,2)). All the weights for a given decoder time step add up to 1 (since the\nsoftmax layer is not time-distributed). This particular attention mechanism is called\nBahdanau attention (named after the paper\u2019s first author). Since it concatenates the\nencoder output with the decoder\u2019s previous hidden state, it is sometimes called con\u2010\ncatenative attention (or additive attention). | Chapter 16: Natural Language Processing with RNNs and Attention\n16 Minh-Thang Luong et al., \u201cEffective Approaches to Attention-Based Neural Machine Translation,\u201d Proceed\u2010\nings of the 2015 Conference on Empirical Methods in Natural Language Processing (2015): 1412\u20131421. If the input sentence is n words long, and assuming the output sen\u2010\ntence is about as long, then this model will need to compute about\nn2 weights. Fortunately, this quadratic computational complexity is\nstill tractable because even long sentences don\u2019t have thousands of\nwords. Another common attention mechanism was proposed shortly after, in a 2015 paper16\nby Minh-Thang Luong et al. Because the goal of the attention mechanism is to meas\u2010\nure the similarity between one of the encoder\u2019s outputs and the decoder\u2019s previous\nhidden state, the authors proposed to simply compute the dot product (see Chapter 4)\nof these two vectors, as this is often a fairly good similarity measure, and modern\nhardware can compute it much faster. For this to be possible, both vectors must have\nthe same dimensionality. This is called Luong attention (again, after the paper\u2019s first\nauthor), or sometimes multiplicative attention. The dot product gives a score, and all\nthe scores (at a given decoder time step) go through a softmax layer to give the final\nweights, just like in Bahdanau attention. Another simplification they proposed was to\nuse the decoder\u2019s hidden state at the current time step rather than at the previous time\nstep (i.e., h(t)) rather than h(t\u20131)), then to use the output of the attention mechanism\n(noted \ufffdt ) directly to compute the decoder\u2019s predictions (rather than using it to\ncompute the decoder\u2019s current hidden state)."
  },
  {
    "id": 370,
    "content": "They also proposed a variant of the dot\nproduct mechanism where the encoder outputs first go through a linear transforma\u2010\ntion (i.e., a time-distributed Dense layer without a bias term) before the dot products\nare computed. This is called the \u201cgeneral\u201d dot product approach. They compared both\ndot product approaches to the concatenative attention mechanism (adding a rescaling\nparameter vector v), and they observed that the dot product variants performed bet\u2010\nter than concatenative attention. For this reason, concatenative attention is much less\nused now. The equations for these three attention mechanisms are summarized in\nEquation 16-1. Attention Mechanisms | 17 Kelvin Xu et al., \u201cShow, Attend and Tell: Neural Image Caption Generation with Visual Attention,\u201d Proceedings\nof the 32nd International Conference on Machine Learning (2015): 2048\u20132057. Equation 16-1. Attention mechanisms\n\ufffdt = \u2211\ni \u03b1 t, i \ufffdi\nwith \u03b1 t, i =\nexp e t, i\n\u2211i\u2032 exp e t, i\u2032\nand e t, i =\n\ufffdt\n\u22ba\ufffdi\ndot\n\ufffdt\n\u22ba\ufffd\ufffdi\ngeneral\n\ufffd\u22batanh \ufffd\ufffdt ; \ufffdi\nconcat\nHere is how you can add Luong attention to an Encoder\u2013Decoder model using Ten\u2010\nsorFlow Addons:\nattention_mechanism = tfa.seq2seq.attention_wrapper.LuongAttention( units, encoder_state, memory_sequence_length=encoder_sequence_length)\nattention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper( decoder_cell, attention_mechanism, attention_layer_size=n_units)\nWe simply wrap the decoder cell in an AttentionWrapper, and we provide the desired\nattention mechanism (Luong attention in this example). Visual Attention\nAttention mechanisms are now used for a variety of purposes. One of their first appli\u2010\ncations beyond NMT was in generating image captions using visual attention:17 a\nconvolutional neural network first processes the image and outputs some feature\nmaps, then a decoder RNN equipped with an attention mechanism generates the cap\u2010\ntion, one word at a time. At each decoder time step (each word), the decoder uses the\nattention model to focus on just the right part of the image. For example, in\nFigure 16-7, the model generated the caption \u201cA woman is throwing a frisbee in a\npark,\u201d and you can see what part of the input image the decoder focused its attention\non when it was about to output the word \u201cfrisbee\u201d: clearly, most of its attention was\nfocused on the frisbee. | Chapter 16: Natural Language Processing with RNNs and Attention\n18 This is a part of figure 3 from the paper. It is reproduced with the kind authorization of the authors. 19 Marco Tulio Ribeiro et al., \u201c\u2018Why Should I Trust You?\u2019: Explaining the Predictions of Any Classifier,\u201d Proceed\u2010\nings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016):\n1135\u20131144. Figure 16-7. Visual attention: an input image (left) and the model\u2019s focus before produc\u2010\ning the word \u201cfrisbee\u201d (right)18\nExplainability\nOne extra benefit of attention mechanisms is that they make it easier to understand\nwhat led the model to produce its output. This is called explainability."
  },
  {
    "id": 371,
    "content": "It can be espe\u2010\ncially useful when the model makes a mistake: for example, if an image of a dog walk\u2010\ning in the snow is labeled as \u201ca wolf walking in the snow,\u201d then you can go back and\ncheck what the model focused on when it output the word \u201cwolf.\u201d You may find that it\nwas paying attention not only to the dog, but also to the snow, hinting at a possible\nexplanation: perhaps the way the model learned to distinguish dogs from wolves is by\nchecking whether or not there\u2019s a lot of snow around. You can then fix this by training\nthe model with more images of wolves without snow, and dogs with snow. This exam\u2010\nple comes from a great 2016 paper19 by Marco Tulio Ribeiro et al. that uses a different\napproach to explainability: learning an interpretable model locally around a classi\u2010\nfier\u2019s prediction. In some applications, explainability is not just a tool to debug a model; it can be a\nlegal requirement (think of a system deciding whether or not it should grant you a\nloan). Attention Mechanisms | 20 Ashish Vaswani et al., \u201cAttention Is All You Need,\u201d Proceedings of the 31st International Conference on Neural\nInformation Processing Systems (2017): 6000\u20136010. 21 Since the Transformer uses time-distributed Dense layers, you could argue that it uses 1D convolutional layers\nwith a kernel size of 1. Attention mechanisms are so powerful that you can actually build state-of-the-art\nmodels using only attention mechanisms. Attention Is All You Need: The Transformer Architecture\nIn a groundbreaking 2017 paper,20 a team of Google researchers suggested that\n\u201cAttention Is All You Need.\u201d They managed to create an architecture called the Trans\u2010\nformer, which significantly improved the state of the art in NMT without using any\nrecurrent or convolutional layers,21 just attention mechanisms (plus embedding lay\u2010\ners, dense layers, normalization layers, and a few other bits and pieces). As an extra\nbonus, this architecture was also much faster to train and easier to parallelize, so they\nmanaged to train it at a fraction of the time and cost of the previous state-of-the-art\nmodels. The Transformer architecture is represented in Figure 16-8. | Chapter 16: Natural Language Processing with RNNs and Attention\n22 This is figure 1 from the paper, reproduced with the kind authorization of the authors. Figure 16-8. The Transformer architecture22\nLet\u2019s walk through this figure:\n\u2022 The lefthand part is the encoder. Just like earlier, it takes as input a batch of sen\u2010\ntences represented as sequences of word IDs (the input shape is [batch size, max\ninput sentence length]), and it encodes each word into a 512-dimensional repre\u2010\nsentation (so the encoder\u2019s output shape is [batch size, max input sentence length,\n512]). Note that the top part of the encoder is stacked N times (in the paper,\nN = 6). Attention Mechanisms | \u2022 The righthand part is the decoder."
  },
  {
    "id": 372,
    "content": "During training, it takes the target sentence as\ninput (also represented as a sequence of word IDs), shifted one time step to the\nright (i.e., a start-of-sequence token is inserted at the beginning). It also receives\nthe outputs of the encoder (i.e., the arrows coming from the left side). Note that\nthe top part of the decoder is also stacked N times, and the encoder stack\u2019s final\noutputs are fed to the decoder at each of these N levels. Just like earlier, the\ndecoder outputs a probability for each possible next word, at each time step (its\noutput shape is [batch size, max output sentence length, vocabulary length]). \u2022 During inference, the decoder cannot be fed targets, so we feed it the previously\noutput words (starting with a start-of-sequence token). So the model needs to be\ncalled repeatedly, predicting one more word at every round (which is fed to the\ndecoder at the next round, until the end-of-sequence token is output). \u2022 Looking more closely, you can see that you are already familiar with most com\u2010\nponents: there are two embedding layers, 5 \u00d7 N skip connections, each of them\nfollowed by a layer normalization layer, 2 \u00d7 N \u201cFeed Forward\u201d modules that are\ncomposed of two dense layers each (the first one using the ReLU activation func\u2010\ntion, the second with no activation function), and finally the output layer is a\ndense layer using the softmax activation function. All of these layers are time-\ndistributed, so each word is treated independently of all the others. But how can\nwe translate a sentence by only looking at one word at a time? Well, that\u2019s where\nthe new components come in:\n\u2014 The encoder\u2019s Multi-Head Attention layer encodes each word\u2019s relationship\nwith every other word in the same sentence, paying more attention to the\nmost relevant ones. For example, the output of this layer for the word \u201cQueen\u201d\nin the sentence \u201cThey welcomed the Queen of the United Kingdom\u201d will\ndepend on all the words in the sentence, but it will probably pay more atten\u2010\ntion to the words \u201cUnited\u201d and \u201cKingdom\u201d than to the words \u201cThey\u201d or \u201cwel\u2010\ncomed.\u201d This attention mechanism is called self-attention (the sentence is\npaying attention to itself). We will discuss exactly how it works shortly. The\ndecoder\u2019s Masked Multi-Head Attention layer does the same thing, but each\nword is only allowed to attend to words located before it. Finally, the decoder\u2019s\nupper Multi-Head Attention layer is where the decoder pays attention to the\nwords in the input sentence. For example, the decoder will probably pay close\nattention to the word \u201cQueen\u201d in the input sentence when it is about to output\nthis word\u2019s translation. \u2014 The positional embeddings are simply dense vectors (much like word embed\u2010\ndings) that represent the position of a word in the sentence. The nth positional\nembedding is added to the word embedding of the nth word in each sentence."
  },
  {
    "id": 373,
    "content": "This gives the model access to each word\u2019s position, which is needed because\nthe Multi-Head Attention layers do not consider the order or the position of\nthe words; they only look at their relationships. Since all the other layers are | Chapter 16: Natural Language Processing with RNNs and Attention\ntime-distributed, they have no way of knowing the position of each word\n(either relative or absolute). Obviously, the relative and absolute word posi\u2010\ntions are important, so we need to give this information to the Transformer\nsomehow, and positional embeddings are a good way to do this. Let\u2019s look a bit closer at both these novel components of the Transformer architecture,\nstarting with the positional embeddings. Positional embeddings\nA positional embedding is a dense vector that encodes the position of a word within a\nsentence: the ith positional embedding is simply added to the word embedding of the\nith word in the sentence. These positional embeddings can be learned by the model,\nbut in the paper the authors preferred to use fixed positional embeddings, defined\nusing the sine and cosine functions of different frequencies. The positional embed\u2010\nding matrix P is defined in Equation 16-2 and represented at the bottom of\nFigure 16-9 (transposed), where Pp,i is the ith component of the embedding for the\nword located at the pth position in the sentence. Equation 16-2. Sine/cosine positional embeddings\nPp, 2i = sin p/100002i/d\nPp, 2i + 1 = cos p/100002i/d\nFigure 16-9. Sine/cosine positional embedding matrix (transposed, top) with a focus on\ntwo values of i (bottom)\nAttention Mechanisms | This solution gives the same performance as learned positional embeddings do, but it\ncan extend to arbitrarily long sentences, which is why it\u2019s favored. After the positional\nembeddings are added to the word embeddings, the rest of the model has access to\nthe absolute position of each word in the sentence because there is a unique posi\u2010\ntional embedding for each position (e.g., the positional embedding for the word loca\u2010\nted at the 22nd position in a sentence is represented by the vertical dashed line at the\nbottom left of Figure 16-9, and you can see that it is unique to that position). More\u2010\nover, the choice of oscillating functions (sine and cosine) makes it possible for the\nmodel to learn relative positions as well. For example, words located 38 words apart\n(e.g., at positions p = 22 and p = 60) always have the same positional embedding val\u2010\nues in the embedding dimensions i = 100 and i = 101, as you can see in Figure 16-9. This explains why we need both the sine and the cosine for each frequency: if we only\nused the sine (the blue wave at i = 100), the model would not be able to distinguish\npositions p = 25 and p = 35 (marked by a cross). There is no PositionalEmbedding layer in TensorFlow, but it is easy to create one."
  },
  {
    "id": 374,
    "content": "For efficiency reasons, we precompute the positional embedding matrix in the con\u2010\nstructor (so we need to know the maximum sentence length, max_steps, and the\nnumber of dimensions for each word representation, max_dims). Then the call()\nmethod crops this embedding matrix to the size of the inputs, and it adds it to the\ninputs. Since we added an extra first dimension of size 1 when creating the positional\nembedding matrix, the rules of broadcasting will ensure that the matrix gets added to\nevery sentence in the inputs:\nclass PositionalEncoding(keras.layers.Layer): def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs): super().__init__(dtype=dtype, **kwargs) if max_dims % 2 == 1: max_dims += 1 # max_dims must be even p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2)) pos_emb = np.empty((1, max_steps, max_dims)) pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T self.positional_embedding = tf.constant(pos_emb.astype(self.dtype)) def call(self, inputs): shape = tf.shape(inputs) return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]\nThen we can create the first layers of the Transformer:\nembed_size = 512; max_steps = 500; vocab_size = 10000\nencoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\ndecoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\nembeddings = keras.layers.Embedding(vocab_size, embed_size)\nencoder_embeddings = embeddings(encoder_inputs)\ndecoder_embeddings = embeddings(decoder_inputs)\npositional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\nencoder_in = positional_encoding(encoder_embeddings)\ndecoder_in = positional_encoding(decoder_embeddings) | Chapter 16: Natural Language Processing with RNNs and Attention\nNow let\u2019s look deeper into the heart of the Transformer model: the Multi-Head Atten\u2010\ntion layer. Multi-Head Attention\nTo understand how a Multi-Head Attention layer works, we must first understand the\nScaled Dot-Product Attention layer, which it is based on. Let\u2019s suppose the encoder\nanalyzed the input sentence \u201cThey played chess,\u201d and it managed to understand that\nthe word \u201cThey\u201d is the subject and the word \u201cplayed\u201d is the verb, so it encoded this\ninformation in the representations of these words. Now suppose the decoder has\nalready translated the subject, and it thinks that it should translate the verb next. For\nthis, it needs to fetch the verb from the input sentence. This is analog to a dictionary\nlookup: it\u2019s as if the encoder created a dictionary {\u201csubject\u201d: \u201cThey\u201d, \u201cverb\u201d: \u201cplayed\u201d,\n\u2026} and the decoder wanted to look up the value that corresponds to the key \u201cverb.\u201d\nHowever, the model does not have discrete tokens to represent the keys (like \u201csubject\u201d\nor \u201cverb\u201d); it has vectorized representations of these concepts (which it learned dur\u2010\ning training), so the key it will use for the lookup (called the query) will not perfectly\nmatch any key in the dictionary. The solution is to compute a similarity measure\nbetween the query and each key in the dictionary, and then use the softmax function\nto convert these similarity scores to weights that add up to 1. If the key that represents\nthe verb is by far the most similar to the query, then that key\u2019s weight will be close to\n1."
  },
  {
    "id": 375,
    "content": "Then the model can compute a weighted sum of the corresponding values, so if the\nweight of the \u201cverb\u201d key is close to 1, then the weighted sum will be very close to the\nrepresentation of the word \u201cplayed.\u201d In short, you can think of this whole process as a\ndifferentiable dictionary lookup. The similarity measure used by the Transformer is\njust the dot product, like in Luong attention. In fact, the equation is the same as for\nLuong attention, except for a scaling factor. The equation is shown in Equation 16-3,\nin a vectorized form. Equation 16-3. Scaled Dot-Product Attention\nAttention \ufffd, \ufffd, \ufffd= softmax\n\ufffd\ufffd\u22ba\ndkeys\n\ufffd\nIn this equation:\n\u2022 Q is a matrix containing one row per query. Its shape is [nqueries, dkeys], where\nnqueries is the number of queries and dkeys is the number of dimensions of each\nquery and each key. \u2022 K is a matrix containing one row per key. Its shape is [nkeys, dkeys], where nkeys is\nthe number of keys and values. Attention Mechanisms | \u2022 V is a matrix containing one row per value. Its shape is [nkeys, dvalues], where dvalues\nis the number of each value. \u2022 The shape of Q K\u22ba is [nqueries, nkeys]: it contains one similarity score for each\nquery/key pair. The output of the softmax function has the same shape, but all\nrows sum up to 1. The final output has a shape of [nqueries, dvalues]: there is one row\nper query, where each row represents the query result (a weighted sum of the val\u2010\nues). \u2022 The scaling factor scales down the similarity scores to avoid saturating the soft\u2010\nmax function, which would lead to tiny gradients. \u2022 It is possible to mask out some key/value pairs by adding a very large negative\nvalue to the corresponding similarity scores, just before computing the softmax. This is useful in the Masked Multi-Head Attention layer. In the encoder, this equation is applied to every input sentence in the batch, with Q,\nK, and V all equal to the list of words in the input sentence (so each word in the sen\u2010\ntence will be compared to every word in the same sentence, including itself). Simi\u2010\nlarly, in the decoder\u2019s masked attention layer, the equation will be applied to every\ntarget sentence in the batch, with Q, K, and V all equal to the list of words in the tar\u2010\nget sentence, but this time using a mask to prevent any word from comparing itself to\nwords located after it (at inference time the decoder will only have access to the words\nit already output, not to future words, so during training we must mask out future\noutput tokens). In the upper attention layer of the decoder, the keys K and values V\nare simply the list of word encodings produced by the encoder, and the queries Q are\nthe list of word encodings produced by the decoder."
  },
  {
    "id": 376,
    "content": "The keras.layers.Attention layer implements Scaled Dot-Product Attention, effi\u2010\nciently applying Equation 16-3 to multiple sentences in a batch. Its inputs are just like\nQ, K, and V, except with an extra batch dimension (the first dimension). In TensorFlow, if A and B are tensors with more than two dimen\u2010\nsions\u2014say, of shape [2, 3, 4, 5] and [2, 3, 5, 6] respectively\u2014then\ntf.matmul(A, B) will treat these tensors as 2 \u00d7 3 arrays where each\ncell contains a matrix, and it will multiply the corresponding matri\u2010\nces: the matrix at the ith row and jth column in A will be multiplied\nby the matrix at the ith row and jth column in B. Since the product of\na 4 \u00d7 5 matrix with a 5 \u00d7 6 matrix is a 4 \u00d7 6 matrix, tf.matmul(A,\nB) will return an array of shape [2, 3, 4, 6]. | Chapter 16: Natural Language Processing with RNNs and Attention\nIf we ignore the skip connections, the layer normalization layers, the Feed Forward\nblocks, and the fact that this is Scaled Dot-Product Attention, not exactly Multi-Head\nAttention, then the rest of the Transformer model can be implemented like this:\nZ = encoder_in\nfor N in range(6): Z = keras.layers.Attention(use_scale=True)([Z, Z])\nencoder_outputs = Z\nZ = decoder_in\nfor N in range(6): Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z]) Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\noutputs = keras.layers.TimeDistributed( keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)\nThe use_scale=True argument creates an additional parameter that lets the layer\nlearn how to properly downscale the similarity scores. This is a bit different from the\nTransformer model, which always downscales the similarity scores by the same factor\n( dkeys). The causal=True argument when creating the second attention layer\nensures that each output token only attends to previous output tokens, not future\nones. Now it\u2019s time to look at the final piece of the puzzle: what is a Multi-Head Attention\nlayer? Its architecture is shown in Figure 16-10. Attention Mechanisms | 23 This is the right part of figure 2 from the paper, reproduced with the kind authorization of the authors. Figure 16-10. Multi-Head Attention layer architecture23\nAs you can see, it is just a bunch of Scaled Dot-Product Attention layers, each pre\u2010\nceded by a linear transformation of the values, keys, and queries (i.e., a time-\ndistributed Dense layer with no activation function). All the outputs are simply\nconcatenated, and they go through a final linear transformation (again, time-\ndistributed). But why? What is the intuition behind this architecture? Well, consider\nthe word \u201cplayed\u201d we discussed earlier (in the sentence \u201cThey played chess\u201d). The\nencoder was smart enough to encode the fact that it is a verb. But the word represen\u2010\ntation also includes its position in the text, thanks to the positional encodings, and it\nprobably includes many other features that are useful for its translation, such as the\nfact that it is in the past tense. In short, the word representation encodes many differ\u2010\nent characteristics of the word."
  },
  {
    "id": 377,
    "content": "If we just used a single Scaled Dot-Product Attention\nlayer, we would only be able to query all of these characteristics in one shot. This is\nwhy the Multi-Head Attention layer applies multiple different linear transformations\nof the values, keys, and queries: this allows the model to apply many different projec\u2010\ntions of the word representation into different subspaces, each focusing on a subset of\nthe word\u2019s characteristics. Perhaps one of the linear layers will project the word repre\u2010\nsentation into a subspace where all that remains is the information that the word is a | Chapter 16: Natural Language Processing with RNNs and Attention\n24 Matthew Peters et al., \u201cDeep Contextualized Word Representations,\u201d Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies 1\n(2018): 2227\u20132237. 25 Jeremy Howard and Sebastian Ruder, \u201cUniversal Language Model Fine-Tuning for Text Classification,\u201d Pro\u2010\nceedings of the 56th Annual Meeting of the Association for Computational Linguistics 1 (2018): 328\u2013339. 26 Alec Radford et al., \u201cImproving Language Understanding by Generative Pre-Training\u201d (2018). verb, another linear layer will extract just the fact that it is past tense, and so on. Then\nthe Scaled Dot-Product Attention layers implement the lookup phase, and finally we\nconcatenate all the results and project them back to the original space. At the time of this writing, there is no Transformer class or MultiHeadAttention\nclass available for TensorFlow 2. However, you can check out TensorFlow\u2019s great tuto\u2010\nrial for building a Transformer model for language understanding. Moreover, the TF\nHub team is currently porting several Transformer-based modules to TensorFlow 2,\nand they should be available soon. In the meantime, I hope I have demonstrated that\nit is not that hard to implement a Transformer yourself, and it is certainly a great\nexercise! Recent Innovations in Language Models\nThe year 2018 has been called the \u201cImageNet moment for NLP\u201d: progress was\nastounding, with larger and larger LSTM and Transformer-based architectures\ntrained on immense datasets. I highly recommend you check out the following\npapers, all published in 2018:\n\u2022 The ELMo paper24 by Matthew Peters introduced Embeddings from Language\nModels (ELMo): these are contextualized word embeddings learned from the\ninternal states of a deep bidirectional language model. For example, the word\n\u201cqueen\u201d will not have the same embedding in \u201cQueen of the United Kingdom\u201d\nand in \u201cqueen bee.\u201d\n\u2022 The ULMFiT paper25 by Jeremy Howard and Sebastian Ruder demonstrated the\neffectiveness of unsupervised pretraining for NLP tasks: the authors trained an\nLSTM language model using self-supervised learning (i.e., generating the labels\nautomatically from the data) on a huge text corpus, then they fine-tuned it on\nvarious tasks. Their model outperformed the state of the art on six text classifica\u2010\ntion tasks by a large margin (reducing the error rate by 18\u201324% in most cases). Moreover, they showed that by fine-tuning the pretrained model on just 100\nlabeled examples, they could achieve the same performance as a model trained\nfrom scratch on 10,000 examples."
  },
  {
    "id": 378,
    "content": "\u2022 The GPT paper26 by Alec Radford and other OpenAI researchers also demon\u2010\nstrated the effectiveness of unsupervised pretraining, but this time using a\nRecent Innovations in Language Models | 27 For example, the sentence \u201cJane had a lot of fun at her friend\u2019s birthday party\u201d entails \u201cJane enjoyed the party,\u201d\nbut it is contradicted by \u201cEveryone hated the party\u201d and it is unrelated to \u201cThe Earth is flat.\u201d\n28 Alec Radford et al., \u201cLanguage Models Are Unsupervised Multitask Learners\u201d (2019). 29 Jacob Devlin et al., \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\u201d\nProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin\u2010\nguistics: Human Language Technologies 1 (2019). Transformer-like architecture. The authors pretrained a large but fairly simple\narchitecture composed of a stack of 12 Transformer modules (using only Masked\nMulti-Head Attention layers) on a large dataset, once again trained using self-\nsupervised learning. Then they fine-tuned it on various language tasks, using\nonly minor adaptations for each task. The tasks were quite diverse: they included\ntext classification, entailment (whether sentence A entails sentence B),27 similarity\n(e.g., \u201cNice weather today\u201d is very similar to \u201cIt is sunny\u201d), and question answer\u2010\ning (given a few paragraphs of text giving some context, the model must answer\nsome multiple-choice questions). Just a few months later, in February 2019, Alec\nRadford, Jeffrey Wu, and other OpenAI researchers published the GPT-2 paper,28\nwhich proposed a very similar architecture, but larger still (with over 1.5 billion\nparameters!) and they showed that it could achieve good performance on many\ntasks without any fine-tuning. This is called zero-shot learning (ZSL). A smaller\nversion of the GPT-2 model (with \u201cjust\u201d 117 million parameters) is available at\n along with its pretrained weights. \u2022 The BERT paper29 by Jacob Devlin and other Google researchers also demon\u2010\nstrates the effectiveness of self-supervised pretraining on a large corpus, using a\nsimilar architecture to GPT but non-masked Multi-Head Attention layers (like in\nthe Transformer\u2019s encoder). This means that the model is naturally bidirectional;\nhence the B in BERT (Bidirectional Encoder Representations from Transformers). Most importantly, the authors proposed two pretraining tasks that explain most\nof the model\u2019s strength:\nMasked language model (MLM)\nEach word in a sentence has a 15% probability of being masked, and the\nmodel is trained to predict the masked words. For example, if the original\nsentence is \u201cShe had fun at the birthday party,\u201d then the model may be given\nthe sentence \u201cShe <mask> fun at the <mask> party\u201d and it must predict the\nwords \u201chad\u201d and \u201cbirthday\u201d (the other outputs will be ignored). To be more\nprecise, each selected word has an 80% chance of being masked, a 10%\nchance of being replaced by a random word (to reduce the discrepancy\nbetween pretraining and fine-tuning, since the model will not see <mask>\ntokens during fine-tuning), and a 10% chance of being left alone (to bias the\nmodel toward the correct answer)."
  },
  {
    "id": 379,
    "content": "| Chapter 16: Natural Language Processing with RNNs and Attention\n30 Maha Elbayad et al., \u201cPervasive Attention: 2D Convolutional Neural Networks for Sequence-to-Sequence Pre\u2010\ndiction,\u201d arXiv preprint arXiv:1808.03867 (2018). 31 Shuai Li et al., \u201cIndependently Recurrent Neural Network (IndRNN): Building a Longer and Deeper RNN,\u201d\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2018): 5457\u20135466. Next sentence prediction (NSP)\nThe model is trained to predict whether two sentences are consecutive or\nnot. For example, it should predict that \u201cThe dog sleeps\u201d and \u201cIt snores\nloudly\u201d are consecutive sentences, while \u201cThe dog sleeps\u201d and \u201cThe Earth\norbits the Sun\u201d are not consecutive. This is a challenging task, and it signifi\u2010\ncantly improves the performance of the model when it is fine-tuned on tasks\nsuch as question answering or entailment. As you can see, the main innovations in 2018 and 2019 have been better subword\ntokenization, shifting from LSTMs to Transformers, and pretraining universal lan\u2010\nguage models using self-supervised learning, then fine-tuning them with very few\narchitectural changes (or none at all). Things are moving fast; no one can say what\narchitectures will prevail next year. Today, it\u2019s clearly Transformers, but tomorrow it\nmight be CNNs (e.g., check out the 2018 paper30 by Maha Elbayad et al., where the\nresearchers use masked 2D convolutional layers for sequence-to-sequence tasks). Or\nit might even be RNNs, if they make a surprise comeback (e.g., check out the 2018\npaper31 by Shuai Li et al. that shows that by making neurons independent of each\nother in a given RNN layer, it is possible to train much deeper RNNs capable of learn\u2010\ning much longer sequences). In the next chapter we will discuss how to learn deep representations in an unsuper\u2010\nvised way using autoencoders, and we will use generative adversarial networks\n(GANs) to produce images and more! Exercises\n1. What are the pros and cons of using a stateful RNN versus a stateless RNN? 2. Why do people use Encoder\u2013Decoder RNNs rather than plain sequence-to-\nsequence RNNs for automatic translation? 3. How can you deal with variable-length input sequences? What about variable-\nlength output sequences? 4. What is beam search and why would you use it? What tool can you use to imple\u2010\nment it? 5. What is an attention mechanism? How does it help? Exercises | 6. What is the most important layer in the Transformer architecture? What is its\npurpose? 7. When would you need to use sampled softmax? 8. Embedded Reber grammars were used by Hochreiter and Schmidhuber in their\npaper about LSTMs. They are artificial grammars that produce strings such as\n\u201cBPBTSXXVPSEPE.\u201d Check out Jenny Orr\u2019s nice introduction to this topic. Choose a particular embedded Reber grammar (such as the one represented on\nJenny Orr\u2019s page), then train an RNN to identify whether a string respects that\ngrammar or not. You will first need to write a function capable of generating a\ntraining batch containing about 50% strings that respect the grammar, and 50%\nthat don\u2019t. 9."
  },
  {
    "id": 380,
    "content": "Train an Encoder\u2013Decoder model that can convert a date string from one format\nto another (e.g., from \u201cApril 22, 2019\u201d to \u201c2019-04-22\u201d). 10. Go through TensorFlow\u2019s Neural Machine Translation with Attention tutorial. 11. Use one of the recent language models (e.g., BERT) to generate more convincing\nShakespearean text. Solutions to these exercises are available in Appendix A. | Chapter 16: Natural Language Processing with RNNs and Attention\nCHAPTER 17\nRepresentation Learning and Generative\nLearning Using Autoencoders and GANs\nAutoencoders are artificial neural networks capable of learning dense representations\nof the input data, called latent representations or codings, without any supervision (i.e.,\nthe training set is unlabeled). These codings typically have a much lower dimension\u2010\nality than the input data, making autoencoders useful for dimensionality reduction\n(see Chapter 8), especially for visualization purposes. Autoencoders also act as feature\ndetectors, and they can be used for unsupervised pretraining of deep neural networks\n(as we discussed in Chapter 11). Lastly, some autoencoders are generative models: they\nare capable of randomly generating new data that looks very similar to the training\ndata. For example, you could train an autoencoder on pictures of faces, and it would\nthen be able to generate new faces. However, the generated images are usually fuzzy\nand not entirely realistic. In contrast, faces generated by generative adversarial networks (GANs) are now so\nconvincing that it is hard to believe that the people they represent do not exist. You\ncan judge so for yourself by visiting  a website that\nshows faces generated by a recent GAN architecture called StyleGAN (you can also\ncheck out  to see some generated Airbnb bed\u2010\nrooms). GANs are now widely used for super resolution (increasing the resolution of\nan image), colorization, powerful image editing (e.g., replacing photo bombers with\nrealistic background), turning a simple sketch into a photorealistic image, predicting\nthe next frames in a video, augmenting a dataset (to train other models), generating\nother types of data (such as text, audio, and time series), identifying the weaknesses in\nother models and strengthening them, and more. Autoencoders and GANs are both unsupervised, they both learn dense representa\u2010\ntions, they can both be used as generative models, and they have many similar appli\u2010\ncations. However, they work very differently:\n\u2022 Autoencoders simply learn to copy their inputs to their outputs. This may sound\nlike a trivial task, but we will see that constraining the network in various ways\ncan make it rather difficult. For example, you can limit the size of the latent rep\u2010\nresentations, or you can add noise to the inputs and train the network to recover\nthe original inputs. These constraints prevent the autoencoder from trivially\ncopying the inputs directly to the outputs, which forces it to learn efficient ways\nof representing the data. In short, the codings are byproducts of the autoencoder\nlearning the identity function under some constraints."
  },
  {
    "id": 381,
    "content": "\u2022 GANs are composed of two neural networks: a generator that tries to generate\ndata that looks similar to the training data, and a discriminator that tries to tell\nreal data from fake data. This architecture is very original in Deep Learning in\nthat the generator and the discriminator compete against each other during\ntraining: the generator is often compared to a criminal trying to make realistic\ncounterfeit money, while the discriminator is like the police investigator trying to\ntell real money from fake. Adversarial training (training competing neural net\u2010\nworks) is widely considered as one of the most important ideas in recent years. In\n2016, Yann LeCun even said that it was \u201cthe most interesting idea in the last 10\nyears in Machine Learning.\u201d\nIn this chapter we will start by exploring in more depth how autoencoders work and\nhow to use them for dimensionality reduction, feature extraction, unsupervised pre\u2010\ntraining, or as generative models. This will naturally lead us to GANs. We will start by\nbuilding a simple GAN to generate fake images, but we will see that training is often\nquite difficult. We will discuss the main difficulties you will encounter with adversa\u2010\nrial training, as well as some of the main techniques to work around these difficulties. Let\u2019s start with autoencoders! | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n1 William G. Chase and Herbert A. Simon, \u201cPerception in Chess,\u201d Cognitive Psychology 4, no. 1 (1973): 55\u201381. Efficient Data Representations\nWhich of the following number sequences do you find the easiest to memorize? \u2022 40, 27, 25, 36, 81, 57, 10, 73, 19, 68\n\u2022 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14\nAt first glance, it would seem that the first sequence should be easier, since it is much\nshorter. However, if you look carefully at the second sequence, you will notice that it\nis just the list of even numbers from 50 down to 14. Once you notice this pattern, the\nsecond sequence becomes much easier to memorize than the first because you only\nneed to remember the pattern (i.e., decreasing even numbers) and the starting and\nending numbers (i.e., 50 and 14). Note that if you could quickly and easily memorize\nvery long sequences, you would not care much about the existence of a pattern in the\nsecond sequence. You would just learn every number by heart, and that would be\nthat. The fact that it is hard to memorize long sequences is what makes it useful to\nrecognize patterns, and hopefully this clarifies why constraining an autoencoder dur\u2010\ning training pushes it to discover and exploit patterns in the data."
  },
  {
    "id": 382,
    "content": "The relationship between memory, perception, and pattern matching was famously\nstudied by William Chase and Herbert Simon in the early 1970s.1 They observed that\nexpert chess players were able to memorize the positions of all the pieces in a game by\nlooking at the board for just five seconds, a task that most people would find impossi\u2010\nble. However, this was only the case when the pieces were placed in realistic positions\n(from actual games), not when the pieces were placed randomly. Chess experts don\u2019t\nhave a much better memory than you and I; they just see chess patterns more easily,\nthanks to their experience with the game. Noticing patterns helps them store infor\u2010\nmation efficiently. Just like the chess players in this memory experiment, an autoencoder looks at the\ninputs, converts them to an efficient latent representation, and then spits out some\u2010\nthing that (hopefully) looks very close to the inputs. An autoencoder is always com\u2010\nposed of two parts: an encoder (or recognition network) that converts the inputs to a\nlatent representation, followed by a decoder (or generative network) that converts the\ninternal representation to the outputs (see Figure 17-1). Efficient Data Representations | Figure 17-1. The chess memory experiment (left) and a simple autoencoder (right)\nAs you can see, an autoencoder typically has the same architecture as a Multi-Layer\nPerceptron (MLP; see Chapter 10), except that the number of neurons in the output\nlayer must be equal to the number of inputs. In this example, there is just one hidden\nlayer composed of two neurons (the encoder), and one output layer composed of\nthree neurons (the decoder). The outputs are often called the reconstructions because\nthe autoencoder tries to reconstruct the inputs, and the cost function contains a\nreconstruction loss that penalizes the model when the reconstructions are different\nfrom the inputs. Because the internal representation has a lower dimensionality than the input data (it\nis 2D instead of 3D), the autoencoder is said to be undercomplete. An undercomplete\nautoencoder cannot trivially copy its inputs to the codings, yet it must find a way to\noutput a copy of its inputs. It is forced to learn the most important features in the\ninput data (and drop the unimportant ones). Let\u2019s see how to implement a very simple undercomplete autoencoder for dimension\u2010\nality reduction. Performing PCA with an Undercomplete Linear\nAutoencoder\nIf the autoencoder uses only linear activations and the cost function is the mean\nsquared error (MSE), then it ends up performing Principal Component Analysis\n(PCA; see Chapter 8)."
  },
  {
    "id": 383,
    "content": "The following code builds a simple linear autoencoder to perform PCA on a 3D data\u2010\nset, projecting it to 2D: | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\nfrom tensorflow import keras\nencoder = keras.models.Sequential([keras.layers.Dense(2, input_shape=[3])])\ndecoder = keras.models.Sequential([keras.layers.Dense(3, input_shape=[2])])\nautoencoder = keras.models.Sequential([encoder, decoder])\nautoencoder.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=0.1))\nThis code is really not very different from all the MLPs we built in past chapters, but\nthere are a few things to note:\n\u2022 We organized the autoencoder into two subcomponents: the encoder and the\ndecoder. Both are regular Sequential models with a single Dense layer each, and\nthe autoencoder is a Sequential model containing the encoder followed by the\ndecoder (remember that a model can be used as a layer in another model). \u2022 The autoencoder\u2019s number of outputs is equal to the number of inputs (i.e., 3). \u2022 To perform simple PCA, we do not use any activation function (i.e., all neurons\nare linear), and the cost function is the MSE. We will see more complex autoen\u2010\ncoders shortly. Now let\u2019s train the model on a simple generated 3D dataset and use it to encode that\nsame dataset (i.e., project it to 2D):\nhistory = autoencoder.fit(X_train, X_train, epochs=20)\ncodings = encoder.predict(X_train)\nNote that the same dataset, X_train, is used as both the inputs and the targets. Figure 17-2 shows the original 3D dataset (on the left) and the output of the autoen\u2010\ncoder\u2019s hidden layer (i.e., the coding layer, on the right). As you can see, the autoen\u2010\ncoder found the best 2D plane to project the data onto, preserving as much variance\nin the data as it could (just like PCA). Figure 17-2. PCA performed by an undercomplete linear autoencoder\nPerforming PCA with an Undercomplete Linear Autoencoder | You can think of autoencoders as a form of self-supervised learning\n(i.e., using a supervised learning technique with automatically gen\u2010\nerated labels, in this case simply equal to the inputs). Stacked Autoencoders\nJust like other neural networks we have discussed, autoencoders can have multiple\nhidden layers. In this case they are called stacked autoencoders (or deep autoencoders). Adding more layers helps the autoencoder learn more complex codings. That said,\none must be careful not to make the autoencoder too powerful. Imagine an encoder\nso powerful that it just learns to map each input to a single arbitrary number (and the\ndecoder learns the reverse mapping). Obviously such an autoencoder will reconstruct\nthe training data perfectly, but it will not have learned any useful data representation\nin the process (and it is unlikely to generalize well to new instances). The architecture of a stacked autoencoder is typically symmetrical with regard to the\ncentral hidden layer (the coding layer). To put it simply, it looks like a sandwich."
  },
  {
    "id": 384,
    "content": "For\nexample, an autoencoder for MNIST (introduced in Chapter 3) may have 784 inputs,\nfollowed by a hidden layer with 100 neurons, then a central hidden layer of 30 neu\u2010\nrons, then another hidden layer with 100 neurons, and an output layer with 784 neu\u2010\nrons. This stacked autoencoder is represented in Figure 17-3. Figure 17-3. Stacked autoencoder\nImplementing a Stacked Autoencoder Using Keras\nYou can implement a stacked autoencoder very much like a regular deep MLP. In par\u2010\nticular, the same techniques we used in Chapter 11 for training deep nets can be\napplied. For example, the following code builds a stacked autoencoder for Fashion | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n2 You might be tempted to use the accuracy metric, but it would not work properly, since this metric expects the\nlabels to be either 0 or 1 for each pixel. You can easily work around this problem by creating a custom metric\nthat computes the accuracy after rounding the targets and predictions to 0 or 1. MNIST (loaded and normalized as in Chapter 10), using the SELU activation\nfunction:\nstacked_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(100, activation=\"selu\"), keras.layers.Dense(30, activation=\"selu\"),\n])\nstacked_decoder = keras.models.Sequential([ keras.layers.Dense(100, activation=\"selu\", input_shape=[30]), keras.layers.Dense(28 * 28, activation=\"sigmoid\"), keras.layers.Reshape([28, 28])\n])\nstacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\nstacked_ae.compile(loss=\"binary_crossentropy\", optimizer=keras.optimizers.SGD(lr=1.5))\nhistory = stacked_ae.fit(X_train, X_train, epochs=10, validation_data=[X_valid, X_valid])\nLet\u2019s go through this code:\n\u2022 Just like earlier, we split the autoencoder model into two submodels: the encoder\nand the decoder. \u2022 The encoder takes 28 \u00d7 28\u2013pixel grayscale images, flattens them so that each\nimage is represented as a vector of size 784, then processes these vectors through\ntwo Dense layers of diminishing sizes (100 units then 30 units), both using the\nSELU activation function (you may want to add LeCun normal initialization as\nwell, but the network is not very deep so it won\u2019t make a big difference). For each\ninput image, the encoder outputs a vector of size 30. \u2022 The decoder takes codings of size 30 (output by the encoder) and processes them\nthrough two Dense layers of increasing sizes (100 units then 784 units), and it\nreshapes the final vectors into 28 \u00d7 28 arrays so the decoder\u2019s outputs have the\nsame shape as the encoder\u2019s inputs. \u2022 When compiling the stacked autoencoder, we use the binary cross-entropy loss\ninstead of the mean squared error. We are treating the reconstruction task as a\nmultilabel binary classification problem: each pixel intensity represents the prob\u2010\nability that the pixel should be black. Framing it this way (rather than as a regres\u2010\nsion problem) tends to make the model converge faster.2\n\u2022 Finally, we train the model using X_train as both the inputs and the targets (and\nsimilarly, we use X_valid as both the validation inputs and targets). Stacked Autoencoders | Visualizing the Reconstructions\nOne way to ensure that an autoencoder is properly trained is to compare the inputs\nand the outputs: the differences should not be too significant."
  },
  {
    "id": 385,
    "content": "Let\u2019s plot a few images\nfrom the validation set, as well as their reconstructions:\ndef plot_image(image): plt.imshow(image, cmap=\"binary\") plt.axis(\"off\")\ndef show_reconstructions(model, n_images=5): reconstructions = model.predict(X_valid[:n_images]) fig = plt.figure(figsize=(n_images * 1.5, 3)) for image_index in range(n_images): plt.subplot(2, n_images, 1 + image_index) plot_image(X_valid[image_index]) plt.subplot(2, n_images, 1 + n_images + image_index) plot_image(reconstructions[image_index])\nshow_reconstructions(stacked_ae)\nFigure 17-4 shows the resulting images. Figure 17-4. Original images (top) and their reconstructions (bottom)\nThe reconstructions are recognizable, but a bit too lossy. We may need to train the\nmodel for longer, or make the encoder and decoder deeper, or make the codings\nlarger. But if we make the network too powerful, it will manage to make perfect\nreconstructions without having learned any useful patterns in the data. For now, let\u2019s\ngo with this model. Visualizing the Fashion MNIST Dataset\nNow that we have trained a stacked autoencoder, we can use it to reduce the dataset\u2019s\ndimensionality. For visualization, this does not give great results compared to other\ndimensionality reduction algorithms (such as those we discussed in Chapter 8), but\none big advantage of autoencoders is that they can handle large datasets, with many\ninstances and many features. So one strategy is to use an autoencoder to reduce the\ndimensionality down to a reasonable level, then use another dimensionality | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\nreduction algorithm for visualization. Let\u2019s use this strategy to visualize Fashion\nMNIST. First, we use the encoder from our stacked autoencoder to reduce the dimen\u2010\nsionality down to 30, then we use Scikit-Learn\u2019s implementation of the t-SNE algo\u2010\nrithm to reduce the dimensionality down to 2 for visualization:\nfrom sklearn.manifold import TSNE\nX_valid_compressed = stacked_encoder.predict(X_valid)\ntsne = TSNE()\nX_valid_2D = tsne.fit_transform(X_valid_compressed)\nNow we can plot the dataset:\nplt.scatter(X_valid_2D[:, 0], X_valid_2D[:, 1], c=y_valid, s=10, cmap=\"tab10\")\nFigure 17-5 shows the resulting scatterplot (beautified a bit by displaying some of the\nimages). The t-SNE algorithm identified several clusters which match the classes rea\u2010\nsonably well (each class is represented with a different color). Figure 17-5. Fashion MNIST visualization using an autoencoder followed by t-SNE\nSo, autoencoders can be used for dimensionality reduction. Another application is for\nunsupervised pretraining. Stacked Autoencoders | Unsupervised Pretraining Using Stacked Autoencoders\nAs we discussed in Chapter 11, if you are tackling a complex supervised task but you\ndo not have a lot of labeled training data, one solution is to find a neural network that\nperforms a similar task and reuse its lower layers. This makes it possible to train a\nhigh-performance model using little training data because your neural network won\u2019t\nhave to learn all the low-level features; it will just reuse the feature detectors learned\nby the existing network. Similarly, if you have a large dataset but most of it is unlabeled, you can first train a\nstacked autoencoder using all the data, then reuse the lower layers to create a neural\nnetwork for your actual task and train it using the labeled data."
  },
  {
    "id": 386,
    "content": "For example,\nFigure 17-6 shows how to use a stacked autoencoder to perform unsupervised pre\u2010\ntraining for a classification neural network. When training the classifier, if you really\ndon\u2019t have much labeled training data, you may want to freeze the pretrained layers\n(at least the lower ones). Figure 17-6. Unsupervised pretraining using autoencoders\nHaving plenty of unlabeled data and little labeled data is common. Building a large unlabeled dataset is often cheap (e.g., a simple\nscript can download millions of images off the internet), but label\u2010\ning those images (e.g., classifying them as cute or not) can usually\nbe done reliably only by humans. Labeling instances is time-\nconsuming and costly, so it\u2019s normal to have only a few thousand\nhuman-labeled instances. | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\nThere is nothing special about the implementation: just train an autoencoder using\nall the training data (labeled plus unlabeled), then reuse its encoder layers to create a\nnew neural network (see the exercises at the end of this chapter for an example). Next, let\u2019s look at a few techniques for training stacked autoencoders. Tying Weights\nWhen an autoencoder is neatly symmetrical, like the one we just built, a common\ntechnique is to tie the weights of the decoder layers to the weights of the encoder lay\u2010\ners. This halves the number of weights in the model, speeding up training and limit\u2010\ning the risk of overfitting. Specifically, if the autoencoder has a total of N layers (not\ncounting the input layer), and WL represents the connection weights of the Lth layer\n(e.g., layer 1 is the first hidden layer, layer N/2 is the coding layer, and layer N is the\noutput layer), then the decoder layer weights can be defined simply as: WN\u2013L+1 = WL\n\u22ba\n(with L = 1, 2, \u2026, N/2). To tie weights between layers using Keras, let\u2019s define a custom layer:\nclass DenseTranspose(keras.layers.Layer): def __init__(self, dense, activation=None, **kwargs): self.dense = dense self.activation = keras.activations.get(activation) super().__init__(**kwargs) def build(self, batch_input_shape): self.biases = self.add_weight(name=\"bias\", initializer=\"zeros\", shape=[self.dense.input_shape[-1]]) super().build(batch_input_shape) def call(self, inputs): z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True) return self.activation(z + self.biases)\nThis custom layer acts like a regular Dense layer, but it uses another Dense layer\u2019s\nweights, transposed (setting transpose_b=True is equivalent to transposing the sec\u2010\nond argument, but it\u2019s more efficient as it performs the transposition on the fly within\nthe matmul() operation). However, it uses its own bias vector. Next, we can build a\nnew stacked autoencoder, much like the previous one, but with the decoder\u2019s Dense\nlayers tied to the encoder\u2019s Dense layers:\ndense_1 = keras.layers.Dense(100, activation=\"selu\")\ndense_2 = keras.layers.Dense(30, activation=\"selu\")\ntied_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), dense_1, dense_2\n])\nStacked Autoencoders | tied_decoder = keras.models.Sequential([ DenseTranspose(dense_2, activation=\"selu\"), DenseTranspose(dense_1, activation=\"sigmoid\"), keras.layers.Reshape([28, 28])\n])\ntied_ae = keras.models.Sequential([tied_encoder, tied_decoder])\nThis model achieves a very slightly lower reconstruction error than the previous\nmodel, with almost half the number of parameters."
  },
  {
    "id": 387,
    "content": "Training One Autoencoder at a Time\nRather than training the whole stacked autoencoder in one go like we just did, it is\npossible to train one shallow autoencoder at a time, then stack all of them into a sin\u2010\ngle stacked autoencoder (hence the name), as shown in Figure 17-7. This technique is\nnot used as much these days, but you may still run into papers that talk about \u201cgreedy\nlayerwise training,\u201d so it\u2019s good to know what it means. Figure 17-7. Training one autoencoder at a time\nDuring the first phase of training, the first autoencoder learns to reconstruct the\ninputs. Then we encode the whole training set using this first autoencoder, and this\ngives us a new (compressed) training set. We then train a second autoencoder on this\nnew dataset. This is the second phase of training. Finally, we build a big sandwich\nusing all these autoencoders, as shown in Figure 17-7 (i.e., we first stack the hidden\nlayers of each autoencoder, then the output layers in reverse order). This gives us the\nfinal stacked autoencoder (see the \u201cTraining One Autoencoder at a Time\u201d section in\nthe notebook for an implementation). We could easily train more autoencoders this\nway, building a very deep stacked autoencoder. | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n3 Yoshua Bengio et al., \u201cGreedy Layer-Wise Training of Deep Networks,\u201d Proceedings of the 19th International\nConference on Neural Information Processing Systems (2006): 153\u2013160. 4 Jonathan Masci et al., \u201cStacked Convolutional Auto-Encoders for Hierarchical Feature Extraction,\u201d Proceed\u2010\nings of the 21st International Conference on Artificial Neural Networks 1 (2011): 52\u201359. As we discussed earlier, one of the triggers of the current tsunami of interest in Deep\nLearning was the discovery in 2006 by Geoffrey Hinton et al. that deep neural net\u2010\nworks can be pretrained in an unsupervised fashion, using this greedy layerwise\napproach. They used restricted Boltzmann machines (RBMs; see Appendix E) for this\npurpose, but in 2007 Yoshua Bengio et al. showed3 that autoencoders worked just as\nwell. For several years this was the only efficient way to train deep nets, until many of\nthe techniques introduced in Chapter 11 made it possible to just train a deep net in\none shot. Autoencoders are not limited to dense networks: you can also build convolutional\nautoencoders, or even recurrent autoencoders. Let\u2019s look at these now. Convolutional Autoencoders\nIf you are dealing with images, then the autoencoders we have seen so far will not\nwork well (unless the images are very small): as we saw in Chapter 14, convolutional\nneural networks are far better suited than dense networks to work with images. So if\nyou want to build an autoencoder for images (e.g., for unsupervised pretraining or\ndimensionality reduction), you will need to build a convolutional autoencoder.4 The\nencoder is a regular CNN composed of convolutional layers and pooling layers."
  },
  {
    "id": 388,
    "content": "It\ntypically reduces the spatial dimensionality of the inputs (i.e., height and width) while\nincreasing the depth (i.e., the number of feature maps). The decoder must do the\nreverse (upscale the image and reduce its depth back to the original dimensions), and\nfor this you can use transpose convolutional layers (alternatively, you could combine\nupsampling layers with convolutional layers). Here is a simple convolutional autoen\u2010\ncoder for Fashion MNIST:\nconv_encoder = keras.models.Sequential([ keras.layers.Reshape([28, 28, 1], input_shape=[28, 28]), keras.layers.Conv2D(16, kernel_size=3, padding=\"same\", activation=\"selu\"), keras.layers.MaxPool2D(pool_size=2), keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"selu\"), keras.layers.MaxPool2D(pool_size=2), keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"selu\"), keras.layers.MaxPool2D(pool_size=2)\n])\nconv_decoder = keras.models.Sequential([ keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding=\"valid\", activation=\"selu\", input_shape=[3, 3, 64]),\nConvolutional Autoencoders | keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding=\"same\", activation=\"selu\"), keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding=\"same\", activation=\"sigmoid\"), keras.layers.Reshape([28, 28])\n])\nconv_ae = keras.models.Sequential([conv_encoder, conv_decoder])\nRecurrent Autoencoders\nIf you want to build an autoencoder for sequences, such as time series or text (e.g., for\nunsupervised learning or dimensionality reduction), then recurrent neural networks\n(see Chapter 15) may be better suited than dense networks. Building a recurrent\nautoencoder is straightforward: the encoder is typically a sequence-to-vector RNN\nwhich compresses the input sequence down to a single vector. The decoder is a\nvector-to-sequence RNN that does the reverse:\nrecurrent_encoder = keras.models.Sequential([ keras.layers.LSTM(100, return_sequences=True, input_shape=[None, 28]), keras.layers.LSTM(30)\n])\nrecurrent_decoder = keras.models.Sequential([ keras.layers.RepeatVector(28, input_shape=[30]), keras.layers.LSTM(100, return_sequences=True), keras.layers.TimeDistributed(keras.layers.Dense(28, activation=\"sigmoid\"))\n])\nrecurrent_ae = keras.models.Sequential([recurrent_encoder, recurrent_decoder])\nThis recurrent autoencoder can process sequences of any length, with 28 dimensions\nper time step. Conveniently, this means it can process Fashion MNIST images by\ntreating each image as a sequence of rows: at each time step, the RNN will process a\nsingle row of 28 pixels. Obviously, you could use a recurrent autoencoder for any\nkind of sequence. Note that we use a RepeatVector layer as the first layer of the\ndecoder, to ensure that its input vector gets fed to the decoder at each time step. OK, let\u2019s step back for a second. So far we have seen various kinds of autoencoders\n(basic, stacked, convolutional, and recurrent), and we have looked at how to train\nthem (either in one shot or layer by layer). We also looked at a couple applications:\ndata visualization and unsupervised pretraining. Up to now, in order to force the autoencoder to learn interesting features, we have\nlimited the size of the coding layer, making it undercomplete. There are actually\nmany other kinds of constraints that can be used, including ones that allow the cod\u2010\ning layer to be just as large as the inputs, or even larger, resulting in an overcomplete\nautoencoder. Let\u2019s look at some of those approaches now. | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n5 Pascal Vincent et al., \u201cExtracting and Composing Robust Features with Denoising Autoencoders,\u201d Proceedings\nof the 25th International Conference on Machine Learning (2008): 1096\u20131103. 6 Pascal Vincent et al., \u201cStacked Denoising Autoencoders: Learning Useful Representations in a Deep Network\nwith a Local Denoising Criterion,\u201d Journal of Machine Learning Research 11 (2010): 3371\u20133408."
  },
  {
    "id": 389,
    "content": "Denoising Autoencoders\nAnother way to force the autoencoder to learn useful features is to add noise to its\ninputs, training it to recover the original, noise-free inputs. This idea has been around\nsince the 1980s (e.g., it is mentioned in Yann LeCun\u2019s 1987 master\u2019s thesis). In a 2008\npaper,5 Pascal Vincent et al. showed that autoencoders could also be used for feature\nextraction. In a 2010 paper,6 Vincent et al. introduced stacked denoising autoencoders. The noise can be pure Gaussian noise added to the inputs, or it can be randomly\nswitched-off inputs, just like in dropout (introduced in Chapter 11). Figure 17-8\nshows both options. Figure 17-8. Denoising autoencoders, with Gaussian noise (left) or dropout (right)\nThe implementation is straightforward: it is a regular stacked autoencoder with an\nadditional Dropout layer applied to the encoder\u2019s inputs (or you could use a Gaus\nsianNoise layer instead). Recall that the Dropout layer is only active during training\n(and so is the GaussianNoise layer):\nDenoising Autoencoders | dropout_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dropout(0.5), keras.layers.Dense(100, activation=\"selu\"), keras.layers.Dense(30, activation=\"selu\")\n])\ndropout_decoder = keras.models.Sequential([ keras.layers.Dense(100, activation=\"selu\", input_shape=[30]), keras.layers.Dense(28 * 28, activation=\"sigmoid\"), keras.layers.Reshape([28, 28])\n])\ndropout_ae = keras.models.Sequential([dropout_encoder, dropout_decoder])\nFigure 17-9 shows a few noisy images (with half the pixels turned off), and the\nimages reconstructed by the dropout-based denoising autoencoder. Notice how the\nautoencoder guesses details that are actually not in the input, such as the top of the\nwhite shirt (bottom row, fourth image). As you can see, not only can denoising\nautoencoders be used for data visualization or unsupervised pretraining, like the\nother autoencoders we\u2019ve discussed so far, but they can also be used quite simply and\nefficiently to remove noise from images. Figure 17-9. Noisy images (top) and their reconstructions (bottom)\nSparse Autoencoders\nAnother kind of constraint that often leads to good feature extraction is sparsity: by\nadding an appropriate term to the cost function, the autoencoder is pushed to reduce\nthe number of active neurons in the coding layer. For example, it may be pushed to\nhave on average only 5% significantly active neurons in the coding layer. This forces\nthe autoencoder to represent each input as a combination of a small number of acti\u2010\nvations. As a result, each neuron in the coding layer typically ends up representing a\nuseful feature (if you could speak only a few words per month, you would probably\ntry to make them worth listening to)."
  },
  {
    "id": 390,
    "content": "A simple approach is to use the sigmoid activation function in the coding layer (to\nconstrain the codings to values between 0 and 1), use a large coding layer (e.g., with | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n300 units), and add some \u21131 regularization to the coding layer\u2019s activations (the\ndecoder is just a regular decoder):\nsparse_l1_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(100, activation=\"selu\"), keras.layers.Dense(300, activation=\"sigmoid\"), keras.layers.ActivityRegularization(l1=1e-3)\n])\nsparse_l1_decoder = keras.models.Sequential([ keras.layers.Dense(100, activation=\"selu\", input_shape=[300]), keras.layers.Dense(28 * 28, activation=\"sigmoid\"), keras.layers.Reshape([28, 28])\n])\nsparse_l1_ae = keras.models.Sequential([sparse_l1_encoder, sparse_l1_decoder])\nThis ActivityRegularization layer just returns its inputs, but as a side effect it adds\na training loss equal to the sum of absolute values of its inputs (this layer only has an\neffect during training). Equivalently, you could remove the ActivityRegularization\nlayer and set activity_regularizer=keras.regularizers.l1(1e-3) in the previous\nlayer. This penalty will encourage the neural network to produce codings close to 0,\nbut since it will also be penalized if it does not reconstruct the inputs correctly, it will\nhave to output at least a few nonzero values. Using the \u21131 norm rather than the \u21132\nnorm will push the neural network to preserve the most important codings while\neliminating the ones that are not needed for the input image (rather than just reduc\u2010\ning all codings). Another approach, which often yields better results, is to measure the actual sparsity\nof the coding layer at each training iteration, and penalize the model when the meas\u2010\nured sparsity differs from a target sparsity. We do so by computing the average activa\u2010\ntion of each neuron in the coding layer, over the whole training batch. The batch size\nmust not be too small, or else the mean will not be accurate. Once we have the mean activation per neuron, we want to penalize the neurons that\nare too active, or not active enough, by adding a sparsity loss to the cost function. For\nexample, if we measure that a neuron has an average activation of 0.3, but the target\nsparsity is 0.1, it must be penalized to activate less. One approach could be simply\nadding the squared error (0.3 \u2013 0.1)2 to the cost function, but in practice a better\napproach is to use the Kullback\u2013Leibler (KL) divergence (briefly discussed in Chap\u2010\nter 4), which has much stronger gradients than the mean squared error, as you can\nsee in Figure 17-10. Sparse Autoencoders | Figure 17-10. Sparsity loss\nGiven two discrete probability distributions P and Q, the KL divergence between\nthese distributions, noted DKL(P \u2225 Q), can be computed using Equation 17-1. Equation 17-1. Kullback\u2013Leibler divergence\nDKL P \u2225Q = \u2211\ni P i log P i\nQ i\nIn our case, we want to measure the divergence between the target probability p that a\nneuron in the coding layer will activate and the actual probability q (i.e., the mean\nactivation over the training batch). So the KL divergence simplifies to Equation 17-2. Equation 17-2."
  },
  {
    "id": 391,
    "content": "KL divergence between the target sparsity p and the actual sparsity q\nDKL p \u2225q = p log p\nq + 1 \u2212p log 1 \u2212p\n1 \u2212q\nOnce we have computed the sparsity loss for each neuron in the coding layer, we sum\nup these losses and add the result to the cost function. In order to control the relative\nimportance of the sparsity loss and the reconstruction loss, we can multiply the spar\u2010\nsity loss by a sparsity weight hyperparameter. If this weight is too high, the model will\nstick closely to the target sparsity, but it may not reconstruct the inputs properly,\nmaking the model useless. Conversely, if it is too low, the model will mostly ignore\nthe sparsity objective and will not learn any interesting features. | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\nWe now have all we need to implement a sparse autoencoder based on the KL diver\u2010\ngence. First, let\u2019s create a custom regularizer to apply KL divergence regularization:\nK = keras.backend\nkl_divergence = keras.losses.kullback_leibler_divergence\nclass KLDivergenceRegularizer(keras.regularizers.Regularizer): def __init__(self, weight, target=0.1): self.weight = weight self.target = target def __call__(self, inputs): mean_activities = K.mean(inputs, axis=0) return self.weight * ( kl_divergence(self.target, mean_activities) + kl_divergence(1. - self.target, 1. - mean_activities))\nNow we can build the sparse autoencoder, using the KLDivergenceRegularizer for\nthe coding layer\u2019s activations:\nkld_reg = KLDivergenceRegularizer(weight=0.05, target=0.1)\nsparse_kl_encoder = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(100, activation=\"selu\"), keras.layers.Dense(300, activation=\"sigmoid\", activity_regularizer=kld_reg)\n])\nsparse_kl_decoder = keras.models.Sequential([ keras.layers.Dense(100, activation=\"selu\", input_shape=[300]), keras.layers.Dense(28 * 28, activation=\"sigmoid\"), keras.layers.Reshape([28, 28])\n])\nsparse_kl_ae = keras.models.Sequential([sparse_kl_encoder, sparse_kl_decoder])\nAfter training this sparse autoencoder on Fashion MNIST, the activations of the neu\u2010\nrons in the coding layer are mostly close to 0 (about 70% of all activations are lower\nthan 0.1), and all neurons have a mean activation around 0.1 (about 90% of all neu\u2010\nrons have a mean activation between 0.1 and 0.2), as shown in Figure 17-11. Figure 17-11. Distribution of all the activations in the coding layer (left) and distribution\nof the mean activation per neuron (right)\nSparse Autoencoders | 7 Diederik Kingma and Max Welling, \u201cAuto-Encoding Variational Bayes,\u201d arXiv preprint arXiv:1312.6114\n(2013). Variational Autoencoders\nAnother important category of autoencoders was introduced in 2013 by Diederik\nKingma and Max Welling and quickly became one of the most popular types of\nautoencoders: variational autoencoders.7\nThey are quite different from all the autoencoders we have discussed so far, in these\nparticular ways:\n\u2022 They are probabilistic autoencoders, meaning that their outputs are partly deter\u2010\nmined by chance, even after training (as opposed to denoising autoencoders,\nwhich use randomness only during training). \u2022 Most importantly, they are generative autoencoders, meaning that they can gener\u2010\nate new instances that look like they were sampled from the training set. Both these properties make them rather similar to RBMs, but they are easier to train,\nand the sampling process is much faster (with RBMs you need to wait for the network\nto stabilize into a \u201cthermal equilibrium\u201d before you can sample a new instance)."
  },
  {
    "id": 392,
    "content": "Indeed, as their name suggests, variational autoencoders perform variational Baye\u2010\nsian inference (introduced in Chapter 9), which is an efficient way to perform\napproximate Bayesian inference. Let\u2019s take a look at how they work. Figure 17-12 (left) shows a variational autoen\u2010\ncoder. You can recognize the basic structure of all autoencoders, with an encoder fol\u2010\nlowed by a decoder (in this example, they both have two hidden layers), but there is a\ntwist: instead of directly producing a coding for a given input, the encoder produces a\nmean coding \u03bc and a standard deviation \u03c3. The actual coding is then sampled ran\u2010\ndomly from a Gaussian distribution with mean \u03bc and standard deviation \u03c3. After that\nthe decoder decodes the sampled coding normally. The right part of the diagram\nshows a training instance going through this autoencoder. First, the encoder pro\u2010\nduces \u03bc and \u03c3, then a coding is sampled randomly (notice that it is not exactly located\nat \u03bc), and finally this coding is decoded; the final output resembles the training\ninstance. | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n8 Variational autoencoders are actually more general; the codings are not limited to Gaussian distributions. Figure 17-12. Variational autoencoder (left) and an instance going through it (right)\nAs you can see in the diagram, although the inputs may have a very convoluted distri\u2010\nbution, a variational autoencoder tends to produce codings that look as though they\nwere sampled from a simple Gaussian distribution:8 during training, the cost function\n(discussed next) pushes the codings to gradually migrate within the coding space\n(also called the latent space) to end up looking like a cloud of Gaussian points. One\ngreat consequence is that after training a variational autoencoder, you can very easily\ngenerate a new instance: just sample a random coding from the Gaussian distribu\u2010\ntion, decode it, and voil\u00e0! Now, let\u2019s look at the cost function. It is composed of two parts. The first is the usual\nreconstruction loss that pushes the autoencoder to reproduce its inputs (we can use\ncross entropy for this, as discussed earlier). The second is the latent loss that pushes\nthe autoencoder to have codings that look as though they were sampled from a simple\nGaussian distribution: it is the KL divergence between the target distribution (i.e., the\nGaussian distribution) and the actual distribution of the codings. The math is a bit\nmore complex than with the sparse autoencoder, in particular because of the Gaus\u2010\nsian noise, which limits the amount of information that can be transmitted to the\ncoding layer (thus pushing the autoencoder to learn useful features). Luckily, the\nVariational Autoencoders | 9 For more mathematical details, check out the original paper on variational autoencoders, or Carl Doersch\u2019s\ngreat tutorial (2016). equations simplify, so the latent loss can be computed quite simply using Equation\n17-3:9\nEquation 17-3."
  },
  {
    "id": 393,
    "content": "Variational autoencoder\u2019s latent loss\n\u2112= \u22121\n2 \u2211\ni = 1\nK\n1 + log \u03c3i\n2 \u2212\u03c3i\n2 \u2212\u03bci In this equation, \u2112 is the latent loss, n is the codings\u2019 dimensionality, and \u03bci and \u03c3i are\nthe mean and standard deviation of the ith component of the codings. The vectors \u03bc\nand \u03c3 (which contain all the \u03bci and \u03c3i) are output by the encoder, as shown in\nFigure 17-12 (left). A common tweak to the variational autoencoder\u2019s architecture is to make the encoder\noutput \u03b3 = log(\u03c32) rather than \u03c3. The latent loss can then be computed as shown in\nEquation 17-4. This approach is more numerically stable and speeds up training. Equation 17-4. Variational autoencoder\u2019s latent loss, rewritten using \u03b3 = log(\u03c32)\n\u2112= \u22121\n2 \u2211\ni = 1\nK\n1 + \u03b3i \u2212exp \u03b3i \u2212\u03bci Let\u2019s start building a variational autoencoder for Fashion MNIST (as shown in\nFigure 17-12, but using the \u03b3 tweak). First, we will need a custom layer to sample the\ncodings, given \u03bc and \u03b3:\nclass Sampling(keras.layers.Layer): def call(self, inputs): mean, log_var = inputs return K.random_normal(tf.shape(log_var)) * K.exp(log_var / 2) + mean\nThis Sampling layer takes two inputs: mean (\u03bc) and log_var (\u03b3). It uses the function\nK.random_normal() to sample a random vector (of the same shape as \u03b3) from the\nNormal distribution, with mean 0 and standard deviation 1. Then it multiplies it by\nexp(\u03b3 / 2) (which is equal to \u03c3, as you can verify), and finally it adds \u03bc and returns the\nresult. This samples a codings vector from the Normal distribution with mean \u03bc and\nstandard deviation \u03c3. Next, we can create the encoder, using the Functional API because the model is not\nentirely sequential: | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\ncodings_size = 10\ninputs = keras.layers.Input(shape=[28, 28])\nz = keras.layers.Flatten()(inputs)\nz = keras.layers.Dense(150, activation=\"selu\")(z)\nz = keras.layers.Dense(100, activation=\"selu\")(z)\ncodings_mean = keras.layers.Dense(codings_size)(z) # \u03bc\ncodings_log_var = keras.layers.Dense(codings_size)(z) # \u03b3\ncodings = Sampling()([codings_mean, codings_log_var])\nvariational_encoder = keras.Model( inputs=[inputs], outputs=[codings_mean, codings_log_var, codings])\nNote that the Dense layers that output codings_mean (\u03bc) and codings_log_var (\u03b3)\nhave the same inputs (i.e., the outputs of the second Dense layer). We then pass both\ncodings_mean and codings_log_var to the Sampling layer. Finally, the varia\ntional_encoder model has three outputs, in case you want to inspect the values of\ncodings_mean and codings_log_var. The only output we will use is the last one (cod\nings). Now let\u2019s build the decoder:\ndecoder_inputs = keras.layers.Input(shape=[codings_size])\nx = keras.layers.Dense(100, activation=\"selu\")(decoder_inputs)\nx = keras.layers.Dense(150, activation=\"selu\")(x)\nx = keras.layers.Dense(28 * 28, activation=\"sigmoid\")(x)\noutputs = keras.layers.Reshape([28, 28])(x)\nvariational_decoder = keras.Model(inputs=[decoder_inputs], outputs=[outputs])\nFor this decoder, we could have used the Sequential API instead of the Functional\nAPI, since it is really just a simple stack of layers, virtually identical to many of the\ndecoders we have built so far."
  },
  {
    "id": 394,
    "content": "Finally, let\u2019s build the variational autoencoder model:\n_, _, codings = variational_encoder(inputs)\nreconstructions = variational_decoder(codings)\nvariational_ae = keras.Model(inputs=[inputs], outputs=[reconstructions])\nNote that we ignore the first two outputs of the encoder (we only want to feed the\ncodings to the decoder). Lastly, we must add the latent loss and the reconstruction\nloss:\nlatent_loss = -0.5 * K.sum( 1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean), axis=-1)\nvariational_ae.add_loss(K.mean(latent_loss) / 784.) variational_ae.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\nWe first apply Equation 17-4 to compute the latent loss for each instance in the batch\n(we sum over the last axis). Then we compute the mean loss over all the instances in\nthe batch, and we divide the result by 784 to ensure it has the appropriate scale com\u2010\npared to the reconstruction loss. Indeed, the variational autoencoder\u2019s reconstruction\nloss is supposed to be the sum of the pixel reconstruction errors, but when Keras\ncomputes the \"binary_crossentropy\" loss, it computes the mean over all 784 pixels,\nVariational Autoencoders | rather than the sum. So, the reconstruction loss is 784 times smaller than we need it\nto be. We could define a custom loss to compute the sum rather than the mean, but it\nis simpler to divide the latent loss by 784 (the final loss will be 784 times smaller than\nit should be, but this just means that we should use a larger learning rate). Note that we use the RMSprop optimizer, which works well in this case. And finally we\ncan train the autoencoder! history = variational_ae.fit(X_train, X_train, epochs=50, batch_size=128, validation_data=[X_valid, X_valid])\nGenerating Fashion MNIST Images\nNow let\u2019s use this variational autoencoder to generate images that look like fashion\nitems. All we need to do is sample random codings from a Gaussian distribution and\ndecode them:\ncodings = tf.random.normal(shape=[12, codings_size])\nimages = variational_decoder(codings).numpy()\nFigure 17-13 shows the 12 generated images. Figure 17-13. Fashion MNIST images generated by the variational autoencoder\nThe majority of these images look fairly convincing, if a bit too fuzzy. The rest are not\ngreat, but don\u2019t be too harsh on the autoencoder\u2014it only had a few minutes to learn! Give it a bit more fine-tuning and training time, and those images should look better. Variational autoencoders make it possible to perform semantic interpolation: instead\nof interpolating two images at the pixel level (which would look as if the two images\nwere overlaid), we can interpolate at the codings level. We first run both images\nthrough the encoder, then we interpolate the two codings we get, and finally we\ndecode the interpolated codings to get the final image. It will look like a regular Fash\u2010\nion MNIST image, but it will be an intermediate between the original images. In the\nfollowing code example, we take the 12 codings we just generated, we organize them | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\nin a 3 \u00d7 4 grid, and we use TensorFlow\u2019s tf.image.resize() function to resize this\ngrid to 5 \u00d7 7."
  },
  {
    "id": 395,
    "content": "By default, the resize() function will perform bilinear interpolation, so\nevery other row and column will contain interpolated codings. We then use the\ndecoder to produce all the images:\ncodings_grid = tf.reshape(codings, [1, 3, 4, codings_size])\nlarger_grid = tf.image.resize(codings_grid, size=[5, 7])\ninterpolated_codings = tf.reshape(larger_grid, [-1, codings_size])\nimages = variational_decoder(interpolated_codings).numpy()\nFigure 17-14 shows the resulting images. The original images are framed, and the rest\nare the result of semantic interpolation between the nearby images. Notice, for exam\u2010\nple, how the shoe in the fourth row and fifth column is a nice interpolation between\nthe two shoes located above and below it. Figure 17-14. Semantic interpolation\nFor several years, variational autoencoders were quite popular, but GANs eventually\ntook the lead, in particular because they are capable of generating much more realistic\nand crisp images. So let\u2019s turn our attention to GANs. Variational Autoencoders | 10 Ian Goodfellow et al., \u201cGenerative Adversarial Nets,\u201d Proceedings of the 27th International Conference on Neu\u2010\nral Information Processing Systems 2 (2014): 2672\u20132680. Generative Adversarial Networks\nGenerative adversarial networks were proposed in a 2014 paper10 by Ian Goodfellow\net al., and although the idea got researchers excited almost instantly, it took a few\nyears to overcome some of the difficulties of training GANs. Like many great ideas, it\nseems simple in hindsight: make neural networks compete against each other in the\nhope that this competition will push them to excel. As shown in Figure 17-15, a GAN\nis composed of two neural networks:\nGenerator\nTakes a random distribution as input (typically Gaussian) and outputs some data\n\u2014typically, an image. You can think of the random inputs as the latent represen\u2010\ntations (i.e., codings) of the image to be generated. So, as you can see, the genera\u2010\ntor offers the same functionality as a decoder in a variational autoencoder, and it\ncan be used in the same way to generate new images (just feed it some Gaussian\nnoise, and it outputs a brand-new image). However, it is trained very differently,\nas we will soon see. Discriminator\nTakes either a fake image from the generator or a real image from the training set\nas input, and must guess whether the input image is fake or real. Figure 17-15. A generative adversarial network | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\nDuring training, the generator and the discriminator have opposite goals: the dis\u2010\ncriminator tries to tell fake images from real images, while the generator tries to pro\u2010\nduce images that look real enough to trick the discriminator. Because the GAN is\ncomposed of two networks with different objectives, it cannot be trained like a regu\u2010\nlar neural network. Each training iteration is divided into two phases:\n\u2022 In the first phase, we train the discriminator. A batch of real images is sampled\nfrom the training set and is completed with an equal number of fake images pro\u2010\nduced by the generator."
  },
  {
    "id": 396,
    "content": "The labels are set to 0 for fake images and 1 for real\nimages, and the discriminator is trained on this labeled batch for one step, using\nthe binary cross-entropy loss. Importantly, backpropagation only optimizes the\nweights of the discriminator during this phase. \u2022 In the second phase, we train the generator. We first use it to produce another\nbatch of fake images, and once again the discriminator is used to tell whether the\nimages are fake or real. This time we do not add real images in the batch, and all\nthe labels are set to 1 (real): in other words, we want the generator to produce\nimages that the discriminator will (wrongly) believe to be real! Crucially, the\nweights of the discriminator are frozen during this step, so backpropagation only\naffects the weights of the generator. The generator never actually sees any real images, yet it gradually\nlearns to produce convincing fake images! All it gets is the gradi\u2010\nents flowing back through the discriminator. Fortunately, the better\nthe discriminator gets, the more information about the real images\nis contained in these secondhand gradients, so the generator can\nmake significant progress. Let\u2019s go ahead and build a simple GAN for Fashion MNIST. First, we need to build the generator and the discriminator. The generator is similar\nto an autoencoder\u2019s decoder, and the discriminator is a regular binary classifier (it\ntakes an image as input and ends with a Dense layer containing a single unit and\nusing the sigmoid activation function). For the second phase of each training itera\u2010\ntion, we also need the full GAN model containing the generator followed by the\ndiscriminator:\ncodings_size = 30\ngenerator = keras.models.Sequential([ keras.layers.Dense(100, activation=\"selu\", input_shape=[codings_size]), keras.layers.Dense(150, activation=\"selu\"), keras.layers.Dense(28 * 28, activation=\"sigmoid\"), keras.layers.Reshape([28, 28])\n])\nGenerative Adversarial Networks | discriminator = keras.models.Sequential([ keras.layers.Flatten(input_shape=[28, 28]), keras.layers.Dense(150, activation=\"selu\"), keras.layers.Dense(100, activation=\"selu\"), keras.layers.Dense(1, activation=\"sigmoid\")\n])\ngan = keras.models.Sequential([generator, discriminator])\nNext, we need to compile these models. As the discriminator is a binary classifier, we\ncan naturally use the binary cross-entropy loss. The generator will only be trained\nthrough the gan model, so we do not need to compile it at all. The gan model is also a\nbinary classifier, so it can use the binary cross-entropy loss. Importantly, the discrimi\u2010\nnator should not be trained during the second phase, so we make it non-trainable\nbefore compiling the gan model:\ndiscriminator.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\ndiscriminator.trainable = False\ngan.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\")\nThe trainable attribute is taken into account by Keras only when\ncompiling a model, so after running this code, the discriminator\nis trainable if we call its fit() method or its train_on_batch()\nmethod (which we will be using), while it is not trainable when we\ncall these methods on the gan model. Since the training loop is unusual, we cannot use the regular fit() method. Instead,\nwe will write a custom training loop."
  },
  {
    "id": 397,
    "content": "For this, we first need to create a Dataset to\niterate through the images:\nbatch_size = 32\ndataset = tf.data.Dataset.from_tensor_slices(X_train).shuffle(1000)\ndataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)\nWe are now ready to write the training loop. Let\u2019s wrap it in a train_gan() function: | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\ndef train_gan(gan, dataset, batch_size, codings_size, n_epochs=50): generator, discriminator = gan.layers for epoch in range(n_epochs): for X_batch in dataset: # phase 1 - training the discriminator noise = tf.random.normal(shape=[batch_size, codings_size]) generated_images = generator(noise) X_fake_and_real = tf.concat([generated_images, X_batch], axis=0) y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size) discriminator.trainable = True discriminator.train_on_batch(X_fake_and_real, y1) # phase 2 - training the generator noise = tf.random.normal(shape=[batch_size, codings_size]) y2 = tf.constant([[1.]] * batch_size) discriminator.trainable = False gan.train_on_batch(noise, y2)\ntrain_gan(gan, dataset, batch_size, codings_size)\nAs discussed earlier, you can see the two phases at each iteration:\n\u2022 In phase one we feed Gaussian noise to the generator to produce fake images,\nand we complete this batch by concatenating an equal number of real images. The targets y1 are set to 0 for fake images and 1 for real images. Then we train\nthe discriminator on this batch. Note that we set the discriminator\u2019s trainable\nattribute to True: this is only to get rid of a warning that Keras displays when it\nnotices that trainable is now False but was True when the model was compiled\n(or vice versa). \u2022 In phase two, we feed the GAN some Gaussian noise. Its generator will start by\nproducing fake images, then the discriminator will try to guess whether these\nimages are fake or real. We want the discriminator to believe that the fake images\nare real, so the targets y2 are set to 1. Note that we set the trainable attribute to\nFalse, once again to avoid a warning. That\u2019s it! If you display the generated images (see Figure 17-16), you will see that at\nthe end of the first epoch, they already start to look like (very noisy) Fashion MNIST\nimages. Unfortunately, the images never really get much better than that, and you may even\nfind epochs where the GAN seems to be forgetting what it learned. Why is that? Well,\nit turns out that training a GAN can be challenging. Let\u2019s see why. Generative Adversarial Networks | Figure 17-16. Images generated by the GAN after one epoch of training\nThe Difficulties of Training GANs\nDuring training, the generator and the discriminator constantly try to outsmart each\nother, in a zero-sum game. As training advances, the game may end up in a state that\ngame theorists call a Nash equilibrium, named after the mathematician John Nash:\nthis is when no player would be better off changing their own strategy, assuming the\nother players do not change theirs. For example, a Nash equilibrium is reached when\neveryone drives on the left side of the road: no driver would be better off being the\nonly one to switch sides."
  },
  {
    "id": 398,
    "content": "Of course, there is a second possible Nash equilibrium:\nwhen everyone drives on the right side of the road. Different initial states and dynam\u2010\nics may lead to one equilibrium or the other. In this example, there is a single optimal\nstrategy once an equilibrium is reached (i.e., driving on the same side as everyone\nelse), but a Nash equilibrium can involve multiple competing strategies (e.g., a preda\u2010\ntor chases its prey, the prey tries to escape, and neither would be better off changing\ntheir strategy). So how does this apply to GANs? Well, the authors of the paper demonstrated that a\nGAN can only reach a single Nash equilibrium: that\u2019s when the generator produces\nperfectly realistic images, and the discriminator is forced to guess (50% real, 50%\nfake). This fact is very encouraging: it would seem that you just need to train the\nGAN for long enough, and it will eventually reach this equilibrium, giving you a per\u2010\nfect generator. Unfortunately, it\u2019s not that simple: nothing guarantees that the equili\u2010\nbrium will ever be reached. | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n11 For a nice comparison of the main GAN losses, check out this great GitHub project by Hwalsuk Lee. 12 Mario Lucic et al., \u201cAre GANs Created Equal? A Large-Scale Study,\u201d Proceedings of the 32nd International Con\u2010\nference on Neural Information Processing Systems (2018): 698\u2013707. The biggest difficulty is called mode collapse: this is when the generator\u2019s outputs\ngradually become less diverse. How can this happen? Suppose that the generator gets\nbetter at producing convincing shoes than any other class. It will fool the discrimina\u2010\ntor a bit more with shoes, and this will encourage it to produce even more images of\nshoes. Gradually, it will forget how to produce anything else. Meanwhile, the only\nfake images that the discriminator will see will be shoes, so it will also forget how to\ndiscriminate fake images of other classes. Eventually, when the discriminator man\u2010\nages to discriminate the fake shoes from the real ones, the generator will be forced to\nmove to another class. It may then become good at shirts, forgetting about shoes, and\nthe discriminator will follow. The GAN may gradually cycle across a few classes,\nnever really becoming very good at any of them. Moreover, because the generator and the discriminator are constantly pushing against\neach other, their parameters may end up oscillating and becoming unstable. Training\nmay begin properly, then suddenly diverge for no apparent reason, due to these insta\u2010\nbilities. And since many factors affect these complex dynamics, GANs are very sensi\u2010\ntive to the hyperparameters: you may have to spend a lot of effort fine-tuning them. These problems have kept researchers very busy since 2014: many papers were pub\u2010\nlished on this topic, some proposing new cost functions11 (though a 2018 paper12 by\nGoogle researchers questions their efficiency) or techniques to stabilize training or to\navoid the mode collapse issue."
  },
  {
    "id": 399,
    "content": "For example, a popular technique called experience\nreplay consists in storing the images produced by the generator at each iteration in a\nreplay buffer (gradually dropping older generated images) and training the discrimi\u2010\nnator using real images plus fake images drawn from this buffer (rather than just fake\nimages produced by the current generator). This reduces the chances that the dis\u2010\ncriminator will overfit the latest generator\u2019s outputs. Another common technique is\ncalled mini-batch discrimination: it measures how similar images are across the batch\nand provides this statistic to the discriminator, so it can easily reject a whole batch of\nfake images that lack diversity. This encourages the generator to produce a greater\nvariety of images, reducing the chance of mode collapse. Other papers simply pro\u2010\npose specific architectures that happen to perform well. In short, this is still a very active field of research, and the dynamics of GANs are still\nnot perfectly understood. But the good news is that great progress has been made,\nand some of the results are truly astounding! So let\u2019s look at some of the most success\u2010\nful architectures, starting with deep convolutional GANs, which were the state of the\nart just a few years ago. Then we will look at two more recent (and more complex)\narchitectures. Generative Adversarial Networks | 13 Alec Radford et al., \u201cUnsupervised Representation Learning with Deep Convolutional Generative Adversarial\nNetworks,\u201d arXiv preprint arXiv:1511.06434 (2015). Deep Convolutional GANs\nThe original GAN paper in 2014 experimented with convolutional layers, but only\ntried to generate small images. Soon after, many researchers tried to build GANs\nbased on deeper convolutional nets for larger images. This proved to be tricky, as\ntraining was very unstable, but Alec Radford et al. finally succeeded in late 2015, after\nexperimenting with many different architectures and hyperparameters. They called\ntheir architecture deep convolutional GANs (DCGANs).13 Here are the main guide\u2010\nlines they proposed for building stable convolutional GANs:\n\u2022 Replace any pooling layers with strided convolutions (in the discriminator) and\ntransposed convolutions (in the generator). \u2022 Use Batch Normalization in both the generator and the discriminator, except in\nthe generator\u2019s output layer and the discriminator\u2019s input layer. \u2022 Remove fully connected hidden layers for deeper architectures. \u2022 Use ReLU activation in the generator for all layers except the output layer, which\nshould use tanh. \u2022 Use leaky ReLU activation in the discriminator for all layers. These guidelines will work in many cases, but not always, so you may still need to\nexperiment with different hyperparameters (in fact, just changing the random seed\nand training the same model again will sometimes work)."
  },
  {
    "id": 400,
    "content": "For example, here is a small\nDCGAN that works reasonably well with Fashion MNIST: | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\ncodings_size = 100\ngenerator = keras.models.Sequential([ keras.layers.Dense(7 * 7 * 128, input_shape=[codings_size]), keras.layers.Reshape([7, 7, 128]), keras.layers.BatchNormalization(), keras.layers.Conv2DTranspose(64, kernel_size=5, strides=2, padding=\"same\", activation=\"selu\"), keras.layers.BatchNormalization(), keras.layers.Conv2DTranspose(1, kernel_size=5, strides=2, padding=\"same\", activation=\"tanh\")\n])\ndiscriminator = keras.models.Sequential([ keras.layers.Conv2D(64, kernel_size=5, strides=2, padding=\"same\", activation=keras.layers.LeakyReLU(0.2), input_shape=[28, 28, 1]), keras.layers.Dropout(0.4), keras.layers.Conv2D(128, kernel_size=5, strides=2, padding=\"same\", activation=keras.layers.LeakyReLU(0.2)), keras.layers.Dropout(0.4), keras.layers.Flatten(), keras.layers.Dense(1, activation=\"sigmoid\")\n])\ngan = keras.models.Sequential([generator, discriminator])\nThe generator takes codings of size 100, and it projects them to 6272 dimensions (7 *\n7 * 128), and reshapes the result to get a 7 \u00d7 7 \u00d7 128 tensor. This tensor is batch nor\u2010\nmalized and fed to a transposed convolutional layer with a stride of 2, which upsam\u2010\nples it from 7 \u00d7 7 to 14 \u00d7 14 and reduces its depth from 128 to 64. The result is batch\nnormalized again and fed to another transposed convolutional layer with a stride of 2,\nwhich upsamples it from 14 \u00d7 14 to 28 \u00d7 28 and reduces the depth from 64 to 1. This\nlayer uses the tanh activation function, so the outputs will range from \u20131 to 1. For this\nreason, before training the GAN, we need to rescale the training set to that same\nrange. We also need to reshape it to add the channel dimension:\nX_train = X_train.reshape(-1, 28, 28, 1) * 2. - 1. # reshape and rescale\nThe discriminator looks much like a regular CNN for binary classification, except\ninstead of using max pooling layers to downsample the image, we use strided convo\u2010\nlutions (strides=2). Also note that we use the leaky ReLU activation function. Overall, we respected the DCGAN guidelines, except we replaced the BatchNormali\nzation layers in the discriminator with Dropout layers (otherwise training was unsta\u2010\nble in this case) and we replaced ReLU with SELU in the generator. Feel free to tweak\nthis architecture: you will see how sensitive it is to the hyperparameters (especially\nthe relative learning rates of the two networks). Lastly, to build the dataset, then compile and train this model, we use the exact same\ncode as earlier. After 50 epochs of training, the generator produces images like those\nGenerative Adversarial Networks | shown in Figure 17-17. It\u2019s still not perfect, but many of these images are pretty\nconvincing. Figure 17-17. Images generated by the DCGAN after 50 epochs of training\nIf you scale up this architecture and train it on a large dataset of faces, you can get\nfairly realistic images. In fact, DCGANs can learn quite meaningful latent representa\u2010\ntions, as you can see in Figure 17-18: many images were generated, and nine of them\nwere picked manually (top left), including three representing men with glasses, three\nmen without glasses, and three women without glasses."
  },
  {
    "id": 401,
    "content": "For each of these categories,\nthe codings that were used to generate the images were averaged, and an image was\ngenerated based on the resulting mean codings (lower left). In short, each of the three\nlower-left images represents the mean of the three images located above it. But this is\nnot a simple mean computed at the pixel level (this would result in three overlapping\nfaces), it is a mean computed in the latent space, so the images still look like normal\nfaces. Amazingly, if you compute men with glasses, minus men without glasses, plus\nwomen without glasses\u2014where each term corresponds to one of the mean codings\u2014\nand you generate the image that corresponds to this coding, you get the image at the\ncenter of the 3 \u00d7 3 grid of faces on the right: a woman with glasses! The eight other\nimages around it were generated based on the same vector plus a bit of noise, to illus\u2010\ntrate the semantic interpolation capabilities of DCGANs. Being able to do arithmetic\non faces feels like science fiction! | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n14 Reproduced with the kind authorization of the authors. 15 Mehdi Mirza and Simon Osindero, \u201cConditional Generative Adversarial Nets,\u201d arXiv preprint arXiv:\n1411.1784 (2014). 16 Tero Karras et al., \u201cProgressive Growing of GANs for Improved Quality, Stability, and Variation,\u201d Proceedings\nof the International Conference on Learning Representations (2018). Figure 17-18. Vector arithmetic for visual concepts (part of figure 7 from the DCGAN\npaper)14\nIf you add each image\u2019s class as an extra input to both the generator\nand the discriminator, they will both learn what each class looks\nlike, and thus you will be able to control the class of each image\nproduced by the generator. This is called a conditional GAN15\n(CGAN). DCGANs aren\u2019t perfect, though. For example, when you try to generate very large\nimages using DCGANs, you often end up with locally convincing features but overall\ninconsistencies (such as shirts with one sleeve much longer than the other). How can\nyou fix this? Progressive Growing of GANs\nAn important technique was proposed in a 2018 paper16 by Nvidia researchers Tero\nKarras et al. : they suggested generating small images at the beginning of training,\nthen gradually adding convolutional layers to both the generator and the discrimina\u2010\ntor to produce larger and larger images (4 \u00d7 4, 8 \u00d7 8, 16 \u00d7 16, \u2026, 512 \u00d7 512, 1,024 \u00d7\n1,024). This approach resembles greedy layer-wise training of stacked autoencoders. Generative Adversarial Networks | The extra layers get added at the end of the generator and at the beginning of the dis\u2010\ncriminator, and previously trained layers remain trainable."
  },
  {
    "id": 402,
    "content": "For example, when growing the generator\u2019s outputs from 4 \u00d7 4 to 8 \u00d7 8 (see\nFigure 17-19), an upsampling layer (using nearest neighbor filtering) is added to the\nexisting convolutional layer, so it outputs 8 \u00d7 8 feature maps, which are then fed to\nthe new convolutional layer (which uses \"same\" padding and strides of 1, so its out\u2010\nputs are also 8 \u00d7 8). This new layer is followed by a new output convolutional layer:\nthis is a regular convolutional layer with kernel size 1 that projects the outputs down\nto the desired number of color channels (e.g., 3). To avoid breaking the trained\nweights of the first convolutional layer when the new convolutional layer is added, the\nfinal output is a weighted sum of the original output layer (which now outputs 8 \u00d7 8\nfeature maps) and the new output layer. The weight of the new outputs is \u03b1, while the\nweight of the original outputs is 1 \u2013 \u03b1, and \u03b1 is slowly increased from 0 to 1. In other\nwords, the new convolutional layers (represented with dashed lines in Figure 17-19)\nare gradually faded in, while the original output layer is gradually faded out. A similar\nfade-in/fade-out technique is used when a new convolutional layer is added to the\ndiscriminator (followed by an average pooling layer for downsampling). Figure 17-19. Progressively growing GAN: a GAN generator outputs 4 \u00d7 4 color images\n(left); we extend it to output 8 \u00d7 8 images (right) | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n17 The dynamic range of a variable is the ratio between the highest and the lowest value it may take. The paper also introduced several other techniques aimed at increasing the diversity\nof the outputs (to avoid mode collapse) and making training more stable:\nMinibatch standard deviation layer\nAdded near the end of the discriminator. For each position in the inputs, it com\u2010\nputes the standard deviation across all channels and all instances in the batch\n(S = tf.math.reduce_std(inputs, axis=[0, -1])). These standard deviations\nare then averaged across all points to get a single value (v = tf.reduce_\nmean(S)). Finally, an extra feature map is added to each instance in the batch and\nfilled with the computed value (tf.concat([inputs, tf.fill([batch_size,\nheight, width, 1], v)], axis=-1)). How does this help? Well, if the genera\u2010\ntor produces images with little variety, then there will be a small standard devia\u2010\ntion across feature maps in the discriminator. Thanks to this layer, the\ndiscriminator will have easy access to this statistic, making it less likely to be\nfooled by a generator that produces too little diversity. This will encourage the\ngenerator to produce more diverse outputs, reducing the risk of mode collapse. Equalized learning rate\nInitializes all weights using a simple Gaussian distribution with mean 0 and stan\u2010\ndard deviation 1 rather than using He initialization."
  },
  {
    "id": 403,
    "content": "However, the weights are\nscaled down at runtime (i.e., every time the layer is executed) by the same factor\nas in He initialization: they are divided by 2/ninputs, where ninputs is the number\nof inputs to the layer. The paper demonstrated that this technique significantly\nimproved the GAN\u2019s performance when using RMSProp, Adam, or other adap\u2010\ntive gradient optimizers. Indeed, these optimizers normalize the gradient updates\nby their estimated standard deviation (see Chapter 11), so parameters that have a\nlarger dynamic range17 will take longer to train, while parameters with a small\ndynamic range may be updated too quickly, leading to instabilities. By rescaling\nthe weights as part of the model itself rather than just rescaling them upon initi\u2010\nalization, this approach ensures that the dynamic range is the same for all param\u2010\neters, throughout training, so they all learn at the same speed. This both speeds\nup and stabilizes training. Pixelwise normalization layer\nAdded after each convolutional layer in the generator. It normalizes each activa\u2010\ntion based on all the activations in the same image and at the same location, but\nacross all channels (dividing by the square root of the mean squared activation). In TensorFlow code, this is inputs / tf.sqrt(tf.reduce_mean(tf.square(X),\naxis=-1, keepdims=True) + 1e-8) (the smoothing term 1e-8 is needed to\nGenerative Adversarial Networks | 18 Tero Karras et al., \u201cA Style-Based Generator Architecture for Generative Adversarial Networks,\u201d arXiv pre\u2010\nprint arXiv:1812.04948 (2018). avoid division by zero). This technique avoids explosions in the activations due\nto excessive competition between the generator and the discriminator. The combination of all these techniques allowed the authors to generate extremely\nconvincing high-definition images of faces. But what exactly do we call \u201cconvincing\u201d? Evaluation is one of the big challenges when working with GANs: although it is possi\u2010\nble to automatically evaluate the diversity of the generated images, judging their qual\u2010\nity is a much trickier and subjective task. One technique is to use human raters, but\nthis is costly and time-consuming. So the authors proposed to measure the similarity\nbetween the local image structure of the generated images and the training images,\nconsidering every scale. This idea led them to another groundbreaking innovation:\nStyleGANs. StyleGANs\nThe state of the art in high-resolution image generation was advanced once again by\nthe same Nvidia team in a 2018 paper18 that introduced the popular StyleGAN archi\u2010\ntecture. The authors used style transfer techniques in the generator to ensure that the\ngenerated images have the same local structure as the training images, at every scale,\ngreatly improving the quality of the generated images. The discriminator and the loss\nfunction were not modified, only the generator. Let\u2019s take a look at the StyleGAN."
  },
  {
    "id": 404,
    "content": "It is\ncomposed of two networks (see Figure 17-20):\nMapping network\nAn eight-layer MLP that maps the latent representations z (i.e., the codings) to a\nvector w. This vector is then sent through multiple affine transformations (i.e.,\nDense layers with no activation functions, represented by the \u201cA\u201d boxes in\nFigure 17-20), which produces multiple vectors. These vectors control the style of\nthe generated image at different levels, from fine-grained texture (e.g., hair color)\nto high-level features (e.g., adult or child). In short, the mapping network maps\nthe codings to multiple style vectors. Synthesis network\nResponsible for generating the images. It has a constant learned input (to be\nclear, this input will be constant after training, but during training it keeps getting\ntweaked by backpropagation). It processes this input through multiple convolu\u2010\ntional and upsampling layers, as earlier, but there are two twists: first, some noise\nis added to the input and to all the outputs of the convolutional layers (before the\nactivation function). Second, each noise layer is followed by an Adaptive Instance\nNormalization (AdaIN) layer: it standardizes each feature map independently (by | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\n19 Reproduced with the kind authorization of the authors. subtracting the feature map\u2019s mean and dividing by its standard deviation), then\nit uses the style vector to determine the scale and offset of each feature map (the\nstyle vector contains one scale and one bias term for each feature map). Figure 17-20. StyleGAN\u2019s generator architecture (part of figure 1 from the StyleGAN\npaper)19\nThe idea of adding noise independently from the codings is very important. Some\nparts of an image are quite random, such as the exact position of each freckle or hair. In earlier GANs, this randomness had to either come from the codings or be some\npseudorandom noise produced by the generator itself. If it came from the codings, it\nmeant that the generator had to dedicate a significant portion of the codings\u2019 repre\u2010\nsentational power to store noise: this is quite wasteful. Moreover, the noise had to be\nGenerative Adversarial Networks | able to flow through the network and reach the final layers of the generator: this\nseems like an unnecessary constraint that probably slowed down training. And\nfinally, some visual artifacts may appear because the same noise was used at different\nlevels. If instead the generator tried to produce its own pseudorandom noise, this\nnoise might not look very convincing, leading to more visual artifacts. Plus, part of\nthe generator\u2019s weights would be dedicated to generating pseudorandom noise, which\nagain seems wasteful. By adding extra noise inputs, all these issues are avoided; the\nGAN is able to use the provided noise to add the right amount of stochasticity to each\npart of the image. The added noise is different for each level."
  },
  {
    "id": 405,
    "content": "Each noise input consists of a single fea\u2010\nture map full of Gaussian noise, which is broadcast to all feature maps (of the given\nlevel) and scaled using learned per-feature scaling factors (this is represented by the\n\u201cB\u201d boxes in Figure 17-20) before it is added. Finally, StyleGAN uses a technique called mixing regularization (or style mixing),\nwhere a percentage of the generated images are produced using two different codings. Specifically, the codings c1 and c2 are sent through the mapping network, giving two\nstyle vectors w1 and w2. Then the synthesis network generates an image based on the\nstyles w1 for the first levels and the styles w2 for the remaining levels. The cutoff level\nis picked randomly. This prevents the network from assuming that styles at adjacent\nlevels are correlated, which in turn encourages locality in the GAN, meaning that\neach style vector only affects a limited number of traits in the generated image. There is such a wide variety of GANs out there that it would require a whole book to\ncover them all. Hopefully this introduction has given you the main ideas, and most\nimportantly the desire to learn more. If you\u2019re struggling with a mathematical con\u2010\ncept, there are probably blog posts out there that will help you understand it better. Then go ahead and implement your own GAN, and do not get discouraged if it has\ntrouble learning at first: unfortunately, this is normal, and it will require quite a bit of\npatience before it works, but the result is worth it. If you\u2019re struggling with an imple\u2010\nmentation detail, there are plenty of Keras or TensorFlow implementations that you\ncan look at. In fact, if all you want is to get some amazing results quickly, then you\ncan just use a pretrained model (e.g., there are pretrained StyleGAN models available\nfor Keras). In the next chapter we will move to an entirely different branch of Deep Learning:\nDeep Reinforcement Learning. | Chapter 17: Representation Learning and Generative Learning Using Autoencoders and GANs\nExercises\n1. What are the main tasks that autoencoders are used for? 2. Suppose you want to train a classifier, and you have plenty of unlabeled training\ndata but only a few thousand labeled instances. How can autoencoders help? How would you proceed? 3. If an autoencoder perfectly reconstructs the inputs, is it necessarily a good\nautoencoder? How can you evaluate the performance of an autoencoder? 4. What are undercomplete and overcomplete autoencoders? What is the main risk\nof an excessively undercomplete autoencoder? What about the main risk of an\novercomplete autoencoder? 5. How do you tie weights in a stacked autoencoder? What is the point of doing so? 6. What is a generative model? Can you name a type of generative autoencoder? 7. What is a GAN? Can you name a few tasks where GANs can shine? 8. What are the main difficulties when training GANs? 9. Try using a denoising autoencoder to pretrain an image classifier."
  },
  {
    "id": 406,
    "content": "You can use\nMNIST (the simplest option), or a more complex image dataset such as CIFAR10\nif you want a bigger challenge. Regardless of the dataset you\u2019re using, follow these\nsteps:\n\u2022 Split the dataset into a training set and a test set. Train a deep denoising\nautoencoder on the full training set. \u2022 Check that the images are fairly well reconstructed. Visualize the images that\nmost activate each neuron in the coding layer. \u2022 Build a classification DNN, reusing the lower layers of the autoencoder. Train\nit using only 500 images from the training set. Does it perform better with or\nwithout pretraining? 10. Train a variational autoencoder on the image dataset of your choice, and use it to\ngenerate images. Alternatively, you can try to find an unlabeled dataset that you\nare interested in and see if you can generate new samples. 11. Train a DCGAN to tackle the image dataset of your choice, and use it to generate\nimages. Add experience replay and see if this helps. Turn it into a conditional\nGAN where you can control the generated class. Solutions to these exercises are available in Appendix A. Exercises | 1 For more details, be sure to check out Richard Sutton and Andrew Barto\u2019s book on RL, Reinforcement Learn\u2010\ning: An Introduction (MIT Press). 2 Volodymyr Mnih et al., \u201cPlaying Atari with Deep Reinforcement Learning,\u201d arXiv preprint arXiv:1312.5602\n(2013). 3 Volodymyr Mnih et al., \u201cHuman-Level Control Through Deep Reinforcement Learning,\u201d Nature 518 (2015):\n529\u2013533. 4 Check out the videos of DeepMind\u2019s system learning to play Space Invaders, Breakout, and other video games\nat \nCHAPTER 18\nReinforcement Learning\nReinforcement Learning (RL) is one of the most exciting fields of Machine Learning\ntoday, and also one of the oldest. It has been around since the 1950s, producing many\ninteresting applications over the years,1 particularly in games (e.g., TD-Gammon, a\nBackgammon-playing program) and in machine control, but seldom making the\nheadline news. But a revolution took place in 2013, when researchers from a British\nstartup called DeepMind demonstrated a system that could learn to play just about\nany Atari game from scratch,2 eventually outperforming humans3 in most of them,\nusing only raw pixels as inputs and without any prior knowledge of the rules of the\ngames.4 This was the first of a series of amazing feats, culminating in March 2016\nwith the victory of their system AlphaGo against Lee Sedol, a legendary professional\nplayer of the game of Go, and in May 2017 against Ke Jie, the world champion. No\nprogram had ever come close to beating a master of this game, let alone the world\nchampion. Today the whole field of RL is boiling with new ideas, with a wide range of\napplications. DeepMind was bought by Google for over $500 million in 2014. So how did DeepMind achieve all this? With hindsight it seems rather simple: they\napplied the power of Deep Learning to the field of Reinforcement Learning, and it\nworked beyond their wildest dreams."
  },
  {
    "id": 407,
    "content": "In this chapter we will first explain what Reinforcement Learning is and what it\u2019s good at, then present two of the most impor\u2010\ntant techniques in Deep Reinforcement Learning: policy gradients and deep Q-\nnetworks (DQNs), including a discussion of Markov decision processes (MDPs). We\nwill use these techniques to train models to balance a pole on a moving cart; then I\u2019ll\nintroduce the TF-Agents library, which uses state-of-the-art algorithms that greatly\nsimplify building powerful RL systems, and we will use the library to train an agent to\nplay Breakout, the famous Atari game. I\u2019ll close the chapter by taking a look at some\nof the latest advances in the field. Learning to Optimize Rewards\nIn Reinforcement Learning, a software agent makes observations and takes actions\nwithin an environment, and in return it receives rewards. Its objective is to learn to act\nin a way that will maximize its expected rewards over time. If you don\u2019t mind a bit of\nanthropomorphism, you can think of positive rewards as pleasure, and negative\nrewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent\nacts in the environment and learns by trial and error to maximize its pleasure and\nminimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few\nexamples (see Figure 18-1):\na. The agent can be the program controlling a robot. In this case, the environment\nis the real world, the agent observes the environment through a set of sensors\nsuch as cameras and touch sensors, and its actions consist of sending signals to\nactivate motors. It may be programmed to get positive rewards whenever it\napproaches the target destination, and negative rewards whenever it wastes time\nor goes in the wrong direction. b. The agent can be the program controlling Ms. Pac-Man. In this case, the environ\u2010\nment is a simulation of the Atari game, the actions are the nine possible joystick\npositions (upper left, down, center, and so on), the observations are screenshots,\nand the rewards are just the game points. c. Similarly, the agent can be the program playing a board game such as Go. d. The agent does not have to control a physically (or virtually) moving thing. For\nexample, it can be a smart thermostat, getting positive rewards whenever it is\nclose to the target temperature and saves energy, and negative rewards when\nhumans need to tweak the temperature, so the agent must learn to anticipate\nhuman needs. e. The agent can observe stock market prices and decide how much to buy or sell\nevery second. Rewards are obviously the monetary gains and losses. | Chapter 18: Reinforcement Learning\n5 Image (a) is from NASA (public domain). (b) is a screenshot from the Ms. Pac-Man game, copyright Atari\n(fair use in this chapter). Images (c) and (d) are reproduced from Wikipedia. (c) was created by user Stever\u2010\ntigo and released under Creative Commons BY-SA 2.0."
  },
  {
    "id": 408,
    "content": "(d) is in the public domain. (e) was reproduced from\nPixabay, released under Creative Commons CC0. Figure 18-1. Reinforcement Learning examples: (a) robotics, (b) Ms. Pac-Man, (c) Go\nplayer, (d) thermostat, (e) automatic trader5\nNote that there may not be any positive rewards at all; for example, the agent may\nmove around in a maze, getting a negative reward at every time step, so it had better\nfind the exit as quickly as possible! There are many other examples of tasks to which\nReinforcement Learning is well suited, such as self-driving cars, recommender sys\u2010\ntems, placing ads on a web page, or controlling where an image classification system\nshould focus its attention. Learning to Optimize Rewards | 6 It is often better to give the poor performers a slight chance of survival, to preserve some diversity in the \u201cgene\npool.\u201d\nPolicy Search\nThe algorithm a software agent uses to determine its actions is called its policy. The\npolicy could be a neural network taking observations as inputs and outputting the\naction to take (see Figure 18-2). Figure 18-2. Reinforcement Learning using a neural network policy\nThe policy can be any algorithm you can think of, and it does not have to be deter\u2010\nministic. In fact, in some cases it does not even have to observe the environment! For\nexample, consider a robotic vacuum cleaner whose reward is the amount of dust it\npicks up in 30 minutes. Its policy could be to move forward with some probability p\nevery second, or randomly rotate left or right with probability 1 \u2013 p. The rotation\nangle would be a random angle between \u2013r and +r. Since this policy involves some\nrandomness, it is called a stochastic policy. The robot will have an erratic trajectory,\nwhich guarantees that it will eventually get to any place it can reach and pick up all\nthe dust. The question is, how much dust will it pick up in 30 minutes? How would you train such a robot? There are just two policy parameters you can\ntweak: the probability p and the angle range r. One possible learning algorithm could\nbe to try out many different values for these parameters, and pick the combination\nthat performs best (see Figure 18-3). This is an example of policy search, in this case\nusing a brute force approach. When the policy space is too large (which is generally\nthe case), finding a good set of parameters this way is like searching for a needle in a\ngigantic haystack. Another way to explore the policy space is to use genetic algorithms. For example, you\ncould randomly create a first generation of 100 policies and try them out, then \u201ckill\u201d\nthe 80 worst policies6 and make the 20 survivors produce 4 offspring each. An | Chapter 18: Reinforcement Learning\n7 If there is a single parent, this is called asexual reproduction. With two (or more) parents, it is called sexual\nreproduction."
  },
  {
    "id": 409,
    "content": "An offspring\u2019s genome (in this case a set of policy parameters) is randomly composed of parts of\nits parents\u2019 genomes. 8 One interesting example of a genetic algorithm used for Reinforcement Learning is the NeuroEvolution of\nAugmenting Topologies (NEAT) algorithm. 9 This is called Gradient Ascent. It\u2019s just like Gradient Descent but in the opposite direction: maximizing instead\nof minimizing. offspring is a copy of its parent7 plus some random variation. The surviving policies\nplus their offspring together constitute the second generation. You can continue to\niterate through generations this way until you find a good policy.8\nFigure 18-3. Four points in policy space (left) and the agent\u2019s corresponding behavior\n(right)\nYet another approach is to use optimization techniques, by evaluating the gradients of\nthe rewards with regard to the policy parameters, then tweaking these parameters by\nfollowing the gradients toward higher rewards.9 We will discuss this approach, is\ncalled policy gradients (PG), in more detail later in this chapter. Going back to the\nvacuum cleaner robot, you could slightly increase p and evaluate whether doing so\nincreases the amount of dust picked up by the robot in 30 minutes; if it does, then\nincrease p some more, or else reduce p. We will implement a popular PG algorithm\nusing TensorFlow, but before we do, we need to create an environment for the agent\nto live in\u2014so it\u2019s time to introduce OpenAI Gym. Introduction to OpenAI Gym\nOne of the challenges of Reinforcement Learning is that in order to train an agent,\nyou first need to have a working environment. If you want to program an agent that\nIntroduction to OpenAI Gym | 10 OpenAI is an artificial intelligence research company, funded in part by Elon Musk. Its stated goal is to pro\u2010\nmote and develop friendly AIs that will benefit humanity (rather than exterminate it). will learn to play an Atari game, you will need an Atari game simulator. If you want to\nprogram a walking robot, then the environment is the real world, and you can\ndirectly train your robot in that environment, but this has its limits: if the robot falls\noff a cliff, you can\u2019t just click Undo. You can\u2019t speed up time either; adding more com\u2010\nputing power won\u2019t make the robot move any faster. And it\u2019s generally too expensive\nto train 1,000 robots in parallel. In short, training is hard and slow in the real world,\nso you generally need a simulated environment at least for bootstrap training. For\nexample, you may use a library like PyBullet or MuJoCo for 3D physics simulation. OpenAI Gym10 is a toolkit that provides a wide variety of simulated environments\n(Atari games, board games, 2D and 3D physical simulations, and so on), so you can\ntrain agents, compare them, or develop new RL algorithms."
  },
  {
    "id": 410,
    "content": "Before installing the toolkit, if you created an isolated environment using virtualenv,\nyou first need to activate it:\n$ cd $ML_PATH # Your ML working directory (e.g., $HOME/ml)\n$ source my_env/bin/activate # on Linux or MacOS\n$ .\\my_env\\Scripts\\activate # on Windows\nNext, install OpenAI Gym (if you are not using a virtual environment, you will need\nto add the --user option, or have administrator rights):\n$ python3 -m pip install -U gym\nDepending on your system, you may also need to install the Mesa OpenGL Utility\n(GLU) library (e.g., on Ubuntu 18.04 you need to run apt install libglu1-mesa). This library will be needed to render the first environment. Next, open up a Python\nshell or a Jupyter notebook and create an environment with make():\n>>> import gym\n>>> env = gym.make(\"CartPole-v1\")\n>>> obs = env.reset()\n>>> obs\narray([-0.01258566, -0.00156614, 0.04207708, -0.00180545])\nHere, we\u2019ve created a CartPole environment. This is a 2D simulation in which a cart\ncan be accelerated left or right in order to balance a pole placed on top of it (see\nFigure 18-4). You can get the list of all available environments by running\ngym.envs.registry.all(). After the environment is created, you must initialize it\nusing the reset() method. This returns the first observation. Observations depend\non the type of environment. For the CartPole environment, each observation is a 1D\nNumPy array containing four floats: these floats represent the cart\u2019s horizontal | Chapter 18: Reinforcement Learning\nposition (0.0 = center), its velocity (positive means right), the angle of the pole (0.0 =\nvertical), and its angular velocity (positive means clockwise). Now let\u2019s display this environment by calling its render() method (see Figure 18-4). On Windows, this requires first installing an X Server, such as VcXsrv or Xming:\n>>> env.render()\nTrue\nFigure 18-4. The CartPole environment\nIf you are using a headless server (i.e., without a screen), such as a\nvirtual machine on the cloud, rendering will fail. The only way to\navoid this is to use a fake X server such as Xvfb or Xdummy. For\nexample, you can install Xvfb (apt install xvfb on Ubuntu or\nDebian) and start Python using the following command: xvfb-run\n-s \"-screen 0 1400x900x24\" python3. Alternatively, install Xvfb\nand the pyvirtualdisplay library (which wraps Xvfb) and run\npyvirtualdisplay.Display(visible=0, size=(1400,\n900)).start() at the beginning of your program. If you want render() to return the rendered image as a NumPy array, you can set\nmode=\"rgb_array\" (oddly, this environment will render the environment to screen as\nwell):\n>>> img = env.render(mode=\"rgb_array\")\n>>> img.shape # height, width, channels (3 = Red, Green, Blue)\n(800, 1200, 3)\nLet\u2019s ask the environment what actions are possible:\n>>> env.action_space\nDiscrete(2)\nDiscrete(2) means that the possible actions are integers 0 and 1, which represent\naccelerating left (0) or right (1). Other environments may have additional discrete\nIntroduction to OpenAI Gym | actions, or other kinds of actions (e.g., continuous)."
  },
  {
    "id": 411,
    "content": "Since the pole is leaning toward\nthe right (obs[2] > 0), let\u2019s accelerate the cart toward the right:\n>>> action = 1 # accelerate right\n>>> obs, reward, done, info = env.step(action)\n>>> obs\narray([-0.01261699, 0.19292789, 0.04204097, -0.28092127])\n>>> reward\n1.0\n>>> done\nFalse\n>>> info\n{}\nThe step() method executes the given action and returns four values:\nobs\nThis is the new observation. The cart is now moving toward the right (obs[1] >\n0). The pole is still tilted toward the right (obs[2] > 0), but its angular velocity is\nnow negative (obs[3] < 0), so it will likely be tilted toward the left after the next\nstep. reward\nIn this environment, you get a reward of 1.0 at every step, no matter what you do,\nso the goal is to keep the episode running as long as possible. done\nThis value will be True when the episode is over. This will happen when the pole\ntilts too much, or goes off the screen, or after 200 steps (in this last case, you have\nwon). After that, the environment must be reset before it can be used again. info\nThis environment-specific dictionary can provide some extra information that\nyou may find useful for debugging or for training. For example, in some games it\nmay indicate how many lives the agent has. Once you have finished using an environment, you should call its\nclose() method to free resources. | Chapter 18: Reinforcement Learning\nLet\u2019s hardcode a simple policy that accelerates left when the pole is leaning toward the\nleft and accelerates right when the pole is leaning toward the right. We will run this\npolicy to see the average rewards it gets over 500 episodes:\ndef basic_policy(obs): angle = obs[2] return 0 if angle < 0 else 1\ntotals = []\nfor episode in range(500): episode_rewards = 0 obs = env.reset() for step in range(200): action = basic_policy(obs) obs, reward, done, info = env.step(action) episode_rewards += reward if done: break totals.append(episode_rewards)\nThis code is hopefully self-explanatory. Let\u2019s look at the result:\n>>> import numpy as np\n>>> np.mean(totals), np.std(totals), np.min(totals), np.max(totals)\n(41.718, 8.858356280936096, 24.0, 68.0)\nEven with 500 tries, this policy never managed to keep the pole upright for more than\n68 consecutive steps. Not great. If you look at the simulation in the Jupyter note\u2010\nbooks, you will see that the cart oscillates left and right more and more strongly until\nthe pole tilts too much. Let\u2019s see if a neural network can come up with a better policy. Neural Network Policies\nLet\u2019s create a neural network policy. Just like with the policy we hardcoded earlier, this\nneural network will take an observation as input, and it will output the action to be\nexecuted. More precisely, it will estimate a probability for each action, and then we\nwill select an action randomly, according to the estimated probabilities (see\nFigure 18-5)."
  },
  {
    "id": 412,
    "content": "In the case of the CartPole environment, there are just two possible\nactions (left or right), so we only need one output neuron. It will output the probabil\u2010\nity p of action 0 (left), and of course the probability of action 1 (right) will be 1 \u2013 p.\nFor example, if it outputs 0.7, then we will pick action 0 with 70% probability, or\naction 1 with 30% probability. Neural Network Policies | Figure 18-5. Neural network policy\nYou may wonder why we are picking a random action based on the probabilities\ngiven by the neural network, rather than just picking the action with the highest\nscore. This approach lets the agent find the right balance between exploring new\nactions and exploiting the actions that are known to work well. Here\u2019s an analogy:\nsuppose you go to a restaurant for the first time, and all the dishes look equally\nappealing, so you randomly pick one. If it turns out to be good, you can increase the\nprobability that you\u2019ll order it next time, but you shouldn\u2019t increase that probability\nup to 100%, or else you will never try out the other dishes, some of which may be\neven better than the one you tried. Also note that in this particular environment, the past actions and observations can\nsafely be ignored, since each observation contains the environment\u2019s full state. If there\nwere some hidden state, then you might need to consider past actions and observa\u2010\ntions as well. For example, if the environment only revealed the position of the cart\nbut not its velocity, you would have to consider not only the current observation but\nalso the previous observation in order to estimate the current velocity. Another exam\u2010\nple is when the observations are noisy; in that case, you generally want to use the past\nfew observations to estimate the most likely current state. The CartPole problem is\nthus as simple as can be; the observations are noise-free, and they contain the envi\u2010\nronment\u2019s full state. | Chapter 18: Reinforcement Learning\nHere is the code to build this neural network policy using tf.keras:\nimport tensorflow as tf\nfrom tensorflow import keras\nn_inputs = 4 # == env.observation_space.shape[0]\nmodel = keras.models.Sequential([ keras.layers.Dense(5, activation=\"elu\", input_shape=[n_inputs]), keras.layers.Dense(1, activation=\"sigmoid\"),\n])\nAfter the imports, we use a simple Sequential model to define the policy network. The number of inputs is the size of the observation space (which in the case of Cart\u2010\nPole is 4), and we have just five hidden units because it\u2019s a simple problem. Finally, we\nwant to output a single probability (the probability of going left), so we have a single\noutput neuron using the sigmoid activation function. If there were more than two\npossible actions, there would be one output neuron per action, and we would use the\nsoftmax activation function instead. OK, we now have a neural network policy that will take observations and output\naction probabilities. But how do we train it?"
  },
  {
    "id": 413,
    "content": "Evaluating Actions: The Credit Assignment Problem\nIf we knew what the best action was at each step, we could train the neural network as\nusual, by minimizing the cross entropy between the estimated probability distribu\u2010\ntion and the target probability distribution. It would just be regular supervised learn\u2010\ning. However, in Reinforcement Learning the only guidance the agent gets is through\nrewards, and rewards are typically sparse and delayed. For example, if the agent man\u2010\nages to balance the pole for 100 steps, how can it know which of the 100 actions it\ntook were good, and which of them were bad? All it knows is that the pole fell after\nthe last action, but surely this last action is not entirely responsible. This is called the\ncredit assignment problem: when the agent gets a reward, it is hard for it to know\nwhich actions should get credited (or blamed) for it. Think of a dog that gets rewar\u2010\nded hours after it behaved well; will it understand what it is being rewarded for? To tackle this problem, a common strategy is to evaluate an action based on the sum\nof all the rewards that come after it, usually applying a discount factor \u03b3 (gamma) at\neach step. This sum of discounted rewards is called the action\u2019s return. Consider the\nexample in Figure 18-6). If an agent decides to go right three times in a row and gets\n+10 reward after the first step, 0 after the second step, and finally \u201350 after the third\nstep, then assuming we use a discount factor \u03b3 = 0.8, the first action will have a return\nof 10 + \u03b3 \u00d7 0 + \u03b32 \u00d7 (\u201350) = \u201322. If the discount factor is close to 0, then future\nrewards won\u2019t count for much compared to immediate rewards. Conversely, if the\ndiscount factor is close to 1, then rewards far into the future will count almost as\nEvaluating Actions: The Credit Assignment Problem | much as immediate rewards. Typical discount factors vary from 0.9 to 0.99. With a\ndiscount factor of 0.95, rewards 13 steps into the future count roughly for half as\nmuch as immediate rewards (since 0.9513 \u2248 0.5), while with a discount factor of 0.99,\nrewards 69 steps into the future count for half as much as immediate rewards. In the\nCartPole environment, actions have fairly short-term effects, so choosing a discount\nfactor of 0.95 seems reasonable. Figure 18-6. Computing an action\u2019s return: the sum of discounted future rewards\nOf course, a good action may be followed by several bad actions that cause the pole to\nfall quickly, resulting in the good action getting a low return (similarly, a good actor\nmay sometimes star in a terrible movie). However, if we play the game enough times,\non average good actions will get a higher return than bad ones. We want to estimate\nhow much better or worse an action is, compared to the other possible actions, on\naverage."
  },
  {
    "id": 414,
    "content": "This is called the action advantage. For this, we must run many episodes and\nnormalize all the action returns (by subtracting the mean and dividing by the stan\u2010\ndard deviation). After that, we can reasonably assume that actions with a negative\nadvantage were bad while actions with a positive advantage were good. Perfect\u2014now\nthat we have a way to evaluate each action, we are ready to train our first agent using\npolicy gradients. Let\u2019s see how. Policy Gradients\nAs discussed earlier, PG algorithms optimize the parameters of a policy by following\nthe gradients toward higher rewards. One popular class of PG algorithms, called | Chapter 18: Reinforcement Learning\n11 Ronald J. Williams, \u201cSimple Statistical Gradient-Following Algorithms for Connectionist Reinforcement\nLeaning,\u201d Machine Learning 8 (1992) : 229\u2013256. REINFORCE algorithms, was introduced back in 199211 by Ronald Williams. Here is\none common variant:\n1. First, let the neural network policy play the game several times, and at each step,\ncompute the gradients that would make the chosen action even more likely\u2014but\ndon\u2019t apply these gradients yet. 2. Once you have run several episodes, compute each action\u2019s advantage (using the\nmethod described in the previous section). 3. If an action\u2019s advantage is positive, it means that the action was probably good,\nand you want to apply the gradients computed earlier to make the action even\nmore likely to be chosen in the future. However, if the action\u2019s advantage is nega\u2010\ntive, it means the action was probably bad, and you want to apply the opposite\ngradients to make this action slightly less likely in the future. The solution is sim\u2010\nply to multiply each gradient vector by the corresponding action\u2019s advantage. 4. Finally, compute the mean of all the resulting gradient vectors, and use it to per\u2010\nform a Gradient Descent step. Let\u2019s use tf.keras to implement this algorithm. We will train the neural network policy\nwe built earlier so that it learns to balance the pole on the cart. First, we need a func\u2010\ntion that will play one step. We will pretend for now that whatever action it takes is\nthe right one so that we can compute the loss and its gradients (these gradients will\njust be saved for a while, and we will modify them later depending on how good or\nbad the action turned out to be):\ndef play_one_step(env, obs, model, loss_fn): with tf.GradientTape() as tape: left_proba = model(obs[np.newaxis]) action = (tf.random.uniform([1, 1]) > left_proba) y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32) loss = tf.reduce_mean(loss_fn(y_target, left_proba)) grads = tape.gradient(loss, model.trainable_variables) obs, reward, done, info = env.step(int(action[0, 0].numpy())) return obs, reward, done, grads\nLet\u2019s walk though this function:\n\u2022 Within the GradientTape block (see Chapter 12), we start by calling the model,\ngiving it a single observation (we reshape the observation so it becomes a batch\ncontaining a single instance, as the model expects a batch). This outputs the\nprobability of going left."
  },
  {
    "id": 415,
    "content": "Policy Gradients | \u2022 Next, we sample a random float between 0 and 1, and we check whether it is\ngreater than left_proba. The action will be False with probability left_proba,\nor True with probability 1 - left_proba. Once we cast this Boolean to a num\u2010\nber, the action will be 0 (left) or 1 (right) with the appropriate probabilities. \u2022 Next, we define the target probability of going left: it is 1 minus the action (cast\nto a float). If the action is 0 (left), then the target probability of going left will be\n1. If the action is 1 (right), then the target probability will be 0. \u2022 Then we compute the loss using the given loss function, and we use the tape to\ncompute the gradient of the loss with regard to the model\u2019s trainable variables. Again, these gradients will be tweaked later, before we apply them, depending on\nhow good or bad the action turned out to be. \u2022 Finally, we play the selected action, and we return the new observation, the\nreward, whether the episode is ended or not, and of course the gradients that we\njust computed. Now let\u2019s create another function that will rely on the play_one_step() function to\nplay multiple episodes, returning all the rewards and gradients for each episode and\neach step:\ndef play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn): all_rewards = [] all_grads = [] for episode in range(n_episodes): current_rewards = [] current_grads = [] obs = env.reset() for step in range(n_max_steps): obs, reward, done, grads = play_one_step(env, obs, model, loss_fn) current_rewards.append(reward) current_grads.append(grads) if done: break all_rewards.append(current_rewards) all_grads.append(current_grads) return all_rewards, all_grads\nThis code returns a list of reward lists (one reward list per episode, containing one\nreward per step), as well as a list of gradient lists (one gradient list per episode, each\ncontaining one tuple of gradients per step and each tuple containing one gradient\ntensor per trainable variable). The algorithm will use the play_multiple_episodes() function to play the game\nseveral times (e.g., 10 times), then it will go back and look at all the rewards, discount\nthem, and normalize them. To do that, we need a couple more functions: the first will\ncompute the sum of future discounted rewards at each step, and the second will | Chapter 18: Reinforcement Learning\nnormalize all these discounted rewards (returns) across many episodes by subtracting\nthe mean and dividing by the standard deviation:\ndef discount_rewards(rewards, discount_factor): discounted = np.array(rewards) for step in range(len(rewards) - 2, -1, -1): discounted[step] += discounted[step + 1] * discount_factor return discounted\ndef discount_and_normalize_rewards(all_rewards, discount_factor): all_discounted_rewards = [discount_rewards(rewards, discount_factor) for rewards in all_rewards] flat_rewards = np.concatenate(all_discounted_rewards) reward_mean = flat_rewards.mean() reward_std = flat_rewards.std() return [(discounted_rewards - reward_mean) / reward_std for discounted_rewards in all_discounted_rewards]\nLet\u2019s check that this works:\n>>> discount_rewards([10, 0, -50], discount_factor=0.8)\narray([-22, -40, -50])\n>>> discount_and_normalize_rewards([[10, 0, -50], [10, 20]],\n... discount_factor=0.8)\n...\n[array([-0.28435071, -0.86597718, -1.18910299]), array([1.26665318, 1.0727777 ])]\nThe call to discount_rewards() returns exactly what we expect (see Figure 18-6)."
  },
  {
    "id": 416,
    "content": "You can verify that the function discount_and_normalize_rewards() does indeed\nreturn the normalized action advantages for each action in both episodes. Notice that\nthe first episode was much worse than the second, so its normalized advantages are\nall negative; all actions from the first episode would be considered bad, and con\u2010\nversely all actions from the second episode would be considered good. We are almost ready to run the algorithm! Now let\u2019s define the hyperparameters. We\nwill run 150 training iterations, playing 10 episodes per iteration, and each episode\nwill last at most 200 steps. We will use a discount factor of 0.95:\nn_iterations = 150\nn_episodes_per_update = 10\nn_max_steps = 200\ndiscount_factor = 0.95\nWe also need an optimizer and the loss function. A regular Adam optimizer with\nlearning rate 0.01 will do just fine, and we will use the binary cross-entropy loss func\u2010\ntion because we are training a binary classifier (there are two possible actions: left or\nright):\noptimizer = keras.optimizers.Adam(lr=0.01)\nloss_fn = keras.losses.binary_crossentropy\nPolicy Gradients | We are now ready to build and run the training loop! for iteration in range(n_iterations): all_rewards, all_grads = play_multiple_episodes( env, n_episodes_per_update, n_max_steps, model, loss_fn) all_final_rewards = discount_and_normalize_rewards(all_rewards, discount_factor) all_mean_grads = [] for var_index in range(len(model.trainable_variables)): mean_grads = tf.reduce_mean( [final_reward * all_grads[episode_index][step][var_index] for episode_index, final_rewards in enumerate(all_final_rewards) for step, final_reward in enumerate(final_rewards)], axis=0) all_mean_grads.append(mean_grads) optimizer.apply_gradients(zip(all_mean_grads, model.trainable_variables))\nLet\u2019s walk through this code:\n\u2022 At each training iteration, this loop calls the play_multiple_episodes() func\u2010\ntion, which plays the game 10 times and returns all the rewards and gradients for\nevery episode and step. \u2022 Then we call the discount_and_normalize_rewards() to compute each action\u2019s\nnormalized advantage (which in the code we call the final_reward). This pro\u2010\nvides a measure of how good or bad each action actually was, in hindsight. \u2022 Next, we go through each trainable variable, and for each of them we compute\nthe weighted mean of the gradients for that variable over all episodes and all\nsteps, weighted by the final_reward. \u2022 Finally, we apply these mean gradients using the optimizer: the model\u2019s trainable\nvariables will be tweaked, and hopefully the policy will be a bit better. And we\u2019re done! This code will train the neural network policy, and it will success\u2010\nfully learn to balance the pole on the cart (you can try it out in the \u201cPolicy Gradients\u201d\nsection of the Jupyter notebook). The mean reward per episode will get very close to\n200 (which is the maximum by default with this environment). Success! Researchers try to find algorithms that work well even when the\nagent initially knows nothing about the environment. However,\nunless you are writing a paper, you should not hesitate to inject\nprior knowledge into the agent, as it will speed up training dramat\u2010\nically. For example, since you know that the pole should be as verti\u2010\ncal as possible, you could add negative rewards proportional to the\npole\u2019s angle. This will make the rewards much less sparse and speed\nup training."
  },
  {
    "id": 417,
    "content": "Also, if you already have a reasonably good policy (e.g.,\nhardcoded), you may want to train the neural network to imitate it\nbefore using policy gradients to improve it. | Chapter 18: Reinforcement Learning\nThe simple policy gradients algorithm we just trained solved the CartPole task, but it\nwould not scale well to larger and more complex tasks. Indeed, it is highly sample\ninefficient, meaning it needs to explore the game for a very long time before it can\nmake significant progress. This is due to the fact that it must run multiple episodes to\nestimate the advantage of each action, as we have seen. However, it is the foundation\nof more powerful algorithms, such as Actor-Critic algorithms (which we will discuss\nbriefly at the end of this chapter). We will now look at another popular family of algorithms. Whereas PG algorithms\ndirectly try to optimize the policy to increase rewards, the algorithms we will look at\nnow are less direct: the agent learns to estimate the expected return for each state, or\nfor each action in each state, then it uses this knowledge to decide how to act. To\nunderstand these algorithms, we must first introduce Markov decision processes. Markov Decision Processes\nIn the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010\ncesses with no memory, called Markov chains. Such a process has a fixed number of\nstates, and it randomly evolves from one state to another at each step. The probability\nfor it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s, s\n\u2032), not on past states (this is why we say that the system has no memory). Figure 18-7 shows an example of a Markov chain with four states. Figure 18-7. Example of a Markov chain\nSuppose that the process starts in state s0, and there is a 70% chance that it will\nremain in that state at the next step. Eventually it is bound to leave that state and\nnever come back because no other state points back to s0. If it goes to state s1, it will\nthen most likely go to state s2 (90% probability), then immediately back to state s1\nMarkov Decision Processes | 12 Richard Bellman, \u201cA Markovian Decision Process,\u201d Journal of Mathematics and Mechanics 6, no. 5 (1957):\n679\u2013684. (with 100% probability). It may alternate a number of times between these two states,\nbut eventually it will fall into state s3 and remain there forever (this is a terminal\nstate). Markov chains can have very different dynamics, and they are heavily used in\nthermodynamics, chemistry, statistics, and much more. Markov decision processes were first described in the 1950s by Richard Bellman.12\nThey resemble Markov chains but with a twist: at each step, an agent can choose one\nof several possible actions, and the transition probabilities depend on the chosen\naction."
  },
  {
    "id": 418,
    "content": "Moreover, some state transitions return some reward (positive or negative),\nand the agent\u2019s goal is to find a policy that will maximize reward over time. For example, the MDP represented in Figure 18-8 has three states (represented by cir\u2010\ncles) and up to three possible discrete actions at each step (represented by diamonds). Figure 18-8. Example of a Markov decision process\nIf it starts in state s0, the agent can choose between actions a0, a1, or a2. If it chooses\naction a1, it just remains in state s0 with certainty, and without any reward. It can thus\ndecide to stay there forever if it wants to. But if it chooses action a0, it has a 70% prob\u2010\nability of gaining a reward of +10 and remaining in state s0. It can then try again and\nagain to gain as much reward as possible, but at one point it is going to end up\ninstead in state s1. In state s1 it has only two possible actions: a0 or a2. It can choose to\nstay put by repeatedly choosing action a0, or it can choose to move on to state s2 and\nget a negative reward of \u201350 (ouch). In state s2 it has no other choice than to take\naction a1, which will most likely lead it back to state s0, gaining a reward of +40 on the | Chapter 18: Reinforcement Learning\nway. You get the picture. By looking at this MDP, can you guess which strategy will\ngain the most reward over time? In state s0 it is clear that action a0 is the best option,\nand in state s2 the agent has no choice but to take action a1, but in state s1 it is not\nobvious whether the agent should stay put (a0) or go through the fire (a2). Bellman found a way to estimate the optimal state value of any state s, noted V*(s),\nwhich is the sum of all discounted future rewards the agent can expect on average\nafter it reaches a state s, assuming it acts optimally. He showed that if the agent acts\noptimally, then the Bellman Optimality Equation applies (see Equation 18-1). This\nrecursive equation says that if the agent acts optimally, then the optimal value of the\ncurrent state is equal to the reward it will get on average after taking one optimal\naction, plus the expected optimal value of all possible next states that this action can\nlead to. Equation 18-1. Bellman Optimality Equation\nV* s = maxa \u2211sT s, a, s\u2032 R s, a, s\u2032 + \u03b3 \u00b7 V* s\u2032\nfor all s\nIn this equation:\n\u2022 T(s, a, s\u2032) is the transition probability from state s to state s\u2032, given that the agent\nchose action a. For example, in Figure 18-8, T(s2, a1, s0) = 0.8. \u2022 R(s, a, s\u2032) is the reward that the agent gets when it goes from state s to state s\u2032,\ngiven that the agent chose action a."
  },
  {
    "id": 419,
    "content": "For example, in Figure 18-8, R(s2, a1,\ns0) = +40. \u2022 \u03b3 is the discount factor. This equation leads directly to an algorithm that can precisely estimate the optimal\nstate value of every possible state: you first initialize all the state value estimates to\nzero, and then you iteratively update them using the Value Iteration algorithm (see\nEquation 18-2). A remarkable result is that, given enough time, these estimates are\nguaranteed to converge to the optimal state values, corresponding to the optimal\npolicy. Equation 18-2. Value Iteration algorithm\nVk + 1 s\nmax\na \u2211\ns\u2032\nT s, a, s\u2032 R s, a, s\u2032 + \u03b3 \u00b7 Vk s\u2032\nfor all s\nIn this equation, Vk(s) is the estimated value of state s at the kth iteration of the\nalgorithm. Markov Decision Processes | This algorithm is an example of Dynamic Programming, which\nbreaks down a complex problem into tractable subproblems that\ncan be tackled iteratively. Knowing the optimal state values can be useful, in particular to evaluate a policy, but\nit does not give us the optimal policy for the agent. Luckily, Bellman found a very\nsimilar algorithm to estimate the optimal state-action values, generally called Q-\nValues (Quality Values). The optimal Q-Value of the state-action pair (s, a), noted\nQ*(s, a), is the sum of discounted future rewards the agent can expect on average\nafter it reaches the state s and chooses action a, but before it sees the outcome of this\naction, assuming it acts optimally after that action. Here is how it works: once again, you start by initializing all the Q-Value estimates to\nzero, then you update them using the Q-Value Iteration algorithm (see Equation\n18-3). Equation 18-3. Q-Value Iteration algorithm\nQk + 1 s, a\n\u2211\ns\u2032\nT s, a, s\u2032 R s, a, s\u2032 + \u03b3 \u00b7 max\na\u2032\nQk s\u2032, a\u2032\nfor all s\u2032a\nOnce you have the optimal Q-Values, defining the optimal policy, noted \u03c0*(s), is triv\u2010\nial: when the agent is in state s, it should choose the action with the highest Q-Value\nfor that state: \u03c0* s = argmax\na\nQ* s, a . Let\u2019s apply this algorithm to the MDP represented in Figure 18-8. First, we need to\ndefine the MDP:\ntransition_probabilities = [ # shape=[s, a, s'] [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], [[0.0, 1.0, 0.0], None, [0.0, 0.0, 1.0]], [None, [0.8, 0.1, 0.1], None]]\nrewards = [ # shape=[s, a, s'] [[+10, 0, 0], [0, 0, 0], [0, 0, 0]], [[0, 0, 0], [0, 0, 0], [0, 0, -50]], [[0, 0, 0], [+40, 0, 0], [0, 0, 0]]]\npossible_actions = [[0, 1, 2], [0, 2], [1]]\nFor example, to know the transition probability from s2 to s0 after playing action a1,\nwe will look up transition_probabilities[2][1][0] (which is 0.8). Similarly, to\nget the corresponding reward, we will look up rewards[2][1][0] (which is +40)."
  },
  {
    "id": 420,
    "content": "And to get the list of possible actions in s2, we will look up possible_actions[2] (in\nthis case, only action a1 is possible). Next, we must initialize all the Q-Values to 0\n(except for the the impossible actions, for which we set the Q-Values to \u2013\u221e): | Chapter 18: Reinforcement Learning\nQ_values = np.full((3, 3), -np.inf) # -np.inf for impossible actions\nfor state, actions in enumerate(possible_actions): Q_values[state, actions] = 0.0 # for all possible actions\nNow let\u2019s run the Q-Value Iteration algorithm. It applies Equation 18-3 repeatedly, to\nall Q-Values, for every state and every possible action:\ngamma = 0.90 # the discount factor\nfor iteration in range(50): Q_prev = Q_values.copy() for s in range(3): for a in possible_actions[s]: Q_values[s, a] = np.sum([ transition_probabilities[s][a][sp] * (rewards[s][a][sp] + gamma * np.max(Q_prev[sp])) for sp in range(3)])\nThat\u2019s it! The resulting Q-Values look like this:\n>>> Q_values\narray([[18.91891892, 17.02702702, 13.62162162], [ 0. , -inf, -4.87971488], [ -inf, 50.13365013, -inf]])\nFor example, when the agent is in state s0 and it chooses action a1, the expected sum\nof discounted future rewards is approximately 17.0. For each state, let\u2019s look at the action that has the highest Q-Value:\n>>> np.argmax(Q_values, axis=1) # optimal action for each state\narray([0, 0, 1])\nThis gives us the optimal policy for this MDP, when using a discount factor of 0.90: in\nstate s0 choose action a0; in state s1 choose action a0 (i.e., stay put); and in state s2\nchoose action a1 (the only possible action). Interestingly, if we increase the discount\nfactor to 0.95, the optimal policy changes: in state s1 the best action becomes a2 (go\nthrough the fire!). This makes sense because the more you value future rewards, the\nmore you are willing to put up with some pain now for the promise of future bliss. Temporal Difference Learning\nReinforcement Learning problems with discrete actions can often be modeled as\nMarkov decision processes, but the agent initially has no idea what the transition\nprobabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards\nare going to be either (it does not know R(s, a, s\u2032)). It must experience each state and\neach transition at least once to know the rewards, and it must experience them multi\u2010\nple times if it is to have a reasonable estimate of the transition probabilities. The Temporal Difference Learning (TD Learning) algorithm is very similar to the\nValue Iteration algorithm, but tweaked to take into account the fact that the agent has\nTemporal Difference Learning | only partial knowledge of the MDP. In general we assume that the agent initially\nknows only the possible states and actions, and nothing more. The agent uses an\nexploration policy\u2014for example, a purely random policy\u2014to explore the MDP, and as\nit progresses, the TD Learning algorithm updates the estimates of the state values\nbased on the transitions and rewards that are actually observed (see Equation 18-4). Equation 18-4."
  },
  {
    "id": 421,
    "content": "TD Learning algorithm\nVk + 1 s\n1 \u2212\u03b1 Vk s + \u03b1 r + \u03b3 \u00b7 Vk s\u2032\nor,\u00a0equivalently: Vk + 1 s\nVk s + \u03b1 \u00b7 \u03b4k s, r, s\u2032\nwith\u00a0\u03b4k s, r, s\u2032 = r + \u03b3 \u00b7 Vk s\u2032 \u2212Vk s\nIn this equation:\n\u2022 \u03b1 is the learning rate (e.g., 0.01). \u2022 r + \u03b3 \u00b7 Vk(s\u2032) is called the TD target. \u2022 \u03b4k(s, r, s\u2032) is called the TD error. A more concise way of writing the first form of this equation is to use the notation\na \u03b1 b, which means ak+1 \u2190 (1 \u2013 \u03b1) \u00b7 ak + \u03b1 \u00b7bk. So, the first line of Equation 18-4 can\nbe rewritten like this: V s\n\u03b1 r + \u03b3 \u00b7 V s\u2032 . TD Learning has many similarities with Stochastic Gradient\nDescent, in particular the fact that it handles one sample at a time. Moreover, just like Stochastic GD, it can only truly converge if you\ngradually reduce the learning rate (otherwise it will keep bouncing\naround the optimum Q-Values). For each state s, this algorithm simply keeps track of a running average of the imme\u2010\ndiate rewards the agent gets upon leaving that state, plus the rewards it expects to get\nlater (assuming it acts optimally). Q-Learning\nSimilarly, the Q-Learning algorithm is an adaptation of the Q-Value Iteration algo\u2010\nrithm to the situation where the transition probabilities and the rewards are initially\nunknown (see Equation 18-5). Q-Learning works by watching an agent play (e.g.,\nrandomly) and gradually improving its estimates of the Q-Values. Once it has | Chapter 18: Reinforcement Learning\naccurate Q-Value estimates (or close enough), then the optimal policy is choosing the\naction that has the highest Q-Value (i.e., the greedy policy). Equation 18-5. Q-Learning algorithm\nQ s, a\n\u03b1 r + \u03b3 \u00b7 max\na\u2032 Q s\u2032, a\u2032\nFor each state-action pair (s, a), this algorithm keeps track of a running average of the\nrewards r the agent gets upon leaving the state s with action a, plus the sum of dis\u2010\ncounted future rewards it expects to get. To estimate this sum, we take the maximum\nof the Q-Value estimates for the next state s\u2032, since we assume that the target policy\nwould act optimally from then on. Let\u2019s implement the Q-Learning algorithm. First, we will need to make an agent\nexplore the environment. For this, we need a step function so that the agent can exe\u2010\ncute one action and get the resulting state and reward:\ndef step(state, action): probas = transition_probabilities[state][action] next_state = np.random.choice([0, 1, 2], p=probas) reward = rewards[state][action][next_state] return next_state, reward\nNow let\u2019s implement the agent\u2019s exploration policy. Since the state space is pretty\nsmall, a simple random policy will be sufficient."
  },
  {
    "id": 422,
    "content": "If we run the algorithm for long\nenough, the agent will visit every state many times, and it will also try every possible\naction many times:\ndef exploration_policy(state): return np.random.choice(possible_actions[state])\nNext, after we initialize the Q-Values just like earlier, we are ready to run the Q-\nLearning algorithm with learning rate decay (using power scheduling, introduced in\nChapter 11):\nalpha0 = 0.05 # initial learning rate\ndecay = 0.005 # learning rate decay\ngamma = 0.90 # discount factor\nstate = 0 # initial state\nfor iteration in range(10000): action = exploration_policy(state) next_state, reward = step(state, action) next_value = np.max(Q_values[next_state]) alpha = alpha0 / (1 + iteration * decay) Q_values[state, action] *= 1 - alpha Q_values[state, action] += alpha * (reward + gamma * next_value) state = next_state\nQ-Learning | This algorithm will converge to the optimal Q-Values, but it will take many iterations,\nand possibly quite a lot of hyperparameter tuning. As you can see in Figure 18-9, the\nQ-Value Iteration algorithm (left) converges very quickly, in fewer than 20 iterations,\nwhile the Q-Learning algorithm (right) takes about 8,000 iterations to converge. Obviously, not knowing the transition probabilities or the rewards makes finding the\noptimal policy significantly harder! Figure 18-9. The Q-Value Iteration algorithm (left) versus the Q-Learning algorithm\n(right)\nThe Q-Learning algorithm is called an off-policy algorithm because the policy being\ntrained is not necessarily the one being executed: in the previous code example, the\npolicy being executed (the exploration policy) is completely random, while the policy\nbeing trained will always choose the actions with the highest Q-Values. Conversely,\nthe Policy Gradients algorithm is an on-policy algorithm: it explores the world using\nthe policy being trained. It is somewhat surprising that Q-Learning is capable of\nlearning the optimal policy by just watching an agent act randomly (imagine learning\nto play golf when your teacher is a drunk monkey). Can we do better? Exploration Policies\nOf course, Q-Learning can work only if the exploration policy explores the MDP\nthoroughly enough. Although a purely random policy is guaranteed to eventually\nvisit every state and every transition many times, it may take an extremely long time\nto do so. Therefore, a better option is to use the \u03b5-greedy policy (\u03b5 is epsilon): at each\nstep it acts randomly with probability \u03b5, or greedily with probability 1\u2013\u03b5 (i.e., choos\u2010\ning the action with the highest Q-Value). The advantage of the \u03b5-greedy policy (com\u2010\npared to a completely random policy) is that it will spend more and more time\nexploring the interesting parts of the environment, as the Q-Value estimates get better\nand better, while still spending some time visiting unknown regions of the MDP. It is\nquite common to start with a high value for \u03b5 (e.g., 1.0) and then gradually reduce it\n(e.g., down to 0.05). | Chapter 18: Reinforcement Learning\nAlternatively, rather than relying only on chance for exploration, another approach is\nto encourage the exploration policy to try actions that it has not tried much before."
  },
  {
    "id": 423,
    "content": "This can be implemented as a bonus added to the Q-Value estimates, as shown in\nEquation 18-6. Equation 18-6. Q-Learning using an exploration function\nQ s, a\n\u03b1 r + \u03b3 \u00b7 max\na\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032\nIn this equation:\n\u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(Q, N) is an exploration function, such as f(Q, N) = Q + \u03ba/(1 + N), where \u03ba is a\ncuriosity hyperparameter that measures how much the agent is attracted to the\nunknown. Approximate Q-Learning and Deep Q-Learning\nThe main problem with Q-Learning is that it does not scale well to large (or even\nmedium) MDPs with many states and actions. For example, suppose you wanted to\nuse Q-Learning to train an agent to play Ms. Pac-Man (see Figure 18-1). There are\nabout 150 pellets that Ms. Pac-Man can eat, each of which can be present or absent\n(i.e., already eaten). So, the number of possible states is greater than 2150 \u2248 1045. And if\nyou add all the possible combinations of positions for all the ghosts and Ms. Pac-\nMan, the number of possible states becomes larger than the number of atoms in our\nplanet, so there\u2019s absolutely no way you can keep track of an estimate for every single\nQ-Value. The solution is to find a function Q\u03b8(s, a) that approximates the Q-Value of any state-\naction pair (s, a) using a manageable number of parameters (given by the parameter\nvector \u03b8). This is called Approximate Q-Learning. For years it was recommended to\nuse linear combinations of handcrafted features extracted from the state (e.g., dis\u2010\ntance of the closest ghosts, their directions, and so on) to estimate Q-Values, but in\n2013, DeepMind showed that using deep neural networks can work much better,\nespecially for complex problems, and it does not require any feature engineering. A\nDNN used to estimate Q-Values is called a Deep Q-Network (DQN), and using a\nDQN for Approximate Q-Learning is called Deep Q-Learning. Now, how can we train a DQN? Well, consider the approximate Q-Value computed\nby the DQN for a given state-action pair (s, a). Thanks to Bellman, we know we want\nthis approximate Q-Value to be as close as possible to the reward r that we actually\nobserve after playing action a in state s, plus the discounted value of playing optimally\nQ-Learning | from then on. To estimate this sum of future discounted rewards, we can simply exe\u2010\ncute the DQN on the next state s\u2032 and for all possible actions a\u2032. We get an approxi\u2010\nmate future Q-Value for each possible action. We then pick the highest (since we\nassume we will be playing optimally) and discount it, and this gives us an estimate of\nthe sum of future discounted rewards."
  },
  {
    "id": 424,
    "content": "By summing the reward r and the future dis\u2010\ncounted value estimate, we get a target Q-Value y(s, a) for the state-action pair (s, a),\nas shown in Equation 18-7. Equation 18-7. Target Q-Value\nQtarget s, a = r + \u03b3 \u00b7 max\na\u2032 Q\u03b8 s\u2032, a\u2032\nWith this target Q-Value, we can run a training step using any Gradient Descent algo\u2010\nrithm. Specifically, we generally try to minimize the squared error between the esti\u2010\nmated Q-Value Q(s, a) and the target Q-Value (or the Huber loss to reduce the\nalgorithm\u2019s sensitivity to large errors). And that\u2019s all for the basic Deep Q-Learning\nalgorithm! Let\u2019s see how to implement it to solve the CartPole environment. Implementing Deep Q-Learning\nThe first thing we need is a Deep Q-Network. In theory, you need a neural net that\ntakes a state-action pair and outputs an approximate Q-Value, but in practice it\u2019s\nmuch more efficient to use a neural net that takes a state and outputs one approxi\u2010\nmate Q-Value for each possible action. To solve the CartPole environment, we do not\nneed a very complicated neural net; a couple of hidden layers will do:\nenv = gym.make(\"CartPole-v0\")\ninput_shape = [4] # == env.observation_space.shape\nn_outputs = 2 # == env.action_space.n\nmodel = keras.models.Sequential([ keras.layers.Dense(32, activation=\"elu\", input_shape=input_shape), keras.layers.Dense(32, activation=\"elu\"), keras.layers.Dense(n_outputs)\n])\nTo select an action using this DQN, we pick the action with the largest predicted Q-\nValue. To ensure that the agent explores the environment, we will use an \u03b5-greedy\npolicy (i.e., we will choose a random action with probability \u03b5):\ndef epsilon_greedy_policy(state, epsilon=0): if np.random.rand() < epsilon: return np.random.randint(2) else: Q_values = model.predict(state[np.newaxis]) return np.argmax(Q_values[0]) | Chapter 18: Reinforcement Learning\nInstead of training the DQN based only on the latest experiences, we will store all\nexperiences in a replay buffer (or replay memory), and we will sample a random train\u2010\ning batch from it at each training iteration. This helps reduce the correlations\nbetween the experiences in a training batch, which tremendously helps training. For\nthis, we will just use a deque list:\nfrom collections import deque\nreplay_buffer = deque(maxlen=2000)\nA deque is a linked list, where each element points to the next one\nand to the previous one. It makes inserting and deleting items very\nfast, but the longer the deque is, the slower random access will be. If you need a very large replay buffer, use a circular buffer; see the\n\u201cDeque vs Rotating List\u201d section of the notebook for an\nimplementation. Each experience will be composed of five elements: a state, the action the agent took,\nthe resulting reward, the next state it reached, and finally a Boolean indicating\nwhether the episode ended at that point (done). We will need a small function to sam\u2010\nple a random batch of experiences from the replay buffer."
  },
  {
    "id": 425,
    "content": "It will return five NumPy\narrays corresponding to the five experience elements:\ndef sample_experiences(batch_size): indices = np.random.randint(len(replay_buffer), size=batch_size) batch = [replay_buffer[index] for index in indices] states, actions, rewards, next_states, dones = [ np.array([experience[field_index] for experience in batch]) for field_index in range(5)] return states, actions, rewards, next_states, dones\nLet\u2019s also create a function that will play a single step using the \u03b5-greedy policy, then\nstore the resulting experience in the replay buffer:\ndef play_one_step(env, state, epsilon): action = epsilon_greedy_policy(state, epsilon) next_state, reward, done, info = env.step(action) replay_buffer.append((state, action, reward, next_state, done)) return next_state, reward, done, info\nFinally, let\u2019s create one last function that will sample a batch of experiences from the\nreplay buffer and train the DQN by performing a single Gradient Descent step on this\nbatch:\nbatch_size = 32\ndiscount_factor = 0.95\noptimizer = keras.optimizers.Adam(lr=1e-3)\nloss_fn = keras.losses.mean_squared_error\nImplementing Deep Q-Learning | def training_step(batch_size): experiences = sample_experiences(batch_size) states, actions, rewards, next_states, dones = experiences next_Q_values = model.predict(next_states) max_next_Q_values = np.max(next_Q_values, axis=1) target_Q_values = (rewards + (1 - dones) * discount_factor * max_next_Q_values) mask = tf.one_hot(actions, n_outputs) with tf.GradientTape() as tape: all_Q_values = model(states) Q_values = tf.reduce_sum(all_Q_values * mask, axis=1, keepdims=True) loss = tf.reduce_mean(loss_fn(target_Q_values, Q_values)) grads = tape.gradient(loss, model.trainable_variables) optimizer.apply_gradients(zip(grads, model.trainable_variables))\nLet\u2019s go through this code:\n\u2022 First we define some hyperparameters, and we create the optimizer and the loss\nfunction. \u2022 Then we create the training_step() function. It starts by sampling a batch of\nexperiences, then it uses the DQN to predict the Q-Value for each possible action\nin each experience\u2019s next state. Since we assume that the agent will be playing\noptimally, we only keep the maximum Q-Value for each next state. Next, we use\nEquation 18-7 to compute the target Q-Value for each experience\u2019s state-action\npair. \u2022 Next, we want to use the DQN to compute the Q-Value for each experienced\nstate-action pair. However, the DQN will also output the Q-Values for the other\npossible actions, not just for the action that was actually chosen by the agent. So\nwe need to mask out all the Q-Values we do not need. The tf.one_hot() func\u2010\ntion makes it easy to convert an array of action indices into such a mask. For\nexample, if the first three experiences contain actions 1, 1, 0, respectively, then\nthe mask will start with [[0, 1], [0, 1], [1, 0],...]. We can then multiply\nthe DQN\u2019s output with this mask, and this will zero out all the Q-Values we do\nnot want. We then sum over axis 1 to get rid of all the zeros, keeping only the Q-\nValues of the experienced state-action pairs. This gives us the Q_values tensor,\ncontaining one predicted Q-Value for each experience in the batch. \u2022 Then we compute the loss: it is the mean squared error between the target and\npredicted Q-Values for the experienced state-action pairs. \u2022 Finally, we perform a Gradient Descent step to minimize the loss with regard to\nthe model\u2019s trainable variables. This was the hardest part."
  },
  {
    "id": 426,
    "content": "Now training the model is straightforward: | Chapter 18: Reinforcement Learning\nfor episode in range(600): obs = env.reset() for step in range(200): epsilon = max(1 - episode / 500, 0.01) obs, reward, done, info = play_one_step(env, obs, epsilon) if done: break if episode > 50: training_step(batch_size)\nWe run 600 episodes, each for a maximum of 200 steps. At each step, we first com\u2010\npute the epsilon value for the \u03b5-greedy policy: it will go from 1 down to 0.01, line\u2010\narly, in a bit under 500 episodes. Then we call the play_one_step() function, which\nwill use the \u03b5-greedy policy to pick an action, then execute it and record the experi\u2010\nence in the replay buffer. If the episode is done, we exit the loop. Finally, if we are past\nthe 50th episode, we call the training_step() function to train the model on one\nbatch sampled from the replay buffer. The reason we play 50 episodes without train\u2010\ning is to give the replay buffer some time to fill up (if we don\u2019t wait enough, then\nthere will not be enough diversity in the replay buffer). And that\u2019s it, we just imple\u2010\nmented the Deep Q-Learning algorithm! Figure 18-10 shows the total rewards the agent got during each episode. Figure 18-10. Learning curve of the Deep Q-Learning algorithm\nAs you can see, the algorithm made no apparent progress at all for almost 300 epi\u2010\nsodes (in part because \u03b5 was very high at the beginning), then its performance sud\u2010\ndenly skyrocketed up to 200 (which is the maximum possible performance in this\nenvironment). That\u2019s great news: the algorithm worked fine, and it actually ran much\nfaster than the Policy Gradient algorithm! But wait\u2026 just a few episodes later, it for\u2010\ngot everything it knew, and its performance dropped below 25! This is called\nImplementing Deep Q-Learning | 13 A great 2018 post by Alex Irpan nicely lays out RL\u2019s biggest difficulties and limitations. catastrophic forgetting, and it is one of the big problems facing virtually all RL algo\u2010\nrithms: as the agent explores the environment, it updates its policy, but what it learns\nin one part of the environment may break what it learned earlier in other parts of the\nenvironment. The experiences are quite correlated, and the learning environment\nkeeps changing\u2014this is not ideal for Gradient Descent! If you increase the size of the\nreplay buffer, the algorithm will be less subject to this problem. Reducing the learning\nrate may also help. But the truth is, Reinforcement Learning is hard: training is often\nunstable, and you may need to try many hyperparameter values and random seeds\nbefore you find a combination that works well. For example, if you try changing the\nnumber of neurons per layer in the preceding from 32 to 30 or 34, the performance\nwill never go above 100 (the DQN may be more stable with one hidden layer instead\nof two)."
  },
  {
    "id": 427,
    "content": "Reinforcement Learning is notoriously difficult, largely because of\nthe training instabilities and the huge sensitivity to the choice of\nhyperparameter values and random seeds.13 As the researcher\nAndrej Karpathy put it: \u201c[Supervised learning] wants to work. [\u2026]\nRL must be forced to work.\u201d You will need time, patience, persever\u2010\nance, and perhaps a bit of luck too. This is a major reason RL is not\nas widely adopted as regular Deep Learning (e.g., convolutional\nnets). But there are a few real-world applications, beyond AlphaGo\nand Atari games: for example, Google uses RL to optimize its data\u2010\ncenter costs, and it is used in some robotics applications, for hyper\u2010\nparameter tuning, and in recommender systems. You might wonder why we didn\u2019t plot the loss. It turns out that loss is a poor indicator\nof the model\u2019s performance. The loss might go down, yet the agent might perform\nworse (e.g., this can happen when the agent gets stuck in one small region of the envi\u2010\nronment, and the DQN starts overfitting this region). Conversely, the loss could go\nup, yet the agent might perform better (e.g., if the DQN was underestimating the Q-\nValues, and it starts correctly increasing its predictions, the agent will likely perform\nbetter, getting more rewards, but the loss might increase because the DQN also sets\nthe targets, which will be larger too). The basic Deep Q-Learning algorithm we\u2019ve been using so far would be too unstable\nto learn to play Atari games. So how did DeepMind do it? Well, they tweaked the\nalgorithm! | Chapter 18: Reinforcement Learning\nDeep Q-Learning Variants\nLet\u2019s look at a few variants of the Deep Q-Learning algorithm that can stabilize and\nspeed up training. Fixed Q-Value Targets\nIn the basic Deep Q-Learning algorithm, the model is used both to make predictions\nand to set its own targets. This can lead to a situation analogous to a dog chasing its\nown tail. This feedback loop can make the network unstable: it can diverge, oscillate,\nfreeze, and so on. To solve this problem, in their 2013 paper the DeepMind research\u2010\ners used two DQNs instead of one: the first is the online model, which learns at each\nstep and is used to move the agent around, and the other is the target model used only\nto define the targets. The target model is just a clone of the online model:\ntarget = keras.models.clone_model(model)\ntarget.set_weights(model.get_weights())\nThen, in the training_step() function, we just need to change one line to use the\ntarget model instead of the online model when computing the Q-Values of the next\nstates:\nnext_Q_values = target.predict(next_states)\nFinally, in the training loop, we must copy the weights of the online model to the tar\u2010\nget model, at regular intervals (e.g., every 50 episodes):\nif episode % 50 == 0: target.set_weights(model.get_weights())\nSince the target model is updated much less often than the online model, the Q-Value\ntargets are more stable, the feedback loop we discussed earlier is dampened, and its\neffects are less severe."
  },
  {
    "id": 428,
    "content": "This approach was one of the DeepMind researchers\u2019 main\ncontributions in their 2013 paper, allowing agents to learn to play Atari games from\nraw pixels. To stabilize training, they used a tiny learning rate of 0.00025, they upda\u2010\nted the target model only every 10,000 steps (instead of the 50 in the previous code\nexample), and they used a very large replay buffer of 1 million experiences. They\ndecreased epsilon very slowly, from 1 to 0.1 in 1 million steps, and they let the algo\u2010\nrithm run for 50 million steps. Later in this chapter, we will use the TF-Agents library to train a DQN agent to play\nBreakout using these hyperparameters, but before we get there, let\u2019s take a look at\nanother DQN variant that managed to beat the state of the art once more. Deep Q-Learning Variants | 14 Hado van Hasselt et al., \u201cDeep Reinforcement Learning with Double Q-Learning,\u201d Proceedings of the 30th\nAAAI Conference on Artificial Intelligence (2015): 2094\u20132100. 15 Tom Schaul et al., \u201cPrioritized Experience Replay,\u201d arXiv preprint arXiv:1511.05952 (2015). Double DQN\nIn a 2015 paper,14 DeepMind researchers tweaked their DQN algorithm, increasing\nits performance and somewhat stabilizing training. They called this variant Double\nDQN. The update was based on the observation that the target network is prone to\noverestimating Q-Values. Indeed, suppose all actions are equally good: the Q-Values\nestimated by the target model should be identical, but since they are approximations,\nsome may be slightly greater than others, by pure chance. The target model will\nalways select the largest Q-Value, which will be slightly greater than the mean Q-\nValue, most likely overestimating the true Q-Value (a bit like counting the height of\nthe tallest random wave when measuring the depth of a pool). To fix this, they pro\u2010\nposed using the online model instead of the target model when selecting the best\nactions for the next states, and using the target model only to estimate the Q-Values\nfor these best actions. Here is the updated training_step() function:\ndef training_step(batch_size): experiences = sample_experiences(batch_size) states, actions, rewards, next_states, dones = experiences next_Q_values = model.predict(next_states) best_next_actions = np.argmax(next_Q_values, axis=1) next_mask = tf.one_hot(best_next_actions, n_outputs).numpy() next_best_Q_values = (target.predict(next_states) * next_mask).sum(axis=1) target_Q_values = (rewards + (1 - dones) * discount_factor * next_best_Q_values) mask = tf.one_hot(actions, n_outputs) [...] # the rest is the same as earlier\nJust a few months later, another improvement to the DQN algorithm was proposed. Prioritized Experience Replay\nInstead of sampling experiences uniformly from the replay buffer, why not sample\nimportant experiences more frequently? This idea is called importance sampling (IS)\nor prioritized experience replay (PER), and it was introduced in a 2015 paper15 by\nDeepMind researchers (once again!). More specifically, experiences are considered \u201cimportant\u201d if they are likely to lead to\nfast learning progress. But how can we estimate this? One reasonable approach is to\nmeasure the magnitude of the TD error \u03b4 = r + \u03b3\u00b7V(s\u2032) \u2013 V(s)."
  },
  {
    "id": 429,
    "content": "A large TD error indi\u2010\ncates that a transition (s, r, s\u2032) is very surprising, and thus probably worth learning | Chapter 18: Reinforcement Learning\n16 It could also just be that the rewards are noisy, in which case there are better methods for estimating an expe\u2010\nrience\u2019s importance (see the paper for some examples). 17 Ziyu Wang et al., \u201cDueling Network Architectures for Deep Reinforcement Learning,\u201d arXiv preprint arXiv:\n1511.06581 (2015). from.16 When an experience is recorded in the replay buffer, its priority is set to a very\nlarge value, to ensure that it gets sampled at least once. However, once it is sampled\n(and every time it is sampled), the TD error \u03b4 is computed, and this experience\u2019s pri\u2010\nority is set to p = |\u03b4| (plus a small constant to ensure that every experience has a non-\nzero probability of being sampled). The probability P of sampling an experience with\npriority p is proportional to p\u03b6, where \u03b6 is a hyperparameter that controls how greedy\nwe want importance sampling to be: when \u03b6 = 0, we just get uniform sampling, and\nwhen \u03b6 = 1, we get full-blown importance sampling. In the paper, the authors used \u03b6 =\n0.6, but the optimal value will depend on the task. There\u2019s one catch, though: since the samples will be biased toward important experi\u2010\nences, we must compensate for this bias during training by downweighting the expe\u2010\nriences according to their importance, or else the model will just overfit the\nimportant experiences. To be clear, we want important experiences to be sampled\nmore often, but this also means we must give them a lower weight during training. To\ndo this, we define each experience\u2019s training weight as w = (n P)\u2013\u03b2, where n is the\nnumber of experiences in the replay buffer, and \u03b2 is a hyperparameter that controls\nhow much we want to compensate for the importance sampling bias (0 means not at\nall, while 1 means entirely). In the paper, the authors used \u03b2 = 0.4 at the beginning of\ntraining and linearly increased it to \u03b2 = 1 by the end of training. Again, the optimal\nvalue will depend on the task, but if you increase one, you will usually want to\nincrease the other as well. Now let\u2019s look at one last important variant of the DQN algorithm. Dueling DQN\nThe Dueling DQN algorithm (DDQN, not to be confused with Double DQN,\nalthough both techniques can easily be combined) was introduced in yet another\n2015 paper17 by DeepMind researchers. To understand how it works, we must first\nnote that the Q-Value of a state-action pair (s, a) can be expressed as Q(s, a) = V(s) +\nA(s, a), where V(s) is the value of state s and A(s, a) is the advantage of taking the\naction a in state s, compared to all other possible actions in that state."
  },
  {
    "id": 430,
    "content": "Moreover, the\nvalue of a state is equal to the Q-Value of the best action a* for that state (since we\nassume the optimal policy will pick the best action), so V(s) = Q(s, a*), which implies\nthat A(s, a*) = 0. In a Dueling DQN, the model estimates both the value of the state\nand the advantage of each possible action. Since the best action should have an\nadvantage of 0, the model subtracts the maximum predicted advantage from all pre\u2010\nDeep Q-Learning Variants | 18 Matteo Hessel et al., \u201cRainbow: Combining Improvements in Deep Reinforcement Learning,\u201d arXiv preprint\narXiv:1710.02298 (2017): 3215\u20133222. 19 If you don\u2019t know this game, it\u2019s simple: a ball bounces around and breaks bricks when it touches them. You\ncontrol a paddle near the bottom of the screen. The paddle can go left or right, and you must get the ball to\nbreak every brick, while preventing it from touching the bottom of the screen. dicted advantages. Here is a simple Dueling DQN model, implemented using the\nFunctional API:\nK = keras.backend\ninput_states = keras.layers.Input(shape=[4])\nhidden1 = keras.layers.Dense(32, activation=\"elu\")(input_states)\nhidden2 = keras.layers.Dense(32, activation=\"elu\")(hidden1)\nstate_values = keras.layers.Dense(1)(hidden2)\nraw_advantages = keras.layers.Dense(n_outputs)(hidden2)\nadvantages = raw_advantages - K.max(raw_advantages, axis=1, keepdims=True)\nQ_values = state_values + advantages\nmodel = keras.Model(inputs=[input_states], outputs=[Q_values])\nThe rest of the algorithm is just the same as earlier. In fact, you can build a Double\nDueling DQN and combine it with prioritized experience replay! More generally,\nmany RL techniques can be combined, as DeepMind demonstrated in a 2017 paper.18\nThe paper\u2019s authors combined six different techniques into an agent called Rainbow,\nwhich largely outperformed the state of the art. Unfortunately, implementing all of these techniques, debugging them, fine-tuning\nthem, and of course training the models can require a huge amount of work. So\ninstead of reinventing the wheel, it is often best to reuse scalable and well-tested libra\u2010\nries, such as TF-Agents. The TF-Agents Library\nThe TF-Agents library is a Reinforcement Learning library based on TensorFlow,\ndeveloped at Google and open sourced in 2018. Just like OpenAI Gym, it provides\nmany off-the-shelf environments (including wrappers for all OpenAI Gym environ\u2010\nments), plus it supports the PyBullet library (for 3D physics simulation), DeepMind\u2019s\nDM Control library (based on MuJoCo\u2019s physics engine), and Unity\u2019s ML-Agents\nlibrary (simulating many 3D environments). It also implements many RL algorithms,\nincluding REINFORCE, DQN, and DDQN, as well as various RL components such\nas efficient replay buffers and metrics. It is fast, scalable, easy to use, and customiza\u2010\nble: you can create your own environments and neural nets, and you can customize\npretty much any component. In this section we will use TF-Agents to train an agent\nto play Breakout, the famous Atari game (see Figure 18-1119), using the DQN algo\u2010\nrithm (you can easily switch to another algorithm if you prefer). | Chapter 18: Reinforcement Learning\nFigure 18-11. The famous Breakout game\nInstalling TF-Agents\nLet\u2019s start by installing TF-Agents."
  },
  {
    "id": 431,
    "content": "This can be done using pip (as always, if you are\nusing a virtual environment, make sure to activate it first; if not, you will need to use\nthe --user option, or have administrator rights):\n$ python3 -m pip install -U tf-agents\nAt the time of this writing, TF-Agents is still quite new and\nimproving every day, so the API may change a bit by the time you\nread this\u2014but the big picture should remain the same, as well as\nmost of the code. If anything breaks, I will update the Jupyter note\u2010\nbook accordingly, so make sure to check it out. Next, let\u2019s create a TF-Agents environment that will just wrap OpenAI GGym\u2019s Break\u2010\nout environment. For this, you must first install OpenAI Gym\u2019s Atari dependencies:\n$ python3 -m pip install -U 'gym[atari]'\nAmong other libraries, this command will install atari-py, which is a Python inter\u2010\nface for the Arcade Learning Environment (ALE), a framework built on top of the\nAtari 2600 emulator Stella. TF-Agents Environments\nIf everything went well, you should be able to import TF-Agents and create a Break\u2010\nout environment:\n>>> from tf_agents.environments import suite_gym\n>>> env = suite_gym.load(\"Breakout-v4\")\n>>> env\n<tf_agents.environments.wrappers.TimeLimit at 0x10c523c18>\nThe TF-Agents Library | This is just a wrapper around an OpenAI Gym environment, which you can access\nthrough the gym attribute:\n>>> env.gym\n<gym.envs.atari.atari_env.AtariEnv at 0x24dcab940>\nTF-Agents environments are very similar to OpenAI Gym environments, but there\nare a few differences. First, the reset() method does not return an observation;\ninstead it returns a TimeStep object that wraps the observation, as well as some extra\ninformation:\n>>> env.reset()\nTimeStep(step_type=array(0, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0., 0., 0. ], [0., 0., 0.],... ]]], dtype=float32))\nThe step() method returns a TimeStep object as well:\n>>> env.step(1) # Fire\nTimeStep(step_type=array(1, dtype=int32), reward=array(0., dtype=float32), discount=array(1., dtype=float32), observation=array([[[0., 0., 0. ], [0., 0., 0.],... ]]], dtype=float32))\nThe reward and observation attributes are self-explanatory, and they are the same as\nfor OpenAI Gym (except the reward is represented as a NumPy array). The\nstep_type attribute is equal to 0 for the first time step in the episode, 1 for intermedi\u2010\nate time steps, and 2 for the final time step. You can call the time step\u2019s is_last()\nmethod to check whether it is the final one or not. Lastly, the discount attribute indi\u2010\ncates the discount factor to use at this time step. In this example it is equal to 1, so\nthere will be no discount at all. You can define the discount factor by setting the dis\ncount parameter when loading the environment. At any time, you can access the environment\u2019s current time step by\ncalling its current_time_step() method. Environment Specifications\nConveniently, a TF-Agents environment provides the specifications of the observa\u2010\ntions, actions, and time steps, including their shapes, data types, and names, as well as\ntheir minimum and maximum values: | Chapter 18: Reinforcement Learning\n>>> env.observation_spec()\nBoundedArraySpec(shape=(210, 160, 3), dtype=dtype('float32'), name=None, minimum=[[[0. 0. 0. ], [0. 0. 0.],... ]], maximum=[[[255., 255., 255."
  },
  {
    "id": 432,
    "content": "], [255., 255., 255. ], ...]])\n>>> env.action_spec()\nBoundedArraySpec(shape=(), dtype=dtype('int64'), name=None, minimum=0, maximum=3)\n>>> env.time_step_spec()\nTimeStep(step_type=ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'), reward=ArraySpec(shape=(), dtype=dtype('float32'), name='reward'), discount=BoundedArraySpec(shape=(), ..., minimum=0.0, maximum=1.0), observation=BoundedArraySpec(shape=(210, 160, 3), ...))\nAs you can see, the observations are simply screenshots of the Atari screen, repre\u2010\nsented as NumPy arrays of shape [210, 160, 3]. To render an environment, you can\ncall env.render(mode=\"human\"), and if you want to get back the image in the form of\na NumPy array, just call env.render(mode=\"rgb_array\") (unlike in OpenAI Gym,\nthis is the default mode). There are four actions available. Gym\u2019s Atari environments have an extra method that\nyou can call to know what each action corresponds to:\n>>> env.gym.get_action_meanings()\n['NOOP', 'FIRE', 'RIGHT', 'LEFT']\nSpecs can be instances of a specification class, nested lists, or dic\u2010\ntionaries of specs. If the specification is nested, then the specified\nobject must match the specification\u2019s nested structure. For example,\nif the observation spec is {\"sensors\": ArraySpec(shape=[2]),\n\"camera\": ArraySpec(shape=[100, 100])}, then a valid observa\u2010\ntion would be {\"sensors\": np.array([1.5, 3.5]), \"camera\":\nnp.array(...)}. The tf.nest package provides tools to handle\nsuch nested structures (a.k.a. nests). The observations are quite large, so we will downsample them and also convert them\nto grayscale. This will speed up training and use less RAM. For this, we can use an\nenvironment wrapper. Environment Wrappers and Atari Preprocessing\nTF-Agents provides several environment wrappers in the tf_agents.environ\nments.wrappers package. As their name suggests, they wrap an environment, for\u2010\nwarding every call to it, but also adding some extra functionality. Here are some of\nthe available wrappers:\nActionClipWrapper\nClips the actions to the action spec. The TF-Agents Library | ActionDiscretizeWrapper\nQuantizes a continuous action space to a discrete action space. For example, if\nthe original environment\u2019s action space is the continuous range from \u20131.0 to\n+1.0, but you want to use an algorithm that only supports discrete action spaces,\nsuch as a DQN, then you can wrap the environment using discrete_env =\nActionDiscretizeWrapper(env, num_actions=5), and the new discrete_env\nwill have a discrete action space with five possible actions: 0, 1, 2, 3, 4. These\nactions correspond to the actions \u20131.0, \u20130.5, 0.0, 0.5, and 1.0 in the original envi\u2010\nronment. ActionRepeat\nRepeats each action over n steps, while accumulating the rewards. In many envi\u2010\nronments, this can speed up training significantly. RunStats\nRecords environment statistics such as the number of steps and the number of\nepisodes. TimeLimit\nInterrupts the environment if it runs for longer than a maximum number of\nsteps. VideoWrapper\nRecords a video of the environment. To create a wrapped environment, you must create a wrapper, passing the wrapped\nenvironment to the constructor. That\u2019s all! For example, the following code will wrap\nour environment in an ActionRepeat wrapper so that every action is repeated four\ntimes:\nfrom tf_agents.environments.wrappers import ActionRepeat\nrepeating_env = ActionRepeat(env, times=4)\nOpenAI Gym has some environment wrappers of its own in the gym.wrappers pack\u2010\nage."
  },
  {
    "id": 433,
    "content": "They are meant to wrap Gym environments, though, not TF-Agents environ\u2010\nments, so to use them you must first wrap the Gym environment with a Gym\nwrapper, then wrap the resulting environment with a TF-Agents wrapper. The\nsuite_gym.wrap_env() function will do this for you, provided you give it a Gym\nenvironment and a list of Gym wrappers and/or a list of TF-Agents wrappers. Alter\u2010\nnatively, the suite_gym.load() function will both create the Gym environment and\nwrap it for you, if you give it some wrappers. Each wrapper will be created without\nany arguments, so if you want to set some arguments, you must pass a lambda. For\nexample, the following code creates a Breakout environment that will run for a maxi\u2010\nmum of 10,000 steps during each episode, and each action will be repeated four\ntimes: | Chapter 18: Reinforcement Learning\nfrom gym.wrappers import TimeLimit\nlimited_repeating_env = suite_gym.load( \"Breakout-v4\", gym_env_wrappers=[lambda env: TimeLimit(env, max_episode_steps=10000)], env_wrappers=[lambda env: ActionRepeat(env, times=4)])\nFor Atari environments, some standard preprocessing steps are applied in most\npapers that use them, so TF-Agents provides a handy AtariPreprocessing wrapper\nthat implements them. Here is the list of preprocessing steps it supports:\nGrayscale and downsampling\nObservations are converted to grayscale and downsampled (by default to 84 \u00d7 84\npixels). Max pooling\nThe last two frames of the game are max-pooled using a 1 \u00d7 1 filter. This is to\nremove the flickering that occurs in some Atari games due to the limited number\nof sprites that the Atari 2600 could display in each frame. Frame skipping\nThe agent only gets to see every n frames of the game (by default n = 4), and its\nactions are repeated for each frame, collecting all the rewards. This effectively\nspeeds up the game from the perspective of the agent, and it also speeds up train\u2010\ning because rewards are less delayed. End on life lost\nIn some games, the rewards are just based on the score, so the agent gets no\nimmediate penalty for losing a life. One solution is to end the game immediately\nwhenever a life is lost. There is some debate over the actual benefits of this strat\u2010\negy, so it is off by default. Since the default Atari environment already applies random frame skipping and\nmax pooling, we will need to load the raw, nonskipping variant called\n\"BreakoutNoFrameskip-v4\". Moreover, a single frame from the Breakout game is\ninsufficient to know the direction and speed of the ball, which will make it very diffi\u2010\ncult for the agent to play the game properly (unless it is an RNN agent, which pre\u2010\nserves some internal state between steps). One way to handle this is to use an\nenvironment wrapper that will output observations composed of multiple frames\nstacked on top of each other along the channels dimension. This strategy is imple\u2010\nmented by the FrameStack4 wrapper, which returns stacks of four frames. Let\u2019s create\nthe wrapped Atari environment!"
  },
  {
    "id": 434,
    "content": "The TF-Agents Library | 20 Since there are only three primary colors, you cannot just display an image with four color channels. For this\nreason, I combined the last channel with the first three to get the RGB image represented here. Pink is actually\na mix of blue and red, but the agent sees four independent channels. from tf_agents.environments import suite_atari\nfrom tf_agents.environments.atari_preprocessing import AtariPreprocessing\nfrom tf_agents.environments.atari_wrappers import FrameStack4\nmax_episode_steps = 27000 # <=> 108k ALE frames since 1 step = 4 frames\nenvironment_name = \"BreakoutNoFrameskip-v4\"\nenv = suite_atari.load( environment_name, max_episode_steps=max_episode_steps, gym_env_wrappers=[AtariPreprocessing, FrameStack4])\nThe result of all this preprocessing is shown in Figure 18-12. You can see that the res\u2010\nolution is much lower, but sufficient to play the game. Moreover, frames are stacked\nalong the channels dimension, so red represents the frame from three steps ago,\ngreen is two steps ago, blue is the previous frame, and pink is the current frame.20\nFrom this single observation, the agent can see that the ball is going toward the\nlower-left corner, and that it should continue to move the paddle to the left (as it did\nin the previous steps). Figure 18-12. Preprocessed Breakout observation\nLastly, we can wrap the environment inside a TFPyEnvironment:\nfrom tf_agents.environments.tf_py_environment import TFPyEnvironment\ntf_env = TFPyEnvironment(env)\nThis will make the environment usable from within a TensorFlow graph (under the\nhood, this class relies on tf.py_function(), which allows a graph to call arbitrary | Chapter 18: Reinforcement Learning\nPython code). Thanks to the TFPyEnvironment class, TF-Agents supports both pure\nPython environments and TensorFlow-based environments. More generally, TF-\nAgents supports and provides both pure Python and TensorFlow-based components\n(agents, replay buffers, metrics, and so on). Now that we have a nice Breakout environment, with all the appropriate preprocess\u2010\ning and TensorFlow support, we must create the DQN agent and the other compo\u2010\nnents we will need to train it. Let\u2019s look at the architecture of the system we will build. Training Architecture\nA TF-Agents training program is usually split into two parts that run in parallel, as\nyou can see in Figure 18-13: on the left, a driver explores the environment using a\ncollect policy to choose actions, and it collects trajectories (i.e., experiences), sending\nthem to an observer, which saves them to a replay buffer; on the right, an agent pulls\nbatches of trajectories from the replay buffer and trains some networks, which the col\u2010\nlect policy uses. In short, the left part explores the environment and collects trajecto\u2010\nries, while the right part learns and updates the collect policy. Figure 18-13. A typical TF-Agents training architecture\nThis figure begs a few questions, which I\u2019ll attempt to answer here:\n\u2022 Why are there multiple environments? Instead of exploring a single environ\u2010\nment, you generally want the driver to explore multiple copies of the environ\u2010\nment in parallel, taking advantage of the power of all your CPU cores, keeping\nThe TF-Agents Library | the training GPUs busy, and providing less-correlated trajectories to the training\nalgorithm."
  },
  {
    "id": 435,
    "content": "\u2022 What is a trajectory? It is a concise representation of a transition from one time\nstep to the next, or a sequence of consecutive transitions from time step n to time\nstep n + t. The trajectories collected by the driver are passed to the observer,\nwhich saves them in the replay buffer, and they are later sampled by the agent\nand used for training. \u2022 Why do we need an observer? Can\u2019t the driver save the trajectories directly? Indeed, it could, but this would make the architecture less flexible. For example,\nwhat if you don\u2019t want to use a replay buffer? What if you want to use the trajec\u2010\ntories for something else, like computing metrics? In fact, an observer is just any\nfunction that takes a trajectory as an argument. You can use an observer to save\nthe trajectories to a replay buffer, or to save them to a TFRecord file (see Chap\u2010\nter 13), or to compute metrics, or for anything else. Moreover, you can pass mul\u2010\ntiple observers to the driver, and it will broadcast the trajectories to all of them. Although this architecture is the most common, you can customize\nit as you please, and even replace some components with your own. In fact, unless you are researching new RL algorithms, you will\nmost likely want to use a custom environment for your task. For\nthis, you just need to create a custom class that inherits from the\nPyEnvironment class in the tf_agents.environments.py_environ\nment package and overrides the appropriate methods, such as\naction_spec(), observation_spec(), _reset(), and _step() (see\nthe \u201cCreating a Custom TF_Agents Environment\u201d section of the\nnotebook for an example). Now we will create all these components: first the Deep Q-Network, then the DQN\nagent (which will take care of creating the collect policy), then the replay buffer and\nthe observer to write to it, then a few training metrics, then the driver, and finally the\ndataset. Once we have all the components in place, we will populate the replay buffer\nwith some initial trajectories, then we will run the main training loop. So, let\u2019s start by\ncreating the Deep Q-Network. Creating the Deep Q-Network\nThe TF-Agents library provides many networks in the tf_agents.networks package\nand its subpackages. We will use the tf_agents.networks.q_network.QNetwork\nclass: | Chapter 18: Reinforcement Learning\nfrom tf_agents.networks.q_network import QNetwork\npreprocessing_layer = keras.layers.Lambda( lambda obs: tf.cast(obs, np.float32) / 255.) conv_layer_params=[(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\nfc_layer_params=[512]\nq_net = QNetwork( tf_env.observation_spec(), tf_env.action_spec(), preprocessing_layers=preprocessing_layer, conv_layer_params=conv_layer_params, fc_layer_params=fc_layer_params)\nThis QNetwork takes an observation as input and outputs one Q-Value per action, so\nwe must give it the specifications of the observations and the actions. It starts with a\npreprocessing layer: a simple Lambda layer that casts the observations to 32-bit floats\nand normalizes them (the values will range from 0.0 to 1.0)."
  },
  {
    "id": 436,
    "content": "The observations contain\nunsigned bytes, which use 4 times less space than 32-bit floats, which is why we did\nnot cast the observations to 32-bit floats earlier; we want to save RAM in the replay\nbuffer. Next, the network applies three convolutional layers: the first has 32 8 \u00d7 8 fil\u2010\nters and uses a stride of 4, the second has 64 4 \u00d7 4 filters and a stride of 2, and the\nthird has 64 3 \u00d7 3 filters and a stride of 1. Lastly, it applies a dense layer with 512\nunits, followed by a dense output layer with 4 units, one per Q-Value to output (i.e.,\none per action). All convolutional layers and all dense layers except the output layer\nuse the ReLU activation function by default (you can change this by setting the acti\nvation_fn argument). The output layer does not use any activation function. Under the hood, a QNetwork is composed of two parts: an encoding network that pro\u2010\ncesses the observations, followed by a dense output layer that outputs one Q-Value\nper action. TF-Agent\u2019s EncodingNetwork class implements a neural network architec\u2010\nture found in various agents (see Figure 18-14). It may have one or more inputs. For example, if each observation is composed of\nsome sensor data plus an image from a camera, you will have two inputs. Each input\nmay require some preprocessing steps, in which case you can specify a list of Keras\nlayers via the preprocessing_layers argument, with one preprocessing layer per\ninput, and the network will apply each layer to the corresponding input (if an input\nrequires multiple layers of preprocessing, you can pass a whole model, since a Keras\nmodel can always be used as a layer). If there are two inputs or more, you must also\npass an extra layer via the preprocessing_combiner argument, to combine the out\u2010\nputs from the preprocessing layers into a single output. Next, the encoding network will optionally apply a list of convolutions sequentially,\nprovided you specify their parameters via the conv_layer_params argument. This\nmust be a list composed of 3-tuples (one per convolutional layer) indicating the\nThe TF-Agents Library | number of filters, the kernel size, and the stride. After these convolutional layers, the\nencoding network will optionally apply a sequence of dense layers, if you set the\nfc_layer_params argument: it must be a list containing the number of neurons for\neach dense layer. Optionally, you can also pass a list of dropout rates (one per dense\nlayer) via the dropout_layer_params argument if you want to apply dropout after\neach dense layer. The QNetwork takes the output of this encoding network and passes\nit to the dense output layer (with one unit per action). Figure 18-14. Architecture of an encoding network\nThe QNetwork class is flexible enough to build many different\narchitectures, but you can always build your own network class if\nyou need extra flexibility: extend the tf_agents.networks.Net\nwork class and implement it like a regular custom Keras layer."
  },
  {
    "id": 437,
    "content": "The\ntf_agents.networks.Network class is a subclass of the keras.lay\ners.Layer class that adds some functionality required by some\nagents, such as the possibility to easily create shallow copies of the\nnetwork (i.e., copying the network\u2019s architecture, but not its\nweights). For example, the DQNAgent uses this to create a copy of\nthe online model. Now that we have the DQN, we are ready to build the DQN agent. Creating the DQN Agent\nThe TF-Agents library implements many types of agents, located in the tf_agents\n.agents package and its subpackages. We will use the tf_agents.agents\n.dqn.dqn_agent.DqnAgent class: | Chapter 18: Reinforcement Learning\nfrom tf_agents.agents.dqn.dqn_agent import DqnAgent\ntrain_step = tf.Variable(0)\nupdate_period = 4 # train the model every 4 steps\noptimizer = keras.optimizers.RMSprop(lr=2.5e-4, rho=0.95, momentum=0.0, epsilon=0.00001, centered=True)\nepsilon_fn = keras.optimizers.schedules.PolynomialDecay( initial_learning_rate=1.0, # initial \u03b5 decay_steps=250000 // update_period, # <=> 1,000,000 ALE frames end_learning_rate=0.01) # final \u03b5\nagent = DqnAgent(tf_env.time_step_spec(), tf_env.action_spec(), q_network=q_net, optimizer=optimizer, target_update_period=2000, # <=> 32,000 ALE frames td_errors_loss_fn=keras.losses.Huber(reduction=\"none\"), gamma=0.99, # discount factor train_step_counter=train_step, epsilon_greedy=lambda: epsilon_fn(train_step))\nagent.initialize()\nLet\u2019s walk through this code:\n\u2022 We first create a variable that will count the number of training steps. \u2022 Then we build the optimizer, using the same hyperparameters as in the 2015\nDQN paper. \u2022 Next, we create a PolynomialDecay object that will compute the \u03b5 value for the \u03b5-\ngreedy collect policy, given the current training step (it is normally used to decay\nthe learning rate, hence the names of the arguments, but it will work just fine to\ndecay any other value). It will go from 1.0 down to 0.01 (the value used during in\nthe 2015 DQN paper) in 1 million ALE frames, which corresponds to 250,000\nsteps, since we use frame skipping with a period of 4. Moreover, we will train the\nagent every 4 steps (i.e., 16 ALE frames), so \u03b5 will actually decay over 62,500\ntraining steps. \u2022 We then build the DQNAgent, passing it the time step and action specs, the QNet\nwork to train, the optimizer, the number of training steps between target model\nupdates, the loss function to use, the discount factor, the train_step variable,\nand a function that returns the \u03b5 value (it must take no argument, which is why\nwe need a lambda to pass the train_step). Note that the loss function must return an error per instance, not the mean error,\nwhich is why we set reduction=\"none\". \u2022 Lastly, we initialize the agent. Next, let\u2019s build the replay buffer and the observer that will write to it. The TF-Agents Library | 21 At the time of this writing, there is no prioritized experience replay buffer yet, but one will likely be open\nsourced soon. Creating the Replay Buffer and the Corresponding Observer\nThe TF-Agents library provides various replay buffer implementations in the\ntf_agents.replay_buffers package. Some are purely written in Python (their mod\u2010\nule names start with py_), and others are written based on TensorFlow (their module\nnames start with tf_). We will use the TFUniformReplayBuffer class in the\ntf_agents.replay_buffers.tf_uniform_replay_buffer package."
  },
  {
    "id": 438,
    "content": "It provides a\nhigh-performance implementation of a replay buffer with uniform sampling:21\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer( data_spec=agent.collect_data_spec, batch_size=tf_env.batch_size, max_length=1000000)\nLet\u2019s look at each of these arguments:\ndata_spec\nThe specification of the data that will be saved in the replay buffer. The DQN\nagent knowns what the collected data will look like, and it makes the data spec\navailable via its collect_data_spec attribute, so that\u2019s what we give the replay\nbuffer. batch_size\nThe number of trajectories that will be added at each step. In our case, it will be\none, since the driver will just execute one action per step and collect one trajec\u2010\ntory. If the environment were a batched environment, meaning an environment\nthat takes a batch of actions at each step and returns a batch of observations, then\nthe driver would have to save a batch of trajectories at each step. Since we are\nusing a TensorFlow replay buffer, it needs to know the size of the batches it will\nhandle (to build the computation graph). An example of a batched environment\nis the ParallelPyEnvironment (from the tf_agents.environments.paral\nlel_py_environment package): it runs multiple environments in parallel in sepa\u2010\nrate processes (they can be different as long as they have the same action and\nobservation specs), and at each step it takes a batch of actions and executes them\nin the environments (one action per environment), then it returns all the result\u2010\ning observations. | Chapter 18: Reinforcement Learning\nmax_length\nThe maximum size of the replay buffer. We created a large replay buffer that can\nstore one million trajectories (as was done in the 2015 DQN paper). This will\nrequire a lot of RAM. When we store two consecutive trajectories, they contain two con\u2010\nsecutive observations with four frames each (since we used the Fra\nmeStack4 wrapper), and unfortunately three of the four frames in\nthe second observation are redundant (they are already present in\nthe first observation). In other words, we are using about four\ntimes more RAM than necessary. To avoid this, you can instead use\na PyHashedReplayBuffer from the tf_agents.replay_buf\nfers.py_hashed_replay_buffer package: it deduplicates data in\nthe stored trajectories along the last axis of the observations. Now we can create the observer that will write the trajectories to the replay buffer. An\nobserver is just a function (or a callable object) that takes a trajectory argument, so we\ncan directly use the add_method() method (bound to the replay_buffer object) as\nour observer:\nreplay_buffer_observer = replay_buffer.add_batch\nIf you wanted to create your own observer, you could write any function with a\ntrajectory argument. If it must have a state, you can write a class with a\n__call__(self, trajectory) method."
  },
  {
    "id": 439,
    "content": "For example, here is a simple observer that\nwill increment a counter every time it is called (except when the trajectory represents\na boundary between two episodes, which does not count as a step), and every 100\nincrements it displays the progress up to a given total (the carriage return \\r along\nwith end=\"\" ensures that the displayed counter remains on the same line):\nclass ShowProgress: def __init__(self, total): self.counter = 0 self.total = total def __call__(self, trajectory): if not trajectory.is_boundary(): self.counter += 1 if self.counter % 100 == 0: print(\"\\r{}/{}\".format(self.counter, self.total), end=\"\")\nNow let\u2019s create a few training metrics. Creating Training Metrics\nTF-Agents implements several RL metrics in the tf_agents.metrics package, some\npurely in Python and some based on TensorFlow. Let\u2019s create a few of them in order\nThe TF-Agents Library | to count the number of episodes, the number of steps taken, and most importantly\nthe average return per episode and the average episode length:\nfrom tf_agents.metrics import tf_metrics\ntrain_metrics = [ tf_metrics.NumberOfEpisodes(), tf_metrics.EnvironmentSteps(), tf_metrics.AverageReturnMetric(), tf_metrics.AverageEpisodeLengthMetric(),\n]\nDiscounting the rewards makes sense for training or to implement\na policy, as it makes it possible to balance the importance of imme\u2010\ndiate rewards with future rewards. However, once an episode is\nover, we can evaluate how good it was overalls by summing the\nundiscounted rewards. For this reason, the AverageReturnMetric\ncomputes the sum of undiscounted rewards for each episode, and it\nkeeps track of the streaming mean of these sums over all the epi\u2010\nsodes it encounters. At any time, you can get the value of each of these metrics by calling its result()\nmethod (e.g., train_metrics[0].result()). Alternatively, you can log all metrics by\ncalling log_metrics(train_metrics) (this function is located in the\ntf_agents.eval.metric_utils package):\n>>> from tf_agents.eval.metric_utils import log_metrics\n>>> import logging\n>>> logging.get_logger().set_level(logging.INFO)\n>>> log_metrics(train_metrics)\n[...]\nNumberOfEpisodes = 0\nEnvironmentSteps = 0\nAverageReturn = 0.0\nAverageEpisodeLength = 0.0\nNext, let\u2019s create the collect driver. Creating the Collect Driver\nAs we explored in Figure 18-13, a driver is an object that explores an environment\nusing a given policy, collects experiences, and broadcasts them to some observers. At\neach step, the following things happen:\n\u2022 The driver passes the current time step to the collect policy, which uses this time\nstep to choose an action and returns an action step object containing the action. | Chapter 18: Reinforcement Learning\n\u2022 The driver then passes the action to the environment, which returns the next\ntime step. \u2022 Finally, the driver creates a trajectory object to represent this transition and\nbroadcasts it to all the observers. Some policies, such as RNN policies, are stateful: they choose an action based on both\nthe given time step and their own internal state. Stateful policies return their own\nstate in the action step, along with the chosen action. The driver will then pass this\nstate back to the policy at the next time step."
  },
  {
    "id": 440,
    "content": "Moreover, the driver saves the policy\nstate to the trajectory (in the policy_info field), so it ends up in the replay buffer. This is essential when training a stateful policy: when the agent samples a trajectory, it\nmust set the policy\u2019s state to the state it was in at the time of the sampled time step. Also, as discussed earlier, the environment may be a batched environment, in which\ncase the driver passes a batched time step to the policy (i.e., a time step object contain\u2010\ning a batch of observations, a batch of step types, a batch of rewards, and a batch of\ndiscounts, all four batches of the same size). The driver also passes a batch of previous\npolicy states. The policy then returns a batched action step containing a batch of\nactions and a batch of policy states. Finally, the driver creates a batched trajectory (i.e.,\na trajectory containing a batch of step types, a batch of observations, a batch of\nactions, a batch of rewards, and more generally a batch for each trajectory attribute,\nwith all batches of the same size). There are two main driver classes: DynamicStepDriver and DynamicEpisodeDriver. The first one collects experiences for a given number of steps, while the second col\u2010\nlects experiences for a given number of episodes. We want to collect experiences for\nfour steps for each training iteration (as was done in the 2015 DQN paper), so let\u2019s\ncreate a DynamicStepDriver:\nfrom tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\ncollect_driver = DynamicStepDriver( tf_env, agent.collect_policy, observers=[replay_buffer_observer] + training_metrics, num_steps=update_period) # collect 4 steps for each training iteration\nWe give it the environment to play with, the agent\u2019s collect policy, a list of observers\n(including the replay buffer observer and the training metrics), and finally the num\u2010\nber of steps to run (in this case, four). We could now run it by calling its run()\nmethod, but it\u2019s best to warm up the replay buffer with experiences collected using a\npurely random policy. For this, we can use the RandomTFPolicy class and create a sec\u2010\nond driver that will run this policy for 20,000 steps (which is equivalent to 80,000\nsimulator frames, as was done in the 2015 DQN paper). We can use our ShowPro\ngress observer to display the progress:\nThe TF-Agents Library | from tf_agents.policies.random_tf_policy import RandomTFPolicy\ninitial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(), tf_env.action_spec())\ninit_driver = DynamicStepDriver( tf_env, initial_collect_policy, observers=[replay_buffer.add_batch, ShowProgress(20000)], num_steps=20000) # <=> 80,000 ALE frames\nfinal_time_step, final_policy_state = init_driver.run()\nWe\u2019re almost ready to run the training loop! We just need one last component: the\ndataset. Creating the Dataset\nTo sample a batch of trajectories from the replay buffer, call its get_next() method. This returns the batch of trajectories plus a BufferInfo object that contains the sam\u2010\nple identifiers and their sampling probabilities (this may be useful for some algo\u2010\nrithms, such as PER). For example, the following code will sample a small batch of\ntwo trajectories (subepisodes), each containing three consecutive steps."
  },
  {
    "id": 441,
    "content": "These\nsubepisodes are shown in Figure 18-15 (each row contains three consecutive steps\nfrom an episode):\n>>> trajectories, buffer_info = replay_buffer.get_next(\n... sample_batch_size=2, num_steps=3)\n...\n>>> trajectories._fields\n('step_type', 'observation', 'action', 'policy_info', 'next_step_type', 'reward', 'discount')\n>>> trajectories.observation.shape\nTensorShape([2, 3, 84, 84, 4])\n>>> trajectories.step_type.numpy()\narray([[1, 1, 1], [1, 1, 1]], dtype=int32)\nThe trajectories object is a named tuple, with seven fields. Each field contains a\ntensor whose first two dimensions are 2 and 3 (since there are two trajectories, each\nwith three steps). This explains why the shape of the observation field is [2, 3, 84, 84,\n4]: that\u2019s two trajectories, each with three steps, and each step\u2019s observation is 84 \u00d7 84\n\u00d7 4. Similarly, the step_type tensor has a shape of [2, 3]: in this example, both trajec\u2010\ntories contain three consecutive steps in the middle on an episode (types 1, 1, 1). In\nthe second trajectory, you can barely see the ball at the lower left of the first observa\u2010\ntion, and it disappears in the next two observations, so the agent is about to lose a life,\nbut the episode will not end immediately because it still has several lives left. | Chapter 18: Reinforcement Learning\nFigure 18-15. Two trajectories containing three consecutive steps each\nEach trajectory is a concise representation of a sequence of consecutive time steps\nand action steps, designed to avoid redundancy. How so? Well, as you can see in\nFigure 18-16, transition n is composed of time step n, action step n, and time step n +\n1, while transition n + 1 is composed of time step n + 1, action step n + 1, and time\nstep n + 2. If we just stored these two transitions directly in the replay buffer, the time\nstep n + 1 would be duplicated. To avoid this duplication, the nth trajectory step\nincludes only the type and observation from time step n (not its reward and dis\u2010\ncount), and it does not contain the observation from time step n + 1 (however, it does\ncontain a copy of the next time step\u2019s type; that\u2019s the only duplication). The TF-Agents Library | Figure 18-16. Trajectories, transitions, time steps, and action steps\nSo if you have a batch of trajectories where each trajectory has t + 1 steps (from time\nstep n to time step n + t), then it contains all the data from time step n to time step n\n+ t, except for the reward and discount from time step n (but it contains the reward\nand discount of time step n + t + 1). This represents t transitions (n to n + 1, n + 1 to\nn + 2, \u2026, n + t \u2013 1 to n + t). The to_transition() function in the tf_agents.trajectories.trajectory mod\u2010\nule converts a batched trajectory into a list containing a batched time_step, a batched\naction_step, and a batched next_time_step."
  },
  {
    "id": 442,
    "content": "Notice that the second dimension is 2\ninstead of 3, since there are t transitions between t + 1 time steps (don\u2019t worry if\nyou\u2019re a bit confused; you\u2019ll get the hang of it):\n>>> from tf_agents.trajectories.trajectory import to_transition\n>>> time_steps, action_steps, next_time_steps = to_transition(trajectories)\n>>> time_steps.observation.shape\nTensorShape([2, 2, 84, 84, 4]) # 3 time steps = 2 transitions\nA sampled trajectory may actually overlap two (or more) episodes! In this case, it will contain boundary transitions, meaning transi\u2010\ntions with a step_type equal to 2 (end) and a next_step_type\nequal to 0 (start). Of course, TF-Agents properly handles such tra\u2010\njectories (e.g., by resetting the policy state when encountering a\nboundary). The trajectory\u2019s is_boundary() method returns a ten\u2010\nsor indicating whether each step is a boundary or not. | Chapter 18: Reinforcement Learning\nFor our main training loop, instead of calling the get_next() method, we will use a\ntf.data.Dataset. This way, we can benefit from the power of the Data API (e.g., par\u2010\nallelism and prefetching). For this, we call the replay buffer\u2019s as_dataset() method:\ndataset = replay_buffer.as_dataset( sample_batch_size=64, num_steps=2, num_parallel_calls=3).prefetch(3)\nWe will sample batches of 64 trajectories at each training step (as in the 2015 DQN\npaper), each with 2 steps (i.e., 2 steps = 1 full transition, including the next step\u2019s\nobservation). This dataset will process three elements in parallel, and prefetch three\nbatches. For on-policy algorithms such as Policy Gradients, each experience\nshould be sampled once, used from training, and then discarded. In\nthis case, you can still use a replay buffer, but instead of using a\nDataset, you would call the replay buffer\u2019s gather_all() method\nat each training iteration to get a tensor containing all the trajecto\u2010\nries recorded so far, then use them to perform a training step, and\nfinally clear the replay buffer by calling its clear() method. Now that we have all the components in place, we are ready to train the model! Creating the Training Loop\nTo speed up training, we will convert the main functions to TensorFlow Functions. For this we will use the tf_agents.utils.common.function() function, which wraps\ntf.function(), with some extra experimental options:\nfrom tf_agents.utils.common import function\ncollect_driver.run = function(collect_driver.run)\nagent.train = function(agent.train)\nLet\u2019s create a small function that will run the main training loop for n_iterations:\ndef train_agent(n_iterations): time_step = None policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size) iterator = iter(dataset) for iteration in range(n_iterations): time_step, policy_state = collect_driver.run(time_step, policy_state) trajectories, buffer_info = next(iterator) train_loss = agent.train(trajectories) print(\"\\r{} loss:{:.5f}\".format( iteration, train_loss.loss.numpy()), end=\"\") if iteration % 1000 == 0: log_metrics(train_metrics)\nThe TF-Agents Library | 22 For a comparison of this algorithm\u2019s performance on various Atari games, see figure 3 in DeepMind\u2019s 2015\npaper. 23 Volodymyr Mnih et al., \u201cAsynchonous Methods for Deep Reinforcement Learning,\u201d Proceedings of the 33rd\nInternational Conference on Machine Learning (2016): 1928\u20131937. The function first asks the collect policy for its initial state (given the environment\nbatch size, which is 1 in this case)."
  },
  {
    "id": 443,
    "content": "Since the policy is stateless, this returns an empty\ntuple (so we could have written policy_state = ()). Next, we create an iterator over\nthe dataset, and we run the training loop. At each iteration, we call the driver\u2019s run()\nmethod, passing it the current time step (initially None) and the current policy state. It\nwill run the collect policy and collect experience for four steps (as we configured ear\u2010\nlier), broadcasting the collected trajectories to the replay buffer and the metrics. Next,\nwe sample one batch of trajectories from the dataset, and we pass it to the agent\u2019s\ntrain() method. It returns a train_loss object which may vary depending on the\ntype of agent. Next, we display the iteration number and the training loss, and every\n1,000 iterations we log all the metrics. Now you can just call train_agent() for some\nnumber of iterations, and see the agent gradually learn to play Breakout! train_agent(10000000)\nThis will take a lot of computing power and a lot of patience (it may take hours, or\neven days, depending on your hardware), plus you may need to run the algorithm\nseveral times with different random seeds to get good results, but once it\u2019s done, the\nagent will be superhuman (at least at Breakout). You can also try training this DQN\nagent on other Atari games: it can achieve superhuman skill at most action games,\nbut it is not so good at games with long-running storylines.22\nOverview of Some Popular RL Algorithms\nBefore we finish this chapter, let\u2019s take a quick look at a few popular RL algorithms:\nActor-Critic algorithms\nA family of RL algorithms that combine Policy Gradients with Deep Q-\nNetworks. An Actor-Critic agent contains two neural networks: a policy net and\na DQN. The DQN is trained normally, by learning from the agent\u2019s experiences. The policy net learns differently (and much faster) than in regular PG: instead of\nestimating the value of each action by going through multiple episodes, then\nsumming the future discounted rewards for each action, and finally normalizing\nthem, the agent (actor) relies on the action values estimated by the DQN (critic). It\u2019s a bit like an athlete (the agent) learning with the help of a coach (the DQN). Asynchronous Advantage Actor-Critic23 (A3C)\nAn important Actor-Critic variant introduced by DeepMind researchers in 2016,\nwhere multiple agents learn in parallel, exploring different copies of the environ\u2010 | Chapter 18: Reinforcement Learning\n24 Tuomas Haarnoja et al., \u201cSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with\na Stochastic Actor,\u201d Proceedings of the 35th International Conference on Machine Learning (2018): 1856\u20131865. 25 John Schulman et al., \u201cProximal Policy Optimization Algorithms,\u201d arXiv preprint arXiv:1707.06347 (2017). 26 John Schulman et al., \u201cTrust Region Policy Optimization,\u201d Proceedings of the 32nd International Conference on\nMachine Learning (2015): 1889\u20131897. ment. At regular intervals, but asynchronously (hence the name), each agent\npushes some weight updates to a master network, then it pulls the latest weights\nfrom that network."
  },
  {
    "id": 444,
    "content": "Each agent thus contributes to improving the master network\nand benefits from what the other agents have learned. Moreover, instead of esti\u2010\nmating the Q-Values, the DQN estimates the advantage of each action (hence the\nsecond A in the name), which stabilizes training. Advantage Actor-Critic (A2C)\nA variant of the A3C algorithm that removes the asynchronicity. All model\nupdates are synchronous, so gradient updates are performed over larger batches,\nwhich allows the model to better utilize the power of the GPU. Soft Actor-Critic24 (SAC)\nAn Actor-Critic variant proposed in 2018 by Tuomas Haarnoja and other UC\nBerkeley researchers. It learns not only rewards, but also to maximize the entropy\nof its actions. In other words, it tries to be as unpredictable as possible while still\ngetting as many rewards as possible. This encourages the agent to explore the\nenvironment, which speeds up training, and makes it less likely to repeatedly exe\u2010\ncute the same action when the DQN produces imperfect estimates. This algo\u2010\nrithm has demonstrated an amazing sample efficiency (contrary to all the\nprevious algorithms, which learn very slowly). SAC is available in TF-Agents. Proximal Policy Optimization (PPO)25\nAn algorithm based on A2C that clips the loss function to avoid excessively large\nweight updates (which often lead to training instabilities). PPO is a simplification\nof the previous Trust Region Policy Optimization26 (TRPO) algorithm, also by\nJohn Schulman and other OpenAI researchers. OpenAI made the news in April\n2019 with their AI called OpenAI Five, based on the PPO algorithm, which\ndefeated the world champions at the multiplayer game Dota 2. PPO is also avail\u2010\nable in TF-Agents. Overview of Some Popular RL Algorithms | 27 Deepak Pathak et al., \u201cCuriosity-Driven Exploration by Self-Supervised Prediction,\u201d Proceedings of the 34th\nInternational Conference on Machine Learning (2017): 2778\u20132787. Curiosity-based exploration27\nA recurring problem in RL is the sparsity of the rewards, which makes learning\nvery slow and inefficient. Deepak Pathak and other UC Berkeley researchers have\nproposed an exciting way to tackle this issue: why not ignore the rewards, and\njust make the agent extremely curious to explore the environment? The rewards\nthus become intrinsic to the agent, rather than coming from the environment. Similarly, stimulating curiosity in a child is more likely to give good results than\npurely rewarding the child for getting good grades. How does this work? The\nagent continuously tries to predict the outcome of its actions, and it seeks situa\u2010\ntions where the outcome does not match its predictions. In other words, it wants\nto be surprised. If the outcome is predictable (boring), it goes elsewhere. How\u2010\never, if the outcome is unpredictable but the agent notices that it has no control\nover it, it also gets bored after a while. With only curiosity, the authors succeeded\nin training an agent at many video games: even though the agent gets no penalty\nfor losing, the game starts over, which is boring so it learns to avoid it."
  },
  {
    "id": 445,
    "content": "We covered many topics in this chapter: Policy Gradients, Markov chains, Markov\ndecision processes, Q-Learning, Approximate Q-Learning, and Deep Q-Learning and\nits main variants (fixed Q-Value targets, Double DQN, Dueling DQN, and prioritized\nexperience replay). We discussed how to use TF-Agents to train agents at scale, and\nfinally we took a quick look at a few other popular algorithms. Reinforcement Learn\u2010\ning is a huge and exciting field, with new ideas and algorithms popping out every day,\nso I hope this chapter sparked your curiosity: there is a whole world to explore! Exercises\n1. How would you define Reinforcement Learning? How is it different from regular\nsupervised or unsupervised learning? 2. Can you think of three possible applications of RL that were not mentioned in\nthis chapter? For each of them, what is the environment? What is the agent? What are some possible actions? What are the rewards? 3. What is the discount factor? Can the optimal policy change if you modify the dis\u2010\ncount factor? 4. How do you measure the performance of a Reinforcement Learning agent? 5. What is the credit assignment problem? When does it occur? How can you allevi\u2010\nate it? 6. What is the point of using a replay buffer? | Chapter 18: Reinforcement Learning\n7. What is an off-policy RL algorithm? 8. Use policy gradients to solve OpenAI Gym\u2019s LunarLander-v2 environment. You\nwill need to install the Box2D dependencies (python3 -m pip install -U\ngym[box2d]). 9. Use TF-Agents to train an agent that can achieve a superhuman level at\nSpaceInvaders-v4 using any of the available algorithms. 10. If you have about $100 to spare, you can purchase a Raspberry Pi 3 plus some\ncheap robotics components, install TensorFlow on the Pi, and go wild! For an\nexample, check out this fun post by Lukas Biewald, or take a look at GoPiGo or\nBrickPi. Start with simple goals, like making the robot turn around to find the\nbrightest angle (if it has a light sensor) or the closest object (if it has a sonar sen\u2010\nsor), and move in that direction. Then you can start using Deep Learning: for\nexample, if the robot has a camera, you can try to implement an object detection\nalgorithm so it detects people and moves toward them. You can also try to use RL\nto make the agent learn on its own how to use the motors to achieve that goal. Have fun! Solutions to these exercises are available in Appendix A. Exercises | 1 An A/B experiment consists in testing two different versions of your product on different subsets of users in\norder to check which version works best and get other insights. CHAPTER 19\nTraining and Deploying TensorFlow\nModels at Scale\nOnce you have a beautiful model that makes amazing predictions, what do you do\nwith it? Well, you need to put it in production!"
  },
  {
    "id": 446,
    "content": "This could be as simple as running the\nmodel on a batch of data and perhaps writing a script that runs this model every\nnight. However, it is often much more involved. Various parts of your infrastructure\nmay need to use this model on live data, in which case you probably want to wrap\nyour model in a web service: this way, any part of your infrastructure can query your\nmodel at any time using a simple REST API (or some other protocol), as we discussed\nin Chapter 2. But as time passes, you need to regularly retrain your model on fresh\ndata and push the updated version to production. You must handle model versioning,\ngracefully transition from one model to the next, possibly roll back to the previous\nmodel in case of problems, and perhaps run multiple different models in parallel to\nperform A/B experiments.1 If your product becomes successful, your service may start\nto get plenty of queries per second (QPS), and it must scale up to support the load. A\ngreat solution to scale up your service, as we will see in this chapter, is to use TF Serv\u2010\ning, either on your own hardware infrastructure or via a cloud service such as Google\nCloud AI Platform. It will take care of efficiently serving your model, handle graceful\nmodel transitions, and more. If you use the cloud platform, you will also get many\nextra features, such as powerful monitoring tools. Moreover, if you have a lot of training data, and compute-intensive models, then\ntraining time may be prohibitively long. If your product needs to adapt to changes\nquickly, then a long training time can be a showstopper (e.g., think of a news 2 A REST (or RESTful) API is an API that uses standard HTTP verbs, such as GET, POST, PUT, and DELETE,\nand uses JSON inputs and outputs. The gRPC protocol is more complex but more efficient. Data is exchanged\nusing protocol buffers (see Chapter 13). recommendation system promoting news from last week). Perhaps even more impor\u2010\ntantly, a long training time will prevent you from experimenting with new ideas. In\nMachine Learning (as in many other fields), it is hard to know in advance which ideas\nwill work, so you should try out as many as possible, as fast as possible. One way to\nspeed up training is to use hardware accelerators such as GPUs or TPUs. To go even\nfaster, you can train a model across multiple machines, each equipped with multiple\nhardware accelerators. TensorFlow\u2019s simple yet powerful Distribution Strategies API\nmakes this easy, as we will see. In this chapter we will look at how to deploy models, first to TF Serving, then to Goo\u2010\ngle Cloud AI Platform. We will also take a quick look at deploying models to mobile\napps, embedded devices, and web apps."
  },
  {
    "id": 447,
    "content": "Lastly, we will discuss how to speed up com\u2010\nputations using GPUs and how to train models across multiple devices and servers\nusing the Distribution Strategies API. That\u2019s a lot of topics to discuss, so let\u2019s get\nstarted! Serving a TensorFlow Model\nOnce you have trained a TensorFlow model, you can easily use it in any Python code:\nif it\u2019s a tf.keras model, just call its predict() method! But as your infrastructure\ngrows, there comes a point where it is preferable to wrap your model in a small ser\u2010\nvice whose sole role is to make predictions and have the rest of the infrastructure\nquery it (e.g., via a REST or gRPC API).2 This decouples your model from the rest of\nthe infrastructure, making it possible to easily switch model versions or scale the ser\u2010\nvice up as needed (independently from the rest of your infrastructure), perform A/B\nexperiments, and ensure that all your software components rely on the same model\nversions. It also simplifies testing and development, and more. You could create your\nown microservice using any technology you want (e.g., using the Flask library), but\nwhy reinvent the wheel when you can just use TF Serving? Using TensorFlow Serving\nTF Serving is a very efficient, battle-tested model server that\u2019s written in C++. It can\nsustain a high load, serve multiple versions of your models and watch a model reposi\u2010\ntory to automatically deploy the latest versions, and more (see Figure 19-1). | Chapter 19: Training and Deploying TensorFlow Models at Scale\nFigure 19-1. TF Serving can serve multiple models and automatically deploy the latest\nversion of each model\nSo let\u2019s suppose you have trained an MNIST model using tf.keras, and you want to\ndeploy it to TF Serving. The first thing you have to do is export this model to Tensor\u2010\nFlow\u2019s SavedModel format. Exporting SavedModels\nTensorFlow provides a simple tf.saved_model.save() function to export models to\nthe SavedModel format. All you need to do is give it the model, specifying its name\nand version number, and the function will save the model\u2019s computation graph and its\nweights:\nmodel = keras.models.Sequential([...])\nmodel.compile([...])\nhistory = model.fit([...])\nmodel_version = \"0001\"\nmodel_name = \"my_mnist_model\"\nmodel_path = os.path.join(model_name, model_version)\ntf.saved_model.save(model, model_path)\nAlternatively, you can just use the model\u2019s save() method (model.save(model_\npath)): as long as the file\u2019s extension is not .h5, the model will be saved using the\nSavedModel format instead of the HDF5 format. It\u2019s usually a good idea to include all the preprocessing layers in the final model you\nexport so that it can ingest data in its natural form once it is deployed to production. This avoids having to take care of preprocessing separately within the application that\nuses the model. Bundling the preprocessing steps within the model also makes it sim\u2010\npler to update them later on and limits the risk of mismatch between a model and the\npreprocessing steps it requires."
  },
  {
    "id": 448,
    "content": "Serving a TensorFlow Model | Since a SavedModel saves the computation graph, it can only be\nused with models that are based exclusively on TensorFlow opera\u2010\ntions, excluding the tf.py_function() operation (which wraps\narbitrary Python code). It also excludes dynamic tf.keras models\n(see Appendix G), since these models cannot be converted to com\u2010\nputation graphs. Dynamic models need to be served using other\ntools (e.g., Flask). A SavedModel represents a version of your model. It is stored as a directory contain\u2010\ning a saved_model.pb file, which defines the computation graph (represented as a seri\u2010\nalized protocol buffer), and a variables subdirectory containing the variable values. For models containing a large number of weights, these variable values may be split\nacross multiple files. A SavedModel also includes an assets subdirectory that may con\u2010\ntain additional data, such as vocabulary files, class names, or some example instances\nfor this model. The directory structure is as follows (in this example, we don\u2019t use\nassets):\nmy_mnist_model\n\u2514\u2500\u2500 0001 \u251c\u2500\u2500 assets \u251c\u2500\u2500 saved_model.pb \u2514\u2500\u2500 variables \u251c\u2500\u2500 variables.data-00000-of-00001 \u2514\u2500\u2500 variables.index\nAs you might expect, you can load a SavedModel using the tf.saved_model.load()\nfunction. However, the returned object is not a Keras model: it represents the Saved\u2010\nModel, including its computation graph and variable values. You can use it like a\nfunction, and it will make predictions (make sure to pass the inputs as tensors of the\nappropriate type):\nsaved_model = tf.saved_model.load(model_path)\ny_pred = saved_model(tf.constant(X_new, dtype=tf.float32))\nAlternatively, you can load this SavedModel directly to a Keras model using the\nkeras.models.load_model() function:\nmodel = keras.models.load_model(model_path)\ny_pred = model.predict(tf.constant(X_new, dtype=tf.float32))\nTensorFlow also comes with a small saved_model_cli command-line tool to inspect\nSavedModels:\n$ export ML_PATH=\"$HOME/ml\" # point to this project, wherever it is\n$ cd $ML_PATH\n$ saved_model_cli show --dir my_mnist_model/0001 --all\nMetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\nsignature_def['__saved_model_init_op']: [...] | Chapter 19: Training and Deploying TensorFlow Models at Scale\nsignature_def['serving_default']: The given SavedModel SignatureDef contains the following input(s): inputs['flatten_input'] tensor_info: dtype: DT_FLOAT shape: (-1, 28, 28) name: serving_default_flatten_input:0 The given SavedModel SignatureDef contains the following output(s): outputs['dense_1'] tensor_info: dtype: DT_FLOAT shape: (-1, 10) name: StatefulPartitionedCall:0 Method name is: tensorflow/serving/predict\nA SavedModel contains one or more metagraphs. A metagraph is a computation\ngraph plus some function signature definitions (including their input and output\nnames, types, and shapes). Each metagraph is identified by a set of tags. For example,\nyou may want to have a metagraph containing the full computation graph, including\nthe training operations (this one may be tagged \"train\", for example), and another\nmetagraph containing a pruned computation graph with only the prediction opera\u2010\ntions, including some GPU-specific operations (this metagraph may be tagged\n\"serve\", \"gpu\"). However, when you pass a tf.keras model to the\ntf.saved_model.save() function, by default the function saves a much simpler\nSavedModel: it saves a single metagraph tagged \"serve\", which contains two signa\u2010\nture definitions, an initialization function (called __saved_model_init_op, which\nyou do not need to worry about) and a default serving function (called serv\ning_default)."
  },
  {
    "id": 449,
    "content": "When saving a tf.keras model, the default serving function corre\u2010\nsponds to the model\u2019s call() function, which of course makes predictions. The saved_model_cli tool can also be used to make predictions (for testing, not\nreally for production). Suppose you have a NumPy array (X_new) containing three\nimages of handwritten digits that you want to make predictions for. You first need to\nexport them to NumPy\u2019s npy format:\nnp.save(\"my_mnist_tests.npy\", X_new)\nNext, use the saved_model_cli command like this:\n$ saved_model_cli run --dir my_mnist_model/0001 --tag_set serve \\ --signature_def serving_default \\ --inputs flatten_input=my_mnist_tests.npy\n[...] Result for output key dense_1:\n[[1.1739199e-04 1.1239604e-07 6.0210604e-04 [...] 3.9471846e-04] [1.2294615e-03 2.9207937e-05 9.8599273e-01 [...] 1.1113169e-07] [6.4066830e-05 9.6359509e-01 9.0598064e-03 [...] 4.2495009e-04]]\nThe tool\u2019s output contains the 10 class probabilities of each of the 3 instances. Great! Now that you have a working SavedModel, the next step is to install TF Serving. Serving a TensorFlow Model | 3 If you are not familiar with Docker, it allows you to easily download a set of applications packaged in a Docker\nimage (including all their dependencies and usually some good default configuration) and then run them on\nyour system using a Docker engine. When you run an image, the engine creates a Docker container that keeps\nthe applications well isolated from your own system (but you can give it some limited access if you want). It is\nsimilar to a virtual machine, but much faster and more lightweight, as the container relies directly on the\nhost\u2019s kernel. This means that the image does not need to include or run its own kernel. Installing TensorFlow Serving\nThere are many ways to install TF Serving: using a Docker image,3 using the system\u2019s\npackage manager, installing from source, and more. Let\u2019s use the Docker option,\nwhich is highly recommended by the TensorFlow team as it is simple to install, it will\nnot mess with your system, and it offers high performance. You first need to install\nDocker. Then download the official TF Serving Docker image:\n$ docker pull tensorflow/serving\nNow you can create a Docker container to run this image:\n$ docker run -it --rm -p 8500:8500 -p 8501:8501 \\ -v \"$ML_PATH/my_mnist_model:/models/my_mnist_model\" \\ -e MODEL_NAME=my_mnist_model \\ tensorflow/serving\n[...]\n2019-06-01 [...] loaded servable version {name: my_mnist_model version: 1}\n2019-06-01 [...] Running gRPC ModelServer at 0.0.0.0:8500 ...\n2019-06-01 [...] Exporting HTTP/REST API at:localhost:8501 ...\n[ev : 237] RAW: Entering the event loop ...\nThat\u2019s it! TF Serving is running. It loaded our MNIST model (version 1), and it is\nserving it through both gRPC (on port 8500) and REST (on port 8501). Here is what\nall the command-line options mean:\n-it\nMakes the container interactive (so you can press Ctrl-C to stop it) and displays\nthe server\u2019s output. --rm\nDeletes the container when you stop it (no need to clutter your machine with\ninterrupted containers). However, it does not delete the image. -p 8500:8500\nMakes the Docker engine forward the host\u2019s TCP port 8500 to the container\u2019s\nTCP port 8500."
  },
  {
    "id": 450,
    "content": "By default, TF Serving uses this port to serve the gRPC API. -p 8501:8501\nForwards the host\u2019s TCP port 8501 to the container\u2019s TCP port 8501. By default,\nTF Serving uses this port to serve the REST API. | Chapter 19: Training and Deploying TensorFlow Models at Scale\n-v \"$ML_PATH/my_mnist_model:/models/my_mnist_model\"\nMakes the host\u2019s $ML_PATH/my_mnist_model directory available to the container\nat the path /models/mnist_model. On Windows, you may need to replace / with \\\nin the host path (but not in the container path). -e MODEL_NAME=my_mnist_model\nSets the container\u2019s MODEL_NAME environment variable, so TF Serving knows\nwhich model to serve. By default, it will look for models in the /models directory,\nand it will automatically serve the latest version it finds. tensorflow/serving\nThis is the name of the image to run. Now let\u2019s go back to Python and query this server, first using the REST API, then the\ngRPC API. Querying TF Serving through the REST API\nLet\u2019s start by creating the query. It must contain the name of the function signature\nyou want to call, and of course the input data:\nimport json\ninput_data_json = json.dumps({ \"signature_name\": \"serving_default\", \"instances\": X_new.tolist(),\n})\nNote that the JSON format is 100% text-based, so the X_new NumPy array had to be\nconverted to a Python list and then formatted as JSON:\n>>> input_data_json\n'{\"signature_name\": \"serving_default\", \"instances\": [[[0.0, 0.0, 0.0, [...]\n0.3294117647058824, 0.725490196078431, [...very long], 0.0, 0.0, 0.0, 0.0]]]}'\nNow let\u2019s send the input data to TF Serving by sending an HTTP POST request. This\ncan be done easily using the requests library (it is not part of Python\u2019s standard\nlibrary, so you will need to install it first, e.g., using pip):\nimport requests\nSERVER_URL = '\nresponse = requests.post(SERVER_URL, data=input_data_json)\nresponse.raise_for_status() # raise an exception in case of error\nresponse = response.json()\nThe response is a dictionary containing a single \"predictions\" key. The correspond\u2010\ning value is the list of predictions. This list is a Python list, so let\u2019s convert it to a\nNumPy array and round the floats it contains to the second decimal:\nServing a TensorFlow Model | 4 To be fair, this can be mitigated by serializing the data first and encoding it to Base64 before creating the REST\nrequest. Moreover, REST requests can be compressed using gzip, which reduces the payload size significantly. >>> y_proba = np.array(response[\"predictions\"])\n>>> y_proba.round(2)\narray([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. ], [0. , 0. , 0.99, 0.01, 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0.96, 0.01, 0. , 0. , 0. , 0. , 0.01, 0.01, 0. ]]) Hurray, we have the predictions! The model is close to 100% confident that the first\nimage is a 7, 99% confident that the second image is a 2, and 96% confident that the\nthird image is a 1."
  },
  {
    "id": 451,
    "content": "The REST API is nice and simple, and it works well when the input and output data\nare not too large. Moreover, just about any client application can make REST queries\nwithout additional dependencies, whereas other protocols are not always so readily\navailable. However, it is based on JSON, which is text-based and fairly verbose. For\nexample, we had to convert the NumPy array to a Python list, and every float ended\nup represented as a string. This is very inefficient, both in terms of serialization/\ndeserialization time (to convert all the floats to strings and back) and in terms of pay\u2010\nload size: many floats end up being represented using over 15 characters, which\ntranslates to over 120 bits for 32-bit floats! This will result in high latency and band\u2010\nwidth usage when transferring large NumPy arrays.4 So let\u2019s use gRPC instead. When transferring large amounts of data, it is much better to use\nthe gRPC API (if the client supports it), as it is based on a compact\nbinary format and an efficient communication protocol (based on\nHTTP/2 framing). Querying TF Serving through the gRPC API\nThe gRPC API expects a serialized PredictRequest protocol buffer as input, and it\noutputs a serialized PredictResponse protocol buffer. These protobufs are part of the\ntensorflow-serving-api library, which you must install (e.g., using pip). First, let\u2019s\ncreate the request:\nfrom tensorflow_serving.apis.predict_pb2 import PredictRequest\nrequest = PredictRequest()\nrequest.model_spec.name = model_name\nrequest.model_spec.signature_name = \"serving_default\"\ninput_name = model.input_names[0]\nrequest.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new))\nThis code creates a PredictRequest protocol buffer and fills in the required fields,\nincluding the model name (defined earlier), the signature name of the function we | Chapter 19: Training and Deploying TensorFlow Models at Scale\nwant to call, and finally the input data, in the form of a Tensor protocol buffer. The\ntf.make_tensor_proto() function creates a Tensor protocol buffer based on the\ngiven tensor or NumPy array, in this case X_new. Next, we\u2019ll send the request to the server and get its response (for this you will need\nthe grpcio library, which you can install using pip):\nimport grpc\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\nchannel = grpc.insecure_channel('localhost:8500')\npredict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\nresponse = predict_service.Predict(request, timeout=10.0)\nThe code is quite straightforward: after the imports, we create a gRPC communica\u2010\ntion channel to localhost on TCP port 8500, then we create a gRPC service over this\nchannel and use it to send a request, with a 10-second timeout (not that the call is\nsynchronous: it will block until it receives the response or the timeout period\nexpires). In this example the channel is insecure (no encryption, no authentication),\nbut gRPC and TensorFlow Serving also support secure channels over SSL/TLS. Next, let\u2019s convert the PredictResponse protocol buffer to a tensor:\noutput_name = model.output_names[0]\noutputs_proto = response.outputs[output_name]\ny_proba = tf.make_ndarray(outputs_proto)\nIf you run this code and print y_proba.numpy().round(2), you will get the exact\nsame estimated class probabilities as earlier."
  },
  {
    "id": 452,
    "content": "And that\u2019s all there is to it: in just a few\nlines of code, you can now access your TensorFlow model remotely, using either\nREST or gRPC. Deploying a new model version\nNow let\u2019s create a new model version and export a SavedModel to the\nmy_mnist_model/0002 directory, just like earlier:\nmodel = keras.models.Sequential([...])\nmodel.compile([...])\nhistory = model.fit([...])\nmodel_version = \"0002\"\nmodel_name = \"my_mnist_model\"\nmodel_path = os.path.join(model_name, model_version)\ntf.saved_model.save(model, model_path)\nAt regular intervals (the delay is configurable), TensorFlow Serving checks for new\nmodel versions. If it finds one, it will automatically handle the transition gracefully:\nby default, it will answer pending requests (if any) with the previous model version,\nServing a TensorFlow Model | 5 If the SavedModel contains some example instances in the assets/extra directory, you can configure TF Serv\u2010\ning to execute the model on these instances before starting to serve new requests with it. This is called model\nwarmup: it will ensure that everything is properly loaded, avoiding long response times for the first requests. while handling new requests with the new version.5 As soon as every pending request\nhas been answered, the previous model version is unloaded. You can see this at work\nin the TensorFlow Serving logs:\n[...]\nreserved resources to load servable {name: my_mnist_model version: 2}\n[...]\nReading SavedModel from: /models/my_mnist_model/0002\nReading meta graph with tags { serve }\nSuccessfully loaded servable version {name: my_mnist_model version: 2}\nQuiescing servable version {name: my_mnist_model version: 1}\nDone quiescing servable version {name: my_mnist_model version: 1}\nUnloading servable version {name: my_mnist_model version: 1}\nThis approach offers a smooth transition, but it may use too much RAM (especially\nGPU RAM, which is generally the most limited). In this case, you can configure TF\nServing so that it handles all pending requests with the previous model version and\nunloads it before loading and using the new model version. This configuration will\navoid having two model versions loaded at the same time, but the service will be\nunavailable for a short period. As you can see, TF Serving makes it quite simple to deploy new models. Moreover, if\nyou discover that version 2 does not work as well as you expected, then rolling back\nto version 1 is as simple as removing the my_mnist_model/0002 directory. Another great feature of TF Serving is its automatic batching capa\u2010\nbility, which you can activate using the --enable_batching option\nupon startup. When TF Serving receives multiple requests within a\nshort period of time (the delay is configurable), it will automatically\nbatch them together before using the model. This offers a signifi\u2010\ncant performance boost by leveraging the power of the GPU. Once\nthe model returns the predictions, TF Serving dispatches each pre\u2010\ndiction to the right client. You can trade a bit of latency for a\ngreater throughput by increasing the batching delay (see the\n--batching_parameters_file option). If you expect to get many queries per second, you will want to deploy TF Serving on\nmultiple servers and load-balance the queries (see Figure 19-2)."
  },
  {
    "id": 453,
    "content": "This will require\ndeploying and managing many TF Serving containers across these servers. One way\nto handle that is to use a tool such as Kubernetes, which is an open source system for\nsimplifying container orchestration across many servers. If you do not want to pur\u2010 | Chapter 19: Training and Deploying TensorFlow Models at Scale\nchase, maintain, and upgrade all the hardware infrastructure, you will want to use\nvirtual machines on a cloud platform such as Amazon AWS, Microsoft Azure, Google\nCloud Platform, IBM Cloud, Alibaba Cloud, Oracle Cloud, or some other Platform-\nas-a-Service (PaaS). Managing all the virtual machines, handling container orchestra\u2010\ntion (even with the help of Kubernetes), taking care of TF Serving configuration,\ntuning and monitoring\u2014all of this can be a full-time job. Fortunately, some service\nproviders can take care of all this for you. In this chapter we will use Google Cloud AI\nPlatform because it\u2019s the only platform with TPUs today, it supports TensorFlow 2, it\noffers a nice suite of AI services (e.g., AutoML, Vision API, Natural Language API),\nand it is the one I have the most experience with. But there are several other provid\u2010\ners in this space, such as Amazon AWS SageMaker and Microsoft AI Platform, which\nare also capable of serving TensorFlow models. Figure 19-2. Scaling up TF Serving with load balancing\nNow let\u2019s see how to serve our wonderful MNIST model on the cloud! Creating a Prediction Service on GCP AI Platform\nBefore you can deploy a model, there\u2019s a little bit of setup to take care of:\n1. Log in to your Google account, and then go to the Google Cloud Platform (GCP)\nconsole (see Figure 19-3). If you don\u2019t have a Google account, you\u2019ll have to cre\u2010\nate one. 2. If it is your first time using GCP, you will have to read and accept the terms and\nconditions. Click Tour Console if you want. At the time of this writing, new users\nare offered a free trial, including $300 worth of GCP credit that you can use over\nthe course of 12 months. You will only need a small portion of that to pay for the\nservices you will use in this chapter. Upon signing up for the free trial, you will\nstill need to create a payment profile and enter your credit card number: it is used\nfor verification purposes (probably to avoid people using the free trial multiple\ntimes), but you will not be billed. Activate and upgrade your account if requested. Serving a TensorFlow Model | Figure 19-3. Google Cloud Platform console\n3. If you have used GCP before and your free trial has expired, then the services you\nwill use in this chapter will cost you some money. It should not be too much,\nespecially if you remember to turn off the services when you do not need them\nanymore. Make sure you understand and agree to the pricing conditions before\nyou run any service."
  },
  {
    "id": 454,
    "content": "I hereby decline any responsibility if services end up costing\nmore than you expected! Also make sure your billing account is active. To check,\nopen the navigation menu on the left and click Billing, and make sure you have\nset up a payment method and that the billing account is active. 4. Every resource in GCP belongs to a project. This includes all the virtual\nmachines you may use, the files you store, and the training jobs you run. When\nyou create an account, GCP automatically creates a project for you, called \u201cMy\nFirst Project.\u201d If you want, you can change its display name by going to the\nproject settings: in the navigation menu (on the left of the screen), select IAM &\nadmin \u2192 Settings, change the project\u2019s display name, and click Save. Note that\nthe project also has a unique ID and number. You can choose the project ID\nwhen you create a project, but you cannot change it later. The project number is\nautomatically generated and cannot be changed. If you want to create a new\nproject, click the project name at the top of the page, then click New Project and\nenter the project ID. Make sure billing is active for this new project. | Chapter 19: Training and Deploying TensorFlow Models at Scale\nAlways set an alarm to remind yourself to turn services off\nwhen you know you will only need them for a few hours, or\nelse you might leave them running for days or months, incur\u2010\nring potentially significant costs. 5. Now that you have a GCP account with billing activated, you can start using the\nservices. The first one you will need is Google Cloud Storage (GCS): this is where\nyou will put the SavedModels, the training data, and more. In the navigation\nmenu, scroll down to the Storage section, and click Storage \u2192 Browser. All your\nfiles will go in one or more buckets. Click Create Bucket and choose the bucket\nname (you may need to activate the Storage API first). GCS uses a single world\u2010\nwide namespace for buckets, so simple names like \u201cmachine-learning\u201d will most\nlikely not be available. Make sure the bucket name conforms to DNS naming\nconventions, as it may be used in DNS records. Moreover, bucket names are pub\u2010\nlic, so do not put anything private in there. It is common to use your domain\nname or your company name as a prefix to ensure uniqueness, or simply use a\nrandom number as part of the name. Choose the location where you want the\nbucket to be hosted, and the rest of the options should be fine by default. Then\nclick Create. 6. Upload the my_mnist_model folder you created earlier (including one or more\nversions) to your bucket. To do this, just go to the GCS Browser, click the bucket,\nthen drag and drop the my_mnist_model folder from your system to the bucket\n(see Figure 19-4)."
  },
  {
    "id": 455,
    "content": "Alternatively, you can click \u201cUpload folder\u201d and select the\nmy_mnist_model folder to upload. By default, the maximum size for a SavedMo\u2010\ndel is 250 MB, but it is possible to request a higher quota. Figure 19-4. Uploading a SavedModel to Google Cloud Storage\nServing a TensorFlow Model | 6 At the time of this writing, TensorFlow version 2 is not available yet on AI Platform, but that\u2019s OK: you can\nuse 1.13, and it will run your TF 2 SavedModels just fine. 7. Now you need to configure AI Platform (formerly known as ML Engine) so that\nit knows which models and versions you want to use. In the navigation menu,\nscroll down to the Artificial Intelligence section, and click AI Platform \u2192 Models. Click Activate API (it takes a few minutes), then click \u201cCreate model.\u201d Fill in the\nmodel details (see Figure 19-5) and click Create. Figure 19-5. Creating a new model on Google Cloud AI Platform\n8. Now that you have a model on AI Platform, you need to create a model version. In the list of models, click the model you just created, then click \u201cCreate version\u201d\nand fill in the version details (see Figure 19-6): set the name, description, Python\nversion (3.5 or above), framework (TensorFlow), framework version (2.0 if avail\u2010\nable, or 1.13),6 ML runtime version (2.0, if available or 1.13), machine type\n(choose \u201cSingle core CPU\u201d for now), model path on GCS (this is the full path to\nthe actual version folder, e.g., gs://my-mnist-model-bucket/my_mnist_model/\n0002/), scaling (choose automatic), and minimum number of TF Serving con\u2010\ntainers to have running at all times (leave this field empty). Then click Save. | Chapter 19: Training and Deploying TensorFlow Models at Scale\nFigure 19-6. Creating a new model version on Google Cloud AI Platform\nCongratulations, you have deployed your first model on the cloud! Because you\nselected automatic scaling, AI Platform will start more TF Serving containers when\nthe number of queries per second increases, and it will load-balance the queries\nbetween them. If the QPS goes down, it will stop containers automatically. The cost is\ntherefore directly linked to the QPS (as well as the type of machine you choose and\nthe amount of data you store on GCS). This pricing model is particularly useful for\noccasional users and for services with important usage spikes, as well as for startups:\nthe price remains low until the startup actually starts up. If you do not use the prediction service, AI Platform will stop all\ncontainers. This means you will only pay for the amount of storage\nyou use (a few cents per gigabyte per month). Note that when you\nquery the service, AI Platform will need to start up a TF Serving\ncontainer, which will take a few seconds. If this delay is unaccepta\u2010\nble, you will have to set the minimum number of TF Serving con\u2010\ntainers to 1 when creating the model version."
  },
  {
    "id": 456,
    "content": "Of course, this means\nat least one machine will run constantly, so the monthly fee will be\nhigher. Now let\u2019s query this prediction service! Serving a TensorFlow Model | Using the Prediction Service\nUnder the hood, AI Platform just runs TF Serving, so in principle you could use the\nsame code as earlier, if you knew which URL to query. There\u2019s just one problem: GCP\nalso takes care of encryption and authentication. Encryption is based on SSL/TLS,\nand authentication is token-based: a secret authentication token must be sent to the\nserver in every request. So before your code can use the prediction service (or any\nother GCP service), it must obtain a token. We will see how to do this shortly, but\nfirst you need to configure authentication and give your application the appropriate\naccess rights on GCP. You have two options for authentication:\n\u2022 Your application (i.e., the client code that will query the prediction service) could\nauthenticate using user credentials with your own Google login and password. Using user credentials would give your application the exact same rights as on\nGCP, which is certainly way more than it needs. Moreover, you would have to\ndeploy your credentials in your application, so anyone with access could steal\nyour credentials and fully access your GCP account. In short, do not choose this\noption; it is only needed in very rare cases (e.g., when your application needs to\naccess its user\u2019s GCP account). \u2022 The client code can authenticate with a service account. This is an account that\nrepresents an application, not a user. It is generally given very restricted access\nrights: strictly what it needs, and no more. This is the recommended option. So, let\u2019s create a service account for your application: in the navigation menu, go to\nIAM & admin \u2192 Service accounts, then click Create Service Account, fill in the form\n(service account name, ID, description), and click Create (see Figure 19-7). Next, you\nmust give this account some access rights. Select the ML Engine Developer role: this\nwill allow the service account to make predictions, and not much more. Optionally,\nyou can grant some users access to the service account (this is useful when your GCP\nuser account is part of an organization, and you wish to authorize other users in the\norganization to deploy applications that will be based on this service account or to\nmanage the service account itself). Next, click Create Key to export the service\naccount\u2019s private key, choose JSON, and click Create. This will download the private\nkey in the form of a JSON file. Make sure to keep it private! | Chapter 19: Training and Deploying TensorFlow Models at Scale\nFigure 19-7. Creating a new service account in Google IAM\nGreat! Now let\u2019s write a small script that will query the prediction service."
  },
  {
    "id": 457,
    "content": "Google\nprovides several libraries to simplify access to its services:\nGoogle API Client Library\nThis is a fairly thin layer on top of OAuth 2.0 (for the authentication) and REST. You can use it with all GCP services, including AI Platform. You can install it\nusing pip: the library is called google-api-python-client. Google Cloud Client Libraries\nThese are a bit more high-level: each one is dedicated to a particular service, such\nas GCS, Google BigQuery, Google Cloud Natural Language, and Google Cloud\nVision. All these libraries can be installed using pip (e.g., the GCS Client Library\nis called google-cloud-storage). When a client library is available for a given\nservice, it is recommended to use it rather than the Google API Client Library, as\nit implements all the best practices and will often use gRPC rather than REST, for\nbetter performance. At the time of this writing there is no client library for AI Platform, so we will use the\nGoogle API Client Library. It will need to use the service account\u2019s private key; you\ncan tell it where it is by setting the GOOGLE_APPLICATION_CREDENTIALS environment\nvariable, either before starting the script or within the script like this:\nimport os\nos.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"my_service_account_key.json\"\nServing a TensorFlow Model | 7 If you get an error saying that module google.appengine was not found, set cache_discovery=False in the\ncall to the build() method; see \nIf you deploy your application to a virtual machine on Google\nCloud Engine (GCE), or within a container using Google Cloud\nKubernetes Engine, or as a web application on Google Cloud App\nEngine, or as a microservice on Google Cloud Functions, and if the\nGOOGLE_APPLICATION_CREDENTIALS environment variable is not\nset, then the library will use the default service account for the host\nservice (e.g., the default GCE service account, if your application\nruns on GCE). Next, you must create a resource object that wraps access to the prediction service:7\nimport googleapiclient.discovery\nproject_id = \"onyx-smoke-242003\" # change this to your project ID\nmodel_id = \"my_mnist_model\"\nmodel_path = \"projects/{}/models/{}\".format(project_id, model_id)\nml_resource = googleapiclient.discovery.build(\"ml\", \"v1\").projects()\nNote that you can append /versions/0001 (or any other version number) to the\nmodel_path to specify the version you want to query: this can be useful for A/B test\u2010\ning or for testing a new version on a small group of users before releasing it widely\n(this is called a canary). Next, let\u2019s write a small function that will use the resource\nobject to call the prediction service and get the predictions back:\ndef predict(X): input_data_json = {\"signature_name\": \"serving_default\", \"instances\": X.tolist()} request = ml_resource.predict(name=model_path, body=input_data_json) response = request.execute() if \"error\" in response: raise RuntimeError(response[\"error\"]) return np.array([pred[output_name] for pred in response[\"predictions\"]])\nThe function takes a NumPy array containing the input images and prepares a dictio\u2010\nnary that the client library will convert to the JSON format (as we did earlier)."
  },
  {
    "id": 458,
    "content": "Then it\nprepares a prediction request, and executes it; it raises an exception if the response\ncontains an error, or else it extracts the predictions for each instance and bundles\nthem in a NumPy array. Let\u2019s see if it works:\n>>> Y_probas = predict(X_new)\n>>> np.round(Y_probas, 2)\narray([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 1. , 0. , 0. ], [0. , 0. , 0.99, 0.01, 0. , 0. , 0. , 0. , 0. , 0. ], [0. , 0.96, 0.01, 0. , 0. , 0. , 0. , 0.01, 0.01, 0. ]]) | Chapter 19: Training and Deploying TensorFlow Models at Scale\n8 Also check out TensorFlow\u2019s Graph Transform Tools for modifying and optimizing computational graphs. Yes! You now have a nice prediction service running on the cloud that can automati\u2010\ncally scale up to any number of QPS, plus you can query it from anywhere securely. Moreover, it costs you close to nothing when you don\u2019t use it: you\u2019ll pay just a few\ncents per month per gigabyte used on GCS. And you can also get detailed logs and\nmetrics using Google Stackdriver. But what if you want to deploy your model to a mobile app? Or to an embedded\ndevice? Deploying a Model to a Mobile or Embedded Device\nIf you need to deploy your model to a mobile or embedded device, a large model may\nsimply take too long to download and use too much RAM and CPU, all of which will\nmake your app unresponsive, heat the device, and drain its battery. To avoid this, you\nneed to make a mobile-friendly, lightweight, and efficient model, without sacrificing\ntoo much of its accuracy. The TFLite library provides several tools8 to help you\ndeploy your models to mobile and embedded devices, with three main objectives:\n\u2022 Reduce the model size, to shorten download time and reduce RAM usage. \u2022 Reduce the amount of computations needed for each prediction, to reduce\nlatency, battery usage, and heating. \u2022 Adapt the model to device-specific constraints. To reduce the model size, TFLite\u2019s model converter can take a SavedModel and com\u2010\npress it to a much lighter format based on FlatBuffers. This is an efficient cross-\nplatform serialization library (a bit like protocol buffers) initially created by Google\nfor gaming. It is designed so you can load FlatBuffers straight to RAM without any\npreprocessing: this reduces the loading time and memory footprint. Once the model\nis loaded into a mobile or embedded device, the TFLite interpreter will execute it to\nmake predictions. Here is how you can convert a SavedModel to a FlatBuffer and save\nit to a .tflite file:\nconverter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\ntflite_model = converter.convert()\nwith open(\"converted_model.tflite\", \"wb\") as f: f.write(tflite_model)\nYou can also save a tf.keras model directly to a FlatBuffer using\nfrom_keras_model(). Deploying a Model to a Mobile or Embedded Device | The converter also optimizes the model, both to shrink it and to reduce its latency."
  },
  {
    "id": 459,
    "content": "It\nprunes all the operations that are not needed to make predictions (such as training\noperations), and it optimizes computations whenever possible; for example, 3\u00d7a +\n4\u00d7a + 5\u00d7a will be converted to (3 + 4 + 5)\u00d7a. It also tries to fuse operations whenever\npossible. For example, Batch Normalization layers end up folded into the previous\nlayer\u2019s addition and multiplication operations, whenever possible. To get a good idea\nof how much TFLite can optimize a model, download one of the pretrained TFLite\nmodels, unzip the archive, then open the excellent Netron graph visualization tool\nand upload the .pb file to view the original model. It\u2019s a big, complex graph, right? Next, open the optimized .tflite model and marvel at its beauty! Another way you can reduce the model size (other than simply using smaller neural\nnetwork architectures) is by using smaller bit-widths: for example, if you use half-\nfloats (16 bits) rather than regular floats (32 bits), the model size will shrink by a fac\u2010\ntor of 2, at the cost of a (generally small) accuracy drop. Moreover, training will be\nfaster, and you will use roughly half the amount of GPU RAM. TFLite\u2019s converter can go further than that, by quantizing the model weights down to\nfixed-point, 8-bit integers! This leads to a fourfold size reduction compared to using\n32-bit floats. The simplest approach is called post-training quantization: it just quanti\u2010\nzes the weights after training, using a fairly basic but efficient symmetrical quantiza\u2010\ntion technique. It finds the maximum absolute weight value, m, then it maps the\nfloating-point range \u2013m to +m to the fixed-point (integer) range \u2013127 to +127. For\nexample (see Figure 19-8), if the weights range from \u20131.5 to +0.8, then the bytes \u2013127,\n0, and +127 will correspond to the floats \u20131.5, 0.0, and +1.5, respectively. Note that\n0.0 always maps to 0 when using symmetrical quantization (also note that the byte\nvalues +68 to +127 will not be used, since they map to floats greater than +0.8). Figure 19-8. From 32-bit floats to 8-bit integers, using symmetrical quantization\nTo perform this post-training quantization, simply add OPTIMIZE_FOR_SIZE to the list\nof converter optimizations before calling the convert() method:\nconverter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\nThis technique dramatically reduces the model\u2019s size, so it\u2019s much faster to download\nand store. However, at runtime the quantized weights get converted back to floats\nbefore they are used (these recovered floats are not perfectly identical to the original | Chapter 19: Training and Deploying TensorFlow Models at Scale\nfloats, but not too far off, so the accuracy loss is usually acceptable). To avoid recom\u2010\nputing them all the time, the recovered floats are cached, so there is no reduction of\nRAM usage. And there is no reduction either in compute speed. The most effective way to reduce latency and power consumption is to also quantize\nthe activations so that the computations can be done entirely with integers, without\nthe need for any floating-point operations."
  },
  {
    "id": 460,
    "content": "Even when using the same bit-width (e.g.,\n32-bit integers instead of 32-bit floats), integer computations use less CPU cycles,\nconsume less energy, and produce less heat. And if you also reduce the bit-width (e.g.,\ndown to 8-bit integers), you can get huge speedups. Moreover, some neural network\naccelerator devices (such as the Edge TPU) can only process integers, so full quanti\u2010\nzation of both weights and activations is compulsory. This can be done post-training;\nit requires a calibration step to find the maximum absolute value of the activations, so\nyou need to provide a representative sample of training data to TFLite (it does not\nneed to be huge), and it will process the data through the model and measure the\nactivation statistics required for quantization (this step is typically fast). The main problem with quantization is that it loses a bit of accuracy: it is equivalent\nto adding noise to the weights and activations. If the accuracy drop is too severe, then\nyou may need to use quantization-aware training. This means adding fake quantiza\u2010\ntion operations to the model so it can learn to ignore the quantization noise during\ntraining; the final weights will then be more robust to quantization. Moreover, the\ncalibration step can be taken care of automatically during training, which simplifies\nthe whole process. I have explained the core concepts of TFLite, but going all the way to coding a mobile\napp or an embedded program would require a whole other book. Fortunately, one\nexists: if you want to learn more about building TensorFlow applications for mobile\nand embedded devices, check out the O\u2019Reilly book TinyML: Machine Learning with\nTensorFlow on Arduino and Ultra-Low Power Micro-Controllers, by Pete Warden (who\nleads the TFLite team) and Daniel Situnayake. Deploying a Model to a Mobile or Embedded Device | 9 If you\u2019re interested in this topic, check out federated learning. TensorFlow in the Browser\nWhat if you want to use your model in a website, running directly in the user\u2019s\nbrowser? This can be useful in many scenarios, such as:\n\u2022 When your web application is often used in situations where the user\u2019s connec\u2010\ntivity is intermittent or slow (e.g., a website for hikers), so running the model\ndirectly on the client side is the only way to make your website reliable. \u2022 When you need the model\u2019s responses to be as fast as possible (e.g., for an online\ngame). Removing the need to query the server to make predictions will definitely\nreduce the latency and make the website much more responsive. \u2022 When your web service makes predictions based on some private user data, and\nyou want to protect the user\u2019s privacy by making the predictions on the client\nside so that the private data never has to leave the user\u2019s machine.9\nFor all these scenarios, you can export your model to a special format that can be\nloaded by the TensorFlow.js JavaScript library."
  },
  {
    "id": 461,
    "content": "This library can then use your model\nto make predictions directly in the user\u2019s browser. The TensorFlow.js project includes\na tensorflowjs_converter tool that can convert a TensorFlow SavedModel or a\nKeras model file to the TensorFlow.js Layers format: this is a directory containing a set\nof sharded weight files in binary format and a model.json file that describes the mod\u2010\nel\u2019s architecture and links to the weight files. This format is optimized to be downloa\u2010\nded efficiently on the web. Users can then download the model and run predictions in\nthe browser using the TensorFlow.js library. Here is a code snippet to give you an idea\nof what the JavaScript API looks like:\nimport * as tf from '@tensorflow/tfjs';\nconst model = await tf.loadLayersModel('\nconst image = tf.fromPixels(webcamElement);\nconst prediction = model.predict(image);\nOnce again, doing justice to this topic would require a whole book. If you want to\nlearn more about TensorFlow.js, check out the O\u2019Reilly book Practical Deep Learning\nfor Cloud, Mobile, and Edge, by Anirudh Koul, Siddha Ganju, and Meher Kasam. Next, we will see how to use GPUs to speed up computations! | Chapter 19: Training and Deploying TensorFlow Models at Scale\nUsing GPUs to Speed Up Computations\nIn Chapter 11 we discussed several techniques that can considerably speed up train\u2010\ning: better weight initialization, Batch Normalization, sophisticated optimizers, and\nso on. But even with all of these techniques, training a large neural network on a sin\u2010\ngle machine with a single CPU can take days or even weeks. In this section we will look at how to speed up your models by using GPUs. We will\nalso see how to split the computations across multiple devices, including the CPU\nand multiple GPU devices (see Figure 19-9). For now we will run everything on a sin\u2010\ngle machine, but later in this chapter we will discuss how to distribute computations\nacross multiple servers. Figure 19-9. Executing a TensorFlow graph across multiple devices in parallel\nThanks to GPUs, instead of waiting for days or weeks for a training algorithm to\ncomplete, you may end up waiting for just a few minutes or hours. Not only does this\nsave an enormous amount of time, but it also means that you can experiment with\nvarious models much more easily and frequently retrain your models on fresh data. You can often get a major performance boost simply by adding\nGPU cards to a single machine. In fact, in many cases this will suf\u2010\nfice; you won\u2019t need to use multiple machines at all. For example,\nyou can typically train a neural network just as fast using four\nGPUs on a single machine rather than eight GPUs across multiple\nmachines, due to the extra delay imposed by network communica\u2010\ntions in a distributed setup. Similarly, using a single powerful GPU\nis often preferable to using multiple slower GPUs. Using GPUs to Speed Up Computations | 10 Please check the docs for detailed and up-to-date installation instructions, as they change quite often."
  },
  {
    "id": 462,
    "content": "The first step is to get your hands on a GPU. There are two options for this: you can\neither purchase your own GPU(s), or you can use GPU-equipped virtual machines\non the cloud. Let\u2019s start with the first option. Getting Your Own GPU\nIf you choose to purchase a GPU card, then take some time to make the right choice. Tim Dettmers wrote an excellent blog post to help you choose, and he updates it reg\u2010\nularly: I encourage you to read it carefully. At the time of this writing, TensorFlow\nonly supports Nvidia cards with CUDA Compute Capability 3.5+ (as well as Google\u2019s\nTPUs, of course), but it may extend its support to other manufacturers. Moreover,\nalthough TPUs are currently only available on GCP, it is highly likely that TPU-like\ncards will be available for sale in the near future, and TensorFlow may support them. In short, make sure to check TensorFlow\u2019s documentation to see what devices are\nsupported at this point. If you go for an Nvidia GPU card, you will need to install the appropriate Nvidia\ndrivers and several Nvidia libraries.10 These include the Compute Unified Device\nArchitecture library (CUDA), which allows developers to use CUDA-enabled GPUs\nfor all sorts of computations (not just graphics acceleration), and the CUDA Deep\nNeural Network library (cuDNN), a GPU-accelerated library of primitives for DNNs. cuDNN provides optimized implementations of common DNN computations such\nas activation layers, normalization, forward and backward convolutions, and pooling\n(see Chapter 14). It is part of Nvidia\u2019s Deep Learning SDK (note that you\u2019ll need to\ncreate an Nvidia developer account in order to download it). TensorFlow uses CUDA\nand cuDNN to control the GPU cards and accelerate computations (see\nFigure 19-10). Figure 19-10. TensorFlow uses CUDA and cuDNN to control GPUs and boost DNNs | Chapter 19: Training and Deploying TensorFlow Models at Scale\nOnce you have installed the GPU card(s) and all the required drivers and libraries,\nyou can use the nvidia-smi command to check that CUDA is properly installed. It\nlists the available GPU cards, as well as processes running on each card:\n$ nvidia-smi\nSun Jun 2 10:05:22 2019\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 418.67 Driver Version: 410.79 CUDA Version: 10.0 |\n|-------------------------------+----------------------+----------------------+\n| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC |\n| Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. |\n|===============================+======================+======================|\n| 0 Tesla T4 Off | 00000000:00:04.0 Off | 0 |\n| N/A 61C P8 17W / 70W | 0MiB / 15079MiB | 0% Default |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes: GPU Memory |\n| GPU PID Type Process name Usage |\n|=============================================================================|\n| No running processes found |\n+-----------------------------------------------------------------------------+\nAt the time of this writing, you\u2019ll also need to install the GPU version of TensorFlow\n(i.e., the tensorflow-gpu library); however, there is ongoing work to have a unified\ninstallation procedure for both CPU-only and GPU machines, so please check the\ninstallation documentation to see which library you should install."
  },
  {
    "id": 463,
    "content": "In any case, since\ninstalling every required library correctly is a bit long and tricky (and all hell breaks\nloose if you do not install the correct library versions), TensorFlow provides a Docker\nimage with everything you need inside. However, in order for the Docker container\nto have access to the GPU, you will still need to install the Nvidia drivers on the host\nmachine. To check that TensorFlow actually sees the GPUs, run the following tests:\n>>> import tensorflow as tf\n>>> tf.test.is_gpu_available()\nTrue\n>>> tf.test.gpu_device_name()\n'/device:GPU:0'\n>>> tf.config.experimental.list_physical_devices(device_type='GPU')\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\nThe is_gpu_available() function checks whether at least one GPU is available. The\ngpu_device_name() function gives the first GPU\u2019s name: by default, operations will\nUsing GPUs to Speed Up Computations | 11 Many code examples in this chapter use experimental APIs. They are very likely to be moved to the core API\nin future versions. So if an experimental function fails, try simply removing the word experimental, and\nhopefully it will work. If not, then perhaps the API has changed a bit; please check the Jupyter notebook, as I\nwill ensure it contains the correct code. 12 Presumably, these quotas are meant to stop bad guys who might be tempted to use GCP with stolen credit\ncards to mine cryptocurrencies. run on this GPU. The list_physical_devices() function returns the list of all avail\u2010\nable GPU devices (just one in this example).11\nNow, what if you don\u2019t want to invest time and money in getting your own GPU\ncard? Just use a GPU VM on the cloud! Using a GPU-Equipped Virtual Machine\nAll major cloud platforms now offer GPU VMs, some preconfigured with all the driv\u2010\ners and libraries you need (including TensorFlow). Google Cloud Platform enforces\nvarious GPU quotas, both worldwide and per region: you cannot just create thou\u2010\nsands of GPU VMs without prior authorization from Google.12 By default, the world\u2010\nwide GPU quota is zero, so you cannot use any GPU VMs. Therefore, the very first\nthing you need to do is to request a higher worldwide quota. In the GCP console,\nopen the navigation menu and go to IAM & admin \u2192 Quotas. Click Metric, click\nNone to uncheck all locations, then search for \u201cGPU\u201d and select \u201cGPUs (all regions)\u201d\nto see the corresponding quota. If this quota\u2019s value is zero (or just insufficient for\nyour needs), then check the box next to it (it should be the only selected one) and\nclick \u201cEdit quotas.\u201d Fill in the requested information, then click \u201cSubmit request.\u201d It\nmay take a few hours (or up to a few days) for your quota request to be processed and\n(generally) accepted. By default, there is also a quota of one GPU per region and per\nGPU type. You can request to increase these quotas too: click Metric, select None to\nuncheck all metrics, search for \u201cGPU,\u201d and select the type of GPU you want (e.g.,\nNVIDIA P4 GPUs)."
  },
  {
    "id": 464,
    "content": "Then click the Location drop-down menu, click None to\nuncheck all metrics, and click the location you want; check the boxes next to the\nquota(s) you want to change, and click \u201cEdit quotas\u201d to file a request. Once your GPU quota requests are approved, you can in no time create a VM equip\u2010\nped with one or more GPUs by using Google Cloud AI Platform\u2019s Deep Learning VM\nImages: go to  click View Console, then click \u201cLaunch on Com\u2010\npute Engine\u201d and fill in the VM configuration form. Note that some locations do not\nhave all types of GPUs, and some have no GPUs at all (change the location to see the\ntypes of GPUs available, if any). Make sure to select TensorFlow 2.0 as the framework,\nand check \u201cInstall NVIDIA GPU driver automatically on first startup.\u201d It is also a\ngood idea to check \u201cEnable access to JupyterLab via URL instead of SSH\u201d: this will\nmake it very easy to start a Jupyter notebook running on this GPU VM, powered by | Chapter 19: Training and Deploying TensorFlow Models at Scale\nJupyterLab (this is an alternative web interface to run Jupyter notebooks). Once the\nVM is created, scroll down the navigation menu to the Artificial Intelligence section,\nthen click AI Platform \u2192 Notebooks. Once the Notebook instance appears in the list\n(this may take a few minutes, so click Refresh once in a while until it appears), click\nits Open JupyterLab link. This will run JupyterLab on the VM and connect your\nbrowser to it. You can create notebooks and run any code you want on this VM, and\nbenefit from its GPUs! But if you just want to run some quick tests or easily share notebooks with your col\u2010\nleagues, then you should try Colaboratory. Colaboratory\nThe simplest and cheapest way to access a GPU VM is to use Colaboratory (or Colab,\nfor short). It\u2019s free! Just go to  and create a new\nPython 3 notebook: this will create a Jupyter notebook, stored on your Google Drive\n(alternatively, you can open any notebook on GitHub, or on Google Drive, or you can\neven upload your own notebooks). Colab\u2019s user interface is similar to Jupyter\u2019s, except\nyou can share and use the notebooks like regular Google Docs, and there are a few\nother minor differences (e.g., you can create handy widgets using special comments\nin your code). When you open a Colab notebook, it runs on a free Google VM dedicated to that\nnotebook, called a Colab Runtime (see Figure 19-11). By default the Runtime is CPU-\nonly, but you can change this by going to Runtime \u2192 \u201cChange runtime type,\u201d select\u2010\ning GPU in the \u201cHardware accelerator\u201d drop-down menu, then clicking Save. In fact,\nyou could even select TPU! (Yes, you can actually use a TPU for free; we will talk\nabout TPUs later in this chapter, though, so for now just select GPU.) Using GPUs to Speed Up Computations | Figure 19-11."
  },
  {
    "id": 465,
    "content": "Colab Runtimes and notebooks\nColab does have some restrictions: first, there is a limit to the number of Colab note\u2010\nbooks you can run simultaneously (currently 5 per Runtime type). Moreover, as the\nFAQ states, \u201cColaboratory is intended for interactive use. Long-running background\ncomputations, particularly on GPUs, may be stopped. Please do not use Colaboratory\nfor cryptocurrency mining.\u201d Also, the web interface will automatically disconnect\nfrom the Colab Runtime if you leave it unattended for a while (~30 minutes). When\nyou reconnect to the Colab Runtime, it may have been reset, so make sure you always\nexport any data you care about (e.g., download it or save it to Google Drive). Even if\nyou never disconnect, the Colab Runtime will automatically shut down after 12\nhours, as it is not meant for long-running computations. Despite these limitations, it\u2019s\na fantastic tool to run tests easily, get quick results, and collaborate with your\ncolleagues. Managing the GPU RAM\nBy default TensorFlow automatically grabs all the RAM in all available GPUs the first\ntime you run a computation. It does this to limit GPU RAM fragmentation. This\nmeans that if you try to start a second TensorFlow program (or any program that\nrequires the GPU), it will quickly run out of RAM. This does not happen as often as\nyou might think, as you will most often have a single TensorFlow program running\non a machine: usually a training script, a TF Serving node, or a Jupyter notebook. If\nyou need to run multiple programs for some reason (e.g., to train two different mod\u2010\nels in parallel on the same machine), then you will need to split the GPU RAM\nbetween these processes more evenly. If you have multiple GPU cards on your machine, a simple solution is to assign each\nof them to a single process. To do this, you can set the CUDA_VISIBLE_DEVICES\nenvironment variable so that each process only sees the appropriate GPU card(s). Also set the CUDA_DEVICE_ORDER environment variable to PCI_BUS_ID to ensure that | Chapter 19: Training and Deploying TensorFlow Models at Scale\neach ID always refers to the same GPU card. For example, if you have four GPU\ncards, you could start two programs, assigning two GPUs to each of them, by execut\u2010\ning commands like the following in two separate terminal windows:\n$ CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=0,1 python3 program_1.py\n# and in another terminal:\n$ CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3,2 python3 program_2.py\nProgram 1 will then only see GPU cards 0 and 1, named /gpu:0 and /gpu:1 respec\u2010\ntively, and program 2 will only see GPU cards 2 and 3, named /gpu:1 and /gpu:0\nrespectively (note the order). Everything will work fine (see Figure 19-12). Of course,\nyou can also define these environment variables in Python by setting os.envi\nron[\"CUDA_DEVICE_ORDER\"] and os.environ[\"CUDA_VISIBLE_DEVICES\"], as long as\nyou do so before using TensorFlow. Figure 19-12. Each program gets two GPUs\nAnother option is to tell TensorFlow to grab only a specific amount of GPU RAM."
  },
  {
    "id": 466,
    "content": "This must be done immediately after importing TensorFlow. For example, to make\nTensorFlow grab only 2 GiB of RAM on each GPU, you must create a virtual GPU\ndevice (also called a logical GPU device) for each physical GPU device and set its\nmemory limit to 2 GiB (i.e., 2,048 MiB):\nfor gpu in tf.config.experimental.list_physical_devices(\"GPU\"): tf.config.experimental.set_virtual_device_configuration( gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\nNow (supposing you have four GPUs, each with at least 4 GiB of RAM) two programs\nlike this one can run in parallel, each using all four GPU cards (see Figure 19-13). Using GPUs to Speed Up Computations | Figure 19-13. Each program gets all four GPUs, but with only 2 GiB of RAM on each\nGPU\nIf you run the nvidia-smi command while both programs are running, you should\nsee that each process holds 2 GiB of RAM on each card:\n$ nvidia-smi\n[...]\n+-----------------------------------------------------------------------------+\n| Processes: GPU Memory |\n| GPU PID Type Process name Usage |\n|=============================================================================|\n| 0 2373 C /usr/bin/python3 2241MiB |\n| 0 2533 C /usr/bin/python3 2241MiB |\n| 1 2373 C /usr/bin/python3 2241MiB |\n| 1 2533 C /usr/bin/python3 2241MiB |\n[...]\nYet another option is to tell TensorFlow to grab memory only when it needs it (this\nalso must be done immediately after importing TensorFlow):\nfor gpu in tf.config.experimental.list_physical_devices(\"GPU\"): tf.config.experimental.set_memory_growth(gpu, True)\nAnother way to do this is to set the TF_FORCE_GPU_ALLOW_GROWTH environment vari\u2010\nable to true. With this option, TensorFlow will never release memory once it has\ngrabbed it (again, to avoid memory fragmentation), except of course when the pro\u2010\ngram ends. It can be harder to guarantee deterministic behavior using this option\n(e.g., one program may crash because another program\u2019s memory usage went through\nthe roof), so in production you\u2019ll probably want to stick with one of the previous\noptions. However, there are some cases where it is very useful: for example, when you\nuse a machine to run multiple Jupyter notebooks, several of which use TensorFlow. This is why the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set to true in\nColab Runtimes. Lastly, in some cases you may want to split a GPU into two or more virtual GPUs\u2014\nfor example, if you want to test a distribution algorithm (this is a handy way to try\nout the code examples in the rest of this chapter even if you have a single GPU, such | Chapter 19: Training and Deploying TensorFlow Models at Scale\n13 Mart\u00edn Abadi et al., \u201cTensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems\u201d\nGoogle Research whitepaper (2015). as in a Colab Runtime). The following code splits the first GPU into two virtual devi\u2010\nces, with 2 GiB of RAM each (again, this must be done immediately after importing\nTensorFlow):\nphysical_gpus = tf.config.experimental.list_physical_devices(\"GPU\")\ntf.config.experimental.set_virtual_device_configuration( physical_gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048), tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\nThese two virtual devices will then be called /gpu:0 and /gpu:1, and you can place\noperations and variables on each of them as if they were really two independent\nGPUs."
  },
  {
    "id": 467,
    "content": "Now let\u2019s see how TensorFlow decides which devices it should place variables\nand execute operations on. Placing Operations and Variables on Devices\nThe TensorFlow whitepaper13 presents a friendly dynamic placer algorithm that auto\u2010\nmagically distributes operations across all available devices, taking into account\nthings like the measured computation time in previous runs of the graph, estimations\nof the size of the input and output tensors for each operation, the amount of RAM\navailable in each device, communication delay when transferring data into and out of\ndevices, and hints and constraints from the user. In practice this algorithm turned out\nto be less efficient than a small set of placement rules specified by the user, so the Ten\u2010\nsorFlow team ended up dropping the dynamic placer. That said, tf.keras and tf.data generally do a good job of placing operations and vari\u2010\nables where they belong (e.g., heavy computations on the GPU, and data preprocess\u2010\ning on the CPU). But you can also place operations and variables manually on each\ndevice, if you want more control:\n\u2022 As just mentioned, you generally want to place the data preprocessing operations\non the CPU, and place the neural network operations on the GPUs. \u2022 GPUs usually have a fairly limited communication bandwidth, so it is important\nto avoid unnecessary data transfers in and out of the GPUs. \u2022 Adding more CPU RAM to a machine is simple and fairly cheap, so there\u2019s usu\u2010\nally plenty of it, whereas the GPU RAM is baked into the GPU: it is an expensive\nand thus limited resource, so if a variable is not needed in the next few training\nsteps, it should probably be placed on the CPU (e.g., datasets generally belong on\nthe CPU). Using GPUs to Speed Up Computations | 14 As we saw in Chapter 12, a kernel is a variable or operation\u2019s implementation for a specific data type and\ndevice type. For example, there is a GPU kernel for the float32 tf.matmul() operation, but there is no GPU\nkernel for int32 tf.matmul() (only a CPU kernel). 15 You can also use tf.debugging.set_log_device_placement(True) to log all device placements. By default, all variables and all operations will be placed on the first GPU\n(named /gpu:0), except for variables and operations that don\u2019t have a GPU kernel:14\nthese are placed on the CPU (named /cpu:0). A tensor or variable\u2019s device attribute\ntells you which device it was placed on:15\n>>> a = tf.Variable(42.0)\n>>> a.device\n'/job:localhost/replica:0/task:0/device:GPU:0'\n>>> b = tf.Variable(42)\n>>> b.device\n'/job:localhost/replica:0/task:0/device:CPU:0'\nYou can safely ignore the prefix /job:localhost/replica:0/task:0 for now (it\nallows you to place operations on other machines when using a TensorFlow cluster;\nwe will talk about jobs, replicas, and tasks later in this chapter). As you can see, the\nfirst variable was placed on GPU 0, which is the default device."
  },
  {
    "id": 468,
    "content": "However, the second\nvariable was placed on the CPU: this is because there are no GPU kernels for integer\nvariables (or for operations involving integer tensors), so TensorFlow fell back to the\nCPU. If you want to place an operation on a different device than the default one, use a\ntf.device() context:\n>>> with tf.device(\"/cpu:0\"):\n... c = tf.Variable(42.0)\n...\n>>> c.device\n'/job:localhost/replica:0/task:0/device:CPU:0'\nThe CPU is always treated as a single device (/cpu:0), even if your\nmachine has multiple CPU cores. Any operation placed on the\nCPU may run in parallel across multiple cores if it has a multi\u2010\nthreaded kernel. If you explicitly try to place an operation or variable on a device that does not exist or\nfor which there is no kernel, then you will get an exception. However, in some cases\nyou may prefer to fall back to the CPU; for example, if your program may run both\non CPU-only machines and on GPU machines, you may want TensorFlow to ignore\nyour tf.device(\"/gpu:*\") on CPU-only machines. To do this, you can call tf.con\nfig.set_soft_device_placement(True) just after importing TensorFlow: when a | Chapter 19: Training and Deploying TensorFlow Models at Scale\nplacement request fails, TensorFlow will fall back to its default placement rules (i.e.,\nGPU 0 by default if it exists and there is a GPU kernel, and CPU 0 otherwise). Now how exactly will TensorFlow execute all these operations across multiple\ndevices? Parallel Execution Across Multiple Devices\nAs we saw in Chapter 12, one of the benefits of using TF Functions is parallelism. Let\u2019s look at this a bit more closely. When TensorFlow runs a TF Function, it starts by\nanalyzing its graph to find the list of operations that need to be evaluated, and it\ncounts how many dependencies each of them has. TensorFlow then adds each opera\u2010\ntion with zero dependencies (i.e., each source operation) to the evaluation queue of\nthis operation\u2019s device (see Figure 19-14). Once an operation has been evaluated, the\ndependency counter of each operation that depends on it is decremented. Once an\noperation\u2019s dependency counter reaches zero, it is pushed to the evaluation queue of\nits device. And once all the nodes that TensorFlow needs have been evaluated, it\nreturns their outputs. Figure 19-14. Parallelized execution of a TensorFlow graph\nOperations in the CPU\u2019s evaluation queue are dispatched to a thread pool called the\ninter-op thread pool. If the CPU has multiple cores, then these operations will effec\u2010\ntively be evaluated in parallel. Some operations have multithreaded CPU kernels:\nthese kernels split their tasks into multiple suboperations, which are placed in\nanother evaluation queue and dispatched to a second thread pool called the intra-op\nUsing GPUs to Speed Up Computations | 16 This can be useful if you want to guarantee perfect reproducibility, as I explain in this video, based on TF 1.\nthread pool (shared by all multithreaded CPU kernels). In short, multiple operations\nand suboperations may be evaluated in parallel on different CPU cores."
  },
  {
    "id": 469,
    "content": "For the GPU, things are a bit simpler. Operations in a GPU\u2019s evaluation queue are\nevaluated sequentially. However, most operations have multithreaded GPU kernels,\ntypically implemented by libraries that TensorFlow depends on, such as CUDA and\ncuDNN. These implementations have their own thread pools, and they typically\nexploit as many GPU threads as they can (which is the reason why there is no need\nfor an inter-op thread pool in GPUs: each operation already floods most GPU\nthreads). For example, in Figure 19-14, operations A, B, and C are source ops, so they can\nimmediately be evaluated. Operations A and B are placed on the CPU, so they are\nsent to the CPU\u2019s evaluation queue, then they are dispatched to the inter-op thread\npool and immediately evaluated in parallel. Operation A happens to have a multi\u2010\nthreaded kernel; its computations are split into three parts, which are executed in par\u2010\nallel by the intra-op thread pool. Operation C goes to GPU 0\u2019s evaluation queue, and\nin this example its GPU kernel happens to use cuDNN, which manages its own intra-\nop thread pool and runs the operation across many GPU threads in parallel. Suppose\nC finishes first. The dependency counters of D and E are decremented and they reach\nzero, so both operations are pushed to GPU 0\u2019s evaluation queue, and they are exe\u2010\ncuted sequentially. Note that C only gets evaluated once, even though both D and E\ndepend on it. Suppose B finishes next. Then F\u2019s dependency counter is decremented\nfrom 4 to 3, and since that\u2019s not 0, it does not run yet. Once A, D, and E are finished,\nthen F\u2019s dependency counter reaches 0, and it is pushed to the CPU\u2019s evaluation\nqueue and evaluated. Finally, TensorFlow returns the requested outputs. An extra bit of magic that TensorFlow performs is when the TF Function modifies a\nstateful resource, such as a variable: it ensures that the order of execution matches the\norder in the code, even if there is no explicit dependency between the statements. For\nexample, if your TF Function contains v.assign_add(1) followed by v.assign(v *\n2), TensorFlow will ensure that these operations are executed in that order. You can control the number of threads in the inter-op thread\npool by calling tf.config.threading.set_inter_op_parallel\nism_threads(). To set the number of intra-op threads, use\ntf.config.threading.set_intra_op_parallelism_threads(). This is useful if you want do not want TensorFlow to use all the\nCPU cores or if you want it to be single-threaded.16 | Chapter 19: Training and Deploying TensorFlow Models at Scale\n17 At the time of this writing it only prefetches the data to the CPU RAM, but you can use tf.data.experimen\ntal.prefetch_to_device() to make it prefetch the data and push it to the device of your choice so that the\nGPU does not waste time waiting for the data to be transferred. With that, you have all you need to run any operation on any device, and exploit the\npower of your GPUs!"
  },
  {
    "id": 470,
    "content": "Here are some of the things you could do:\n\u2022 You could train several models in parallel, each on its own GPU: just write a\ntraining script for each model and run them in parallel, setting\nCUDA_DEVICE_ORDER and CUDA_VISIBLE_DEVICES so that each script only sees a\nsingle GPU device. This is great for hyperparameter tuning, as you can train in\nparallel multiple models with different hyperparameters. If you have a single\nmachine with two GPUs, and it takes one hour to train one model on one GPU,\nthen training two models in parallel, each on its own dedicated GPU, will take\njust one hour. Simple! \u2022 You could train a model on a single GPU and perform all the preprocessing in\nparallel on the CPU, using the dataset\u2019s prefetch() method17 to prepare the next\nfew batches in advance so that they are ready when the GPU needs them (see\nChapter 13). \u2022 If your model takes two images as input and processes them using two CNNs\nbefore joining their outputs, then it will probably run much faster if you place\neach CNN on a different GPU. \u2022 You can create an efficient ensemble: just place a different trained model on each\nGPU so that you can get all the predictions much faster to produce the ensem\u2010\nble\u2019s final prediction. But what if you want to train a single model across multiple GPUs? Training Models Across Multiple Devices\nThere are two main approaches to training a single model across multiple devices:\nmodel parallelism, where the model is split across the devices, and data parallelism,\nwhere the model is replicated across every device, and each replica is trained on a\nsubset of the data. Let\u2019s look at these two options closely before we train a model on\nmultiple GPUs. Model Parallelism\nSo far we have trained each neural network on a single device. What if we want to\ntrain a single neural network across multiple devices? This requires chopping the\nmodel into separate chunks and running each chunk on a different device. Training Models Across Multiple Devices | Unfortunately, such model parallelism turns out to be pretty tricky, and it really\ndepends on the architecture of your neural network. For fully connected networks,\nthere is generally not much to be gained from this approach (see Figure 19-15). Intui\u2010\ntively, it may seem that an easy way to split the model is to place each layer on a dif\u2010\nferent device, but this does not work because each layer needs to wait for the output\nof the previous layer before it can do anything. So perhaps you can slice it vertically\u2014\nfor example, with the left half of each layer on one device, and the right part on\nanother device?"
  },
  {
    "id": 471,
    "content": "This is slightly better, since both halves of each layer can indeed work\nin parallel, but the problem is that each half of the next layer requires the output of\nboth halves, so there will be a lot of cross-device communication (represented by the\ndashed arrows). This is likely to completely cancel out the benefit of the parallel com\u2010\nputation, since cross-device communication is slow (especially when the devices are\nlocated on different machines). Figure 19-15. Splitting a fully connected neural network\nSome neural network architectures, such as convolutional neural networks (see\nChapter 14), contain layers that are only partially connected to the lower layers, so it\nis much easier to distribute chunks across devices in an efficient way (Figure 19-16). | Chapter 19: Training and Deploying TensorFlow Models at Scale\nFigure 19-16. Splitting a partially connected neural network\nDeep recurrent neural networks (see Chapter 15) can be split a bit more efficiently\nacross multiple GPUs. If you split the network horizontally by placing each layer on a\ndifferent device, and you feed the network with an input sequence to process, then at\nthe first time step only one device will be active (working on the sequence\u2019s first\nvalue), at the second step two will be active (the second layer will be handling the out\u2010\nput of the first layer for the first value, while the first layer will be handling the second\nvalue), and by the time the signal propagates to the output layer, all devices will be\nactive simultaneously (Figure 19-17). There is still a lot of cross-device communica\u2010\ntion going on, but since each cell may be fairly complex, the benefit of running multi\u2010\nple cells in parallel may (in theory) outweigh the communication penalty. However,\nin practice a regular stack of LSTM layers running on a single GPU actually runs much\nfaster. Training Models Across Multiple Devices | 18 If you are interested in going further with model parallelism, check out Mesh TensorFlow. Figure 19-17. Splitting a deep recurrent neural network\nIn short, model parallelism may speed up running or training some types of neural\nnetworks, but not all, and it requires special care and tuning, such as making sure\nthat devices that need to communicate the most run on the same machine.18 Let\u2019s look\nat a much simpler and generally more efficient option: data parallelism. Data Parallelism\nAnother way to parallelize the training of a neural network is to replicate it on every\ndevice and run each training step simultaneously on all replicas, using a different\nmini-batch for each. The gradients computed by each replica are then averaged, and\nthe result is used to update the model parameters. This is called data parallelism. There are many variants of this idea, so let\u2019s look at the most important ones. Data parallelism using the mirrored strategy\nArguably the simplest approach is to completely mirror all the model parameters\nacross all the GPUs and always apply the exact same parameter updates on every\nGPU."
  },
  {
    "id": 472,
    "content": "This way, all replicas always remain perfectly identical. This is called the mir\u2010\nrored strategy, and it turns out to be quite efficient, especially when using a single\nmachine (see Figure 19-18). | Chapter 19: Training and Deploying TensorFlow Models at Scale\nFigure 19-18. Data parallelism using the mirrored strategy\nThe tricky part when using this approach is to efficiently compute the mean of all the\ngradients from all the GPUs and distribute the result across all the GPUs. This can be\ndone using an AllReduce algorithm, a class of algorithms where multiple nodes col\u2010\nlaborate to efficiently perform a reduce operation (such as computing the mean, sum,\nand max), while ensuring that all nodes obtain the same final result. Fortunately,\nthere are off-the-shelf implementations of such algorithms, as we will see. Data parallelism with centralized parameters\nAnother approach is to store the model parameters outside of the GPU devices per\u2010\nforming the computations (called workers), for example on the CPU (see\nFigure 19-19). In a distributed setup, you may place all the parameters on one or\nmore CPU-only servers called parameter servers, whose only role is to host and\nupdate the parameters. Training Models Across Multiple Devices | 19 This name is slightly confusing because it sounds like some replicas are special, doing nothing. In reality, all\nreplicas are equivalent: they all work hard to be among the fastest at each training step, and the losers vary at\nevery step (unless some devices are really slower than others). However, it does mean that if a server crashes,\ntraining will continue just fine. Figure 19-19. Data parallelism with centralized parameters\nWhereas the mirrored strategy imposes synchronous weight updates across all GPUs,\nthis centralized approach allows either synchronous or asynchronous updates. Let\u2019s\nsee the pros and cons of both options. Synchronous updates. With synchronous updates, the aggregator waits until all gradi\u2010\nents are available before it computes the average gradients and passes them to the\noptimizer, which will update the model parameters. Once a replica has finished com\u2010\nputing its gradients, it must wait for the parameters to be updated before it can pro\u2010\nceed to the next mini-batch. The downside is that some devices may be slower than\nothers, so all other devices will have to wait for them at every step. Moreover, the\nparameters will be copied to every device almost at the same time (immediately after\nthe gradients are applied), which may saturate the parameter servers\u2019 bandwidth. To reduce the waiting time at each step, you could ignore the gradi\u2010\nents from the slowest few replicas (typically ~10%). For example,\nyou could run 20 replicas, but only aggregate the gradients from\nthe fastest 18 replicas at each step, and just ignore the gradients\nfrom the last 2. As soon as the parameters are updated, the first 18\nreplicas can start working again immediately, without having to\nwait for the 2 slowest replicas."
  },
  {
    "id": 473,
    "content": "This setup is generally described as\nhaving 18 replicas plus 2 spare replicas.19 | Chapter 19: Training and Deploying TensorFlow Models at Scale\nAsynchronous updates. With asynchronous updates, whenever a replica has finished\ncomputing the gradients, it immediately uses them to update the model parameters. There is no aggregation (it removes the \u201cmean\u201d step in Figure 19-19) and no synchro\u2010\nnization. Replicas work independently of the other replicas. Since there is no waiting\nfor the other replicas, this approach runs more training steps per minute. Moreover,\nalthough the parameters still need to be copied to every device at every step, this hap\u2010\npens at different times for each replica, so the risk of bandwidth saturation is reduced. Data parallelism with asynchronous updates is an attractive choice because of its sim\u2010\nplicity, the absence of synchronization delay, and a better use of the bandwidth. How\u2010\never, although it works reasonably well in practice, it is almost surprising that it\nworks at all! Indeed, by the time a replica has finished computing the gradients based\non some parameter values, these parameters will have been updated several times by\nother replicas (on average N \u2013 1 times, if there are N replicas), and there is no guaran\u2010\ntee that the computed gradients will still be pointing in the right direction (see\nFigure 19-20). When gradients are severely out-of-date, they are called stale gradients:\nthey can slow down convergence, introducing noise and wobble effects (the learning\ncurve may contain temporary oscillations), or they can even make the training algo\u2010\nrithm diverge. Figure 19-20. Stale gradients when using asynchronous updates\nThere are a few ways you can reduce the effect of stale gradients:\n\u2022 Reduce the learning rate. \u2022 Drop stale gradients or scale them down. \u2022 Adjust the mini-batch size. Training Models Across Multiple Devices | 20 Jianmin Chen et al., \u201cRevisiting Distributed Synchronous SGD,\u201d arXiv preprint arXiv:1604.00981 (2016). \u2022 Start the first few epochs using just one replica (this is called the warmup phase). Stale gradients tend to be more damaging at the beginning of training, when gra\u2010\ndients are typically large and the parameters have not settled into a valley of the\ncost function yet, so different replicas may push the parameters in quite different\ndirections. A paper published by the Google Brain team in 201620 benchmarked various\napproaches and found that using synchronous updates with a few spare replicas was\nmore efficient than using asynchronous updates, not only converging faster but also\nproducing a better model. However, this is still an active area of research, so you\nshould not rule out asynchronous updates just yet. Bandwidth saturation\nWhether you use synchronous or asynchronous updates, data parallelism with cen\u2010\ntralized parameters still requires communicating the model parameters from the\nparameter servers to every replica at the beginning of each training step, and the gra\u2010\ndients in the other direction at the end of each training step."
  },
  {
    "id": 474,
    "content": "Similarly, when using the\nmirrored strategy, the gradients produced by each GPU will need to be shared with\nevery other GPU. Unfortunately, there always comes a point where adding an extra\nGPU will not improve performance at all because the time spent moving the data into\nand out of GPU RAM (and across the network in a distributed setup) will outweigh\nthe speedup obtained by splitting the computation load. At that point, adding more\nGPUs will just worsen the bandwidth saturation and actually slow down training. For some models, typically relatively small and trained on a very\nlarge training set, you are often better off training the model on a\nsingle machine with a single powerful GPU with a large memory\nbandwidth. Saturation is more severe for large dense models, since they have a lot of parameters\nand gradients to transfer. It is less severe for small models (but the parallelization gain\nis limited) and for large sparse models, where the gradients are typically mostly zeros\nand so can be communicated efficiently. Jeff Dean, initiator and lead of the Google\nBrain project, reported typical speedups of 25\u201340\u00d7 when distributing computations\nacross 50 GPUs for dense models, and a 300\u00d7 speedup for sparser models trained\nacross 500 GPUs. As you can see, sparse models really do scale better. Here are a few\nconcrete examples: | Chapter 19: Training and Deploying TensorFlow Models at Scale\n\u2022 Neural machine translation: 6\u00d7 speedup on 8 GPUs\n\u2022 Inception/ImageNet: 32\u00d7 speedup on 50 GPUs\n\u2022 RankBrain: 300\u00d7 speedup on 500 GPUs\nBeyond a few dozen GPUs for a dense model or few hundred GPUs for a sparse\nmodel, saturation kicks in and performance degrades. There is plenty of research\ngoing on to solve this problem (exploring peer-to-peer architectures rather than cen\u2010\ntralized parameter servers, using lossy model compression, optimizing when and\nwhat the replicas need to communicate, and so on), so there will likely be a lot of pro\u2010\ngress in parallelizing neural networks in the next few years. In the meantime, to reduce the saturation problem, you probably want to use a few\npowerful GPUs rather than plenty of weak GPUs, and you should also group your\nGPUs on few and very well interconnected servers. You can also try dropping the\nfloat precision from 32 bits (tf.float32) to 16 bits (tf.bfloat16). This will cut in\nhalf the amount of data to transfer, often without much impact on the convergence\nrate or the model\u2019s performance. Lastly, if you are using centralized parameters, you\ncan shard (split) the parameters across multiple parameter servers: adding more\nparameter servers will reduce the network load on each server and limit the risk of\nbandwidth saturation. OK, now let\u2019s train a model across multiple GPUs! Training at Scale Using the Distribution Strategies API\nMany models can be trained quite well on a single GPU, or even on a CPU. But if\ntraining is too slow, you can try distributing it across multiple GPUs on the same\nmachine."
  },
  {
    "id": 475,
    "content": "If that\u2019s still too slow, try using more powerful GPUs, or add more GPUs to\nthe machine. If your model performs heavy computations (such as large matrix mul\u2010\ntiplications), then it will run much faster on powerful GPUs, and you could even try\nto use TPUs on Google Cloud AI Platform, which will usually run even faster for such\nmodels. But if you can\u2019t fit any more GPUs on the same machine, and if TPUs aren\u2019t\nfor you (e.g., perhaps your model doesn\u2019t benefit much from TPUs, or perhaps you\nwant to use your own hardware infrastructure), then you can try training it across\nseveral servers, each with multiple GPUs (if this is still not enough, as a last resort you\ncan try adding some model parallelism, but this requires a lot more effort). In this\nsection we will see how to train models at scale, starting with multiple GPUs on the\nsame machine (or TPUs) and then moving on to multiple GPUs across multiple\nmachines. Luckily, TensorFlow comes with a very simple API that takes care of all the complex\u2010\nity for you: the Distribution Strategies API. To train a Keras model across all available\nGPUs (on a single machine, for now) using data parallelism with the mirrored\nTraining Models Across Multiple Devices | 21 For more details on AllReduce algorithms, read this great post by Yuichiro Ueno, and this page on scaling\nwith NCCL. strategy, create a MirroredStrategy object, call its scope() method to get a distribu\u2010\ntion context, and wrap the creation and compilation of your model inside that con\u2010\ntext. Then call the model\u2019s fit() method normally:\ndistribution = tf.distribute.MirroredStrategy()\nwith distribution.scope(): mirrored_model = keras.models.Sequential([...]) mirrored_model.compile([...])\nbatch_size = 100 # must be divisible by the number of replicas\nhistory = mirrored_model.fit(X_train, y_train, epochs=10)\nUnder the hood, tf.keras is distribution-aware, so in this MirroredStrategy context it\nknows that it must replicate all variables and operations across all available GPU\ndevices. Note that the fit() method will automatically split each training batch\nacross all the replicas, so it\u2019s important that the batch size be divisible by the number\nof replicas. And that\u2019s all! Training will generally be significantly faster than using a\nsingle device, and the code change was really minimal. Once you have finished training your model, you can use it to make predictions effi\u2010\nciently: call the predict() method, and it will automatically split the batch across all\nreplicas, making predictions in parallel (again, the batch size must be divisible by the\nnumber of replicas). If you call the model\u2019s save() method, it will be saved as a regu\u2010\nlar model, not as a mirrored model with multiple replicas. So when you load it, it will\nrun like a regular model, on a single device (by default GPU 0, or the CPU if there are\nno GPUs)."
  },
  {
    "id": 476,
    "content": "If you want to load a model and run it on all available devices, you must\ncall keras.models.load_model() within a distribution context:\nwith distribution.scope(): mirrored_model = keras.models.load_model(\"my_mnist_model.h5\")\nIf you only want to use a subset of all the available GPU devices, you can pass the list\nto the MirroredStrategy\u2019s constructor:\ndistribution = tf.distribute.MirroredStrategy([\"/gpu:0\", \"/gpu:1\"])\nBy default, the MirroredStrategy class uses the NVIDIA Collective Communications\nLibrary (NCCL) for the AllReduce mean operation, but you can change it by setting\nthe cross_device_ops argument to an instance of the tf.distribute.Hierarchical\nCopyAllReduce class, or an instance of the tf.distribute.ReductionToOneDevice\nclass. The default NCCL option is based on the tf.distribute.NcclAllReduce class,\nwhich is usually faster, but this depends on the number and types of GPUs, so you\nmay want to give the alternatives a try.21 | Chapter 19: Training and Deploying TensorFlow Models at Scale\nIf you want to try using data parallelism with centralized parameters, replace the\nMirroredStrategy with the CentralStorageStrategy:\ndistribution = tf.distribute.experimental.CentralStorageStrategy()\nYou can optionally set the compute_devices argument to specify the list of devices\nyou want to use as workers (by default it will use all available GPUs), and you can\noptionally set the parameter_device argument to specify the device you want to store\nthe parameters on (by default it will use the CPU, or the GPU if there is just one). Now let\u2019s see how to train a model across a cluster of TensorFlow servers! Training a Model on a TensorFlow Cluster\nA TensorFlow cluster is a group of TensorFlow processes running in parallel, usually\non different machines, and talking to each other to complete some work\u2014for exam\u2010\nple, training or executing a neural network. Each TF process in the cluster is called a\ntask, or a TF server. It has an IP address, a port, and a type (also called its role or its\njob). The type can be either \"worker\", \"chief\", \"ps\" (parameter server), or\n\"evaluator\":\n\u2022 Each worker performs computations, usually on a machine with one or more\nGPUs. \u2022 The chief performs computations as well (it is a worker), but it also handles extra\nwork such as writing TensorBoard logs or saving checkpoints. There is a single\nchief in a cluster. If no chief is specified, then the first worker is the chief. \u2022 A parameter server only keeps track of variable values, and it is usually on a CPU-\nonly machine. This type of task is only used with the ParameterServerStrategy. \u2022 An evaluator obviously takes care of evaluation. To start a TensorFlow cluster, you must first specify it. This means defining each\ntask\u2019s IP address, TCP port, and type. For example, the following cluster specification\ndefines a cluster with three tasks (two workers and one parameter server; see\nFigure 19-21)."
  },
  {
    "id": 477,
    "content": "The cluster spec is a dictionary with one key per job, and the values are\nlists of task addresses (IP:port):\ncluster_spec = { \"worker\": [ \"machine-a.example.com:2222\", # /job:worker/task:0 \"machine-b.example.com:2222\" # /job:worker/task:1 ], \"ps\": [\"machine-a.example.com:2221\"] # /job:ps/task:0\n}\nTraining Models Across Multiple Devices | Figure 19-21. TensorFlow cluster\nIn general there will be a single task per machine, but as this example shows, you can\nconfigure multiple tasks on the same machine if you want (if they share the same\nGPUs, make sure the RAM is split appropriately, as discussed earlier). By default, every task in the cluster may communicate with every\nother task, so make sure to configure your firewall to authorize all\ncommunications between these machines on these ports (it\u2019s usu\u2010\nally simpler if you use the same port on every machine). When you start a task, you must give it the cluster spec, and you must also tell it what\nits type and index are (e.g., worker 0). The simplest way to specify everything at once\n(both the cluster spec and the current task\u2019s type and index) is to set the TF_CONFIG\nenvironment variable before starting TensorFlow. It must be a JSON-encoded dictio\u2010\nnary containing a cluster specification (under the \"cluster\" key) and the type and\nindex of the current task (under the \"task\" key). For example, the following TF_CON\nFIG environment variable uses the cluster we just defined and specifies that the task\nto start is the first worker:\nimport os\nimport json\nos.environ[\"TF_CONFIG\"] = json.dumps({ \"cluster\": cluster_spec, \"task\": {\"type\": \"worker\", \"index\": 0}\n}) | Chapter 19: Training and Deploying TensorFlow Models at Scale\nIn general you want to define the TF_CONFIG environment variable\noutside of Python, so the code does not need to include the current\ntask\u2019s type and index (this makes it possible to use the same code\nacross all workers). Now let\u2019s train a model on a cluster! We will start with the mirrored strategy\u2014it\u2019s sur\u2010\nprisingly simple! First, you need to set the TF_CONFIG environment variable appropri\u2010\nately for each task. There should be no parameter server (remove the \u201cps\u201d key in the\ncluster spec), and in general you will want a single worker per machine. Make extra\nsure you set a different task index for each task. Finally, run the following training\ncode on every worker:\ndistribution = tf.distribute.experimental.MultiWorkerMirroredStrategy()\nwith distribution.scope(): mirrored_model = keras.models.Sequential([...]) mirrored_model.compile([...])\nbatch_size = 100 # must be divisible by the number of replicas\nhistory = mirrored_model.fit(X_train, y_train, epochs=10)\nYes, that\u2019s exactly the same code we used earlier, except this time we are using the\nMultiWorkerMirroredStrategy (in future versions, the MirroredStrategy will prob\u2010\nably handle both the single machine and multimachine cases). When you start this\nscript on the first workers, they will remain blocked at the AllReduce step, but as soon\nas the last worker starts up training will begin, and you will see them all advancing at\nexactly the same rate (since they synchronize at each step)."
  },
  {
    "id": 478,
    "content": "You can choose from two AllReduce implementations for this distribution strategy: a\nring AllReduce algorithm based on gRPC for the network communications, and\nNCCL\u2019s implementation. The best algorithm to use depends on the number of work\u2010\ners, the number and types of GPUs, and the network. By default, TensorFlow will\napply some heuristics to select the right algorithm for you, but if you want to force\none algorithm, pass CollectiveCommunication.RING or CollectiveCommunica\ntion.NCCL (from tf.distribute.experimental) to the strategy\u2019s constructor. If you prefer to implement asynchronous data parallelism with parameter servers,\nchange the strategy to ParameterServerStrategy, add one or more parameter\nservers, and configure TF_CONFIG appropriately for each task. Note that although the\nworkers will work asynchronously, the replicas on each worker will work\nsynchronously. Lastly, if you have access to TPUs on Google Cloud, you can create a TPUStrategy\nlike this (then use it like the other strategies):\nTraining Models Across Multiple Devices | resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.tpu.experimental.initialize_tpu_system(resolver)\ntpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)\nIf you are a researcher, you may be eligible to use TPUs for free; see\n for more details. You can now train models across multiple GPUs and multiple servers: give yourself a\npat on the back! If you want to train a large model, you will need many GPUs, across\nmany servers, which will require either buying a lot of hardware or managing a lot of\ncloud VMs. In many cases, it\u2019s going to be less hassle and less expensive to use a cloud\nservice that takes care of provisioning and managing all this infrastructure for you,\njust when you need it. Let\u2019s see how to do that on GCP. Running Large Training Jobs on Google Cloud AI Platform\nIf you decide to use Google AI Platform, you can deploy a training job with the same\ntraining code as you would run on your own TF cluster, and the platform will take\ncare of provisioning and configuring as many GPU VMs as you desire (within your\nquotas). To start the job, you will need the gcloud command-line tool, which is part of the\nGoogle Cloud SDK. You can either install the SDK on your own machine, or just use\nthe Google Cloud Shell on GCP. This is a terminal you can use directly in your web\nbrowser; it runs on a free Linux VM (Debian), with the SDK already installed and\npreconfigured for you. The Cloud Shell is available anywhere in GCP: just click the\nActivate Cloud Shell icon at the top right of the page (see Figure 19-22). Figure 19-22. Activating the Google Cloud Shell\nIf you prefer to install the SDK on your machine, once you have installed it, you need\nto initialize it by running gcloud init: you will need to log in to GCP and grant\naccess to your GCP resources, then select the GCP project you want to use (if you\nhave more than one), as well as the region where you want the job to run."
  },
  {
    "id": 479,
    "content": "The gcloud\ncommand gives you access to every GCP feature, including the ones we used earlier. You don\u2019t have to go through the web interface every time; you can write scripts that\nstart or stop VMs for you, deploy models, or perform any other GCP action. | Chapter 19: Training and Deploying TensorFlow Models at Scale\n22 At the time of this writing, the 2.0 runtime is not yet available, but it should be ready by the time you read\nthis. Check out the list of available runtimes. Before you can run the training job, you need to write the training code, exactly like\nyou did earlier for a distributed setup (e.g., using the ParameterServerStrategy). AI\nPlatform will take care of setting TF_CONFIG for you on each VM. Once that\u2019s done,\nyou can deploy it and run it on a TF cluster with a command line like this:\n$ gcloud ai-platform jobs submit training my_job_20190531_164700 \\ --region asia-southeast1 \\ --scale-tier PREMIUM_1 \\ --runtime-version 2.0 \\ --python-version 3.5 \\ --package-path /my_project/src/trainer \\ --module-name trainer.task \\ --staging-bucket gs://my-staging-bucket \\ --job-dir gs://my-mnist-model-bucket/trained_model \\ -- --my-extra-argument1 foo --my-extra-argument2 bar\nLet\u2019s go through these options. The command will start a training job named\nmy_job_20190531_164700, in the asia-southeast1 region, using a PREMIUM_1 scale\ntier: this corresponds to 20 workers (including a chief) and 11 parameter servers\n(check out the other available scale tiers). All these VMs will be based on AI Plat\u2010\nform\u2019s 2.0 runtime (a VM configuration that includes TensorFlow 2.0 and many other\npackages)22 and Python 3.5. The training code is located in the /my_project/src/trainer\ndirectory, and the gcloud command will automatically bundle it into a pip package\nand upload it to GCS at gs://my-staging-bucket. Next, AI Platform will start several\nVMs, deploy the package to them, and run the trainer.task module. Lastly, the --\njob-dir argument and the extra arguments (i.e., all the arguments located after the\n-- separator) will be passed to the training program: the chief task will usually use the\n--job-dir argument to find out where to save the final model on GCS, in this case at\ngs://my-mnist-model-bucket/trained_model. And that\u2019s it! In the GCP console, you can\nthen open the navigation menu, scroll down to the Artificial Intelligence section, and\nopen AI Platform \u2192 Jobs. You should see your job running, and if you click it you\nwill see graphs showing the CPU, GPU, and RAM utilization for every task. You can\nclick View Logs to access the detailed logs using Stackdriver. If you place the training data on GCS, you can create a\ntf.data.TextLineDataset or tf.data.TFRecordDataset to access\nit: just use the GCS paths as the filenames (e.g., gs://my-data-\nbucket/my_data_001.csv). These datasets rely on the tf.io.gfile\npackage to access files: it supports both local files and GCS files\n(but make sure the service account you use has access to GCS)."
  },
  {
    "id": 480,
    "content": "Training Models Across Multiple Devices | 23 Daniel Golovin et al., \u201cGoogle Vizier: A Service for Black-Box Optimization,\u201d Proceedings of the 23rd ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining (2017): 1487\u20131495. If you want to explore a few hyperparameter values, you can simply run multiple jobs\nand specify the hyperparameter values using the extra arguments for your tasks. However, if you want to explore many hyperparameters efficiently, it\u2019s a good idea to\nuse AI Platform\u2019s hyperparameter tuning service instead. Black Box Hyperparameter Tuning on AI Platform\nAI Platform provides a powerful Bayesian optimization hyperparameter tuning ser\u2010\nvice called Google Vizier.23 To use it, you need to pass a YAML configuration file\nwhen creating the job (--config tuning.yaml). For example, it may look like this:\ntrainingInput: hyperparameters: goal: MAXIMIZE hyperparameterMetricTag: accuracy maxTrials: 10 maxParallelTrials: 2 params: - parameterName: n_layers type: INTEGER minValue: 10 maxValue: 100 scaleType: UNIT_LINEAR_SCALE - parameterName: momentum type: DOUBLE minValue: 0.1 maxValue: 1.0 scaleType: UNIT_LOG_SCALE\nThis tells AI Platform that we want to maximize the metric named \"accuracy\", the\njob will run a maximum of 10 trials (each trial will run our training code to train the\nmodel from scratch), and it will run a maximum of 2 trials in parallel. We want it to\ntune two hyperparameters: the n_layers hyperparameter (an integer between 10 and\n100) and the momentum hyperparameter (a float between 0.1 and 1.0). The scaleType\nargument specifies the prior for the hyperparameter value: UNIT_LINEAR_SCALE\nmeans a flat prior (i.e., no a priori preference), while UNIT_LOG_SCALE says we have a\nprior belief that the optimal value lies closer to the max value (the other possible prior\nis UNIT_REVERSE_LOG_SCALE, when we believe the optimal value to be close to the min\nvalue). The n_layers and momentum arguments will be passed as command-line arguments\nto the training code, and of course it is expected to use them. The question is, how\nwill the training code communicate the metric back to the AI Platform so that it can | Chapter 19: Training and Deploying TensorFlow Models at Scale\ndecide which hyperparameter values to use during the next trial? Well, AI Platform\njust monitors the output directory (specified via --job-dir) for any event file (intro\u2010\nduced in Chapter 10) containing summaries for a metric named \"accuracy\" (or\nwhatever metric name is specified as the hyperparameterMetricTag), and it reads\nthose values. So your training code simply has to use the TensorBoard() callback\n(which you will want to do anyway for monitoring), and you\u2019re good to go! Once the job is finished, all the hyperparameter values used in each trial and the\nresulting accuracy will be available in the job\u2019s output (available via the AI Platform \u2192\nJobs page). AI Platform jobs can also be used to efficiently execute your model\non large amounts of data: each worker can read part of the data\nfrom GCS, make predictions, and save them to GCS."
  },
  {
    "id": 481,
    "content": "Now you have all the tools and knowledge you need to create state-of-the-art neural\nnet architectures and train them at scale using various distribution strategies, on your\nown infrastructure or on the cloud\u2014and you can even perform powerful Bayesian\noptimization to fine-tune the hyperparameters! Exercises\n1. What does a SavedModel contain? How do you inspect its content? 2. When should you use TF Serving? What are its main features? What are some\ntools you can use to deploy it? 3. How do you deploy a model across multiple TF Serving instances? 4. When should you use the gRPC API rather than the REST API to query a model\nserved by TF Serving? 5. What are the different ways TFLite reduces a model\u2019s size to make it run on a\nmobile or embedded device? 6. What is quantization-aware training, and why would you need it? 7. What are model parallelism and data parallelism? Why is the latter generally\nrecommended? 8. When training a model across multiple servers, what distribution strategies can\nyou use? How do you choose which one to use? 9. Train a model (any model you like) and deploy it to TF Serving or Google Cloud\nAI Platform. Write the client code to query it using the REST API or the gRPC\nExercises | API. Update the model and deploy the new version. Your client code will now\nquery the new version. Roll back to the first version. 10. Train any model across multiple GPUs on the same machine using the Mirrored\nStrategy (if you do not have access to GPUs, you can use Colaboratory with a\nGPU Runtime and create two virtual GPUs). Train the model again using the\nCentralStorageStrategy and compare the training time. 11. Train a small model on Google Cloud AI Platform, using black box hyperpara\u2010\nmeter tuning. Thank You! Before we close the last chapter of this book, I would like to thank you for reading it\nup to the last paragraph. I truly hope that you had as much pleasure reading this book\nas I had writing it, and that it will be useful for your projects, big or small. If you find errors, please send feedback. More generally, I would love to know what\nyou think, so please don\u2019t hesitate to contact me via O\u2019Reilly, through the ageron/\nhandson-ml2 GitHub project, or on Twitter at @aureliengeron. Going forward, my best advice to you is to practice and practice: try going through all\nthe exercises (if you have not done so already), play with the Jupyter notebooks, join\nKaggle.com or some other ML community, watch ML courses, read papers, attend\nconferences, and meet experts. It also helps tremendously to have a concrete project\nto work on, whether it is for work or for fun (ideally for both), so if there\u2019s anything\nyou have always dreamt of building, give it a shot! Work incrementally; don\u2019t shoot\nfor the moon right away, but stay focused on your project and build it piece by piece."
  },
  {
    "id": 482,
    "content": "It will require patience and perseverance, but when you have a walking robot, or a\nworking chatbot, or whatever else you fancy to build, it will be immensely rewarding. My greatest hope is that this book will inspire you to build a wonderful ML applica\u2010\ntion that will benefit all of us! What will it be? \u2014Aur\u00e9lien G\u00e9ron, June 17, 2019 | Chapter 19: Training and Deploying TensorFlow Models at Scale\nAPPENDIX A\nExercise Solutions\nSolutions to the coding exercises are available in the online Jupyter\nnotebooks at \nChapter 1: The Machine Learning Landscape\n1. Machine Learning is about building systems that can learn from data. Learning\nmeans getting better at some task, given some performance measure. 2. Machine Learning is great for complex problems for which we have no algorith\u2010\nmic solution, to replace long lists of hand-tuned rules, to build systems that adapt\nto fluctuating environments, and finally to help humans learn (e.g., data mining). 3. A labeled training set is a training set that contains the desired solution (a.k.a. a\nlabel) for each instance. 4. The two most common supervised tasks are regression and classification. 5. Common unsupervised tasks include clustering, visualization, dimensionality\nreduction, and association rule learning. 6. Reinforcement Learning is likely to perform best if we want a robot to learn to\nwalk in various unknown terrains, since this is typically the type of problem that\nReinforcement Learning tackles. It might be possible to express the problem as a\nsupervised or semisupervised learning problem, but it would be less natural. 7. If you don\u2019t know how to define the groups, then you can use a clustering algo\u2010\nrithm (unsupervised learning) to segment your customers into clusters of similar\ncustomers. However, if you know what groups you would like to have, then you can feed many examples of each group to a classification algorithm (supervised\nlearning), and it will classify all your customers into these groups. 8. Spam detection is a typical supervised learning problem: the algorithm is fed\nmany emails along with their labels (spam or not spam). 9. An online learning system can learn incrementally, as opposed to a batch learn\u2010\ning system. This makes it capable of adapting rapidly to both changing data and\nautonomous systems, and of training on very large quantities of data. 10. Out-of-core algorithms can handle vast quantities of data that cannot fit in a\ncomputer\u2019s main memory. An out-of-core learning algorithm chops the data into\nmini-batches and uses online learning techniques to learn from these mini-\nbatches. 11. An instance-based learning system learns the training data by heart; then, when\ngiven a new instance, it uses a similarity measure to find the most similar learned\ninstances and uses them to make predictions. 12. A model has one or more model parameters that determine what it will predict\ngiven a new instance (e.g., the slope of a linear model). A learning algorithm tries\nto find optimal values for these parameters such that the model generalizes well\nto new instances."
  },
  {
    "id": 483,
    "content": "A hyperparameter is a parameter of the learning algorithm\nitself, not of the model (e.g., the amount of regularization to apply). 13. Model-based learning algorithms search for an optimal value for the model\nparameters such that the model will generalize well to new instances. We usually\ntrain such systems by minimizing a cost function that measures how bad the sys\u2010\ntem is at making predictions on the training data, plus a penalty for model com\u2010\nplexity if the model is regularized. To make predictions, we feed the new\ninstance\u2019s features into the model\u2019s prediction function, using the parameter val\u2010\nues found by the learning algorithm. 14. Some of the main challenges in Machine Learning are the lack of data, poor data\nquality, nonrepresentative data, uninformative features, excessively simple mod\u2010\nels that underfit the training data, and excessively complex models that overfit\nthe data. 15. If a model performs great on the training data but generalizes poorly to new\ninstances, the model is likely overfitting the training data (or we got extremely\nlucky on the training data). Possible solutions to overfitting are getting more\ndata, simplifying the model (selecting a simpler algorithm, reducing the number\nof parameters or features used, or regularizing the model), or reducing the noise\nin the training data. 16. A test set is used to estimate the generalization error that a model will make on\nnew instances, before the model is launched in production. | Appendix A: Exercise Solutions\n17. A validation set is used to compare models. It makes it possible to select the best\nmodel and tune the hyperparameters. 18. The train-dev set is used when there is a risk of mismatch between the training\ndata and the data used in the validation and test datasets (which should always be\nas close as possible to the data used once the model is in production). The train-\ndev set is a part of the training set that\u2019s held out (the model is not trained on it). The model is trained on the rest of the training set, and evaluated on both the\ntrain-dev set and the validation set. If the model performs well on the training set\nbut not on the train-dev set, then the model is likely overfitting the training set. If\nit performs well on both the training set and the train-dev set, but not on the val\u2010\nidation set, then there is probably a significant data mismatch between the train\u2010\ning data and the validation + test data, and you should try to improve the\ntraining data to make it look more like the validation + test data. 19. If you tune hyperparameters using the test set, you risk overfitting the test set,\nand the generalization error you measure will be optimistic (you may launch a\nmodel that performs worse than you expect). Chapter 2: End-to-End Machine Learning Project\nSee the Jupyter notebooks available at \nChapter 3: Classification\nSee the Jupyter notebooks available at \nChapter 4: Training Models\n1."
  },
  {
    "id": 484,
    "content": "If you have a training set with millions of features you can use Stochastic Gradi\u2010\nent Descent or Mini-batch Gradient Descent, and perhaps Batch Gradient\nDescent if the training set fits in memory. But you cannot use the Normal Equa\u2010\ntion or the SVD approach because the computational complexity grows quickly\n(more than quadratically) with the number of features. 2. If the features in your training set have very different scales, the cost function will\nhave the shape of an elongated bowl, so the Gradient Descent algorithms will take\na long time to converge. To solve this you should scale the data before training\nthe model. Note that the Normal Equation or SVD approach will work just fine\nwithout scaling. Moreover, regularized models may converge to a suboptimal sol\u2010\nution if the features are not scaled: since regularization penalizes large weights,\nfeatures with smaller values will tend to be ignored compared to features with\nlarger values. Exercise Solutions | 1 If you draw a straight line between any two points on the curve, the line never crosses the curve. 3. Gradient Descent cannot get stuck in a local minimum when training a Logistic\nRegression model because the cost function is convex.1\n4. If the optimization problem is convex (such as Linear Regression or Logistic\nRegression), and assuming the learning rate is not too high, then all Gradient\nDescent algorithms will approach the global optimum and end up producing\nfairly similar models. However, unless you gradually reduce the learning rate,\nStochastic GD and Mini-batch GD will never truly converge; instead, they will\nkeep jumping back and forth around the global optimum. This means that even\nif you let them run for a very long time, these Gradient Descent algorithms will\nproduce slightly different models. 5. If the validation error consistently goes up after every epoch, then one possibility\nis that the learning rate is too high and the algorithm is diverging. If the training\nerror also goes up, then this is clearly the problem and you should reduce the\nlearning rate. However, if the training error is not going up, then your model is\noverfitting the training set and you should stop training. 6. Due to their random nature, neither Stochastic Gradient Descent nor Mini-batch\nGradient Descent is guaranteed to make progress at every single training itera\u2010\ntion. So if you immediately stop training when the validation error goes up, you\nmay stop much too early, before the optimum is reached. A better option is to\nsave the model at regular intervals; then, when it has not improved for a long\ntime (meaning it will probably never beat the record), you can revert to the best\nsaved model. 7. Stochastic Gradient Descent has the fastest training iteration since it considers\nonly one training instance at a time, so it is generally the first to reach the vicinity\nof the global optimum (or Mini-batch GD with a very small mini-batch size)."
  },
  {
    "id": 485,
    "content": "However, only Batch Gradient Descent will actually converge, given enough\ntraining time. As mentioned, Stochastic GD and Mini-batch GD will bounce\naround the optimum, unless you gradually reduce the learning rate. 8. If the validation error is much higher than the training error, this is likely because\nyour model is overfitting the training set. One way to try to fix this is to reduce\nthe polynomial degree: a model with fewer degrees of freedom is less likely to\noverfit. Another thing you can try is to regularize the model\u2014for example, by\nadding an \u21132 penalty (Ridge) or an \u21131 penalty (Lasso) to the cost function. This\nwill also reduce the degrees of freedom of the model. Lastly, you can try to\nincrease the size of the training set. | Appendix A: Exercise Solutions\n9. If both the training error and the validation error are almost equal and fairly\nhigh, the model is likely underfitting the training set, which means it has a high\nbias. You should try reducing the regularization hyperparameter \u03b1. 10. Let\u2019s see:\n\u2022 A model with some regularization typically performs better than a model\nwithout any regularization, so you should generally prefer Ridge Regression\nover plain Linear Regression. \u2022 Lasso Regression uses an \u21131 penalty, which tends to push the weights down to\nexactly zero. This leads to sparse models, where all weights are zero except for\nthe most important weights. This is a way to perform feature selection auto\u2010\nmatically, which is good if you suspect that only a few features actually matter. When you are not sure, you should prefer Ridge Regression. \u2022 Elastic Net is generally preferred over Lasso since Lasso may behave erratically\nin some cases (when several features are strongly correlated or when there are\nmore features than training instances). However, it does add an extra hyper\u2010\nparameter to tune. If you want Lasso without the erratic behavior, you can just\nuse Elastic Net with an l1_ratio close to 1. 11. If you want to classify pictures as outdoor/indoor and daytime/nighttime, since\nthese are not exclusive classes (i.e., all four combinations are possible) you should\ntrain two Logistic Regression classifiers. 12. See the Jupyter notebooks available at \nChapter 5: Support Vector Machines\n1. The fundamental idea behind Support Vector Machines is to fit the widest possi\u2010\nble \u201cstreet\u201d between the classes. In other words, the goal is to have the largest pos\u2010\nsible margin between the decision boundary that separates the two classes and\nthe training instances. When performing soft margin classification, the SVM\nsearches for a compromise between perfectly separating the two classes and hav\u2010\ning the widest possible street (i.e., a few instances may end up on the street). Another key idea is to use kernels when training on nonlinear datasets. 2. After training an SVM, a support vector is any instance located on the \u201cstreet\u201d (see\nthe previous answer), including its border. The decision boundary is entirely\ndetermined by the support vectors."
  },
  {
    "id": 486,
    "content": "Any instance that is not a support vector (i.e.,\nis off the street) has no influence whatsoever; you could remove them, add more\ninstances, or move them around, and as long as they stay off the street they won\u2019t\naffect the decision boundary. Computing the predictions only involves the sup\u2010\nport vectors, not the whole training set. Exercise Solutions | 3. SVMs try to fit the largest possible \u201cstreet\u201d between the classes (see the first\nanswer), so if the training set is not scaled, the SVM will tend to neglect small\nfeatures (see Figure 5-2). 4. An SVM classifier can output the distance between the test instance and the deci\u2010\nsion boundary, and you can use this as a confidence score. However, this score\ncannot be directly converted into an estimation of the class probability. If you set\nprobability=True when creating an SVM in Scikit-Learn, then after training it\nwill calibrate the probabilities using Logistic Regression on the SVM\u2019s scores\n(trained by an additional five-fold cross-validation on the training data). This\nwill add the predict_proba() and predict_log_proba() methods to the SVM. 5. This question applies only to linear SVMs since kernelized SVMs can only use\nthe dual form. The computational complexity of the primal form of the SVM\nproblem is proportional to the number of training instances m, while the compu\u2010\ntational complexity of the dual form is proportional to a number between m2 and\nm3. So if there are millions of instances, you should definitely use the primal\nform, because the dual form will be much too slow. 6. If an SVM classifier trained with an RBF kernel underfits the training set, there\nmight be too much regularization. To decrease it, you need to increase gamma or C\n(or both). 7. Let\u2019s call the QP parameters for the hard margin problem H\u2032, f\u2032, A\u2032, and b\u2032 (see\n\u201cQuadratic Programming\u201d on page 167). The QP parameters for the soft margin\nproblem have m additional parameters (np = n + 1 + m) and m additional con\u2010\nstraints (nc = 2m). They can be defined like so:\n\u2022 H is equal to H\u2032, plus m columns of 0s on the right and m rows of 0s at the\nbottom: H =\nH\u2032 0 \u22ef\n0 0\n\u22ee\n\u22f1\n\u2022 f is equal to f\u2032 with m additional elements, all equal to the value of the hyper\u2010\nparameter C.\n\u2022 b is equal to b\u2032 with m additional elements, all equal to 0. \u2022 A is equal to A\u2032, with an extra m \u00d7 m identity matrix Im appended to the right,\n\u2013*I*m just below it, and the rest filled with 0s: A =\nA\u2032 Im\n0 \u2212Im\nFor the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available\nat  | Appendix A: Exercise Solutions\n2 log2 is the binary log; log2(m) = log(m) / log(2). Chapter 6: Decision Trees\n1. The depth of a well-balanced binary tree containing m leaves is equal to log2(m),2\nrounded up."
  },
  {
    "id": 487,
    "content": "A binary Decision Tree (one that makes only binary decisions, as is\nthe case with all trees in Scikit-Learn) will end up more or less well balanced at\nthe end of training, with one leaf per training instance if it is trained without\nrestrictions. Thus, if the training set contains one million instances, the Decision\nTree will have a depth of log2(106) \u2248 20 (actually a bit more since the tree will\ngenerally not be perfectly well balanced). 2. A node\u2019s Gini impurity is generally lower than its parent\u2019s. This is due to the\nCART training algorithm\u2019s cost function, which splits each node in a way that\nminimizes the weighted sum of its children\u2019s Gini impurities. However, it is possi\u2010\nble for a node to have a higher Gini impurity than its parent, as long as this\nincrease is more than compensated for by a decrease in the other child\u2019s impurity. For example, consider a node containing four instances of class A and one of\nclass B. Its Gini impurity is 1 \u2013 (1/5)2 \u2013 (4/5)2 = 0.32. Now suppose the dataset is\none-dimensional and the instances are lined up in the following order: A, B, A,\nA, A. You can verify that the algorithm will split this node after the second\ninstance, producing one child node with instances A, B, and the other child node\nwith instances A, A, A. The first child node\u2019s Gini impurity is 1 \u2013 (1/2)2 \u2013 (1/2)2 =\n0.5, which is higher than its parent\u2019s. This is compensated for by the fact that the\nother node is pure, so its overall weighted Gini impurity is 2/5 \u00d7 0.5 + 3/5 \u00d7 0 =\n0.2, which is lower than the parent\u2019s Gini impurity. 3. If a Decision Tree is overfitting the training set, it may be a good idea to decrease\nmax_depth, since this will constrain the model, regularizing it. 4. Decision Trees don\u2019t care whether or not the training data is scaled or centered;\nthat\u2019s one of the nice things about them. So if a Decision Tree underfits the train\u2010\ning set, scaling the input features will just be a waste of time. 5. The computational complexity of training a Decision Tree is O(n \u00d7 m log(m)). So\nif you multiply the training set size by 10, the training time will be multiplied by\nK = (n \u00d7 10m \u00d7 log(10m)) / (n \u00d7 m \u00d7 log(m)) = 10 \u00d7 log(10m) / log(m). If m =\n106, then K \u2248 11.7, so you can expect the training time to be roughly 11.7 hours. 6. Presorting the training set speeds up training only if the dataset is smaller than a\nfew thousand instances. If it contains 100,000 instances, setting presort=True\nwill considerably slow down training. For the solutions to exercises 7 and 8, please see the Jupyter notebooks available at\n\nExercise Solutions | Chapter 7: Ensemble Learning and Random Forests\n1."
  },
  {
    "id": 488,
    "content": "If you have trained five different models and they all achieve 95% precision, you\ncan try combining them into a voting ensemble, which will often give you even\nbetter results. It works better if the models are very different (e.g., an SVM classi\u2010\nfier, a Decision Tree classifier, a Logistic Regression classifier, and so on). It is\neven better if they are trained on different training instances (that\u2019s the whole\npoint of bagging and pasting ensembles), but if not this will still be effective as\nlong as the models are very different. 2. A hard voting classifier just counts the votes of each classifier in the ensemble\nand picks the class that gets the most votes. A soft voting classifier computes the\naverage estimated class probability for each class and picks the class with the\nhighest probability. This gives high-confidence votes more weight and often per\u2010\nforms better, but it works only if every classifier is able to estimate class probabil\u2010\nities (e.g., for the SVM classifiers in Scikit-Learn you must set\nprobability=True). 3. It is quite possible to speed up training of a bagging ensemble by distributing it\nacross multiple servers, since each predictor in the ensemble is independent of\nthe others. The same goes for pasting ensembles and Random Forests, for the\nsame reason. However, each predictor in a boosting ensemble is built based on\nthe previous predictor, so training is necessarily sequential, and you will not gain\nanything by distributing training across multiple servers. Regarding stacking\nensembles, all the predictors in a given layer are independent of each other, so\nthey can be trained in parallel on multiple servers. However, the predictors in one\nlayer can only be trained after the predictors in the previous layer have all been\ntrained. 4. With out-of-bag evaluation, each predictor in a bagging ensemble is evaluated\nusing instances that it was not trained on (they were held out). This makes it pos\u2010\nsible to have a fairly unbiased evaluation of the ensemble without the need for an\nadditional validation set. Thus, you have more instances available for training,\nand your ensemble can perform slightly better. 5. When you are growing a tree in a Random Forest, only a random subset of the\nfeatures is considered for splitting at each node. This is true as well for Extra-\nTrees, but they go one step further: rather than searching for the best possible\nthresholds, like regular Decision Trees do, they use random thresholds for each\nfeature. This extra randomness acts like a form of regularization: if a Random\nForest overfits the training data, Extra-Trees might perform better. Moreover,\nsince Extra-Trees don\u2019t search for the best possible thresholds, they are much\nfaster to train than Random Forests. However, they are neither faster nor slower\nthan Random Forests when making predictions. | Appendix A: Exercise Solutions\n6. If your AdaBoost ensemble underfits the training data, you can try increasing the\nnumber of estimators or reducing the regularization hyperparameters of the base\nestimator."
  },
  {
    "id": 489,
    "content": "You may also try slightly increasing the learning rate. 7. If your Gradient Boosting ensemble overfits the training set, you should try\ndecreasing the learning rate. You could also use early stopping to find the right\nnumber of predictors (you probably have too many). For the solutions to exercises 8 and 9, please see the Jupyter notebooks available at\n\nChapter 8: Dimensionality Reduction\n1. The main motivations for dimensionality reduction are:\n\u2022 To speed up a subsequent training algorithm (in some cases it may even\nremove noise and redundant features, making the training algorithm perform\nbetter)\n\u2022 To visualize the data and gain insights on the most important features\n\u2022 To save space (compression)\nThe main drawbacks are:\n\u2022 Some information is lost, possibly degrading the performance of subsequent\ntraining algorithms. \u2022 It can be computationally intensive. \u2022 It adds some complexity to your Machine Learning pipelines. \u2022 Transformed features are often hard to interpret. 2. The curse of dimensionality refers to the fact that many problems that do not\nexist in low-dimensional space arise in high-dimensional space. In Machine\nLearning, one common manifestation is the fact that randomly sampled high-\ndimensional vectors are generally very sparse, increasing the risk of overfitting\nand making it very difficult to identify patterns in the data without having plenty\nof training data. 3. Once a dataset\u2019s dimensionality has been reduced using one of the algorithms we\ndiscussed, it is almost always impossible to perfectly reverse the operation,\nbecause some information gets lost during dimensionality reduction. Moreover,\nwhile some algorithms (such as PCA) have a simple reverse transformation\nprocedure that can reconstruct a dataset relatively similar to the original, other\nalgorithms (such as T-SNE) do not. Exercise Solutions | 4. PCA can be used to significantly reduce the dimensionality of most datasets, even\nif they are highly nonlinear, because it can at least get rid of useless dimensions. However, if there are no useless dimensions\u2014as in a Swiss roll dataset\u2014then\nreducing dimensionality with PCA will lose too much information. You want to\nunroll the Swiss roll, not squash it. 5. That\u2019s a trick question: it depends on the dataset. Let\u2019s look at two extreme exam\u2010\nples. First, suppose the dataset is composed of points that are almost perfectly\naligned. In this case, PCA can reduce the dataset down to just one dimension\nwhile still preserving 95% of the variance. Now imagine that the dataset is com\u2010\nposed of perfectly random points, scattered all around the 1,000 dimensions. In\nthis case roughly 950 dimensions are required to preserve 95% of the variance. So\nthe answer is, it depends on the dataset, and it could be any number between 1\nand 950. Plotting the explained variance as a function of the number of dimen\u2010\nsions is one way to get a rough idea of the dataset\u2019s intrinsic dimensionality. 6. Regular PCA is the default, but it works only if the dataset fits in memory."
  },
  {
    "id": 490,
    "content": "Incre\u2010\nmental PCA is useful for large datasets that don\u2019t fit in memory, but it is slower\nthan regular PCA, so if the dataset fits in memory you should prefer regular\nPCA. Incremental PCA is also useful for online tasks, when you need to apply\nPCA on the fly, every time a new instance arrives. Randomized PCA is useful\nwhen you want to considerably reduce dimensionality and the dataset fits in\nmemory; in this case, it is much faster than regular PCA. Finally, Kernel PCA is\nuseful for nonlinear datasets. 7. Intuitively, a dimensionality reduction algorithm performs well if it eliminates a\nlot of dimensions from the dataset without losing too much information. One\nway to measure this is to apply the reverse transformation and measure the\nreconstruction error. However, not all dimensionality reduction algorithms pro\u2010\nvide a reverse transformation. Alternatively, if you are using dimensionality\nreduction as a preprocessing step before another Machine Learning algorithm\n(e.g., a Random Forest classifier), then you can simply measure the performance\nof that second algorithm; if dimensionality reduction did not lose too much\ninformation, then the algorithm should perform just as well as when using the\noriginal dataset. 8. It can absolutely make sense to chain two different dimensionality reduction\nalgorithms. A common example is using PCA to quickly get rid of a large num\u2010\nber of useless dimensions, then applying another much slower dimensionality\nreduction algorithm, such as LLE. This two-step approach will likely yield the\nsame performance as using LLE only, but in a fraction of the time. For the solutions to exercises 9 and 10, please see the Jupyter notebooks available at\n | Appendix A: Exercise Solutions\nChapter 9: Unsupervised Learning Techniques\n1. In Machine Learning, clustering is the unsupervised task of grouping similar\ninstances together. The notion of similarity depends on the task at hand: for\nexample, in some cases two nearby instances will be considered similar, while in\nothers similar instances may be far apart as long as they belong to the same\ndensely packed group. Popular clustering algorithms include K-Means,\nDBSCAN, agglomerative clustering, BIRCH, Mean-Shift, affinity propagation,\nand spectral clustering. 2. The main applications of clustering algorithms include data analysis, customer\nsegmentation, recommender systems, search engines, image segmentation, semi-\nsupervised learning, dimensionality reduction, anomaly detection, and novelty\ndetection. 3. The elbow rule is a simple technique to select the number of clusters when using\nK-Means: just plot the inertia (the mean squared distance from each instance to\nits nearest centroid) as a function of the number of clusters, and find the point in\nthe curve where the inertia stops dropping fast (the \u201celbow\u201d). This is generally\nclose to the optimal number of clusters. Another approach is to plot the silhou\u2010\nette score as a function of the number of clusters. There will often be a peak, and\nthe optimal number of clusters is generally nearby. The silhouette score is the\nmean silhouette coefficient over all instances."
  },
  {
    "id": 491,
    "content": "This coefficient varies from +1 for\ninstances that are well inside their cluster and far from other clusters, to \u20131 for\ninstances that are very close to another cluster. You may also plot the silhouette\ndiagrams and perform a more thorough analysis. 4. Labeling a dataset is costly and time-consuming. Therefore, it is common to have\nplenty of unlabeled instances, but few labeled instances. Label propagation is a\ntechnique that consists in copying some (or all) of the labels from the labeled\ninstances to similar unlabeled instances. This can greatly extend the number of\nlabeled instances, and thereby allow a supervised algorithm to reach better per\u2010\nformance (this is a form of semi-supervised learning). One approach is to use a\nclustering algorithm such as K-Means on all the instances, then for each cluster\nfind the most common label or the label of the most representative instance (i.e.,\nthe one closest to the centroid) and propagate it to the unlabeled instances in the\nsame cluster. 5. K-Means and BIRCH scale well to large datasets. DBSCAN and Mean-Shift look\nfor regions of high density. 6. Active learning is useful whenever you have plenty of unlabeled instances but\nlabeling is costly. In this case (which is very common), rather than randomly\nselecting instances to label, it is often preferable to perform active learning,\nwhere human experts interact with the learning algorithm, providing labels for\nExercise Solutions | specific instances when the algorithm requests them. A common approach is\nuncertainty sampling (see the description in \u201cActive Learning\u201d on page 255). 7. Many people use the terms anomaly detection and novelty detection interchangea\u2010\nbly, but they are not exactly the same. In anomaly detection, the algorithm is\ntrained on a dataset that may contain outliers, and the goal is typically to identify\nthese outliers (within the training set), as well as outliers among new instances. In novelty detection, the algorithm is trained on a dataset that is presumed to be\n\u201cclean,\u201d and the objective is to detect novelties strictly among new instances. Some algorithms work best for anomaly detection (e.g., Isolation Forest), while\nothers are better suited for novelty detection (e.g., one-class SVM). 8. A Gaussian mixture model (GMM) is a probabilistic model that assumes that the\ninstances were generated from a mixture of several Gaussian distributions whose\nparameters are unknown. In other words, the assumption is that the data is grou\u2010\nped into a finite number of clusters, each with an ellipsoidal shape (but the clus\u2010\nters may have different ellipsoidal shapes, sizes, orientations, and densities), and\nwe don\u2019t know which cluster each instance belongs to. This model is useful for\ndensity estimation, clustering, and anomaly detection. 9. One way to find the right number of clusters when using a Gaussian mixture\nmodel is to plot the Bayesian information criterion (BIC) or the Akaike informa\u2010\ntion criterion (AIC) as a function of the number of clusters, then choose the\nnumber of clusters that minimizes the BIC or AIC."
  },
  {
    "id": 492,
    "content": "Another technique is to use a\nBayesian Gaussian mixture model, which automatically selects the number of\nclusters. For the solutions to exercises 10 to 13, please see the Jupyter notebooks available at\n\nChapter 10: Introduction to Artificial Neural Networks\nwith Keras\n1. Visit the TensorFlow Playground and play around with it, as described in this\nexercise. 2. Here is a neural network based on the original artificial neurons that computes A\n\u2295 B (where \u2295 represents the exclusive OR), using the fact that A \u2295 B = (A \u2227 \u00ac B)\n\u2228 (\u00ac A \u2227 B). There are other solutions\u2014for example, using the fact that A \u2295 B =\n(A \u2228 B) \u2227 \u00ac(A \u2227 B), or the fact that A \u2295 B = (A \u2228 B) \u2227 (\u00ac A \u2228 \u2227 B), and so on. | Appendix A: Exercise Solutions\n3. A classical Perceptron will converge only if the dataset is linearly separable, and it\nwon\u2019t be able to estimate class probabilities. In contrast, a Logistic Regression\nclassifier will converge to a good solution even if the dataset is not linearly sepa\u2010\nrable, and it will output class probabilities. If you change the Perceptron\u2019s activa\u2010\ntion function to the logistic activation function (or the softmax activation\nfunction if there are multiple neurons), and if you train it using Gradient Descent\n(or some other optimization algorithm minimizing the cost function, typically\ncross entropy), then it becomes equivalent to a Logistic Regression classifier. 4. The logistic activation function was a key ingredient in training the first MLPs\nbecause its derivative is always nonzero, so Gradient Descent can always roll\ndown the slope. When the activation function is a step function, Gradient\nDescent cannot move, as there is no slope at all. 5. Popular activation functions include the step function, the logistic (sigmoid)\nfunction, the hyperbolic tangent (tanh) function, and the Rectified Linear Unit\n(ReLU) function (see Figure 10-8). See Chapter 11 for other examples, such as\nELU and variants of the ReLU function. 6. Considering the MLP described in the question, composed of one input layer\nwith 10 passthrough neurons, followed by one hidden layer with 50 artificial neu\u2010\nrons, and finally one output layer with 3 artificial neurons, where all artificial\nneurons use the ReLU activation function: ..The shape of the input matrix X is m\n\u00d7 10, where m represents the training batch size. a. The shape of the hidden layer\u2019s weight vector Wh is 10 \u00d7 50, and the length of\nits bias vector bh is 50.\nb. The shape of the output layer\u2019s weight vector Wo is 50 \u00d7 3, and the length of its\nbias vector bo is 3.\nc. The shape of the network\u2019s output matrix Y is m \u00d7 3.\nd. Y* = ReLU(ReLU(X Wh + bh) Wo + bo). Recall that the ReLU function just sets\nevery negative number in the matrix to zero."
  },
  {
    "id": 493,
    "content": "Also note that when you are\nadding a bias vector to a matrix, it is added to every single row in the matrix,\nwhich is called broadcasting. Exercise Solutions | 3 When the values to predict can vary by many orders of magnitude, you may want to predict the logarithm of\nthe target value rather than the target value directly. Simply computing the exponential of the neural network\u2019s\noutput will give you the estimated value (since exp(log v) = v). 4 In Chapter 11 we discuss many techniques that introduce additional hyperparameters: type of weight initiali\u2010\nzation, activation function hyperparameters (e.g., the amount of leak in leaky ReLU), Gradient Clipping thres\u2010\nhold, type of optimizer and its hyperparameters (e.g., the momentum hyperparameter when using a\nMomentumOptimizer), type of regularization for each layer and regularization hyperparameters (e.g., dropout\nrate when using dropout), and so on. 7. To classify email into spam or ham, you just need one neuron in the output layer\nof a neural network\u2014for example, indicating the probability that the email is\nspam. You would typically use the logistic activation function in the output layer\nwhen estimating a probability. If instead you want to tackle MNIST, you need 10\nneurons in the output layer, and you must replace the logistic function with the\nsoftmax activation function, which can handle multiple classes, outputting one\nprobability per class. If you want your neural network to predict housing prices\nlike in Chapter 2, then you need one output neuron, using no activation function\nat all in the output layer.3\n8. Backpropagation is a technique used to train artificial neural networks. It first\ncomputes the gradients of the cost function with regard to every model parame\u2010\nter (all the weights and biases), then it performs a Gradient Descent step using\nthese gradients. This backpropagation step is typically performed thousands or\nmillions of times, using many training batches, until the model parameters con\u2010\nverge to values that (hopefully) minimize the cost function. To compute the gra\u2010\ndients, backpropagation uses reverse-mode autodiff (although it wasn\u2019t called\nthat when backpropagation was invented, and it has been reinvented several\ntimes). Reverse-mode autodiff performs a forward pass through a computation\ngraph, computing every node\u2019s value for the current training batch, and then it\nperforms a reverse pass, computing all the gradients at once (see Appendix D for\nmore details). So what\u2019s the difference? Well, backpropagation refers to the whole\nprocess of training an artificial neural network using multiple backpropagation\nsteps, each of which computes gradients and uses them to perform a Gradient\nDescent step. In contrast, reverse-mode autodiff is just a technique to compute\ngradients efficiently, and it happens to be used by backpropagation. 9."
  },
  {
    "id": 494,
    "content": "Here is a list of all the hyperparameters you can tweak in a basic MLP: the num\u2010\nber of hidden layers, the number of neurons in each hidden layer, and the activa\u2010\ntion function used in each hidden layer and in the output layer.4 In general, the\nReLU activation function (or one of its variants; see Chapter 11) is a good default\nfor the hidden layers. For the output layer, in general you will want the logistic\nactivation function for binary classification, the softmax activation function for\nmulticlass classification, or no activation function for regression. | Appendix A: Exercise Solutions\nIf the MLP overfits the training data, you can try reducing the number of hidden\nlayers and reducing the number of neurons per hidden layer. 10. See the Jupyter notebooks available at \nChapter 11: Training Deep Neural Networks\n1. No, all weights should be sampled independently; they should not all have the\nsame initial value. One important goal of sampling weights randomly is to break\nsymmetry: if all the weights have the same initial value, even if that value is not\nzero, then symmetry is not broken (i.e., all neurons in a given layer are equiva\u2010\nlent), and backpropagation will be unable to break it. Concretely, this means that\nall the neurons in any given layer will always have the same weights. It\u2019s like hav\u2010\ning just one neuron per layer, and much slower. It is virtually impossible for such\na configuration to converge to a good solution. 2. It is perfectly fine to initialize the bias terms to zero. Some people like to initialize\nthem just like weights, and that\u2019s okay too; it does not make much difference. 3. A few advantages of the SELU function over the ReLU function are:\n\u2022 It can take on negative values, so the average output of the neurons in any\ngiven layer is typically closer to zero than when using the ReLU activation\nfunction (which never outputs negative values). This helps alleviate the vanish\u2010\ning gradients problem. \u2022 It always has a nonzero derivative, which avoids the dying units issue that can\naffect ReLU units. \u2022 When the conditions are right (i.e., if the model is sequential, and the weights\nare initialized using LeCun initialization, and the inputs are standardized, and\nthere\u2019s no incompatible layer or regularization, such as dropout or \u21131 regulari\u2010\nzation), then the SELU activation function ensures the model is self-\nnormalized, which solves the exploding/vanishing gradients problems. 4. The SELU activation function is a good default. If you need the neural network to\nbe as fast as possible, you can use one of the leaky ReLU variants instead (e.g., a\nsimple leaky ReLU using the default hyperparameter value). The simplicity of the\nReLU activation function makes it many people\u2019s preferred option, despite the\nfact that it is generally outperformed by SELU and leaky ReLU. However, the\nReLU activation function\u2019s ability to output precisely zero can be useful in some\ncases (e.g., see Chapter 17)."
  },
  {
    "id": 495,
    "content": "Moreover, it can sometimes benefit from optimized\nimplementation as well as from hardware acceleration. The hyperbolic tangent\n(tanh) can be useful in the output layer if you need to output a number between\n\u20131 and 1, but nowadays it is not used much in hidden layers (except in recurrent\nExercise Solutions | nets). The logistic activation function is also useful in the output layer when you\nneed to estimate a probability (e.g., for binary classification), but is rarely used in\nhidden layers (there are exceptions\u2014for example, for the coding layer of varia\u2010\ntional autoencoders; see Chapter 17). Finally, the softmax activation function is\nuseful in the output layer to output probabilities for mutually exclusive classes,\nbut it is rarely (if ever) used in hidden layers. 5. If you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using\nan SGD optimizer, then the algorithm will likely pick up a lot of speed, hopefully\nmoving roughly toward the global minimum, but its momentum will carry it\nright past the minimum. Then it will slow down and come back, accelerate again,\novershoot again, and so on. It may oscillate this way many times before converg\u2010\ning, so overall it will take much longer to converge than with a smaller momentum\nvalue. 6. One way to produce a sparse model (i.e., with most weights equal to zero) is to\ntrain the model normally, then zero out tiny weights. For more sparsity, you can\napply \u21131 regularization during training, which pushes the optimizer toward spar\u2010\nsity. A third option is to use the TensorFlow Model Optimization Toolkit. 7. Yes, dropout does slow down training, in general roughly by a factor of two. However, it has no impact on inference speed since it is only turned on during\ntraining. MC Dropout is exactly like dropout during training, but it is still active\nduring inference, so each inference is slowed down slightly. More importantly,\nwhen using MC Dropout you generally want to run inference 10 times or more\nto get better predictions. This means that making predictions is slowed down by\na factor of 10 or more. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available\nat \nChapter 12: Custom Models and Training with TensorFlow\n1. TensorFlow is an open-source library for numerical computation, particularly\nwell suited and fine-tuned for large-scale Machine Learning. Its core is similar to\nNumPy, but it also features GPU support, support for distributed computing,\ncomputation graph analysis and optimization capabilities (with a portable graph\nformat that allows you to train a TensorFlow model in one environment and run\nit in another), an optimization API based on reverse-mode autodiff, and several\npowerful APIs such as tf.keras, tf.data, tf.image, tf.signal, and more. Other popu\u2010\nlar Deep Learning libraries include PyTorch, MXNet, Microsoft Cognitive Tool\u2010\nkit, Theano, Caffe2, and Chainer. 2. Although TensorFlow offers most of the functionalities provided by NumPy, it is\nnot a drop-in replacement, for a few reasons."
  },
  {
    "id": 496,
    "content": "First, the names of the functions are | Appendix A: Exercise Solutions\nnot always the same (for example, tf.reduce_sum() versus np.sum()). Second,\nsome functions do not behave in exactly the same way (for example, tf.trans\npose() creates a transposed copy of a tensor, while NumPy\u2019s T attribute creates a\ntransposed view, without actually copying any data). Lastly, NumPy arrays are\nmutable, while TensorFlow tensors are not (but you can use a tf.Variable if you\nneed a mutable object). 3. Both tf.range(10) and tf.constant(np.arange(10)) return a one-\ndimensional tensor containing the integers 0 to 9. However, the former uses 32-\nbit integers while the latter uses 64-bit integers. Indeed, TensorFlow defaults to\n32 bits, while NumPy defaults to 64 bits. 4. Beyond regular tensors, TensorFlow offers several other data structures, includ\u2010\ning sparse tensors, tensor arrays, ragged tensors, queues, string tensors, and sets. The last two are actually represented as regular tensors, but TensorFlow provides\nspecial functions to manipulate them (in tf.strings and tf.sets). 5. When you want to define a custom loss function, in general you can just imple\u2010\nment it as a regular Python function. However, if your custom loss function must\nsupport some hyperparameters (or any other state), then you should subclass the\nkeras.losses.Loss class and implement the __init__() and call() methods. If\nyou want the loss function\u2019s hyperparameters to be saved along with the model,\nthen you must also implement the get_config() method. 6. Much like custom loss functions, most metrics can be defined as regular Python\nfunctions. But if you want your custom metric to support some hyperparameters\n(or any other state), then you should subclass the keras.metrics.Metric class. Moreover, if computing the metric over a whole epoch is not equivalent to com\u2010\nputing the mean metric over all batches in that epoch (e.g., as for the precision\nand recall metrics), then you should subclass the keras.metrics.Metric class\nand implement the __init__(), update_state(), and result() methods to keep\ntrack of a running metric during each epoch. You should also implement the\nreset_states() method unless all it needs to do is reset all variables to 0.0. If\nyou want the state to be saved along with the model, then you should implement\nthe get_config() method as well. 7. You should distinguish the internal components of your model (i.e., layers or\nreusable blocks of layers) from the model itself (i.e., the object you will train). The former should subclass the keras.layers.Layer class, while the latter\nshould subclass the keras.models.Model class. 8. Writing your own custom training loop is fairly advanced, so you should only do\nit if you really need to. Keras provides several tools to customize training without\nhaving to write a custom training loop: callbacks, custom regularizers, custom\nconstraints, custom losses, and so on. You should use these instead of writing a\ncustom training loop whenever possible: writing a custom training loop is more\nExercise Solutions | error-prone, and it will be harder to reuse the custom code you write."
  },
  {
    "id": 497,
    "content": "However,\nin some cases writing a custom training loop is necessary\u2014for example, if you\nwant to use different optimizers for different parts of your neural network, like in\nthe Wide & Deep paper. A custom training loop can also be useful when debug\u2010\nging, or when trying to understand exactly how training works. 9. Custom Keras components should be convertible to TF Functions, which means\nthey should stick to TF operations as much as possible and respect all the rules\nlisted in \u201cTF Function Rules\u201d on page 409. If you absolutely need to include arbi\u2010\ntrary Python code in a custom component, you can either wrap it in a\ntf.py_function() operation (but this will reduce performance and limit your\nmodel\u2019s portability) or set dynamic=True when creating the custom layer or\nmodel (or set run_eagerly=True when calling the model\u2019s compile() method). 10. Please refer to \u201cTF Function Rules\u201d on page 409 for the list of rules to respect\nwhen creating a TF Function. 11. Creating a dynamic Keras model can be useful for debugging, as it will not com\u2010\npile any custom component to a TF Function, and you can use any Python\ndebugger to debug your code. It can also be useful if you want to include arbi\u2010\ntrary Python code in your model (or in your training code), including calls to\nexternal libraries. To make a model dynamic, you must set dynamic=True when\ncreating it. Alternatively, you can set run_eagerly=True when calling the model\u2019s\ncompile() method. Making a model dynamic prevents Keras from using any of\nTensorFlow\u2019s graph features, so it will slow down training and inference, and you\nwill not have the possibility to export the computation graph, which will limit\nyour model\u2019s portability. For the solutions to exercises 12 and 13, please see the Jupyter notebooks available at\n\nChapter 13: Loading and Preprocessing Data with\nTensorFlow\n1. Ingesting a large dataset and preprocessing it efficiently can be a complex engi\u2010\nneering challenge. The Data API makes it fairly simple. It offers many features,\nincluding loading data from various sources (such as text or binary files), reading\ndata in parallel from multiple sources, transforming it, interleaving the records,\nshuffling the data, batching it, and prefetching it. 2. Splitting a large dataset into multiple files makes it possible to shuffle it at a\ncoarse level before shuffling it at a finer level using a shuffling buffer. It also\nmakes it possible to handle huge datasets that do not fit on a single machine. It\u2019s\nalso simpler to manipulate thousands of small files rather than one huge file; for | Appendix A: Exercise Solutions\nexample, it\u2019s easier to split the data into multiple subsets. Lastly, if the data is split\nacross multiple files spread across multiple servers, it is possible to download sev\u2010\neral files from different servers simultaneously, which improves the bandwidth\nusage. 3."
  },
  {
    "id": 498,
    "content": "You can use TensorBoard to visualize profiling data: if the GPU is not fully uti\u2010\nlized then your input pipeline is likely to be the bottleneck. You can fix it by mak\u2010\ning sure it reads and preprocesses the data in multiple threads in parallel, and\nensuring it prefetches a few batches. If this is insufficient to get your GPU to\n100% usage during training, make sure your preprocessing code is optimized. You can also try saving the dataset into multiple TFRecord files, and if necessary\nperform some of the preprocessing ahead of time so that it does not need to be\ndone on the fly during training (TF Transform can help with this). If necessary,\nuse a machine with more CPU and RAM, and ensure that the GPU bandwidth is\nlarge enough. 4. A TFRecord file is composed of a sequence of arbitrary binary records: you can\nstore absolutely any binary data you want in each record. However, in practice\nmost TFRecord files contain sequences of serialized protocol buffers. This makes\nit possible to benefit from the advantages of protocol buffers, such as the fact that\nthey can be read easily across multiple platforms and languages and their defini\u2010\ntion can be updated later in a backward-compatible way. 5. The Example protobuf format has the advantage that TensorFlow provides some\noperations to parse it (the tf.io.parse*example() functions) without you hav\u2010\ning to define your own format. It is sufficiently flexible to represent instances in\nmost datasets. However, if it does not cover your use case, you can define your\nown protocol buffer, compile it using protoc (setting the --descriptor_set_out\nand --include_imports arguments to export the protobuf descriptor), and use\nthe tf.io.decode_proto() function to parse the serialized protobufs (see the\n\u201cCustom protobuf\u201d section of the notebook for an example). It\u2019s more compli\u2010\ncated, and it requires deploying the descriptor along with the model, but it can be\ndone. 6. When using TFRecords, you will generally want to activate compression if the\nTFRecord files will need to be downloaded by the training script, as compression\nwill make files smaller and thus reduce download time. But if the files are located\non the same machine as the training script, it\u2019s usually preferable to leave com\u2010\npression off, to avoid wasting CPU for decompression. 7. Let\u2019s look at the pros and cons of each preprocessing option:\n\u2022 If you preprocess the data when creating the data files, the training script will\nrun faster, since it will not have to perform preprocessing on the fly. In some\ncases, the preprocessed data will also be much smaller than the original data, so\nyou can save some space and speed up downloads. It may also be helpful to\nExercise Solutions | materialize the preprocessed data, for example to inspect it or archive it. How\u2010\never, this approach has a few cons. First, it\u2019s not easy to experiment with vari\u2010\nous preprocessing logics if you need to generate a preprocessed dataset for\neach variant."
  },
  {
    "id": 499,
    "content": "Second, if you want to perform data augmentation, you have to\nmaterialize many variants of your dataset, which will use a large amount of\ndisk space and take a lot of time to generate. Lastly, the trained model will\nexpect preprocessed data, so you will have to add preprocessing code in your\napplication before it calls the model. \u2022 If the data is preprocessed with the tf.data pipeline, it\u2019s much easier to tweak\nthe preprocessing logic and apply data augmentation. Also, tf.data makes it\neasy to build highly efficient preprocessing pipelines (e.g., with multithreading\nand prefetching). However, preprocessing the data this way will slow down\ntraining. Moreover, each training instance will be preprocessed once per epoch\nrather than just once if the data was preprocessed when creating the data files. Lastly, the trained model will still expect preprocessed data. \u2022 If you add preprocessing layers to your model, you will only have to write the\npreprocessing code once for both training and inference. If your model needs\nto be deployed to many different platforms, you will not need to write the pre\u2010\nprocessing code multiple times. Plus, you will not run the risk of using the\nwrong preprocessing logic for your model, since it will be part of the model. On the downside, preprocessing the data will slow down training, and each\ntraining instance will be preprocessed once per epoch. Moreover, by default\nthe preprocessing operations will run on the GPU for the current batch (you\nwill not benefit from parallel preprocessing on the CPU, and prefetching). For\u2010\ntunately, the upcoming Keras preprocessing layers should be able to lift the\npreprocessing operations from the preprocessing layers and run them as part\nof the tf.data pipeline, so you will benefit from multithreaded execution on the\nCPU and prefetching. \u2022 Lastly, using TF Transform for preprocessing gives you many of the benefits\nfrom the previous options: the preprocessed data is materialized, each instance\nis preprocessed just once (speeding up training), and preprocessing layers get\ngenerated automatically so you only need to write the preprocessing code\nonce. The main drawback is the fact that you need to learn how to use this\ntool. 8. Let\u2019s look at how to encode categorical features and text:\n\u2022 To encode a categorical feature that has a natural order, such as a movie rating\n(e.g., \u201cbad,\u201d \u201caverage,\u201d \u201cgood\u201d), the simplest option is to use ordinal encoding:\nsort the categories in their natural order and map each category to its rank\n(e.g., \u201cbad\u201d maps to 0, \u201caverage\u201d maps to 1, and \u201cgood\u201d maps to 2). However,\nmost categorical features don\u2019t have such a natural order. For example, there\u2019s | Appendix A: Exercise Solutions\nno natural order for professions or countries. In this case, you can use one-hot\nencoding or, if there are many categories, embeddings. \u2022 For text, one option is to use a bag-of-words representation: a sentence is rep\u2010\nresented by a vector counting the counts of each possible word."
  },
  {
    "id": 500,
    "content": "Since common\nwords are usually not very important, you\u2019ll want to use TF-IDF to reduce\ntheir weight. Instead of counting words, it is also common to count n-grams,\nwhich are sequences of n consecutive words\u2014nice and simple. Alternatively,\nyou can encode each word using word embeddings, possibly pretrained. Rather than encoding words, it is also possible to encode each letter, or sub\u2010\nword tokens (e.g., splitting \u201csmartest\u201d into \u201csmart\u201d and \u201cest\u201d). These last two\noptions are discussed in Chapter 16. For the solutions to exercises 9 and 10, please see the Jupyter notebooks available at\n\nChapter 14: Deep Computer Vision Using Convolutional\nNeural Networks\n1. These are the main advantages of a CNN over a fully connected DNN for image\nclassification:\n\u2022 Because consecutive layers are only partially connected and because it heavily\nreuses its weights, a CNN has many fewer parameters than a fully connected\nDNN, which makes it much faster to train, reduces the risk of overfitting, and\nrequires much less training data. \u2022 When a CNN has learned a kernel that can detect a particular feature, it can\ndetect that feature anywhere in the image. In contrast, when a DNN learns a\nfeature in one location, it can detect it only in that particular location. Since\nimages typically have very repetitive features, CNNs are able to generalize\nmuch better than DNNs for image processing tasks such as classification, using\nfewer training examples. \u2022 Finally, a DNN has no prior knowledge of how pixels are organized; it does not\nknow that nearby pixels are close. A CNN\u2019s architecture embeds this prior\nknowledge. Lower layers typically identify features in small areas of the images,\nwhile higher layers combine the lower-level features into larger features. This\nworks well with most natural images, giving CNNs a decisive head start com\u2010\npared to DNNs. 2. Let\u2019s compute how many parameters the CNN has. Since its first convolutional\nlayer has 3 \u00d7 3 kernels, and the input has three channels (red, green, and blue),\neach feature map has 3 \u00d7 3 \u00d7 3 weights, plus a bias term. That\u2019s 28 parameters per\nExercise Solutions | feature map. Since this first convolutional layer has 100 feature maps, it has a\ntotal of 2,800 parameters. The second convolutional layer has 3 \u00d7 3 kernels and\nits input is the set of 100 feature maps of the previous layer, so each feature map\nhas 3 \u00d7 3 \u00d7 100 = 900 weights, plus a bias term. Since it has 200 feature maps, this\nlayer has 901 \u00d7 200 = 180,200 parameters. Finally, the third and last convolu\u2010\ntional layer also has 3 \u00d7 3 kernels, and its input is the set of 200 feature maps of\nthe previous layers, so each feature map has 3 \u00d7 3 \u00d7 200 = 1,800 weights, plus a\nbias term. Since it has 400 feature maps, this layer has a total of 1,801 \u00d7 400 =\n720,400 parameters."
  },
  {
    "id": 501,
    "content": "All in all, the CNN has 2,800 + 180,200 + 720,400 = 903,400\nparameters. Now let\u2019s compute how much RAM this neural network will require (at least)\nwhen making a prediction for a single instance. First let\u2019s compute the feature\nmap size for each layer. Since we are using a stride of 2 and \"same\" padding, the\nhorizontal and vertical dimensions of the feature maps are divided by 2 at each\nlayer (rounding up if necessary). So, as the input channels are 200 \u00d7 300 pixels,\nthe first layer\u2019s feature maps are 100 \u00d7 150, the second layer\u2019s feature maps are 50\n\u00d7 75, and the third layer\u2019s feature maps are 25 \u00d7 38. Since 32 bits is 4 bytes and\nthe first convolutional layer has 100 feature maps, this first layer takes up 4 \u00d7 100\n\u00d7 150 \u00d7 100 = 6 million bytes (6 MB). The second layer takes up 4 \u00d7 50 \u00d7 75 \u00d7\n200 = 3 million bytes (3 MB). Finally, the third layer takes up 4 \u00d7 25 \u00d7 38 \u00d7 400 =\n1,520,000 bytes (about 1.5 MB). However, once a layer has been computed, the\nmemory occupied by the previous layer can be released, so if everything is well\noptimized, only 6 + 3 = 9 million bytes (9 MB) of RAM will be required (when\nthe second layer has just been computed, but the memory occupied by the first\nlayer has not been released yet). But wait, you also need to add the memory occu\u2010\npied by the CNN\u2019s parameters! We computed earlier that it has 903,400 parame\u2010\nters, each using up 4 bytes, so this adds 3,613,600 bytes (about 3.6 MB). The total\nRAM required is therefore (at least) 12,613,600 bytes (about 12.6 MB). Lastly, let\u2019s compute the minimum amount of RAM required when training the\nCNN on a mini-batch of 50 images. During training TensorFlow uses backpropa\u2010\ngation, which requires keeping all values computed during the forward pass until\nthe reverse pass begins. So we must compute the total RAM required by all layers\nfor a single instance and multiply that by 50. At this point, let\u2019s start counting in\nmegabytes rather than bytes. We computed before that the three layers require\nrespectively 6, 3, and 1.5 MB for each instance. That\u2019s a total of 10.5 MB per\ninstance, so for 50 instances the total RAM required is 525 MB. Add to that the\nRAM required by the input images, which is 50 \u00d7 4 \u00d7 200 \u00d7 300 \u00d7 3 = 36 million\nbytes (36 MB), plus the RAM required for the model parameters, which is about\n3.6 MB (computed earlier), plus some RAM for the gradients (we will neglect this\nsince it can be released gradually as backpropagation goes down the layers during\nthe reverse pass). We are up to a total of roughly 525 + 36 + 3.6 = 564.6 MB, and\nthat\u2019s really an optimistic bare minimum. | Appendix A: Exercise Solutions\n3."
  },
  {
    "id": 502,
    "content": "If your GPU runs out of memory while training a CNN, here are five things you\ncould try to solve the problem (other than purchasing a GPU with more RAM):\n\u2022 Reduce the mini-batch size. \u2022 Reduce dimensionality using a larger stride in one or more layers. \u2022 Remove one or more layers. \u2022 Use 16-bit floats instead of 32-bit floats. \u2022 Distribute the CNN across multiple devices. 4. A max pooling layer has no parameters at all, whereas a convolutional layer has\nquite a few (see the previous questions). 5. A local response normalization layer makes the neurons that most strongly acti\u2010\nvate inhibit neurons at the same location but in neighboring feature maps, which\nencourages different feature maps to specialize and pushes them apart, forcing\nthem to explore a wider range of features. It is typically used in the lower layers to\nhave a larger pool of low-level features that the upper layers can build upon. 6. The main innovations in AlexNet compared to LeNet-5 are that it is much larger\nand deeper, and it stacks convolutional layers directly on top of each other,\ninstead of stacking a pooling layer on top of each convolutional layer. The main\ninnovation in GoogLeNet is the introduction of inception modules, which make it\npossible to have a much deeper net than previous CNN architectures, with fewer\nparameters. ResNet\u2019s main innovation is the introduction of skip connections,\nwhich make it possible to go well beyond 100 layers. Arguably, its simplicity and\nconsistency are also rather innovative. SENet\u2019s main innovation was the idea of\nusing an SE block (a two-layer dense network) after every inception module in\nan inception network or every residual unit in a ResNet to recalibrate the relative\nimportance of feature maps. Finally, Xception\u2019s main innovation was the use of\ndepthwise separable convolutional layers, which look at spatial patterns and\ndepthwise patterns separately. 7. Fully convolutional networks are neural networks composed exclusively of con\u2010\nvolutional and pooling layers. FCNs can efficiently process images of any width\nand height (at least above the minimum size). They are most useful for object\ndetection and semantic segmentation because they only need to look at the image\nonce (instead of having to run a CNN multiple times on different parts of the\nimage). If you have a CNN with some dense layers on top, you can convert these\ndense layers to convolutional layers to create an FCN: just replace the lowest\ndense layer with a convolutional layer with a kernel size equal to the layer\u2019s input\nsize, with one filter per neuron in the dense layer, and using \"valid\" padding. Generally the stride should be 1, but you can set it to a higher value if you want. The activation function should be the same as the dense layer\u2019s. The other dense\nlayers should be converted the same way, but using 1 \u00d7 1 filters."
  },
  {
    "id": 503,
    "content": "It is actually pos\u2010\nExercise Solutions | sible to convert a trained CNN this way by appropriately reshaping the dense lay\u2010\ners\u2019 weight matrices. 8. The main technical difficulty of semantic segmentation is the fact that a lot of the\nspatial information gets lost in a CNN as the signal flows through each layer,\nespecially in pooling layers and layers with a stride greater than 1. This spatial\ninformation needs to be restored somehow to accurately predict the class of each\npixel. For the solutions to exercises 9 to 12, please see the Jupyter notebooks available at\n\nChapter 15: Processing Sequences Using RNNs and CNNs\n1. Here are a few RNN applications:\n\u2022 For a sequence-to-sequence RNN: predicting the weather (or any other time\nseries), machine translation (using an Encoder\u2013Decoder architecture), video\ncaptioning, speech to text, music generation (or other sequence generation),\nidentifying the chords of a song\n\u2022 For a sequence-to-vector RNN: classifying music samples by music genre, ana\u2010\nlyzing the sentiment of a book review, predicting what word an aphasic patient\nis thinking of based on readings from brain implants, predicting the probabil\u2010\nity that a user will want to watch a movie based on their watch history (this is\none of many possible implementations of collaborative filtering for a recom\u2010\nmender system)\n\u2022 For a vector-to-sequence RNN: image captioning, creating a music playlist\nbased on an embedding of the current artist, generating a melody based on a\nset of parameters, locating pedestrians in a picture (e.g., a video frame from a\nself-driving car\u2019s camera)\n2. An RNN layer must have three-dimensional inputs: the first dimension is the\nbatch dimension (its size is the batch size), the second dimension represents the\ntime (its size is the number of time steps), and the third dimension holds the\ninputs at each time step (its size is the number of input features per time step). For example, if you want to process a batch containing 5 time series of 10 time\nsteps each, with 2 values per time step (e.g., the temperature and the wind speed),\nthe shape will be [5, 10, 2]. The outputs are also three-dimensional, with the\nsame first two dimensions, but the last dimension is equal to the number of\nneurons. For example, if an RNN layer with 32 neurons processes the batch we\njust discussed, the output will have a shape of [5, 10, 32]. | Appendix A: Exercise Solutions\n3. To build a deep sequence-to-sequence RNN using Keras, you must set\nreturn_sequences=True for all RNN layers. To build a sequence-to-vector RNN,\nyou must set return_sequences=True for all RNN layers except for the top RNN\nlayer, which must have return_sequences=False (or do not set this argument at\nall, since False is the default). 4."
  },
  {
    "id": 504,
    "content": "If you have a daily univariate time series, and you want to forecast the next seven\ndays, the simplest RNN architecture you can use is a stack of RNN layers (all with\nreturn_sequences=True except for the top RNN layer), using seven neurons in\nthe output RNN layer. You can then train this model using random windows\nfrom the time series (e.g., sequences of 30 consecutive days as the inputs, and a\nvector containing the values of the next 7 days as the target). This is a sequence-\nto-vector RNN. Alternatively, you could set return_sequences=True for all RNN\nlayers to create a sequence-to-sequence RNN. You can train this model using\nrandom windows from the time series, with sequences of the same length as the\ninputs as the targets. Each target sequence should have seven values per time step\n(e.g., for time step t, the target should be a vector containing the values at time\nsteps t + 1 to t + 7). 5. The two main difficulties when training RNNs are unstable gradients (exploding\nor vanishing) and a very limited short-term memory. These problems both get\nworse when dealing with long sequences. To alleviate the unstable gradients\nproblem, you can use a smaller learning rate, use a saturating activation function\nsuch as the hyperbolic tangent (which is the default), and possibly use gradient\nclipping, Layer Normalization, or dropout at each time step. To tackle the limited\nshort-term memory problem, you can use LSTM or GRU layers (this also helps with\nthe unstable gradients problem). 6. An LSTM cell\u2019s architecture looks complicated, but it\u2019s actually not too hard if\nyou understand the underlying logic. The cell has a short-term state vector and a\nlong-term state vector. At each time step, the inputs and the previous short-term\nstate are fed to a simple RNN cell and three gates: the forget gate decides what to\nremove from the long-term state, the input gate decides which part of the output\nof the simple RNN cell should be added to the long-term state, and the output\ngate decides which part of the long-term state should be output at this time step\n(after going through the tanh activation function). The new short-term state is\nequal to the output of the cell. See Figure 15-9. 7. An RNN layer is fundamentally sequential: in order to compute the outputs at\ntime step t, it has to first compute the outputs at all earlier time steps. This makes\nit impossible to parallelize. On the other hand, a 1D convolutional layer lends\nitself well to parallelization since it does not hold a state between time steps. In\nother words, it has no memory: the output at any time step can be computed\nbased only on a small window of values from the inputs without having to know\nall the past values. Moreover, since a 1D convolutional layer is not recurrent, it\nExercise Solutions | suffers less from unstable gradients."
  },
  {
    "id": 505,
    "content": "One or more 1D convolutional layers can be\nuseful in an RNN to efficiently preprocess the inputs, for example to reduce their\ntemporal resolution (downsampling) and thereby help the RNN layers detect\nlong-term patterns. In fact, it is possible to use only convolutional layers, for\nexample by building a WaveNet architecture. 8. To classify videos based on their visual content, one possible architecture could\nbe to take (say) one frame per second, then run every frame through the same\nconvolutional neural network (e.g., a pretrained Xception model, possibly frozen\nif your dataset is not large), feed the sequence of outputs from the CNN to a\nsequence-to-vector RNN, and finally run its output through a softmax layer, giv\u2010\ning you all the class probabilities. For training you would use cross entropy as the\ncost function. If you wanted to use the audio for classification as well, you could\nuse a stack of strided 1D convolutional layers to reduce the temporal resolution\nfrom thousands of audio frames per second to just one per second (to match the\nnumber of images per second), and concatenate the output sequence to the\ninputs of the sequence-to-vector RNN (along the last dimension). For the solutions to exercises 9 and 10, please see the Jupyter notebooks available at\n\nChapter 16: Natural Language Processing with RNNs and\nAttention\n1. Stateless RNNs can only capture patterns whose length is less than, or equal to,\nthe size of the windows the RNN is trained on. Conversely, stateful RNNs can\ncapture longer-term patterns. However, implementing a stateful RNN is much\nharder\u2014especially preparing the dataset properly. Moreover, stateful RNNs do\nnot always work better, in part because consecutive batches are not independent\nand identically distributed (IID). Gradient Descent is not fond of non-IID\ndatasets. 2. In general, if you translate a sentence one word at a time, the result will be terri\u2010\nble. For example, the French sentence \u201cJe vous en prie\u201d means \u201cYou are welcome,\u201d\nbut if you translate it one word at a time, you get \u201cI you in pray.\u201d Huh? It is much\nbetter to read the whole sentence first and then translate it. A plain sequence-to-\nsequence RNN would start translating a sentence immediately after reading the\nfirst word, while an Encoder\u2013Decoder RNN will first read the whole sentence\nand then translate it. That said, one could imagine a plain sequence-to-sequence\nRNN that would output silence whenever it is unsure about what to say next (just\nlike human translators do when they must translate a live broadcast). 3. Variable-length input sequences can be handled by padding the shorter sequen\u2010\nces so that all sequences in a batch have the same length, and using masking to | Appendix A: Exercise Solutions\nensure the RNN ignores the padding token. For better performance, you may\nalso want to create batches containing sequences of similar sizes."
  },
  {
    "id": 506,
    "content": "Ragged tensors\ncan hold sequences of variable lengths, and tf.keras will likely support them even\u2010\ntually, which will greatly simplify handling variable-length input sequences (at\nthe time of this writing, it is not the case yet). Regarding variable-length output\nsequences, if the length of the output sequence is known in advance (e.g., if you\nknow that it is the same as the input sequence), then you just need to configure\nthe loss function so that it ignores tokens that come after the end of the sequence. Similarly, the code that will use the model should ignore tokens beyond the end\nof the sequence. But generally the length of the output sequence is not known\nahead of time, so the solution is to train the model so that it outputs an end-of-\nsequence token at the end of each sequence. 4. Beam search is a technique used to improve the performance of a trained\nEncoder\u2013Decoder model, for example in a neural machine translation system. The algorithm keeps track of a short list of the k most promising output senten\u2010\nces (say, the top three), and at each decoder step it tries to extend them by one\nword; then it keeps only the k most likely sentences. The parameter k is called the\nbeam width: the larger it is, the more CPU and RAM will be used, but also the\nmore accurate the system will be. Instead of greedily choosing the most likely\nnext word at each step to extend a single sentence, this technique allows the sys\u2010\ntem to explore several promising sentences simultaneously. Moreover, this tech\u2010\nnique lends itself well to parallelization. You can implement beam search fairly\neasily using TensorFlow Addons. 5. An attention mechanism is a technique initially used in Encoder\u2013Decoder mod\u2010\nels to give the decoder more direct access to the input sequence, allowing it to\ndeal with longer input sequences. At each decoder time step, the current decod\u2010\ner\u2019s state and the full output of the encoder are processed by an alignment model\nthat outputs an alignment score for each input time step. This score indicates\nwhich part of the input is most relevant to the current decoder time step. The\nweighted sum of the encoder output (weighted by their alignment score) is then\nfed to the decoder, which produces the next decoder state and the output for this\ntime step. The main benefit of using an attention mechanism is the fact that the\nEncoder\u2013Decoder model can successfully process longer input sequences. Another benefit is that the alignment scores makes the model easier to debug and\ninterpret: for example, if the model makes a mistake, you can look at which part\nof the input it was paying attention to, and this can help diagnose the issue. An\nattention mechanism is also at the core of the Transformer architecture, in the\nMulti-Head Attention layers. See the next answer. 6."
  },
  {
    "id": 507,
    "content": "The most important layer in the Transformer architecture is the Multi-Head\nAttention layer (the original Transformer architecture contains 18 of them,\nincluding 6 Masked Multi-Head Attention layers). It is at the core of language\nExercise Solutions | models such as BERT and GPT-2. Its purpose is to allow the model to identify\nwhich words are most aligned with each other, and then improve each word\u2019s\nrepresentation using these contextual clues. 7. Sampled softmax is used when training a classification model when there are\nmany classes (e.g., thousands). It computes an approximation of the cross-\nentropy loss based on the logit predicted by the model for the correct class, and\nthe predicted logits for a sample of incorrect words. This speeds up training con\u2010\nsiderably compared to computing the softmax over all logits and then estimating\nthe cross-entropy loss. After training, the model can be used normally, using the\nregular softmax function to compute all the class probabilities based on all the\nlogits. For the solutions to exercises 8 to 11, please see the Jupyter notebooks available at\n\nChapter 17: Representation Learning and Generative\nLearning Using Autoencoders and GANs\n1. Here are some of the main tasks that autoencoders are used for:\n\u2022 Feature extraction\n\u2022 Unsupervised pretraining\n\u2022 Dimensionality reduction\n\u2022 Generative models\n\u2022 Anomaly detection (an autoencoder is generally bad at reconstructing outliers)\n2. If you want to train a classifier and you have plenty of unlabeled training data but\nonly a few thousand labeled instances, then you could first train a deep autoen\u2010\ncoder on the full dataset (labeled + unlabeled), then reuse its lower half for the\nclassifier (i.e., reuse the layers up to the codings layer, included) and train the\nclassifier using the labeled data. If you have little labeled data, you probably want\nto freeze the reused layers when training the classifier. 3. The fact that an autoencoder perfectly reconstructs its inputs does not necessarily\nmean that it is a good autoencoder; perhaps it is simply an overcomplete autoen\u2010\ncoder that learned to copy its inputs to the codings layer and then to the outputs. In fact, even if the codings layer contained a single neuron, it would be possible\nfor a very deep autoencoder to learn to map each training instance to a different\ncoding (e.g., the first instance could be mapped to 0.001, the second to 0.002, the\nthird to 0.003, and so on), and it could learn \u201cby heart\u201d to reconstruct the right\ntraining instance for each coding. It would perfectly reconstruct its inputs | Appendix A: Exercise Solutions\nwithout really learning any useful pattern in the data. In practice such a mapping\nis unlikely to happen, but it illustrates the fact that perfect reconstructions are not\na guarantee that the autoencoder learned anything useful. However, if it produces\nvery bad reconstructions, then it is almost guaranteed to be a bad autoencoder."
  },
  {
    "id": 508,
    "content": "To evaluate the performance of an autoencoder, one option is to measure the\nreconstruction loss (e.g., compute the MSE, or the mean square of the outputs\nminus the inputs). Again, a high reconstruction loss is a good sign that the\nautoencoder is bad, but a low reconstruction loss is not a guarantee that it is\ngood. You should also evaluate the autoencoder according to what it will be used\nfor. For example, if you are using it for unsupervised pretraining of a classifier,\nthen you should also evaluate the classifier\u2019s performance. 4. An undercomplete autoencoder is one whose codings layer is smaller than the\ninput and output layers. If it is larger, then it is an overcomplete autoencoder. The main risk of an excessively undercomplete autoencoder is that it may fail to\nreconstruct the inputs. The main risk of an overcomplete autoencoder is that it\nmay just copy the inputs to the outputs, without learning any useful features. 5. To tie the weights of an encoder layer and its corresponding decoder layer, you\nsimply make the decoder weights equal to the transpose of the encoder weights. This reduces the number of parameters in the model by half, often making train\u2010\ning converge faster with less training data and reducing the risk of overfitting the\ntraining set. 6. A generative model is a model capable of randomly generating outputs that\nresemble the training instances. For example, once trained successfully on the\nMNIST dataset, a generative model can be used to randomly generate realistic\nimages of digits. The output distribution is typically similar to the training data. For example, since MNIST contains many images of each digit, the generative\nmodel would output roughly the same number of images of each digit. Some\ngenerative models can be parametrized\u2014for example, to generate only some\nkinds of outputs. An example of a generative autoencoder is the variational\nautoencoder. 7. A generative adversarial network is a neural network architecture composed of\ntwo parts, the generator and the discriminator, which have opposing objectives. The generator\u2019s goal is to generate instances similar to those in the training set, to\nfool the discriminator. The discriminator must distinguish the real instances\nfrom the generated ones. At each training iteration, the discriminator is trained\nlike a normal binary classifier, then the generator is trained to maximize the\ndiscriminator\u2019s error. GANs are used for advanced image processing tasks such as\nsuper resolution, colorization, image editing (replacing objects with realistic\nbackground), turning a simple sketch into a photorealistic image, or predicting\nthe next frames in a video. They are also used to augment a dataset (to train other\nExercise Solutions | models), to generate other types of data (such as text, audio, and time series), and\nto identify the weaknesses in other models and strengthen them. 8. Training GANs is notoriously difficult, because of the complex dynamics\nbetween the generator and the discriminator. The biggest difficulty is mode col\u2010\nlapse, where the generator produces outputs with very little diversity."
  },
  {
    "id": 509,
    "content": "Moreover,\ntraining can be terribly unstable: it may start out fine and then suddenly start\noscillating or diverging, without any apparent reason. GANs are also very sensi\u2010\ntive to the choice of hyperparameters. For the solutions to exercises 9, 10, and 11, please see the Jupyter notebooks available\nat \nChapter 18: Reinforcement Learning\n1. Reinforcement Learning is an area of Machine Learning aimed at creating agents\ncapable of taking actions in an environment in a way that maximizes rewards\nover time. There are many differences between RL and regular supervised and\nunsupervised learning. Here are a few:\n\u2022 In supervised and unsupervised learning, the goal is generally to find patterns\nin the data and use them to make predictions. In Reinforcement Learning, the\ngoal is to find a good policy. \u2022 Unlike in supervised learning, the agent is not explicitly given the \u201cright\u201d\nanswer. It must learn by trial and error. \u2022 Unlike in unsupervised learning, there is a form of supervision, through\nrewards. We do not tell the agent how to perform the task, but we do tell it\nwhen it is making progress or when it is failing. \u2022 A Reinforcement Learning agent needs to find the right balance between\nexploring the environment, looking for new ways of getting rewards, and\nexploiting sources of rewards that it already knows. In contrast, supervised and\nunsupervised learning systems generally don\u2019t need to worry about explora\u2010\ntion; they just feed on the training data they are given. \u2022 In supervised and unsupervised learning, training instances are typically inde\u2010\npendent (in fact, they are generally shuffled). In Reinforcement Learning, con\u2010\nsecutive observations are generally not independent. An agent may remain in\nthe same region of the environment for a while before it moves on, so consecu\u2010\ntive observations will be very correlated. In some cases a replay memory\n(buffer) is used to ensure that the training algorithm gets fairly independent\nobservations. | Appendix A: Exercise Solutions\n2. Here are a few possible applications of Reinforcement Learning, other than those\nmentioned in Chapter 18:\nMusic personalization\nThe environment is a user\u2019s personalized web radio. The agent is the software\ndeciding what song to play next for that user. Its possible actions are to play\nany song in the catalog (it must try to choose a song the user will enjoy) or to\nplay an advertisement (it must try to choose an ad that the user will be inter\u2010\nested in). It gets a small reward every time the user listens to a song, a larger\nreward every time the user listens to an ad, a negative reward when the user\nskips a song or an ad, and a very negative reward if the user leaves. Marketing\nThe environment is your company\u2019s marketing department. The agent is the\nsoftware that defines which customers a mailing campaign should be sent to,\ngiven their profile and purchase history (for each customer it has two possi\u2010\nble actions: send or don\u2019t send)."
  },
  {
    "id": 510,
    "content": "It gets a negative reward for the cost of the\nmailing campaign, and a positive reward for estimated revenue generated\nfrom this campaign. Product delivery\nLet the agent control a fleet of delivery trucks, deciding what they should\npick up at the depots, where they should go, what they should drop off, and\nso on. It will get positive rewards for each product delivered on time, and\nnegative rewards for late deliveries. 3. When estimating the value of an action, Reinforcement Learning algorithms typ\u2010\nically sum all the rewards that this action led to, giving more weight to immediate\nrewards and less weight to later rewards (considering that an action has more\ninfluence on the near future than on the distant future). To model this, a discount\nfactor is typically applied at each time step. For example, with a discount factor of\n0.9, a reward of 100 that is received two time steps later is counted as only 0.92 \u00d7\n100 = 81 when you are estimating the value of the action. You can think of the\ndiscount factor as a measure of how much the future is valued relative to the\npresent: if it is very close to 1, then the future is valued almost as much as the\npresent; if it is close to 0, then only immediate rewards matter. Of course, this\nimpacts the optimal policy tremendously: if you value the future, you may be\nwilling to put up with a lot of immediate pain for the prospect of eventual\nrewards, while if you don\u2019t value the future, you will just grab any immediate\nreward you can find, never investing in the future. 4. To measure the performance of a Reinforcement Learning agent, you can simply\nsum up the rewards it gets. In a simulated environment, you can run many epi\u2010\nsodes and look at the total rewards it gets on average (and possibly look at the\nmin, max, standard deviation, and so on). Exercise Solutions | 5. The credit assignment problem is the fact that when a Reinforcement Learning\nagent receives a reward, it has no direct way of knowing which of its previous\nactions contributed to this reward. It typically occurs when there is a large delay\nbetween an action and the resulting reward (e.g., during a game of Atari\u2019s Pong,\nthere may be a few dozen time steps between the moment the agent hits the ball\nand the moment it wins the point). One way to alleviate it is to provide the agent\nwith shorter-term rewards, when possible. This usually requires prior knowledge\nabout the task. For example, if we want to build an agent that will learn to play\nchess, instead of giving it a reward only when it wins the game, we could give it a\nreward every time it captures one of the opponent\u2019s pieces. 6."
  },
  {
    "id": 511,
    "content": "An agent can often remain in the same region of its environment for a while, so\nall of its experiences will be very similar for that period of time. This can intro\u2010\nduce some bias in the learning algorithm. It may tune its policy for this region of\nthe environment, but it will not perform well as soon as it moves out of this\nregion. To solve this problem, you can use a replay memory; instead of using\nonly the most immediate experiences for learning, the agent will learn based on a\nbuffer of its past experiences, recent and not so recent (perhaps this is why we\ndream at night: to replay our experiences of the day and better learn from them?). 7. An off-policy RL algorithm learns the value of the optimal policy (i.e., the sum of\ndiscounted rewards that can be expected for each state if the agent acts optimally)\nwhile the agent follows a different policy. Q-Learning is a good example of such\nan algorithm. In contrast, an on-policy algorithm learns the value of the policy\nthat the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available\nat \nChapter 19: Training and Deploying TensorFlow Models\nat Scale\n1. A SavedModel contains a TensorFlow model, including its architecture (a com\u2010\nputation graph) and its weights. It is stored as a directory containing a\nsaved_model.pb file, which defines the computation graph (represented as a seri\u2010\nalized protocol buffer), and a variables subdirectory containing the variable val\u2010\nues. For models containing a large number of weights, these variable values may\nbe split across multiple files. A SavedModel also includes an assets subdirectory\nthat may contain additional data, such as vocabulary files, class names, or some\nexample instances for this model. To be more accurate, a SavedModel can con\u2010\ntain one or more metagraphs. A metagraph is a computation graph plus some\nfunction signature definitions (including their input and output names, types,\nand shapes). Each metagraph is identified by a set of tags. To inspect a SavedMo\u2010 | Appendix A: Exercise Solutions\ndel, you can use the command-line tool saved_model_cli or just load it using\ntf.saved_model.load() and inspect it in Python. 2. TF Serving allows you to deploy multiple TensorFlow models (or multiple ver\u2010\nsions of the same model) and make them accessible to all your applications easily\nvia a REST API or a gRPC API. Using your models directly in your applications\nwould make it harder to deploy a new version of a model across all applications. Implementing your own microservice to wrap a TF model would require extra\nwork, and it would be hard to match TF Serving\u2019s features."
  },
  {
    "id": 512,
    "content": "TF Serving has many\nfeatures: it can monitor a directory and autodeploy the models that are placed\nthere, and you won\u2019t have to change or even restart any of your applications to\nbenefit from the new model versions; it\u2019s fast, well tested, and scales very well;\nand it supports A/B testing of experimental models and deploying a new model\nversion to just a subset of your users (in this case the model is called a canary). TF Serving is also capable of grouping individual requests into batches to run\nthem jointly on the GPU. To deploy TF Serving, you can install it from source,\nbut it is much simpler to install it using a Docker image. To deploy a cluster of TF\nServing Docker images, you can use an orchestration tool such as Kubernetes, or\nuse a fully hosted solution such as Google Cloud AI Platform. 3. To deploy a model across multiple TF Serving instances, all you need to do is\nconfigure these TF Serving instances to monitor the same models directory, and\nthen export your new model as a SavedModel into a subdirectory. 4. The gRPC API is more efficient than the REST API. However, its client libraries\nare not as widely available, and if you activate compression when using the REST\nAPI, you can get almost the same performance. So, the gRPC API is most useful\nwhen you need the highest possible performance and the clients are not limited\nto the REST API. 5. To reduce a model\u2019s size so it can run on a mobile or embedded device, TFLite\nuses several techniques:\n\u2022 It provides a converter which can optimize a SavedModel: it shrinks the model\nand reduces its latency. To do this, it prunes all the operations that are not\nneeded to make predictions (such as training operations), and it optimizes and\nfuses operations whenever possible. \u2022 The converter can also perform post-training quantization: this technique dra\u2010\nmatically reduces the model\u2019s size, so it\u2019s much faster to download and store. \u2022 It saves the optimized model using the FlatBuffer format, which can be loaded\nto RAM directly, without parsing. This reduces the loading time and memory\nfootprint. Exercise Solutions | 6. Quantization-aware training consists in adding fake quantization operations to\nthe model during training. This allows the model to learn to ignore the quantiza\u2010\ntion noise; the final weights will be more robust to quantization. 7. Model parallelism means chopping your model into multiple parts and running\nthem in parallel across multiple devices, hopefully speeding up the model during\ntraining or inference. Data parallelism means creating multiple exact replicas of\nyour model and deploying them across multiple devices. At each iteration during\ntraining, each replica is given a different batch of data, and it computes the gradi\u2010\nents of the loss with regard to the model parameters. In synchronous data paral\u2010\nlelism, the gradients from all replicas are then aggregated and the optimizer\nperforms a Gradient Descent step."
  },
  {
    "id": 513,
    "content": "The parameters may be centralized (e.g., on\nparameter servers) or replicated across all replicas and kept in sync using AllRe\u2010\nduce. In asynchronous data parallelism, the parameters are centralized and the\nreplicas run independently from each other, each updating the central parame\u2010\nters directly at the end of each training iteration, without having to wait for the\nother replicas. To speed up training, data parallelism turns out to work better\nthan model parallelism, in general. This is mostly because it requires less com\u2010\nmunication across devices. Moreover, it is much easier to implement, and it\nworks the same way for any model, whereas model parallelism requires analyzing\nthe model to determine the best way to chop it into pieces. 8. When training a model across multiple servers, you can use the following distri\u2010\nbution strategies:\n\u2022 The MultiWorkerMirroredStrategy performs mirrored data parallelism. The\nmodel is replicated across all available servers and devices, and each replica\ngets a different batch of data at each training iteration and computes its own\ngradients. The mean of the gradients is computed and shared across all replicas\nusing a distributed AllReduce implementation (NCCL by default), and all rep\u2010\nlicas perform the same Gradient Descent step. This strategy is the simplest to\nuse since all servers and devices are treated in exactly the same way, and it per\u2010\nforms fairly well. In general, you should use this strategy. Its main limitation is\nthat it requires the model to fit in RAM on every replica. \u2022 The ParameterServerStrategy performs asynchronous data parallelism. The\nmodel is replicated across all devices on all workers, and the parameters are\nsharded across all parameter servers. Each worker has its own training loop,\nrunning asynchronously with the other workers; at each training iteration,\neach worker gets its own batch of data and fetches the latest version of the\nmodel parameters from the parameter servers, then it computes the gradients\nof the loss with regard to these parameters, and it sends them to the parameter\nservers. Lastly, the parameter servers perform a Gradient Descent step using\nthese gradients. This strategy is generally slower than the previous strategy, | Appendix A: Exercise Solutions\nand a bit harder to deploy, since it requires managing parameter servers. How\u2010\never, it is useful to train huge models that don\u2019t fit in GPU RAM. For the solutions to exercises 9, 10, and 11, please see the Jupyter notebooks available\nat \nExercise Solutions | APPENDIX B\nMachine Learning Project Checklist\nThis checklist can guide you through your Machine Learning projects. There are\neight main steps:\n1. Frame the problem and look at the big picture. 2. Get the data. 3. Explore the data to gain insights. 4. Prepare the data to better expose the underlying data patterns to Machine Learn\u2010\ning algorithms. 5. Explore many different models and shortlist the best ones. 6. Fine-tune your models and combine them into a great solution. 7. Present your solution. 8. Launch, monitor, and maintain your system."
  },
  {
    "id": 514,
    "content": "Obviously, you should feel free to adapt this checklist to your needs. Frame the Problem and Look at the Big Picture\n1. Define the objective in business terms. 2. How will your solution be used? 3. What are the current solutions/workarounds (if any)? 4. How should you frame this problem (supervised/unsupervised, online/offline,\netc.)? 5. How should performance be measured? 6. Is the performance measure aligned with the business objective? 7. What would be the minimum performance needed to reach the business objec\u2010\ntive? 8. What are comparable problems? Can you reuse experience or tools? 9. Is human expertise available? 10. How would you solve the problem manually? 11. List the assumptions you (or others) have made so far. 12. Verify assumptions if possible. Get the Data\nNote: automate as much as possible so you can easily get fresh data. 1. List the data you need and how much you need. 2. Find and document where you can get that data. 3. Check how much space it will take. 4. Check legal obligations, and get authorization if necessary. 5. Get access authorizations. 6. Create a workspace (with enough storage space). 7. Get the data. 8. Convert the data to a format you can easily manipulate (without changing the\ndata itself). 9. Ensure sensitive information is deleted or protected (e.g., anonymized). 10. Check the size and type of data (time series, sample, geographical, etc.). 11. Sample a test set, put it aside, and never look at it (no data snooping!). Explore the Data\nNote: try to get insights from a field expert for these steps. 1. Create a copy of the data for exploration (sampling it down to a manageable size\nif necessary). 2. Create a Jupyter notebook to keep a record of your data exploration. 3. Study each attribute and its characteristics:\n\u2022 Name\n\u2022 Type (categorical, int/float, bounded/unbounded, text, structured, etc.) | Appendix B: Machine Learning Project Checklist\n\u2022 % of missing values\n\u2022 Noisiness and type of noise (stochastic, outliers, rounding errors, etc.) \u2022 Usefulness for the task\n\u2022 Type of distribution (Gaussian, uniform, logarithmic, etc.) 4. For supervised learning tasks, identify the target attribute(s). 5. Visualize the data. 6. Study the correlations between attributes. 7. Study how you would solve the problem manually. 8. Identify the promising transformations you may want to apply. 9. Identify extra data that would be useful (go back to \u201cGet the Data\u201d on page 756). 10. Document what you have learned. Prepare the Data\nNotes:\n\u2022 Work on copies of the data (keep the original dataset intact). \u2022 Write functions for all data transformations you apply, for five reasons:\n\u2014 So you can easily prepare the data the next time you get a fresh dataset\n\u2014 So you can apply these transformations in future projects\n\u2014 To clean and prepare the test set\n\u2014 To clean and prepare new data instances once your solution is live\n\u2014 To make it easy to treat your preparation choices as hyperparameters\n1."
  },
  {
    "id": 515,
    "content": "Data cleaning:\n\u2022 Fix or remove outliers (optional). \u2022 Fill in missing values (e.g., with zero, mean, median\u2026) or drop their rows (or\ncolumns). 2. Feature selection (optional):\n\u2022 Drop the attributes that provide no useful information for the task. 3. Feature engineering, where appropriate:\n\u2022 Discretize continuous features. Machine Learning Project Checklist | \u2022 Decompose features (e.g., categorical, date/time, etc.). \u2022 Add promising transformations of features (e.g., log(x), sqrt(x), x2, etc.). \u2022 Aggregate features into promising new features. 4. Feature scaling:\n\u2022 Standardize or normalize features. Shortlist Promising Models\nNotes:\n\u2022 If the data is huge, you may want to sample smaller training sets so you can train\nmany different models in a reasonable time (be aware that this penalizes complex\nmodels such as large neural nets or Random Forests). \u2022 Once again, try to automate these steps as much as possible. 1. Train many quick-and-dirty models from different categories (e.g., linear, naive\nBayes, SVM, Random Forest, neural net, etc.) using standard parameters. 2. Measure and compare their performance. \u2022 For each model, use N-fold cross-validation and compute the mean and stan\u2010\ndard deviation of the performance measure on the N folds. 3. Analyze the most significant variables for each algorithm. 4. Analyze the types of errors the models make. \u2022 What data would a human have used to avoid these errors? 5. Perform a quick round of feature selection and engineering. 6. Perform one or two more quick iterations of the five previous steps. 7. Shortlist the top three to five most promising models, preferring models that\nmake different types of errors. Fine-Tune the System\nNotes:\n\u2022 You will want to use as much data as possible for this step, especially as you move\ntoward the end of fine-tuning. | Appendix B: Machine Learning Project Checklist\n1 Jasper Snoek et al., \u201cPractical Bayesian Optimization of Machine Learning Algorithms,\u201d Proceedings of the 25th\nInternational Conference on Neural Information Processing Systems 2 (2012): 2951\u20132959. \u2022 As always, automate what you can. 1. Fine-tune the hyperparameters using cross-validation:\n\u2022 Treat your data transformation choices as hyperparameters, especially when\nyou are not sure about them (e.g., if you\u2019re not sure whether to replace missing\nvalues with zeros or with the median value, or to just drop the rows). \u2022 Unless there are very few hyperparameter values to explore, prefer random\nsearch over grid search. If training is very long, you may prefer a Bayesian\noptimization approach (e.g., using Gaussian process priors, as described by\nJasper Snoek et al. ).1\n2. Try Ensemble methods. Combining your best models will often produce better\nperformance than running them individually. 3. Once you are confident about your final model, measure its performance on the\ntest set to estimate the generalization error. Don\u2019t tweak your model after measuring the generalization error:\nyou would just start overfitting the test set. Present Your Solution\n1. Document what you have done. 2. Create a nice presentation. \u2022 Make sure you highlight the big picture first. 3."
  },
  {
    "id": 516,
    "content": "Explain why your solution achieves the business objective. 4. Don\u2019t forget to present interesting points you noticed along the way. \u2022 Describe what worked and what did not. \u2022 List your assumptions and your system\u2019s limitations. Machine Learning Project Checklist | 5. Ensure your key findings are communicated through beautiful visualizations or\neasy-to-remember statements (e.g., \u201cthe median income is the number-one pre\u2010\ndictor of housing prices\u201d). Launch! 1. Get your solution ready for production (plug into production data inputs, write\nunit tests, etc.). 2. Write monitoring code to check your system\u2019s live performance at regular inter\u2010\nvals and trigger alerts when it drops. \u2022 Beware of slow degradation: models tend to \u201crot\u201d as data evolves. \u2022 Measuring performance may require a human pipeline (e.g., via a crowdsourc\u2010\ning service). \u2022 Also monitor your inputs\u2019 quality (e.g., a malfunctioning sensor sending ran\u2010\ndom values, or another team\u2019s output becoming stale). This is particularly\nimportant for online learning systems. 3. Retrain your models on a regular basis on fresh data (automate as much as\npossible). | Appendix B: Machine Learning Project Checklist\nAPPENDIX C\nSVM Dual Problem\nTo understand duality, you first need to understand the Lagrange multipliers method. The general idea is to transform a constrained optimization objective into an uncon\u2010\nstrained one, by moving the constraints into the objective function. Let\u2019s look at a\nsimple example. Suppose you want to find the values of x and y that minimize the\nfunction f(x, y) = x2 + 2y, subject to an equality constraint: 3x + 2y + 1 = 0. Using the\nLagrange multipliers method, we start by defining a new function called the Lagran\u2010\ngian (or Lagrange function): g(x, y, \u03b1) = f(x, y) \u2013 \u03b1(3x + 2y + 1). Each constraint (in\nthis case just one) is subtracted from the original objective, multiplied by a new vari\u2010\nable called a Lagrange multiplier. Joseph-Louis Lagrange showed that if x, y is a solution to the constrained optimiza\u2010\ntion problem, then there must exist an \u03b1 such that x, y, \u03b1 is a stationary point of the\nLagrangian (a stationary point is a point where all partial derivatives are equal to\nzero). In other words, we can compute the partial derivatives of g(x, y, \u03b1) with regard\nto x, y, and \u03b1; we can find the points where these derivatives are all equal to zero; and\nthe solutions to the constrained optimization problem (if they exist) must be among\nthese stationary points. In this example the partial derivatives are: \u2202\n\u2202x g x, y, \u03b1 = 2x \u22123\u03b1\n\u2202\n\u2202y g x, y, \u03b1 = 2 \u22122\u03b1\n\u2202\n\u2202\u03b1 g x, y, \u03b1 = \u22123x \u22122y \u22121\nWhen all these partial derivatives are equal to 0, we find that\n2x \u22123\u03b1 = 2 \u22122\u03b1 = \u22123x \u22122y \u22121 = 0, from which we can easily find that x = 3\n2,\ny = \u221211\n4 , and \u03b1 = 1."
  },
  {
    "id": 517,
    "content": "This is the only stationary point, and as it respects the con\u2010\nstraint, it must be the solution to the constrained optimization problem. However, this method applies only to equality constraints. Fortunately, under some\nregularity conditions (which are respected by the SVM objectives), this method can\nbe generalized to inequality constraints as well (e.g., 3x + 2y + 1 \u2265 0). The generalized\nLagrangian for the hard margin problem is given by Equation C-1, where the \u03b1(i) vari\u2010\nables are called the Karush\u2013Kuhn\u2013Tucker (KKT) multipliers, and they must be greater\nor equal to zero. Equation C-1. Generalized Lagrangian for the hard margin problem\n\u2112w, b, \u03b1 = 1\n2w\u22baw \u2212\u2211\ni = 1\nm\n\u03b1 i t i w\u22bax i + b \u22121\nwith\n\u03b1 i \u22650\nfor i = 1, 2, \u22ef, m\nJust like with the Lagrange multipliers method, you can compute the partial deriva\u2010\ntives and locate the stationary points. If there is a solution, it will necessarily be\namong the stationary points w, b, \u03b1 that respect the KKT conditions:\n\u2022 Respect the problem\u2019s constraints: t i w\u22bax i + b \u22651 for\u00a0i = 1, 2, \u2026, m.\n\u2022 Verify \u03b1 i \u22650\nfor i = 1, 2, \u22ef, m.\n\u2022 Either \u03b1 i = 0 or the ith constraint must be an active constraint, meaning it must\nhold by equality: t i w\u22bax i + b = 1. This condition is called the complementary\nslackness condition. It implies that either \u03b1 i = 0 or the ith instance lies on the\nboundary (it is a support vector). Note that the KKT conditions are necessary conditions for a stationary point to be a\nsolution of the constrained optimization problem. Under some conditions, they are\nalso sufficient conditions. Luckily, the SVM optimization problem happens to meet\nthese conditions, so any stationary point that meets the KKT conditions is guaranteed\nto be a solution to the constrained optimization problem. We can compute the partial derivatives of the generalized Lagrangian with regard to\nw and b with Equation C-2. Equation C-2. Partial derivatives of the generalized Lagrangian\n\u2207w\u2112w, b, \u03b1 = w \u2212\u2211\ni = 1\nm\n\u03b1 i t i x i\n\u2202\n\u2202b\u2112w, b, \u03b1 = \u2212\u2211\ni = 1\nm\n\u03b1 i t i | Appendix C: SVM Dual Problem\nWhen these partial derivatives are equal to zero, we have Equation C-3. Equation C-3. Properties of the stationary points\nw = \u2211\ni = 1\nm\n\u03b1 i t i x i\n\u2211\ni = 1\nm\n\u03b1 i t i = 0\nIf we plug these results into the definition of the generalized Lagrangian, some terms\ndisappear and we find Equation C-4. Equation C-4."
  },
  {
    "id": 518,
    "content": "Dual form of the SVM problem\n\u2112w, b, \u03b1 = 1\n2 \u2211\ni = 1\nm\n\u2211\nj = 1\nm\n\u03b1 i \u03b1 j t i t j x i \u22bax j\n\u2212\n\u2211\ni = 1\nm\n\u03b1 i\nwith\n\u03b1 i \u22650\nfor i = 1, 2, \u22ef, m\nThe goal is now to find the vector \u03b1 that minimizes this function, with \u03b1 i \u22650 for all\ninstances. This constrained optimization problem is the dual problem we were look\u2010\ning for. Once you find the optimal \u03b1, you can compute w using the first line of Equation C-3. To compute b, you can use the fact that a support vector must verify t(i)(w\u22ba x(i) + b) =\n1, so if the kth instance is a support vector (i.e., \u03b1 k > 0), you can use it to compute\nb = t k \u2212w\u22bax k . However, it is often preferred to compute the average over all sup\u2010\nport vectors to get a more stable and precise value, as in Equation C-5. Equation C-5. Bias term estimation using the dual form\nb = 1\nns \u2211\ni = 1\n\u03b1 i > 0\nm\nt i \u2212w\u22bax i\nSVM Dual Problem | APPENDIX D\nAutodiff\nThis appendix explains how TensorFlow\u2019s autodifferentiation (autodiff) feature\nworks, and how it compares to other solutions. Suppose you define a function f(x, y) = x2y + y + 2, and you need its partial derivatives\n\u2202f/\u2202x and \u2202f/\u2202y, typically to perform Gradient Descent (or some other optimization\nalgorithm). Your main options are manual differentiation, finite difference approxi\u2010\nmation, forward-mode autodiff, and reverse-mode autodiff. TensorFlow implements\nreverse-mode autodiff, but to understand it, it\u2019s useful to look at the other options\nfirst. So let\u2019s go through each of them, starting with manual differentiation. Manual Differentiation\nThe first approach to compute derivatives is to pick up a pencil and a piece of paper\nand use your calculus knowledge to derive the appropriate equation. For the function\nf(x, y) just defined, it is not too hard; you just need to use five rules:\n\u2022 The derivative of a constant is 0. \u2022 The derivative of \u03bbx is \u03bb (where \u03bb is a constant). \u2022 The derivative of x\u03bb is \u03bbx\u03bb \u2013 1, so the derivative of x2 is 2x. \u2022 The derivative of a sum of functions is the sum of these functions\u2019 derivatives. \u2022 The derivative of \u03bb times a function is \u03bb times its derivative. From these rules, you can derive Equation D-1. Equation D-1. Partial derivatives of f(x, y)\n\u2202f\n\u2202x =\n\u2202x2y\n\u2202x\n+ \u2202y\n\u2202x + \u22022\n\u2202x = y\n\u2202x2\n\u2202x\n+ 0 + 0 = 2xy\n\u2202f\n\u2202y =\n\u2202x2y\n\u2202y\n+ \u2202y\n\u2202y + \u22022\n\u2202y = x2 + 1 + 0 = x2 + 1\nThis approach can become very tedious for more complex functions, and you run the\nrisk of making mistakes. Fortunately, there are other options. Let\u2019s look at finite dif\u2010\nference approximation now."
  },
  {
    "id": 519,
    "content": "Finite Difference Approximation\nRecall that the derivative h\u2032(x0) of a function h(x) at a point x0 is the slope of the func\u2010\ntion at that point. More precisely, the derivative is defined as the limit of the slope of a\nstraight line going through this point x0 and another point x on the function, as x gets\ninfinitely close to x0 (see Equation D-2). Equation D-2. Definition of the derivative of a function h(x) at point x0\nh\u2032 x0 =\nlim\nx\nx0\nh x \u2212h x0\nx \u2212x0\n=\nlim\n\u03b5 h x0 + \u03b5 \u2212h x0\n\u03b5\nSo, if we wanted to calculate the partial derivative of f(x, y) with regard to x at x = 3\nand y = 4, we could compute f(3 + \u03b5, 4) \u2013 f(3, 4) and divide the result by \u03b5, using a\nvery small value for \u03b5. This type of numerical approximation of the derivative is called\na finite difference approximation, and this specific equation is called Newton\u2019s differ\u2010\nence quotient. That\u2019s exactly what the following code does:\ndef f(x, y): return x**2*y + y + 2\ndef derivative(f, x, y, x_eps, y_eps): return (f(x + x_eps, y + y_eps) - f(x, y)) / (x_eps + y_eps)\ndf_dx = derivative(f, 3, 4, 0.00001, 0)\ndf_dy = derivative(f, 3, 4, 0, 0.00001)\nUnfortunately, the result is imprecise (and it gets worse for more complicated func\u2010\ntions). The correct results are respectively 24 and 10, but instead we get: | Appendix D: Autodiff\n>>> print(df_dx)\n24.000039999805264\n>>> print(df_dy)\n10.000000000331966\nNotice that to compute both partial derivatives, we have to call f() at least three times\n(we called it four times in the preceding code, but it could be optimized). If there\nwere 1,000 parameters, we would need to call f() at least 1,001 times. When you are\ndealing with large neural networks, this makes finite difference approximation way\ntoo inefficient. However, this method is so simple to implement that it is a great tool to check that the\nother methods are implemented correctly. For example, if it disagrees with your man\u2010\nually derived function, then your function probably contains a mistake. So far, we have considered two ways to compute gradients: using manual differentia\u2010\ntion and using finite difference approximation. Unfortunately, both were fatally\nflawed to train a large-scale neural network. So let\u2019s turn to autodiff, starting with for\u2010\nward mode. Forward-Mode Autodiff\nFigure D-1 shows how forward-mode autodiff works on an even simpler function,\ng(x, y) = 5 + xy. The graph for that function is represented on the left. After forward-\nmode autodiff, we get the graph on the right, which represents the partial derivative\n\u2202g/\u2202x = 0 + (0 \u00d7 x + y \u00d7 1) = y (we could similarly obtain the partial derivative with\nregard to y). Figure D-1. Forward-mode autodiff\nAutodiff | The algorithm will go through the computation graph from the inputs to the outputs\n(hence the name \u201cforward mode\u201d)."
  },
  {
    "id": 520,
    "content": "It starts by getting the partial derivatives of the\nleaf nodes. The constant node (5) returns the constant 0, since the derivative of a con\u2010\nstant is always 0. The variable x returns the constant 1 since \u2202x/\u2202x = 1, and the vari\u2010\nable y returns the constant 0 since \u2202y/\u2202x = 0 (if we were looking for the partial\nderivative with regard to y, it would be the reverse). Now we have all we need to move up the graph to the multiplication node in function\ng. Calculus tells us that the derivative of the product of two functions u and v is\n\u2202(u \u00d7 v)/\u2202x = \u2202v/\u2202x \u00d7 u + v \u00d7 \u2202u/\u2202x. We can therefore construct a large part of the\ngraph on the right, representing 0 \u00d7 x + y \u00d7 1. Finally, we can go up to the addition node in function g. As mentioned, the derivative\nof a sum of functions is the sum of these functions\u2019 derivatives. So we just need to\ncreate an addition node and connect it to the parts of the graph we have already com\u2010\nputed. We get the correct partial derivative: \u2202g/\u2202x = 0 + (0 \u00d7 x + y \u00d7 1). However, this equation can be simplified (a lot). A few pruning steps can be applied\nto the computation graph to get rid of all unnecessary operations, and we get a much\nsmaller graph with just one node: \u2202g/\u2202x = y. In this case simplification is fairly easy,\nbut for a more complex function forward-mode autodiff can produce a huge graph\nthat may be tough to simplify and lead to suboptimal performance. Note that we started with a computation graph, and forward-mode autodiff produced\nanother computation graph. This is called symbolic differentiation, and it has two nice\nfeatures: first, once the computation graph of the derivative has been produced, we\ncan use it as many times as we want to compute the derivatives of the given function\nfor any value of x and y; second, we can run forward-mode autodiff again on the\nresulting graph to get second-order derivatives if we ever need to (i.e., derivatives of\nderivatives). We could even compute third-order derivatives, and so on. But it is also possible to run forward-mode autodiff without constructing a graph\n(i.e., numerically, not symbolically), just by computing intermediate results on the fly. One way to do this is to use dual numbers, which are weird but fascinating numbers\nof the form a + b\u03b5, where a and b are real numbers and \u03b5 is an infinitesimal number\nsuch that \u03b52 = 0 (but \u03b5 \u2260 0). You can think of the dual number 42 + 24\u03b5 as something\nakin to 42.0000\u22ef000024 with an infinite number of 0s (but of course this is simpli\u2010\nfied just to give you some idea of what dual numbers are). A dual number is repre\u2010\nsented in memory as a pair of floats."
  },
  {
    "id": 521,
    "content": "For example, 42 + 24\u03b5 is represented by the pair\n(42.0, 24.0). | Appendix D: Autodiff\nDual numbers can be added, multiplied, and so on, as shown in Equation D-3. Equation D-3. A few operations with dual numbers\n\u03bb a + b\u03b5 = \u03bba + \u03bbb\u03b5\na + b\u03b5 + c + d\u03b5 = a + c + b + d \u03b5\na + b\u03b5 \u00d7 c + d\u03b5 = ac + ad + bc \u03b5 + bd \u03b52 = ac + ad + bc \u03b5\nMost importantly, it can be shown that h(a + b\u03b5) = h(a) + b \u00d7 h\u2032(a)\u03b5, so computing\nh(a + \u03b5) gives you both h(a) and the derivative h\u2032(a) in just one shot. Figure D-2\nshows that the partial derivative of f(x, y) with regard to x at x = 3 and y = 4 (which\nwe will write \u2202f/\u2202x (3, 4)) can be computed using dual numbers. All we need to do is\ncompute f(3 + \u03b5, 4); this will output a dual number whose first component is equal to\nf(3, 4) and whose second component is equal to \u2202f/\u2202x (3, 4). Figure D-2. Forward-mode autodiff using dual numbers\nTo compute \u2202f/\u2202x (3, 4) we would have to go through the graph again, but this time\nwith x = 3 and y = 4 + \u03b5. So forward-mode autodiff is much more accurate than finite difference approxima\u2010\ntion, but it suffers from the same major flaw, at least when there are many inputs and\nfew outputs (as is the case when dealing with neural networks): if there were 1,000\nparameters, it would require 1,000 passes through the graph to compute all the partial\nAutodiff | derivatives. This is where reverse-mode autodiff shines: it can compute all of them in\njust two passes through the graph. Let\u2019s see how. Reverse-Mode Autodiff\nReverse-mode autodiff is the solution implemented by TensorFlow. It first goes\nthrough the graph in the forward direction (i.e., from the inputs to the output) to\ncompute the value of each node. Then it does a second pass, this time in the reverse\ndirection (i.e., from the output to the inputs), to compute all the partial derivatives. The name \u201creverse mode\u201d comes from this second pass through the graph, where gra\u2010\ndients flow in the reverse direction. Figure D-3 represents the second pass. During\nthe first pass, all the node values were computed, starting from x = 3 and y = 4. You\ncan see those values at the bottom right of each node (e.g., x \u00d7 x = 9). The nodes are\nlabeled n1 to n7 for clarity. The output node is n7: f(3, 4) = n7 = 42. Figure D-3. Reverse-mode autodiff | Appendix D: Autodiff\nThe idea is to gradually go down the graph, computing the partial derivative of f(x, y)\nwith regard to each consecutive node, until we reach the variable nodes. For this,\nreverse-mode autodiff relies heavily on the chain rule, shown in Equation D-4. Equation D-4."
  },
  {
    "id": 522,
    "content": "Chain rule\n\u2202f\n\u2202x = \u2202f\n\u2202ni\n\u00d7\n\u2202ni\n\u2202x\nSince n7 is the output node, f = n7 so \u2202f/\u2202n7 = 1. Let\u2019s continue down the graph to n5: how much does f vary when n5 varies? The\nanswer is \u2202f/\u2202n5 = \u2202f/\u2202n7 \u00d7 \u2202n7/\u2202n5. We already know that \u2202f/\u2202n7 = 1, so all we need is\n\u2202n7/\u2202n5. Since n7 simply performs the sum n5 + n6, we find that \u2202n7/\u2202n5 = 1, so \u2202f/\u2202n5\n= 1 \u00d7 1 = 1. Now we can proceed to node n4: how much does f vary when n4 varies? The answer is\n\u2202f/\u2202n4 = \u2202f/\u2202n5 \u00d7 \u2202n5/\u2202n4. Since n5 = n4 \u00d7 n2, we find that \u2202n5/\u2202n4 = n2, so \u2202f/\u2202n4 = 1 \u00d7\nn2 = 4. The process continues until we reach the bottom of the graph. At that point we will\nhave calculated all the partial derivatives of f(x, y) at the point x = 3 and y = 4. In this\nexample, we find \u2202f/\u2202x = 24 and \u2202f/\u2202y = 10. Sounds about right! Reverse-mode autodiff is a very powerful and accurate technique, especially when\nthere are many inputs and few outputs, since it requires only one forward pass plus\none reverse pass per output to compute all the partial derivatives for all outputs with\nregard to all the inputs. When training neural networks, we generally want to mini\u2010\nmize the loss, so there is a single output (the loss), and hence only two passes through\nthe graph are needed to compute the gradients. Reverse-mode autodiff can also han\u2010\ndle functions that are not entirely differentiable, as long as you ask it to compute the\npartial derivatives at points that are differentiable. In Figure D-3, the numerical results are computed on the fly, at each node. However,\nthat\u2019s not exactly what TensorFlow does: instead, it creates a new computation graph. In other words, it implements symbolic reverse-mode autodiff. This way, the compu\u2010\ntation graph to compute the gradients of the loss with regard to all the parameters in\nthe neural network only needs to be generated once, and then it can be executed over\nand over again, whenever the optimizer needs to compute the gradients. Moreover,\nthis makes it possible to compute higher-order derivatives if needed. Autodiff | If you ever want to implement a new type of low-level TensorFlow\noperation in C++, and you want to make it compatible with auto\u2010\ndiff, then you will need to provide a function that returns the par\u2010\ntial derivatives of the function\u2019s outputs with regard to its inputs. For example, suppose you implement a function that computes the\nsquare of its input: f(x) = x2. In that case you would need to provide\nthe corresponding derivative function: f\u2032(x) = 2x."
  },
  {
    "id": 523,
    "content": "| Appendix D: Autodiff\nAPPENDIX E\nOther Popular ANN Architectures\nIn this appendix I will give a quick overview of a few historically important neural\nnetwork architectures that are much less used today than deep Multilayer Perceptrons\n(Chapter 10), convolutional neural networks (Chapter 14), recurrent neural networks\n(Chapter 15), or autoencoders (Chapter 17). They are often mentioned in the litera\u2010\nture, and some are still used in a range of applications, so it is worth knowing about\nthem. Additionally, we will discuss deep belief nets, which were the state of the art in\nDeep Learning until the early 2010s. They are still the subject of very active research,\nso they may well come back with a vengeance in the future. Hopfield Networks\nHopfield networks were first introduced by W. A. Little in 1974, then popularized by J.\nHopfield in 1982. They are associative memory networks: you first teach them some\npatterns, and then when they see a new pattern they (hopefully) output the closest\nlearned pattern. This made them useful for character recognition, in particular,\nbefore they were outperformed by other approaches: you first train the network by\nshowing it examples of character images (each binary pixel maps to one neuron), and\nthen when you show it a new character image, after a few iterations it outputs the\nclosest learned character. Hopfield networks are fully connected graphs (see Figure E-1); that is, every neuron\nis connected to every other neuron. Note that in the diagram the images are 6 \u00d7 6\npixels, so the neural network on the left should contain 36 neurons (and 630 connec\u2010\ntions), but for visual clarity a much smaller network is represented. Figure E-1. Hopfield network\nThe training algorithm works by using Hebb\u2019s rule (see \u201cThe Perceptron\u201d on page\n284): for each training image, the weight between two neurons is increased if the cor\u2010\nresponding pixels are both on or both off, but decreased if one pixel is on and the\nother is off. To show a new image to the network, you just activate the neurons that correspond to\nactive pixels. The network then computes the output of every neuron, and this gives\nyou a new image. You can then take this new image and repeat the whole process. After a while, the network reaches a stable state. Generally, this corresponds to the\ntraining image that most resembles the input image. A so-called energy function is associated with Hopfield nets. At each iteration, the\nenergy decreases, so the network is guaranteed to eventually stabilize to a low-energy\nstate. The training algorithm tweaks the weights in a way that decreases the energy\nlevel of the training patterns, so the network is likely to stabilize in one of these low-\nenergy configurations. Unfortunately, some patterns that were not in the training set\nalso end up with low energy, so the network sometimes stabilizes in a configuration\nthat was not learned. These are called spurious patterns."
  },
  {
    "id": 524,
    "content": "Another major flaw with Hopfield nets is that they don\u2019t scale very well\u2014their mem\u2010\nory capacity is roughly equal to 14% of the number of neurons. For example, to clas\u2010\nsify 28 \u00d7 28\u2013pixel images, you would need a Hopfield net with 784 fully connected\nneurons and 306,936 weights. Such a network would only be able to learn about 110\ndifferent characters (14% of 784). That\u2019s a lot of parameters for such a small memory. | Appendix E: Other Popular ANN Architectures\nBoltzmann Machines\nBoltzmann machines were invented in 1985 by Geoffrey Hinton and Terrence Sejnow\u2010\nski. Just like Hopfield nets, they are fully connected ANNs, but they are based on sto\u2010\nchastic neurons: instead of using a deterministic step function to decide what value to\noutput, these neurons output 1 with some probability, and 0 otherwise. The probabil\u2010\nity function that these ANNs use is based on the Boltzmann distribution (used in\nstatistical mechanics), hence their name. Equation E-1 gives the probability that a\nparticular neuron will output 1. Equation E-1. Probability that the ith neuron will output 1\np si\nnext step = 1 = \u03c3\n\u2211j = 1\nN\nwi, jsj + bi\nT\n\u2022 sj is the jth neuron\u2019s state (0 or 1). \u2022 wi,j is the connection weight between the ith and jth neurons. Note that wi,i = 0. \u2022 bi is the ith neuron\u2019s bias term. We can implement this term by adding a bias neu\u2010\nron to the network. \u2022 N is the number of neurons in the network. \u2022 T is a number called the network\u2019s temperature; the higher the temperature, the\nmore random the output is (i.e., the more the probability approaches 50%). \u2022 \u03c3 is the logistic function. Neurons in Boltzmann machines are separated into two groups: visible units and hid\u2010\nden units (see Figure E-2). All neurons work in the same stochastic way, but the visi\u2010\nble units are the ones that receive the inputs and from which outputs are read. Because of its stochastic nature, a Boltzmann machine will never stabilize into a fixed\nconfiguration; instead, it will keep switching between many configurations. If it is left\nrunning for a sufficiently long time, the probability of observing a particular configu\u2010\nration will only be a function of the connection weights and bias terms, not of the\noriginal configuration (similarly, after you shuffle a deck of cards for long enough, the\nconfiguration of the deck does not depend on the initial state). When the network\nreaches this state where the original configuration is \u201cforgotten,\u201d it is said to be in\nthermal equilibrium (although its configuration keeps changing all the time). By set\u2010\nting the network parameters appropriately, letting the network reach thermal equili\u2010\nbrium, and then observing its state, we can simulate a wide range of probability\ndistributions. This is called a generative model. Other Popular ANN Architectures | Figure E-2."
  },
  {
    "id": 525,
    "content": "Boltzmann machine\nTraining a Boltzmann machine means finding the parameters that will make the net\u2010\nwork approximate the training set\u2019s probability distribution. For example, if there are\nthree visible neurons and the training set contains 75% (0, 1, 1) triplets, 10% (0, 0, 1)\ntriplets, and 15% (1, 1, 1) triplets, then after training a Boltzmann machine, you could\nuse it to generate random binary triplets with about the same probability distribu\u2010\ntion. For example, about 75% of the time it would output the (0, 1, 1) triplet. Such a generative model can be used in a variety of ways. For example, if it is trained\non images, and you provide an incomplete or noisy image to the network, it will\nautomatically \u201crepair\u201d the image in a reasonable way. You can also use a generative\nmodel for classification. Just add a few visible neurons to encode the training image\u2019s\nclass (e.g., add 10 visible neurons and turn on only the fifth neuron when the training\nimage represents a 5). Then, when given a new image, the network will automatically\nturn on the appropriate visible neurons, indicating the image\u2019s class (e.g., it will turn\non the fifth visible neuron if the image represents a 5). Unfortunately, there is no efficient technique to train Boltzmann machines. However,\nfairly efficient algorithms have been developed to train restricted Boltzmann machines\n(RBMs). Restricted Boltzmann Machines\nAn RBM is simply a Boltzmann machine in which there are no connections between\nvisible units or between hidden units, only between visible and hidden units. For\nexample, Figure E-3 represents an RBM with three visible units and four hidden\nunits. | Appendix E: Other Popular ANN Architectures\n1 Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n and Geoffrey E. Hinton, \u201cOn Contrastive Divergence Learning,\u201d Proceedings of\nthe 10th International Workshop on Artificial Intelligence and Statistics (2005): 59\u201366. Figure E-3. Restricted Boltzmann machine\nA very efficient training algorithm called Contrastive Divergence was introduced in\n2005 by Miguel \u00c1. Carreira-Perpi\u00f1\u00e1n and Geoffrey Hinton.1 Here is how it works: for\neach training instance x, the algorithm starts by feeding it to the network by setting\nthe state of the visible units to x1, x2, \u22ef, xn. Then you compute the state of the hidden\nunits by applying the stochastic equation described before (Equation E-1). This gives\nyou a hidden vector h (where hi is equal to the state of the ith unit). Next you compute\nthe state of the visible units, by applying the same stochastic equation. This gives you\na vector x\u02b9. Then once again you compute the state of the hidden units, which gives\nyou a vector h\u02b9. Now you can update each connection weight by applying the rule in\nEquation E-2, where \u03b7 is the learning rate. Equation E-2. Contrastive divergence weight update\nwi, j\nwi, j + \u03b7 xh\u22ba\u2212x\u2032h\u2032\u22ba\nThe great benefit of this algorithm is that it does not require waiting for the network\nto reach thermal equilibrium: it just goes forward, backward, and forward again, and\nthat\u2019s it."
  },
  {
    "id": 526,
    "content": "This makes it incomparably more efficient than previous algorithms, and it\nwas a key ingredient to the first success of Deep Learning based on multiple stacked\nRBMs. Deep Belief Nets\nSeveral layers of RBMs can be stacked; the hidden units of the first-level RBM serve\nas the visible units for the second-layer RBM, and so on. Such an RBM stack is called\na deep belief net (DBN). Other Popular ANN Architectures | 2 Geoffrey E. Hinton et al., \u201cA Fast Learning Algorithm for Deep Belief Nets,\u201d Neural Computation 18 (2006):\n1527\u20131554. Yee-Whye Teh, one of Geoffrey Hinton\u2019s students, observed that it was possible to\ntrain DBNs one layer at a time using Contrastive Divergence, starting with the lower\nlayers and then gradually moving up to the top layers. This led to the groundbreaking\narticle that kickstarted the Deep Learning tsunami in 2006.2\nJust like RBMs, DBNs learn to reproduce the probability distribution of their inputs,\nwithout any supervision. However, they are much better at it, for the same reason that\ndeep neural networks are more powerful than shallow ones: real-world data is often\norganized in hierarchical patterns, and DBNs take advantage of that. Their lower lay\u2010\ners learn low-level features in the input data, while higher layers learn high-level\nfeatures. Just like RBMs, DBNs are fundamentally unsupervised, but you can also train them\nin a supervised manner by adding some visible units to represent the labels. More\u2010\nover, one great feature of DBNs is that they can be trained in a semisupervised fash\u2010\nion. Figure E-4 represents such a DBN configured for semisupervised learning. Figure E-4. A deep belief network configured for semisupervised learning\nFirst, RBM 1 is trained without supervision. It learns low-level features in the training\ndata. Then RBM 2 is trained with RBM 1\u2019s hidden units as inputs, again without | Appendix E: Other Popular ANN Architectures\n3 See this video by Geoffrey Hinton for more details and a demo: \nsupervision: it learns higher-level features (note that RBM 2\u2019s hidden units include\nonly the three rightmost units, not the label units). Several more RBMs could be\nstacked this way, but you get the idea. So far, training was 100% unsupervised. Lastly,\nRBM 3 is trained using RBM 2\u2019s hidden units as inputs, as well as extra visible units\nused to represent the target labels (e.g., a one-hot vector representing the instance\nclass). It learns to associate high-level features with training labels. This is the super\u2010\nvised step. At the end of training, if you feed RBM 1 a new instance, the signal will propagate up\nto RBM 2, then up to the top of RBM 3, and then back down to the label units; hope\u2010\nfully, the appropriate label will light up. This is how a DBN can be used for\nclassification. One great benefit of this semisupervised approach is that you don\u2019t need much\nlabeled training data."
  },
  {
    "id": 527,
    "content": "If the unsupervised RBMs do a good enough job, then only a\nsmall amount of labeled training instances per class will be necessary. Similarly, a\nbaby learns to recognize objects without supervision, so when you point to a chair\nand say \u201cchair,\u201d the baby can associate the word \u201cchair\u201d with the class of objects it has\nalready learned to recognize on its own. You don\u2019t need to point to every single chair\nand say \u201cchair\u201d; only a few examples will suffice (just enough so the baby can be sure\nthat you are indeed referring to the chair, not to its color or one of the chair\u2019s parts). Quite amazingly, DBNs can also work in reverse. If you activate one of the label units,\nthe signal will propagate up to the hidden units of RBM 3, then down to RBM 2, and\nthen RBM 1, and a new instance will be output by the visible units of RBM 1. This\nnew instance will usually look like a regular instance of the class whose label unit you\nactivated. This generative capability of DBNs is quite powerful. For example, it has\nbeen used to automatically generate captions for images, and vice versa: first a DBN is\ntrained (without supervision) to learn features in images, and another DBN is trained\n(again without supervision) to learn features in sets of captions (e.g., \u201ccar\u201d often\ncomes with \u201cautomobile\u201d). Then an RBM is stacked on top of both DBNs and trained\nwith a set of images along with their captions; it learns to associate high-level features\nin images with high-level features in captions. Next, if you feed the image DBN an\nimage of a car, the signal will propagate through the network, up to the top-level\nRBM, and back down to the bottom of the caption DBN, producing a caption. Due to\nthe stochastic nature of RBMs and DBNs, the caption will keep changing randomly,\nbut it will generally be appropriate for the image. If you generate a few hundred cap\u2010\ntions, the most frequently generated ones will likely be a good description of the\nimage.3\nOther Popular ANN Architectures | Self-Organizing Maps\nSelf-organizing maps (SOMs) are quite different from all the other types of neural net\u2010\nworks we have discussed so far. They are used to produce a low-dimensional repre\u2010\nsentation of a high-dimensional dataset, generally for visualization, clustering, or\nclassification. The neurons are spread across a map (typically 2D for visualization,\nbut it can be any number of dimensions you want), as shown in Figure E-5, and each\nneuron has a weighted connection to every input (note that the diagram shows just\ntwo inputs, but there are typically a very large number, since the whole point of\nSOMs is to reduce dimensionality). Figure E-5. Self-organizing map\nOnce the network is trained, you can feed it a new instance and this will activate only\none neuron (i.e., one point on the map): the neuron whose weight vector is closest to\nthe input vector."
  },
  {
    "id": 528,
    "content": "In general, instances that are nearby in the original input space will\nactivate neurons that are nearby on the map. This makes SOMs useful not only for\nvisualization (in particular, you can easily identify clusters on the map), but also for\napplications like speech recognition. For example, if each instance represents an\naudio recording of a person pronouncing a vowel, then different pronunciations of\nthe vowel \u201ca\u201d will activate neurons in the same area of the map, while instances of the\nvowel \u201ce\u201d will activate neurons in another area, and intermediate sounds will gener\u2010\nally activate intermediate neurons on the map. | Appendix E: Other Popular ANN Architectures\n4 You can imagine a class of young children with roughly similar skills. One child happens to be slightly better\nat basketball. This motivates them to practice more, especially with their friends. After a while, this group of\nfriends gets so good at basketball that other kids cannot compete. But that\u2019s okay, because the other kids spe\u2010\ncialize in other areas. After a while, the class is full of little specialized groups. One important difference from the other dimensionality reduction\ntechniques discussed in Chapter 8 is that all instances get mapped\nto a discrete number of points in the low-dimensional space (one\npoint per neuron). When there are very few neurons, this techni\u2010\nque is better described as clustering rather than dimensionality\nreduction. The training algorithm is unsupervised. It works by having all the neurons compete\nagainst each other. First, all the weights are initialized randomly. Then a training\ninstance is picked randomly and fed to the network. All neurons compute the dis\u2010\ntance between their weight vector and the input vector (this is very different from the\nartificial neurons we have seen so far). The neuron that measures the smallest dis\u2010\ntance wins and tweaks its weight vector to be slightly closer to the input vector, mak\u2010\ning it more likely to win future competitions for other inputs similar to this one. It\nalso recruits its neighboring neurons, and they too update their weight vectors to be\nslightly closer to the input vector (but they don\u2019t update their weights as much as the\nwinning neuron). Then the algorithm picks another training instance and repeats the\nprocess, again and again. This algorithm tends to make nearby neurons gradually\nspecialize in similar inputs.4\nOther Popular ANN Architectures | 1 If you are not familiar with Unicode code points, please check out \nAPPENDIX F\nSpecial Data Structures\nIn this appendix we will take a very quick look at the data structures supported by\nTensorFlow, beyond regular float or integer tensors. This includes strings, ragged ten\u2010\nsors, sparse tensors, tensor arrays, sets, and queues."
  },
  {
    "id": 529,
    "content": "Strings\nTensors can hold byte strings, which is useful in particular for natural language pro\u2010\ncessing (see Chapter 16):\n>>> tf.constant(b\"hello world\")\n<tf.Tensor: id=149, shape=(), dtype=string, numpy=b'hello world'>\nIf you try to build a tensor with a Unicode string, TensorFlow automatically encodes\nit to UTF-8:\n>>> tf.constant(\"caf\u00e9\")\n<tf.Tensor: id=138, shape=(), dtype=string, numpy=b'caf\\xc3\\xa9'>\nIt is also possible to create tensors representing Unicode strings. Just create an array\nof 32-bit integers, each representing a single Unicode code point:1\n>>> tf.constant([ord(c) for c in \"caf\u00e9\"])\n<tf.Tensor: id=211, shape=(4,), dtype=int32, numpy=array([ 99, 97, 102, 233], dtype=int32)> In tensors of type tf.string, the string length is not part of the ten\u2010\nsor\u2019s shape. In other words, strings are considered as atomic values. However, in a Unicode string tensor (i.e., an int32 tensor), the\nlength of the string is part of the tensor\u2019s shape. The tf.strings package contains several functions to manipulate string tensors,\nsuch as length() to count the number of bytes in a byte string (or the number of\ncode points if you set unit=\"UTF8_CHAR\"), unicode_encode() to convert a Unicode\nstring tensor (i.e., int32 tensor) to a byte string tensor, and unicode_decode() to do\nthe reverse:\n>>> b = tf.strings.unicode_encode(u, \"UTF-8\")\n>>> tf.strings.length(b, unit=\"UTF8_CHAR\")\n<tf.Tensor: id=386, shape=(), dtype=int32, numpy=4>\n>>> tf.strings.unicode_decode(b, \"UTF-8\")\n<tf.Tensor: id=393, shape=(4,), dtype=int32, numpy=array([ 99, 97, 102, 233], dtype=int32)>\nYou can also manipulate tensors containing multiple strings:\n>>> p = tf.constant([\"Caf\u00e9\", \"Coffee\", \"caff\u00e8\", \"\u5496\u5561\"])\n>>> tf.strings.length(p, unit=\"UTF8_CHAR\")\n<tf.Tensor: id=299, shape=(4,), dtype=int32, numpy=array([4, 6, 5, 2], dtype=int32)>\n>>> r = tf.strings.unicode_decode(p, \"UTF8\")\n>>> r\ntf.RaggedTensor(values=tf.Tensor(\n[ 67 97 102 233 67 111 102 102 101 101 99 97 102 102 232 21654 21857], shape=(17,), dtype=int32), row_splits=tf.Tensor([ 0 4 10 15 17], shape=(5,), dtype=int64))\n>>> print(r)\n<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97, 102, 102, 232], [21654, 21857]]>\nNotice that the decoded strings are stored in a RaggedTensor. What is that? Ragged Tensors\nA ragged tensor is a special kind of tensor that represents a list of arrays of different\nsizes. More generally, it is a tensor with one or more ragged dimensions, meaning\ndimensions whose slices may have different lengths. In the ragged tensor r, the sec\u2010\nond dimension is a ragged dimension. In all ragged tensors, the first dimension is\nalways a regular dimension (also called a uniform dimension). | Appendix F: Special Data Structures\nAll the elements of the ragged tensor r are regular tensors. For example, let\u2019s look at\nthe second element of the ragged tensor:\n>>> print(r[1])\ntf.Tensor([ 67 111 102 102 101 101], shape=(6,), dtype=int32)\nThe tf.ragged package contains several functions to create and manipulate ragged\ntensors."
  },
  {
    "id": 530,
    "content": "Let\u2019s create a second ragged tensor using tf.ragged.constant() and concat\u2010\nenate it with the first ragged tensor, along axis 0:\n>>> r2 = tf.ragged.constant([[65, 66], [], [67]])\n>>> print(tf.concat([r, r2], axis=0))\n<tf.RaggedTensor [[67, 97, 102, 233], [67, 111, 102, 102, 101, 101], [99, 97,\n102, 102, 232], [21654, 21857], [65, 66], [], [67]]>\nThe result is not too surprising: the tensors in r2 were appended after the tensors in r\nalong axis 0. But what if we concatenate r and another ragged tensor along axis 1? >>> r3 = tf.ragged.constant([[68, 69, 70], [71], [], [72, 73]])\n>>> print(tf.concat([r, r3], axis=1))\n<tf.RaggedTensor [[67, 97, 102, 233, 68, 69, 70], [67, 111, 102, 102, 101, 101,\n71], [99, 97, 102, 102, 232], [21654, 21857, 72, 73]]>\nThis time, notice that the ith tensor in r and the ith tensor in r3 were concatenated. Now that\u2019s more unusual, since all of these tensors can have different lengths. If you call the to_tensor() method, it gets converted to a regular tensor, padding\nshorter tensors with zeros to get tensors of equal lengths (you can change the default\nvalue by setting the default_value argument):\n>>> r.to_tensor()\n<tf.Tensor: id=1056, shape=(4, 6), dtype=int32, numpy=\narray([[ 67, 97, 102, 233, 0, 0], [ 67, 111, 102, 102, 101, 101], [ 99, 97, 102, 102, 232, 0], [21654, 21857, 0, 0, 0, 0]], dtype=int32)>\nMany TF operations support ragged tensors. For the full list, see the documentation\nof the tf.RaggedTensor class. Sparse Tensors\nTensorFlow can also efficiently represent sparse tensors (i.e., tensors containing\nmostly zeros). Just create a tf.SparseTensor, specifying the indices and values of the\nnonzero elements and the tensor\u2019s shape. The indices must be listed in \u201creading\norder\u201d (from left to right, and top to bottom). If you are unsure, just use\ntf.sparse.reorder(). You can convert a sparse tensor to a dense tensor (i.e., a regu\u2010\nlar tensor) using tf.sparse.to_dense():\nSpecial Data Structures | >>> s = tf.SparseTensor(indices=[[0, 1], [1, 0], [2, 3]], values=[1., 2., 3. ], dense_shape=[3, 4])\n>>> tf.sparse.to_dense(s)\n<tf.Tensor: id=1074, shape=(3, 4), dtype=float32, numpy=\narray([[0., 1., 0., 0. ], [2., 0., 0., 0. ], [0., 0., 0., 3. ]], dtype=float32)>\nNote that sparse tensors do not support as many operations as dense tensors. For\nexample, you can multiply a sparse tensor by any scalar value, and you get a new\nsparse tensor, but you cannot add a scalar value to a sparse tensor, as this would not\nreturn a sparse tensor:\n>>> s * 3.14\n<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x13205d470>\n>>> s + 42.0\n[...] TypeError: unsupported operand type(s) for +: 'SparseTensor' and 'float'\nTensor Arrays\nA tf.TensorArray represents a list of tensors. This can be handy in dynamic models\ncontaining loops, to accumulate results and later compute some statistics. You can\nread or write tensors at any location in the array:\narray = tf.TensorArray(dtype=tf.float32, size=3)\narray = array.write(0, tf.constant([1., 2.])) array = array.write(1, tf.constant([3., 10.])) array = array.write(2, tf.constant([5., 7.])) tensor1 = array.read(1) # => returns (and pops!) tf.constant([3., 10.])"
  },
  {
    "id": 531,
    "content": "Notice that reading an item pops it from the array, replacing it with a tensor of the\nsame shape, full of zeros. When you write to the array, you must assign the output back to\nthe array, as shown in this code example. If you don\u2019t, although\nyour code will work fine in eager mode, it will break in graph mode\n(these modes were presented in Chapter 12). When creating a TensorArray, you must provide its size, except in graph mode. Alternatively, you can leave the size unset and instead set dynamic_size=True, but\nthis will hinder performance, so if you know the size in advance, you should set it. You must also specify the dtype, and all elements must have the same shape as the\nfirst one written to the array. You can stack all the items into a regular tensor by calling the stack() method: | Appendix F: Special Data Structures\n>>> array.stack()\n<tf.Tensor: id=2110875, shape=(3, 2), dtype=float32, numpy=\narray([[1., 2. ], [0., 0. ], [5., 7. ]], dtype=float32)>\nSets\nTensorFlow supports sets of integers or strings (but not floats). It represents them\nusing regular tensors. For example, the set {1, 5, 9} is just represented as the tensor\n[[1, 5, 9]]. Note that the tensor must have at least two dimensions, and the sets\nmust be in the last dimension. For example, [[1, 5, 9], [2, 5, 11]] is a tensor\nholding two independent sets: {1, 5, 9} and {2, 5, 11}. If some sets are shorter\nthan others, you must pad them with a padding value (0 by default, but you can use\nany other value you prefer). The tf.sets package contains several functions to manipulate sets. For example, let\u2019s\ncreate two sets and compute their union (the result is a sparse tensor, so we call\nto_dense() to display it):\n>>> a = tf.constant([[1, 5, 9]])\n>>> b = tf.constant([[5, 6, 9, 11]])\n>>> u = tf.sets.union(a, b)\n>>> u\n<tensorflow.python.framework.sparse_tensor.SparseTensor at 0x132b60d30>\n>>> tf.sparse.to_dense(u)\n<tf.Tensor: [...] numpy=array([[ 1, 5, 6, 9, 11]], dtype=int32)>\nYou can also compute the union of multiple pairs of sets simultaneously:\n>>> a = tf.constant([[1, 5, 9], [10, 0, 0]])\n>>> b = tf.constant([[5, 6, 9, 11], [13, 0, 0, 0, 0]])\n>>> u = tf.sets.union(a, b)\n>>> tf.sparse.to_dense(u)\n<tf.Tensor: [...] numpy=array([[ 1, 5, 6, 9, 11], [ 0, 10, 13, 0, 0]], dtype=int32)>\nIf you prefer to use a different padding value, you must set default_value when call\u2010\ning to_dense():\n>>> tf.sparse.to_dense(u, default_value=-1)\n<tf.Tensor: [...] numpy=array([[ 1, 5, 6, 9, 11], [ 0, 10, 13, -1, -1]], dtype=int32)>\nThe default default_value is 0, so when dealing with string sets,\nyou must set the default_value (e.g., to an empty string). Special Data Structures | Other functions available in tf.sets include difference(), intersection(), and\nsize(), which are self-explanatory. If you want to check whether or not a set contains\nsome given values, you can compute the intersection of that set and the values."
  },
  {
    "id": 532,
    "content": "If you\nwant to add some values to a set, you can compute the union of the set and the values. Queues\nA queue is a data structure to which you can push data records, and later pull them\nout. TensorFlow implements several types of queues in the tf.queue package. They\nused to be very important when implementing efficient data loading and preprocess\u2010\ning pipelines, but the tf.data API has essentially rendered them useless (except per\u2010\nhaps in some rare cases) because it is much simpler to use and provides all the tools\nyou need to build efficient pipelines. For the sake of completeness, though, let\u2019s take a\nquick look at them. The simplest kind of queue is the first-in, first-out (FIFO) queue. To build it, you\nneed to specify the maximum number of records it can contain. Moreover, each\nrecord is a tuple of tensors, so you must specify the type of each tensor, and option\u2010\nally their shapes. For example, the following code example creates a FIFO queue with\nmaximum three records, each containing a tuple with a 32-bit integer and a string. Then it pushes two records to it, looks at the size (which is 2 at this point), and pulls a\nrecord out:\n>>> q = tf.queue.FIFOQueue(3, [tf.int32, tf.string], shapes=[(), ()])\n>>> q.enqueue([10, b\"windy\"])\n>>> q.enqueue([15, b\"sunny\"])\n>>> q.size()\n<tf.Tensor: id=62, shape=(), dtype=int32, numpy=2>\n>>> q.dequeue()\n[<tf.Tensor: id=6, shape=(), dtype=int32, numpy=10>, <tf.Tensor: id=7, shape=(), dtype=string, numpy=b'windy'>]\nIt is also possible to enqueue and dequeue multiple records at once (the latter requires\nspecifying the shapes when creating the queue):\n>>> q.enqueue_many([[13, 16], [b'cloudy', b'rainy']])\n>>> q.dequeue_many(3)\n[<tf.Tensor: [...] numpy=array([15, 13, 16], dtype=int32)>, <tf.Tensor: [...] numpy=array([b'sunny', b'cloudy', b'rainy'], dtype=object)>]\nOther queue types include:\nPaddingFIFOQueue\nSame as FIFOQueue, but its dequeue_many() method supports dequeueing multi\u2010\nple records of different shapes. It automatically pads the shortest records to\nensure all the records in the batch have the same shape. | Appendix F: Special Data Structures\nPriorityQueue\nA queue that dequeues records in a prioritized order. The priority must be a 64-\nbit integer included as the first element of each record. Surprisingly, records with\na lower priority will be dequeued first. Records with the same priority will be\ndequeued in FIFO order. RandomShuffleQueue\nA queue whose records are dequeued in random order. This was useful to imple\u2010\nment a shuffle buffer before tf.data existed. If a queue is already full and you try to enqueue another record, the enqueue*()\nmethod will freeze until a record is dequeued by another thread. Similarly, if a queue\nis empty and you try to dequeue a record, the dequeue*() method will freeze until\nrecords are pushed to the queue by another thread. Special Data Structures | APPENDIX G\nTensorFlow Graphs\nIn this appendix, we will explore the graphs generated by TF Functions (see Chap\u2010\nter 12). TF Functions and Concrete Functions\nTF Functions are polymorphic, meaning they support inputs of different types (and\nshapes)."
  },
  {
    "id": 533,
    "content": "For example, consider the following tf_cube() function:\n@tf.function\ndef tf_cube(x): return x ** 3\nEvery time you call a TF Function with a new combination of input types or shapes, it\ngenerates a new concrete function, with its own graph specialized for this particular\ncombination. Such a combination of argument types and shapes is called an input sig\u2010\nnature. If you call the TF Function with an input signature it has already seen before,\nit will reuse the concrete function it generated earlier. For example, if you call\ntf_cube(tf.constant(3.0)), the TF Function will reuse the same concrete function\nit used for tf_cube(tf.constant(2.0)) (for float32 scalar tensors). But it will gener\u2010\nate a new concrete function if you call tf_cube(tf.constant([2.0])) or\ntf_cube(tf.constant([3.0])) (for float32 tensors of shape [1]), and yet another for\ntf_cube(tf.constant([[1.0, 2.0], [3.0, 4.0]])) (for float32 tensors of shape\n[2, 2]). You can get the concrete function for a particular combination of inputs by\ncalling the TF Function\u2019s get_concrete_function() method. It can then be called\nlike a regular function, but it will only support one input signature (in this example,\nfloat32 scalar tensors): >>> concrete_function = tf_cube.get_concrete_function(tf.constant(2.0))\n>>> concrete_function\n<tensorflow.python.eager.function.ConcreteFunction at 0x155c29240>\n>>> concrete_function(tf.constant(2.0))\n<tf.Tensor: id=19068249, shape=(), dtype=float32, numpy=8.0>\nFigure G-1 shows the tf_cube() TF Function, after we called tf_cube(2) and\ntf_cube(tf.constant(2.0)): two concrete functions were generated, one for each\nsignature, each with its own optimized function graph (FuncGraph), and its own func\u2010\ntion definition (FunctionDef). A function definition points to the parts of the graph\nthat correspond to the function\u2019s inputs and outputs. In each FuncGraph, the nodes\n(ovals) represent operations (e.g., power, constants, or placeholders for arguments\nlike x), while the edges (the solid arrows between the operations) represent the ten\u2010\nsors that will flow through the graph. The concrete function on the left is specialized\nfor x = 2, so TensorFlow managed to simplify it to just output 8 all the time (note\nthat the function definition does not even have an input). The concrete function on\nthe right is specialized for float32 scalar tensors, and it could not be simplified. If we\ncall tf_cube(tf.constant(5.0)), the second concrete function will be called, the\nplaceholder operation for x will output 5.0, then the power operation will compute\n5.0 ** 3, so the output will be 125.0. Figure G-1. The tf_cube() TF Function, with its ConcreteFunctions and their Function\u2010\nGraphs\nThe tensors in these graphs are symbolic tensors, meaning they don\u2019t have an actual\nvalue, just a data type, a shape, and a name. They represent the future tensors that will\nflow through the graph once an actual value is fed to the placeholder x and the graph\nis executed. Symbolic tensors make it possible to specify ahead of time how to | Appendix G: TensorFlow Graphs\n1 You can safely ignore it\u2014it is only here for technical reasons, to ensure that TF Functions don\u2019t leak internal\nstructures."
  },
  {
    "id": 534,
    "content": "connect operations, and they also allow TensorFlow to recursively infer the data types\nand shapes of all tensors, given the data types and shapes of their inputs. Now let\u2019s continue to peek under the hood, and see how to access function definitions\nand function graphs and how to explore a graph\u2019s operations and tensors. Exploring Function Definitions and Graphs\nYou can access a concrete function\u2019s computation graph using the graph attribute,\nand get the list of its operations by calling the graph\u2019s get_operations() method:\n>>> concrete_function.graph\n<tensorflow.python.framework.func_graph.FuncGraph at 0x14db5ef98>\n>>> ops = concrete_function.graph.get_operations()\n>>> ops\n[<tf.Operation 'x' type=Placeholder>, <tf.Operation 'pow/y' type=Const>, <tf.Operation 'pow' type=Pow>, <tf.Operation 'Identity' type=Identity>]\nIn this example, the first operation represents the input argument x (it is called a\nplaceholder), the second \u201coperation\u201d represents the constant 3, the third operation\nrepresents the power operation (**), and the final operation represents the output of\nthis function (it is an identity operation, meaning it will do nothing more than copy\nthe output of the addition operation1). Each operation has a list of input and output\ntensors that you can easily access using the operation\u2019s inputs and outputs attributes. For example, let\u2019s get the list of inputs and outputs of the power operation:\n>>> pow_op = ops[2]\n>>> list(pow_op.inputs)\n[<tf.Tensor 'x:0' shape=() dtype=float32>, <tf.Tensor 'pow/y:0' shape=() dtype=float32>]\n>>> pow_op.outputs\n[<tf.Tensor 'pow:0' shape=() dtype=float32>]\nThis computation graph is represented in Figure G-2. TensorFlow Graphs | 2 A popular binary format discussed in Chapter 13. Figure G-2. Example of a computation graph\nNote that each operation has a name. It defaults to the name of the operation (e.g.,\n\"pow\"), but you can define it manually when calling the operation (e.g., tf.pow(x,\n3, name=\"other_name\")). If a name already exists, TensorFlow automatically adds a\nunique index (e.g., \"pow_1\", \"pow_2\", etc.). Each tensor also has a unique name: it is\nalways the name of the operation that outputs this tensor, plus :0 if it is the opera\u2010\ntion\u2019s first output, or :1 if it is the second output, and so on. You can fetch an opera\u2010\ntion or a tensor by name using the graph\u2019s get_operation_by_name() or\nget_tensor_by_name() methods:\n>>> concrete_function.graph.get_operation_by_name('x')\n<tf.Operation 'x' type=Placeholder>\n>>> concrete_function.graph.get_tensor_by_name('Identity:0')\n<tf.Tensor 'Identity:0' shape=() dtype=float32>\nThe concrete function also contains the function definition (represented as a protocol\nbuffer2), which includes the function\u2019s signature. This signature allows the concrete\nfunction to know which placeholders to feed with the input values, and which tensors\nto return:\n>>> concrete_function.function_def.signature\nname: \"__inference_cube_19068241\"\ninput_arg { name: \"x\" type: DT_FLOAT\n}\noutput_arg { name: \"identity\" type: DT_FLOAT\n} | Appendix G: TensorFlow Graphs\nNow let\u2019s look more closely at tracing. A Closer Look at Tracing\nLet\u2019s tweak the tf_cube() function to print its input:\n@tf.function\ndef tf_cube(x): print(\"x =\", x) return x ** 3\nNow let\u2019s call it:\n>>> result = tf_cube(tf.constant(2.0))\nx = Tensor(\"x:0\", shape=(), dtype=float32)\n>>> result\n<tf.Tensor: id=19068290, shape=(), dtype=float32, numpy=8.0>\nThe result looks good, but look at what was printed: x is a symbolic tensor!"
  },
  {
    "id": 535,
    "content": "It has a\nshape and a data type, but no value. Plus it has a name (\"x:0\"). This is because the\nprint() function is not a TensorFlow operation, so it will only run when the Python\nfunction is traced, which happens in graph mode, with arguments replaced with sym\u2010\nbolic tensors (same type and shape, but no value). Since the print() function was not\ncaptured into the graph, the next times we call tf_cube() with float32 scalar tensors,\nnothing is printed:\n>>> result = tf_cube(tf.constant(3.0))\n>>> result = tf_cube(tf.constant(4.0))\nBut if we call tf_cube() with a tensor of a different type or shape, or with a new\nPython value, the function will be traced again, so the print() function will be called:\n>>> result = tf_cube(2) # new Python value: trace! x = 2\n>>> result = tf_cube(3) # new Python value: trace! x = 3\n>>> result = tf_cube(tf.constant([[1., 2.]])) # New shape: trace! x = Tensor(\"x:0\", shape=(1, 2), dtype=float32)\n>>> result = tf_cube(tf.constant([[3., 4. ], [5., 6.]])) # New shape: trace! x = Tensor(\"x:0\", shape=(None, 2), dtype=float32)\n>>> result = tf_cube(tf.constant([[7., 8. ], [9., 10.]])) # Same shape: no trace\nIf your function has Python side effects (e.g., it saves some logs to\ndisk), be aware that this code will only run when the function is\ntraced (i.e., every time the TF Function is called with a new input\nsignature). It best to assume that the function may be traced (or\nnot) any time the TF Function is called. TensorFlow Graphs | In some cases, you may want to restrict a TF Function to a specific input signature. For example, suppose you know that you will only ever call a TF Function with\nbatches of 28 \u00d7 28\u2013pixel images, but the batches will have very different sizes. You\nmay not want TensorFlow to generate a different concrete function for each batch\nsize, or count on it to figure out on its own when to use None. In this case, you can\nspecify the input signature like this:\n@tf.function(input_signature=[tf.TensorSpec([None, 28, 28], tf.float32)])\ndef shrink(images): return images[:, ::2, ::2] # drop half the rows and columns\nThis TF Function will accept any float32 tensor of shape [*, 28, 28], and it will reuse\nthe same concrete function every time:\nimg_batch_1 = tf.random.uniform(shape=[100, 28, 28])\nimg_batch_2 = tf.random.uniform(shape=[50, 28, 28])\npreprocessed_images = shrink(img_batch_1) # Works fine. Traces the function. preprocessed_images = shrink(img_batch_2) # Works fine. Same concrete function. However, if you try to call this TF Function with a Python value, or a tensor of an\nunexpected data type or shape, you will get an exception:\nimg_batch_3 = tf.random.uniform(shape=[2, 2, 2])\npreprocessed_images = shrink(img_batch_3) # ValueError! Unexpected signature. Using AutoGraph to Capture Control Flow\nIf your function contains a simple for loop, what do you expect will happen?"
  },
  {
    "id": 536,
    "content": "For\nexample, let\u2019s write a function that will add 10 to its input, by just adding 1 10 times:\n@tf.function\ndef add_10(x): for i in range(10): x += 1 return x\nIt works fine, but when we look at its graph, we find that it does not contain a loop: it\njust contains 10 addition operations! >>> add_10(tf.constant(0))\n<tf.Tensor: id=19280066, shape=(), dtype=int32, numpy=10>\n>>> add_10.get_concrete_function(tf.constant(0)).graph.get_operations()\n[<tf.Operation 'x' type=Placeholder>, [...], <tf.Operation 'add' type=Add>, [...], <tf.Operation 'add_1' type=Add>, [...], <tf.Operation 'add_2' type=Add>, [...], [...] <tf.Operation 'add_9' type=Add>, [...], <tf.Operation 'Identity' type=Identity>] | Appendix G: TensorFlow Graphs\nThis actually makes sense: when the function got traced, the loop ran 10 times, so the\nx += 1 operation was run 10 times, and since it was in graph mode, it recorded this\noperation 10 times in the graph. You can think of this for loop as a \u201cstatic\u201d loop that\ngets unrolled when the graph is created. If you want the graph to contain a \u201cdynamic\u201d loop instead (i.e., one that runs when\nthe graph is executed), you can create one manually using the tf.while_loop() oper\u2010\nation, but it is not very intuitive (see the \u201cUsing AutoGraph to Capture Control Flow\u201d\nsection of the Chapter 12 notebook for an example). Instead, it is much simpler to use\nTensorFlow\u2019s AutoGraph feature, discussed in Chapter 12. AutoGraph is actually acti\u2010\nvated by default (if you ever need to turn it off, you can pass autograph=False to\ntf.function()). So if it is on, why didn\u2019t it capture the for loop in the add_10()\nfunction? Well, it only captures for loops that iterate over tf.range(), not range(). This is to give you the choice:\n\u2022 If you use range(), the for loop will be static, meaning it will only be executed\nwhen the function is traced. The loop will be \u201cunrolled\u201d into a set of operations\nfor each iteration, as we saw. \u2022 If you use tf.range(), the loop will be dynamic, meaning that it will be included\nin the graph itself (but it will not run during tracing). Let\u2019s look at the graph that gets generated if you just replace range() with tf.range()\nin the add_10() function:\n>>> add_10.get_concrete_function(tf.constant(0)).graph.get_operations()\n[<tf.Operation 'x' type=Placeholder>, [...], <tf.Operation 'range' type=Range>, [...], <tf.Operation 'while' type=While>, [...], <tf.Operation 'Identity' type=Identity>]\nAs you can see, the graph now contains a While loop operation, as if you had called\nthe tf.while_loop() function. Handling Variables and Other Resources in TF Functions\nIn TensorFlow, variables and other stateful objects, such as queues or datasets, are\ncalled resources. TF Functions treat them with special care: any operation that reads\nor updates a resource is considered stateful, and TF Functions ensure that stateful\noperations are executed in the order they appear (as opposed to stateless operations,\nwhich may be run in parallel, so their order of execution is not guaranteed). More\u2010\nover, when you pass a resource as an argument to a TF Function, it gets passed by\nreference, so the function may modify it."
  },
  {
    "id": 537,
    "content": "For example:\nTensorFlow Graphs | counter = tf.Variable(0)\n@tf.function\ndef increment(counter, c=1): return counter.assign_add(c)\nincrement(counter) # counter is now equal to 1\nincrement(counter) # counter is now equal to 2\nIf you peek at the function definition, the first argument is marked as a resource:\n>>> function_def = increment.get_concrete_function(counter).function_def\n>>> function_def.signature.input_arg[0]\nname: \"counter\"\ntype: DT_RESOURCE\nIt is also possible to use a tf.Variable defined outside of the function, without\nexplicitly passing it as an argument:\ncounter = tf.Variable(0)\n@tf.function\ndef increment(c=1): return counter.assign_add(c)\nThe TF Function will treat this as an implicit first argument, so it will actually end up\nwith the same signature (except for the name of the argument). However, using global\nvariables can quickly become messy, so you should generally wrap variables (and\nother resources) inside classes. The good news is @tf.function works fine with\nmethods too:\nclass Counter: def __init__(self): self.counter = tf.Variable(0) @tf.function def increment(self, c=1): return self.counter.assign_add(c)\nDo not use =, +=, -=, or any other Python assignment operator with\nTF variables. Instead, you must use the assign(), assign_add(),\nor assign_sub() methods. If you try to use a Python assignment\noperator, you will get an exception when you call the method. A good example of this object-oriented approach is, of course, tf.keras. Let\u2019s see how\nto use TF Functions with tf.keras. | Appendix G: TensorFlow Graphs\nUsing TF Functions with tf.keras (or Not)\nBy default, any custom function, layer, or model you use with tf.keras will automati\u2010\ncally be converted to a TF Function; you do not need to do anything at all! However,\nin some cases you may want to deactivate this automatic conversion\u2014for example, if\nyour custom code cannot be turned into a TF Function, or if you just want to debug\nyour code, which is much easier in eager mode. To do this, you can simply pass\ndynamic=True when creating the model or any of its layers:\nmodel = MyModel(dynamic=True)\nIf your custom model or layer will always be dynamic, you can instead call the base\nclass\u2019s constructor with dynamic=True:\nclass MyLayer(keras.layers.Layer): def __init__(self, units, **kwargs): super().__init__(dynamic=True, **kwargs) [...]\nAlternatively, you can pass run_eagerly=True when calling the compile() method:\nmodel.compile(loss=my_mse, optimizer=\"nadam\", metrics=[my_mae], run_eagerly=True)\nNow you know how TF Functions handle polymorphism (with multiple concrete\nfunctions), how graphs are automatically generated using AutoGraph and tracing,\nwhat graphs look like, how to explore their symbolic operations and tensors, how to\nhandle variables and resources, and how to use TF Functions with tf.keras."
  },
  {
    "id": 538,
    "content": "TensorFlow Graphs | Index\nSymbols\n1cycle scheduling, 361\n1D convolutional layers, 520\nA\nA/B experiments, 667\naccelerated K-Means, 244\naccuracy\ndefined, 89\nexample of, 2\nmeasuring using cross-validation, 89\naction advantage, 620\naction step, 656\nactions\nevaluating, 619\nexploiting versus exploring, 618\nactivation functions\nexponential linear unit (ELU), 336-338\nhyperbolic tangent (tanh), 291\nLogistic (sigmoid), 143, 293, 302, 332\nnonsaturating, 335\nRectified Linear Unit function (ReLU),\n292-293\nScaled Exponential Linear Unit (SELU), 334,\n337-338, 368\nsoftmax, 294, 299, 470, 482, 488, 543\nsoftplus, 293\nactive constraint, 762\nactive learning, 255\nActor-Critic algorithms, 625, 662\nAdaBoost, 200\nAdaGrad, 354\nAdam and Nadam optimization, 356\nAdaptive Boosting, 200\nadaptive instance normalization (AdaIN), 604\nadaptive learning rate, 355\nadaptive moment estimation, 356\nadditive attention, 550\nAdvantage Actor-Critic (A2C), 663\nadversarial learning, 495, 568\naffine transformations, 604\naffinity, 237\naffinity propagation, 259\nagents, 14\nagglomerative clustering, 258\nAI Platform, 680\nAkaike information criterion (AIC), 267\nAlexNet, 464\nalgorithms\nActor-Critic algorithms, 625, 662\nAdvantage Actor-Critic (A2C), 663\nAllReduce algorithm, 705\nAsynchronous Advantage Actor-Critic\n(A3C), 662\nBIRCH algorithm, 259\nCART training algorithm, 177, 179\nclustering algorithms, 10\nDueling DQN algorithm, 641\ndynamic placer algorithm, 697\nExpectation-Maximization (EM) algorithm, for anomaly detection, 274\ngenetic algorithms, 612\ngreedy algorithms, 180\nhierarchical clustering algorithms, 10\nimportance of data over, 24\nIsolation Forest algorithm, 274\nisomap algorithm, 233 K-Means algorithm, 238\nLloyd\u2013Forgy algorithm, 238\nMean-Shift algorithm, 259\noff-policy algorithms, 632\non-policy algorithms, 632\none-class SVM algorithm, 275\nProximal Policy Optimization (PPO), 663\nRandomized PCA algorithm, 225\nREINFORCE algorithms, 620\nSoft Actor-Critic algorithm, 663\nsupervised learning, 8\nunsupervised learning, 9\nValue Iteration algorithm, 627\nvisualization algorithms, 11\nAllReduce algorithm, 705\nalpha channels, 250\nanchor boxes, 490\nanomaly detection\nadditional algorithms for, 274\nexamples of, 12\ngoal of, 236\nusing clustering, 237\nusing Gaussian Mixtures, 266\nApproximate Q-Learning, 633\narea under the curve (AUC), 98\nargmax operator, 149\nartificial neural networks (ANNs)\nBoltzmann machines, 775\nfine-tuning hyperparameters for, 320-327\nfrom biological to artificial neurons,\n280-295\nHopfield networks, 773\nimplementing MLPs with Keras, 295-320\noverview of, 279\nrestricted Boltzmann machines (RBMs), 776\nself-organizing maps (SOMs), 780\nartificial neurons, 283\nassociation rule learning, 12\nassociative memory networks, 773\nAsynchronous Advantage Actor-Critic (A3C), asynchronous updates, 707\nAtari preprocessing, 645\nattention mechanisms\ndefined, 526\nexplainability and, 553\noverview of, 549\nTransformer architecture, 554\nvisual attention, 552\nattributes, 8\nautoencoders\nconvolutional, 579\ndenoising, 581\nefficient data representations, 569\ngenerative, 586\nversus Generative Adversarial Networks\n(GANs), 568\noverview of, 567\nparts of, 569\nPCA with undercomplete linear autoencod\u2010\ners, 570\nprobabilistic, 586\nrecurrent, 580\nsparse, 582\nstacked, 572-575\nundercomplete, 570\nunsupervised pretraining using stacked,\n576-579\nvariational, 586-591\nAutoGraphs, 407\nautomatic differentiation (autodiff), 290, 399,\n765-772\nAutoML, 323\nautonomous driving systems, 497\nautoregressive integrated moving average\n(ARIMA) models, 506\naverage absolute deviation, 41\naverage pooling layer, 459\nAverage Precision (AP), 491\nB\nbackpropagation, 289-292\nbackpropagation through time (BPTT), 502\nbag of words, 438\nbagging and pasting\nout-of-bag evaluation, 195\noverview of, 192\nin Scikit-Learn, 194\nBahdanau attention, 550\nbandwidth saturation, 708\nbasic cells, 500\nBatch Gradient Descent, 121\nbatch learning, 15\nBatch Normalization (BN), 339\nbatch size, 325\nbatched action step, 657\nbatched time step, 657\nbatched trajectory, 657 | Index\nBayesian Gaussian Mixture models, 270\nBayesian inference, 586\nBayesian information criterion (BIC), 267\nbeam search, 547\nbeam width, 547\nBellman Optimality Equation, 627\nBetter Life Index, 19\nbias neurons, 285\nbias terms, 112\nbias/variance trade-off, 134\nbidirectional recurrent layers, 546\nbidirectional RNNs, 546\nbinary classifiers, 88\nbinary trees, 177\nbiological neural networks (BNN), 282\nbiological neurons, 280\nBIRCH algorithm, 259\nblack box models, 178\nblack box stochastic variational inference\n(BBSVI), 273\nblenders, 208\nBoltzmann machines, 775\nboosting\nAdaBoost, 200\nGradient Boosting, 203\noverview of, 199\nbottleneck layers, 467\nboundary transitions, 660\nbounding box priors, 490\nbreak the symmetry, 291\nByte-Pair Encoding, 536\nC\ncalculus, 112\nCalifornia Housing Prices dataset, 36\ncallbacks, 315\ncanary testing, 684\nCART training algorithm, 177, 179\ncatastrophic forgetting, 637\ncategorical distribution, 261\ncategorical features\nencoding using embeddings, 433\nencoding using one-hot vectors, 431\ncausal models, 510\ncentroids, 238\nchain rule, 290\nchaining transformations, 415\ncharacter RNNs (Char-RNNs)\nbuilding and training, 530\nchopping sequential datasets, 528\ngenerating Shakespearean text, 531\noverview of, 526\nsplitting sequential datasets, 527\nstateful RNNs and, 532\ntraining dataset creation, 527\nusing, 531\nchatbots, 525\nchi-squared test, 182\nClassification and Regression Tree (CART),\n177, 179\nclassification problems\nAdaBoost classifiers, 200\nbinary classifiers, 88\nclassification and localization, 483\nclassification MLPs, 294\nerror analysis, 102\nexample of, 8\nExtra-Trees classifier, 198\nhard margin classification, 154\nimage classifiers using Sequential APIs,\n297-307\nlarge margin classification, 153\nlinear SVM classification, 153\nMNIST dataset, 85\nmulticlass classification, 100\nmultilabel classification, 106\nmultioutput classification, 107\nmultitask classification, 311\nnonlinear SVM classification, 157-162\nperformance measures, 88-100\nsoft margin classification, 154\nvoting classifiers, 189\nclosed-form solution, 114\ncluster specification, 711\nclustering algorithms\nadditional algorithms, 258\napplications for, 10, 237\nDBSCAN, 255\ngoal of, 236\nfor image segmentation, 238, 249\nK-Means, 238-249\noverview of, 236\nfor preprocessing, 251\nfor semi-supervised learning, 253\ncode examples, obtaining and using, xxi\ncodings, 567\nColab Runtime, 693\nColaboratory (Colab), 693\nIndex | collect policy, 649\ncolor channels, 451\ncolor segmentation, 249\ncolumn vectors, 113\ncomments and questions, xxiii, 718\ncomplementary slackness, 762\ncomponents, 38\ncompression, 224\ncomputation graphs, 376\nCompute Unified Device Architecture library\n(CUDA), 690\nconcatenative attention, 550\nconcrete functions, 791\nconditional probability, 547\nconfusion matrix, 90\nconnectionism, 280\nconstrained optimization, 166\nContrastive Divergence, 777\nconvergence, 118\nconvex function, 120\nconvolution kernels, 450\nconvolutional autoencoders, 579\nconvolutional layer\nfilters, 450\nmemory requirements, 456\noverview of, 448\nstacking multiple feature maps, 451\nTensorFlow implementation, 453\nConvolutional Neural Networks (CNNs)\narchitecture of visual cortex, 446\nclassification and localization, 483\nCNN architectures, 460-478\nconvolutional layer, 448-456\nobject detection, 485-492\noverview of, 445\npooling layer, 456\npretrained models for transfer learning, 481\npretrained models from Keras, 479\nResNet-34 using Keras, 478\nsemantic segmentation, 492\ncore instances, 255\ncorpus development, 24\ncorrelation coefficient, 58\ncost functions\ncross-entropy loss (log loss), 149\nhinge loss, 155, 173\nmean absolute error (MAE), 41, 293\nmean squared error, 120, 293, 308, 384, 570,\n573, 583, 636\nrole of, 20\ncredit assignment problem, 619\ncross-entropy loss (log loss), 149, 295\ncross-validation, 31, 73, 89\nCUDA Deep Neural Network library (cuDNN), curiosity-based exploration, 664\ncurse of dimensionality, 214\ncustom models\nabout, 375\nactivation functions, initializers, regulariz\u2010\ners, and constraints, 387\ncomputing gradients using Autodiff, 399,\n765-772\nlayers, 391\nloss functions, 384\nlosses and metrics, 397\nmetrics, 388\nmodels, 394\nsaving and loading, 385\ntraining loops, 402\ncustomer segmentation, 237\nD\ndata (see also data preparation; data visualiza\u2010\ntion; training data)\nanalyzing through clustering, 237\nCalifornia Housing Prices dataset, 36\nchopping sequential datasets, 528\ncompressing, 224\ndata mismatch, 32\ndecompressing, 224\ndownloading, 46\nefficient data representations, 569\nFashion MNIST dataset, 297, 574, 590\nflat datasets, 529\ngeographical data, 56\nGoogle News 7B corpus, 541\nhelper function creation, 420\nimportance of over algorithms, 24\nInternet Movie Database, 534\niris dataset, 145\nloading and preprocessing with TensorFlow,\n413-442\nMNIST dataset, 85\nnested datasets, 529\nnoisy data, 19\nprefetching, 421\npreprocessing, 251, 419, 430-439 | Index\nreconstruction error, 224\nreducing dimensionality of, 222\nshuffling, 416\nskewed datasets, 89\nsources for, 35\nsplitting sequential datasets, 527\ntraining dataset creation, 527\ntraining sparse models, 359\nusing datasets with tf.Keras, 423\nData API (TensorFlow)\nchaining transformations, 415\nhelper function creation, 420\noverview of, 414\nprefetching data, 421\npreprocessing data, 419\nshuffling data, 416\nusing datasets with tf.keras, 423\ndata augmentation, 464\ndata parallelism, 701, 704\ndata preparation\nbenefits of functions for, 62\ncustom transformers, 68\ndata cleaning, 63\nfeature scaling, 69\nhandling text and categorical attributes, 65\ntransformation pipelines, 70\ndata snooping bias, 51\ndata visualization\nattribute combinations, 61\ncomputing correlations, 58\ndimensionality reduction, 213\ngeographical data, 56\ntest, training, and exploration sets, 56\nusing TensorBoard for, 317\nvisualizing Fashion MNIST Dataset, 574\nvisualizing reconstructions, 574\ndatasets, defined, 414\nDataViz (see data visualization)\nDBSCAN (density-based spatial clustering of\napplications with noise), 255\ndecision boundaries, 145\ndecision function, 93\nDecision Stumps, 203\nDecision Trees\nbenefits of, 175\nCART training algorithm, 179\ncomputational complexity, 180\nestimating class probabilities, 178\nevaluating, 73\nGini impurity versus entropy, 180\ninstability drawbacks, 185\nmaking predictions, 176\nregression tasks, 183\nregularization hyperparameters, 181\ntraining and visualizing, 175\ndecoders, 501, 569\ndecompression, 224\ndeep autoencoders, 572\ndeep belief networks (DBNs), 13, 777\ndeep computer vision (see Convolutional Neu\u2010\nral Networks (CNNs))\ndeep convolutional GANs, 598\nDeep Learning VM Images, 692\ndeep neural networks (DNNs)\navoiding overfitting, 364-371\ndefault configuration, 371\ndefined, xv, 289\nfaster optimizers, 351-364\noverview of, 331\nreusing pretrained layers, 345-351\nvanishing/exploding gradients problems,\n332-345\nDeep Neuroevolution, 323\nDeep Q-Learning\nDouble DQN, 640\nDueling DQN, 641\nfixed Q-Value targets, 639\nimplementing, 634\noverview of, 633\nprioritized experience replay, 640\nvariants of, 639\ndeep Q-networks (DQNs), 633, 650, 650\ndenoising autoencoders, 581\ndense layer, 285\ndense vectors, 556\ndensity estimation, 236, 264\ndepth concat layer, 467\ndepth radius, 466\ndepthwise separable convolution, 474\ndeques, 635\ndevelopment sets (dev sets), 31\ndifferencing, 506\ndimensionality reduction\nadditional techniques, 232\napproaches for, 215-218\nusing clustering, 237\ncurse of dimensionality, 214\ngoal of, 12\nIndex | LLE (Locally Linear Embedding), 230\noverview of, 213\nPCA (Principal Component Analysis),\n219-230\ndiscount factors, 619\ndiscriminators, 568\nDistribution Strategies API, 668, 709\ndot product, 551\nDouble DQN, 640\nDouble Dueling DQN, 642\nDQN agents, 652\ndropout, 365\ndual numbers, 768\ndual problem, 168, 761\nduck typing, 68\nDueling DQN algorithm, 641\ndummy attributes, 67\ndying ReLUs problem, 335\ndynamic models, 313\ndynamic placer algorithm, 697\nDynamic Programming, 628\nE\neager execution/eager mode, 408\nearly stopping, 141\nElastic Net, 140\nELU (exponential linear unit), 336-338\nembedded devices, 685\nEmbedded Reber grammars, 566\nembedding, 68, 413, 433\nembedding matrix, 435\nencoders, 501, 569\nEncoder\u2013Decoder model, 501, 542-548\nend-of-sequence (EoS) token, 542, 556\nenergy function, 774\nEnsemble Learning\nbagging and pasting, 192-196\nbenefits of, 74\nbest uses of, 191\nboosting, 199-208\ndefined, 189\nexamples of, 189\nRandom Forests, 189, 197\nrandom patches and random subspaces, 196\nstacking, 208\nvoting classifiers, 189\nEnsemble methods, 189\nensembles, 189\nentailment, 564\nentropy impurity measure, 180\nepochs, 125, 290\nequalized learning rates, 603\nequivariance, 458\nerror analysis, 102\nestimators, 64\nEuclidean norm, 41\nevent files, 317\nevidence lower bound (ELBO), 272\nexample project\ndata downloading, 42-55, 756\ndata preparation, 62-72, 757\ndata visualization, 56-62, 756\nframing the problem, 37, 755\nlaunching, monitoring, and maintaining, 80, Machine Learning project checklist, 37, 755\nmodel fine-tuning, 75-80, 759\nmodel selection and training, 72, 758\noverview of, 35\nproject goals, 37\nreal-world data for, 35\nselecting performance measure, 39\nverifying assumptions, 42\nExclusive OR (XOR) classification problem, exercise solutions, 719-753\nexpectation step, 262\nExpectation-Maximization (EM) algorithm, experience replay, 597\nexplainability, 553\nexplained variance ratio, 222\nexploding gradients problem, 332\nexploration policy, 630, 632\nexploration sets, 56\nexponential linear unit (ELU), 336-338\nexponential scheduling, 360\nExtra-Trees classifier, 198\nExtremely Randomized Trees ensemble, 198\nF\nF1 score, 92\nfake quantization, 687\nfalse positive rate (FPR), 97\nfan-in/fan-out numbers, 333\nFashion MNIST dataset, 297, 574, 590\nFast-MCD (minimum covariance determi\u2010\nnant), 274 | Index\nfeature engineering, 27\nfeature extraction, 12, 27\nfeature maps, 228, 450\nfeature scaling, 69\nfeature selection, 27\nfeature space, 226\nfeature vector, 113\nfeatures, 8\nfeedforward neural networks (FNNs), 289\nfilters, 450\nfinal trained models, 20\nfinite difference approximation, 766\nFirst In, First Out (FIFO) queues, 383\nfirst-order partial derivatives (Jacobians), 358\nfitness functions, 20\nfixed Q-Value targets, 639\nflat datasets, 529\nfolds, 73, 89\nforecasting, 503\nforget gate, 516\nforward pass, 290\nforward-mode autodiff, 767\nfraud detection, 237\nFull Gradient Descent, 122\nfully connected layer, 285\nfully convolutional networks (FCNs), 487\nfully-specified model architecture, 20\nfunction definitions, 792\nfunction graphs, 792\nFunctional API, 308-313\nG\ngate controllers, 516\nGated Recurrent Unit (GRU) cell, 518\nGaussian mixture model (GMM)\nadditional algorithms for anomaly and nov\u2010\nelty detection, 274\nanomaly detection using, 266\nBayesian Gaussian Mixture models, 270\ngraphical model of, 260\noverview of, 260\nselecting cluster number, 267\nvariants, 260\nGaussian Radial Basis Function (RBF), 159\ngeneralization error, 30\ngeneralized Lagrangian, 762\nGenerative Adversarial Networks (GANs)\nversus autoencoders, 568\ndeep convolutional GANs (DCGANs), 598\ndifficulties of training, 596\noverview of, 592\nprogressive growing of, 601\nStyleGANs, 604\nuses for, 567\ngenerative autoencoders, 586\ngenerative models, 263, 567, 775 (see also\nautencoders; Generative Adversarial Net\u2010\nworks (GANs))\ngenerative network, 569\ngenerators, 568\ngenetic algorithms, 612\nGini impurity measure, 180\nglobal average pooling layer, 460\nglobal minimum, 119\nGlorot and He initialization, 333\nGoogle Cloud Platform (GCP)\nprediction service creation, 677-681\nprediction service use, 682-685\nGoogle Cloud Storage (GCS), 679\nGoogle News 7B corpus, 541\nGoogLeNet, 466\nGPUs (graphics processing units)\nadding to single machines, 689\nColaboratory (Colab), 693\nGPU-equipped virtual machines, 692\nmanaging GPU RAM, 694\nparallel execution across multiple devices, placing operations and variables on devices, selecting, 690\nspeeding computations with, 689\nGradient Boosted Regression Trees (GBRT), Gradient Boosting, 203\ngradient clipping, 345\nGradient Descent (GD)\nBatch Gradient Descent, 121\nMini-batch Gradient Descent, 127\noverview of, 111, 118\nStochastic Gradient Descent, 124\nGradient Tree Boosting, 203\ngraph mode, 408\ngreedy algorithms, 180\ngreedy layer-wise pretraining, 349\ngreedy layer-wise training, 578\nIndex | H\nhard clustering, 240\nhard margin classification, 154\nhard voting classifiers, 190\nharmonic mean, 92\nHDF5 format, 314\nHe initialization, 333\nHeaviside step function, 285\nHebb's rule, 286\nHebbian learning, 286\nhelper functions, 420\nhidden layers\nin MLPs, 289\nneurons per hidden layer, 324\nnumber of, 323\nhidden units, 775\nhierarchical clustering algorithms, 10\nHierarchical DBSCAN (HDBSCAN), 258\nhigh-dimensional training sets, 213\nhinge loss function, 155, 173\nHinton, Geoffrey, xv\nhistograms, 50\nhold outs, 31\nholdout validation, 31\nHopfield networks, 773\nHuber loss, 293, 384\nHyperas, 322\nHyperband, 323\nhyperbolic tangent function (tanh), 291\nHyperopt, 322\nhyperparameters\ndefined, 29\nfine-tuning for neural networks, 320-327\nhyperparameter tuning, 31, 75\nlearning rate, 118\nPython libraries for optimization, 322\nregularization hyperparameters, 181\nhyperplanes, 165\nhypothesis boosting, 199\nI\nidentity matrix, 137\nimage classification\nmultitask classification, 311\nusing Sequential API, 297-307\nimage generation, 495\nimage segmentation, 238, 249\nimportance sampling (IS), 640\nimpurity, 177, 180\nimputation, 503\nincremental learning, 16\nIncremental PCA (IPCA), 225\nindependent and identically distributed (IID), inequality constraints, 762\ninertia, 243\ninference, 23\ninformation theory, 180\ninitialization\ncentroid initialization methods, 243\nGlorot and He initialization, 333\nLeCun initialization, 334\nrandom initialization, 118\nXavier initialization, 333\ninliers, 266\ninput and output sequences, 501\ninput gate, 516\ninput layers, 289\ninput neurons, 285\ninput signatures, 791\ninstability, 185\ninstance segmentation, 249, 495\ninstance-based learning, 17, 22\ninter-op thread pool, 699\nintercept terms, 112\nInternet Movie Database, 534\nintra-op thread pool, 699\ninvariance, 457\ninverse transformation, 225\niris dataset, 145\nisolated environments, 43\nIsolation Forest algorithm, 274\nisomap algorithm, 233\nJ\nJupyterLab, 692\njust-in-time (JIT) compiler, 376\nK\nK-fold cross-validation, 73, 89\nK-Means\naccelerated and mini-batch, 244\ncentroid initialization methods, 243\nhard and soft clustering, 240\nimage segmentation, 249\nK-Means algorithm, 241\nlimits of, 248\noptimal cluster number, 245 | Index\noverview of, 238\npreprocessing with, 251\nproposed improvement to, 243\nscaling input features, 249\nfor semi-supervised learning, 253\nk-Nearest Neighbors regression, 22\nKarush\u2013Kuhn\u2013Tucker (KKT) multipliers, 762\nkeep probability, 367\nKeras\nbenefits of, xvi\ncomplex architectures, 314\ngradient clipping in, 345\nimplementing Batch Normalization with, implementing dropout using, 367\nimplementing MLPs with, 295-320\nimplementing ResNet-34 with, 478\nkeras.callbacks package, 316\nloading datasets with, 297\nlow-level API, 381\nmultibackend Keras, 295\npreprocessing layers, 437\nsaving and restoring models in, 314\nstacked autoencoders using, 572\ntransfer learning with, 347\nusing code examples from keras.io, 300\nusing pretrained models from, 479\nKeras Tuner, 322\nKernel PCA (kPCA), 226-230\nkernel trick, 158, 228\nkernelized SVM, 169\nkernels, 170, 226, 377\nkopt library, 322\nKullback\u2013Leibler divergence, 150\nL\nlabel propagation, 254\nlabels, 8, 39, 239\nLagrange multipliers, 761\nlandmarks, 159\nlanguage models, 563 (see also natural language\nprocessing (NLP))\nlarge margin classification, 153\nLasso Regression, 137\nlatent loss, 587\nlatent representations, 567\nlatent variables, 262\nlaw of large numbers, 191\nLayer Normalization, 512\nlayers\n1D convolutional layer, 520\nadaptive instance normalization (AdaIN), bidirectional recurrent layer, 546\nconvolutional layer, 448-456\ndense (fully connected) layer, 285\nhidden layer, 289\ninput layer, 289\nMasked Multi-Head Attention layer, 556\nminibatch standard deviation layer, 603\nMulti-Head Attention layer, 556, 559\noutput layer, 289\npooling layer, 456\nrecurrent, 498-502\nreusing pretrained, 345-351\nScaled Dot-Product Attention layer, 559\nleaf nodes, 176\nleaky ReLU function, 335\nlearning curves, 130-134\nlearning rate, 16, 118, 325, 603\nlearning rate scheduling, 359\nlearning schedules, 125, 360\nLeCun initialization, 334\nLeNet-5, 463\nLevenshtein distance, 161\nliblinear library, 162\nlibsvm library, 162\nlikelihood function, 267\nlinear algebra, 112\nlinear autoencoders, 570\nLinear Discriminant Analysis (LDA), 233\nlinear models, 19\nLinear Regression model\napproaches to training, 111, 113\ncomputational complexity, 117\nNormal Equation, 114\noverview of, 112\nlinear SVM classification, 153\nlists of lists, using SequenceExample Protobuf, LLE (Locally Linear Embedding), 230\nLloyd-Forgy algorithm, 238\nlocal minimum, 119\nLocal Outlier Factor (LOF), 274\nlocal response normalization, 465\nlocalization, 483\nlog loss, 144\nlog-odds, 144\nIndex | logical computations, 283\nlogical GPU devices, 695\nLogistic (sigmoid) function, 143, 293-294, 302, Logistic Regression\nclassification with, 8\ndecision boundaries, 145\nestimating probabilities, 143\noverview of, 142\nSoftmax Regression, 148\ntraining and cost function, 144\nlogit, 144\nLogit Regression (see Logistic Regression)\nlong sequences\noverview of, 511\nshort-term memory problems, 514-523\nunstable gradients problem, 512\nLong Short-Term Memory (LSTM) cell, 514\nloss functions (see cost functions)\nLuong attention, 551\nM\nMachine Learning (ML)\nadditional resources, xix\napplications for, xv, 5\napproach to learning, xvi\nbenefits of, 2\nchallenges of, 23-30\ndefined, 1\nhistory of, xv\nlocating papers on, 378\nnotations for, 40, 164\noverview of, 30\nprerequisites to learning, xvii\ntesting and validating, 30-33\ntopics covered, xvii\ntypes of, 7-23\nMachine Learning project checklist, 37, 755\nmajority-vote classifiers, 190\nmajority-vote predictions, 187\nManhattan norm, 41\nmanifold assumption, 218\nmanifold hypothesis, 218\nManifold Learning, 218\nmanual differentiation, 765\nmargin violations, 155\nMarkov chains, 625\nMarkov Decision Processes (MDP), 625-629\nMask R-CNN, 495\nmask tensors, 539\nmasked language model (MLM), 564\nMasked Multi-Head Attention layer, 556\nmasking, 538\nmax pooling layer, 457\nmax-norm regularization, 370\nmaximization step, 262\nmaximum a-posteriori (MAP) estimation, 269\nmaximum likelihood estimate (MLE), 269\nmean absolute error (MAE), 41\nmean Average Precision (mAP), 491\nmean coding, 586\nmean field variational inference, 273\nMean-Shift algorithm, 259\nmeasure of similarity, 18\nmemory bandwidth, 422\nmemory cells, 500\nMercer's conditions, 171\nMercer's theorem, 171\nmeta learners, 208\nmetagraphs, 671\nmetrics\naccuracy, 388\narea under the curve (AUC), 98\nconfusion matrix, 90, 90\nF1 score, 92\nmean absolute error (MAE), 41, 293\nmean average precision, 491\nmean squared error, 183, 505\nprecision, 91-97\nrecall, 91-97\nRMSE, 39\nROC curve, 97\nMicrosoft Cognitive Toolkit (CNTK), 295\nmin-max scaling, 69\nMini-batch Gradient Descent, 127\nmini-batch K-Means, 244\nmini-batches, 15, 127\nminibatch discrimination, 597\nminibatch standard deviation layer, 603\nmirrored strategy, 704\nmixing regularization, 606\nML Engine, 680\nMNIST dataset, 85\nmobile devices, 685\nmode collapse, 597\nmodel parallelism, 701\nmodel parameters, 20\nmodel selection, 19, 31, 72 | Index\nmodel-based learning, 18\nmodels (see also custom models)\ncausal models, 510\ncomplex using Functional API, 308-313\ncustom with TensorFlow, 384-405\ndefined, 20\ndynamic using Subclassing API, 313\nfine-tuning, 75-80\nparametric versus nonparametric, 181\npretrained models for transfer learning, 481\npretrained models from Keras, 479\nsaving and restoring, 314\nsequence-to-sequence models, 510\ntraining, 20, 72 (see also training models)\ntraining across multiple devices, 701-717\ntraining sparse models, 359\nusing callbacks, 315\nusing TensorBoard for visualization, 317\nwhite versus black box, 178\nmodules, 540\nmomentum optimization, 351\nmomentum vector, 352\nMonte Carlo (MC) dropout, 368\nMulti-Head Attention layer, 556, 559\nmultibackend Keras, 295\nmulticlass classification, 100\nMultidimensional Scaling (MDS), 232\nmultilabel classification, 106\nMultilayer Perceptrons (MLPs)\nbackpropagation and, 289-292\nclassification MLPs, 294\nregression MLPs, 292\nmultinomial classifiers, 100\nMultinomial Logistic Regression, 148\nmultioutput classification, 107\nmultiple outputs, 311\nmultiple regression problems, 39\nmultiplicative attention, 551\nmultitask classification, 311\nmultivariate regression problems, 39\nmultivariate time series, 503\nN\nnaive forecasting, 505\nNash equilibrium, 596\nnatural language processing (NLP)\nattention mechanisms, 549-563\nCNNs for, 445\nEncoder\u2013Decoder network for, 542-548\ngenerating text using character RNNs,\n526-534\noverview of, 525\nrecent innovations in, 563\nRNNS for, 497\nsentiment analysis, 534-542\nuses for, 351\nnested datasets, 529\nNesterov Accelerated Gradient (NAG), 353\nNesterov momentum optimization, 353\nneural machine translation (NMT), 542-563\n(see also natural language processing\n(NLP))\nneurons\nbias neurons, 285\nfan-in/fan-out numbers, 333\nfrom biological to artificial, 280-295\ninput neurons, 285\nlogical computations with, 283\nper hidden layer, 324\nrecurrent neurons, 498-502\nstochastic neurons, 775\nNewton's difference quotient, 766\nnext sentence prediction (NSP), 565\nNo Free Lunch (NFL) theorem, 33\nnoisy data, 19\nnon-max suppression, 486\nnonlinear dimensionality reduction (NLDR), nonlinear SVM classification, 157-162\nnonparametric models, 181\nnonsaturating activation functions, 335\nnonsequential neural networks, 308\nNormal Equation, 114\nnormalization, 69, 339, 603\nnormalized exponential, 148\nnovelty detection, 12, 267, 274\nNP-Complete problem, 180\nnull hypothesis, 182\nNumPy\narray_split() function, 226\ndense arrays, 67\ninstalling, 42\ninv() function, 115\nmemmap class, 226\nrandint() function, 107\nserializing large arrays, 75\nsvd() function, 221\nusing TensorFlow like, 379-384\nIndex | NVIDIA Collective Communications Library\n(NCCL), 710\nNvidia GPU cards, 690\nO\nobject detection\nfully convolutional networks (FCNs), 487\noverview of, 485\nYou Only Look Once (YOLO), 489\nobjectness output, 486\nobserved variables, 262\nobservers, 654\noff-policy algorithms, 632\noffline learning, 15\non-policy algorithms, 632\none-class SVM algorithm, 275\none-hot encoding, 67\none-hot vectors, 431\none-versus-all (OvA) strategy, 100\none-versus-one (OvO) strategy, 100\none-versus-the-rest (OvR) strategy, 100\nonline learning, 15, 88\nonline model, 639\nonline SVMs, 172\nOpenAI Gym, 613-617\nOptical Character Recognition (OCR), 1\noptimal state value, 627\noptimizers\nAdaGrad, 354\nAdam and Nadam optimization, 356\ncreating faster, 351\nfirst- and second-order partial derivatives, learning rate scheduling, 359\nmomentum optimization, 351\nNesterov Accelerated Gradient (NAG), 353\nRMSProp, 355\nStochastic Gradient Descent (SGD), 88, 124\noriginal space, 226\nout-of-core learning, 16\nout-of-sample error, 30\nout-of-vocabulary (oov) buckets, 432\noutlier detection, 237, 266\noutput gate, 516\noutput layers, 289\novercomplete autoencoders, 580, 580\noverfitting\navoiding through regularization, 364-371\ndefined, 27\nlimiting risk of, 457\nP\np (posterior) distribution, 272\np (prior) distribution, 271\np-value, 182\nparameter efficiency, 323\nparameter matrix, 148\nparameter servers, 705\nparameter space, 121\nparameter vector, 113\nparametric leaky ReLU (PReLU), 335\nparametric models, 181\npartial derivatives, 121\npasting (see bagging and pasting)\npattern matching, 569\nPCA (Principal Component Analysis)\nanomaly and novelty detection using, 274\nchoosing dimension number, 223\nfor compression, 224\nexplained variance ratio, 222\nincremental, 225\nKernel PCA (kPCA), 226-230\noverview of, 219\npreserving variance, 219\nprincipal component axis, 220\nprojecting down to d dimensions, 221\nrandomized, 225\nusing Scikit-Learn, 222\nundercomplete linear autoencoders for, 570\nPearson's r, 58\npeephole connections, 518\npenalties, 14\nPerceptron, 284-288\nPerceptron convergence theorem, 287\nperformance measures (see metrics)\nperformance scheduling, 361\npiecewise constant scheduling, 361\npipelines, 38, 424\npixelwise normalization layers, 603\npolicies, 14, 612\npolicy gradients (PG), 613, 620-625\npolicy parameters, 612\npolicy search, 612\npolicy space, 612\npolynomial features, 158\npolynomial kernels, 170\nPolynomial Regression, 112, 128\npooling kernel, 457 | Index\npooling layer, 456\npositional embeddings, 556\npost-training quantization, 686\npower scheduling, 360\npre-images, 228\nprecision, 91-97\nprediction problems, 8, 17, 189\nprediction service\ncreating on GCP AI, 677-681\nusing, 682-685\npredictors, 65\npreprocessing, 251, 430-439\npretraining\nfor transfer learning, 481\ngreedy layer-wise pretraining, 349\nmodels from Keras, 479\non auxiliary tasks, 350\nreusing pretrained embeddings, 540\nreusing pretrained layers, 345-351\nunsupervised pretraining, 349\nusing stacked autoencoders, 576-579\nprimal problem, 168\nprioritized experience replay (PER), 640\nprobabilistic autoencoders, 586\nprobability density function (PDF), 236, 264\nprojection, 215\npropositional logic, 280\nprotocol buffers (protobufs), 425\nProximal Policy Optimization (PPO), 663\npruning, 182\nPyTorch library, 296\nQ\nQ-Learning\nApproximate Q-Learning and Deep Q-\nLearning, 633\nexploration policy, 632\nimplementing, 631\noverview of, 630\nQ-Value Iteration, 628\nQ-Values, 628\nQuadratic Programming (QP) problems, 167\nquantization-aware training, 687\nqueries per second (QPS), 667\nquestions and comments, xxiii, 718\nqueues, 383, 788\nR\nRadial Basis Function (RBF), 159\nragged tensors, 383, 784\nRainbow agent, 642\nRandom Forests\nbenefits of, 189\nExtra-Trees, 198\nfeature importance, 198\noverview of, 197\nrandom initialization, 118\nrandom patches and random subspaces, 196\nrandom projections, 232\nrandomized leaky ReLU (RReLU), 335\nRandomized PCA, 225\nrecall, 91-97\nreceiver operating characteristic (ROC) curve, recognition network, 569\nrecommender systems, 237\nreconstruction error, 224\nreconstruction loss, 397, 570\nreconstruction pre-images, 228\nreconstructions, 570\nRectified Linear Unit function (ReLU), 292-293\nrecurrent autoencoders, 580\nrecurrent neural networks (RNNs)\nbidirectional RNNs, 546\nforecasting time series, 503-511\ngenerating text using character RNNS,\n526-534\nhandling long sequences, 511-523\noverview of, 497\nrecurrent neurons and layers, 498-502\nstateless and stateful, 525, 532\ntraining, 502\nrecurrent neurons, 498\nRegion Proposal Network (RPN), 492\nregression problems\nDecision Trees, 183\ndefined, 8\nk-Nearest Neighbors regression, 22\nLasso Regression, 137\nLinear Regression, 112-117\nLogistic Regression, 142-151\nmultiple regression problems, 39\nmultivariate regression problems, 39\nPolynomial Regression, 128\nregression MLPs, 292\nregression MLPs using Sequential API, 307\nRidge Regression, 135\nSoftmax Regression, 148-151\nIndex | SVM regression, 162\nunivariate regression problems, 39\nregular expressions, 536\nregularization\navoiding overfitting through, 364-371\ndefined, 28\nhyperparameters for Decision Trees, 181\nmultiple outputs for, 311\nshrinkage technique, 205\nregularization terms, 135\nregularized linear models\nElastic Net, 140\nLasso Regression, 137\noverview of, 134\nRidge Regression, 135\nREINFORCE algorithms, 620\nReinforcement Learning (RL)\nalgorithms for, 662\nDeep Q-Learning, 633-638\nevaluating actions, 619\nMarkov Decision Processes (MDP), 625-629\nneural network policies, 617\nOpenAI Gym, 613-617\noptimizing rewards, 610\noverview of, 14, 609\npolicy gradients, 620-625\npolicy search, 612\nQ-Learning, 630-634\nTemporal Difference Learning, 629\nTF-Agents library, 642-662\nReLU (Rectified Linear Unit function), 292-293\nreplay buffers, 635, 649, 654\nreplay memory, 635\nrepresentation learning, 68, 434 (see also\nautoencoders)\nresidual blocks, 395\nresidual errors, 203\nresidual learning, 471\nresidual units, 471\nResNet (Residual Network), 471\nResNet-34 CNN, 478\nresponsibilities (clustering), 262\nrestoring models, 314\nrestricted Boltzmann machines (RBMs), 13,\n349, 776\nreverse-mode autodiff, 290, 770\nrewards, 14\nRidge Regression, 135\nRMSProp, 355\nRoot Mean Square Error (RMSE), 39, 120\nroot nodes, 176\nS\nSAMME (Stagewise Additive Modeling using a\nMulticlass Exponential loss function), 203\nsample inefficiency, 625\nsampled softmax technique, 544\nsampling bias, 25\nsampling noise, 25\nSavedModel format, 669\nsaving and restoring models, 314\nScaled Dot-Product Attention layer, 559\nScaled Exponential Linear Unit (SELU) func\u2010\ntion, 334, 337-338, 368\nScikit-Learn\nAdaBoost version used in, 203\nanomaly and novelty detection, 274\nautomatic reconstruction with, 229\nbagging and pasting in, 194\nbenefits of, xvi\nCART training algorithm, 177, 179\nclustering algorithms in, 258\ncomputing classifier metrics, 92-107\nconverting text to numbers, 66\ncross_val_score() function, 89\ndata centering in, 221\ndataset dictionary structure, 85\nDecisionTreeRegressor class, 183\ndesign principles, 64\ndimensionality reduction in, 232\nExtraTreesClassifier class, 198\nfeature importance scoring, 198\nfeature scaling, 154\nfull SVD approach, 225\nGBRT ensemble training in, 204\nGridSearchCV, 76\nincremental training in, 207\nIncrementalPCA class, 226\ninstalling, 42\nK-fold cross-validation feature, 73\nKernelPCA class, 227\nlaunching, monitoring, and maintaining\nyour system, 80\nlinear model using, 21\nlinear regression using, 116\nLLE (Locally Linear Embedding), 230, 232\nmax_depth hyperparameter, 181\nmean_squared_error function, 72 | Index\nmissing value handling, 63\none-hot vectors, 67\nout-of-bag evaluation, 195\nPCA using, 222\nPerceptron class, 287\npresorting data with, 180\nRandomized PCA algorithm, 225\nrandom_state hyperparameter, 185\nsaving models, 75\nSGDClassifier class, 88\nsplitting datasets into subsets, 53\nstratified sampling using, 54\nSVM classification classes, 162\nSVM models, 155\ntolerance hyperparameter, 162\ntransformation sequences, 70\ntransformers and, 68\nvoting classifiers in, 191\nScikit-Optimize, 322\nSE block, 476\nSE-Inception, 476\nSE-ResNet, 476\nsearch engines, 238\nsecond-order partial derivatives (Hessians), 358\nself-attention mechanism, 556\nself-normalization, 337\nself-organizing maps (SOMs), 780\nself-supervised learning, 351\nSELU (Scaled Exponential Linear Unit) func\u2010\ntion (see Scaled Exponential Linear Unit\n(SELU) function)\nsemantic interpolation, 590\nsemantic segmentation, 249, 458, 492\nsemi-supervised learning\nclustering algorithms for, 237, 253\ndefined, 13\nexamples of, 13\nSENet (Squeeze-and-Excitation Network), 476\nsensitivity, 91\nsentence encoders, 541\nsentiment analysis\ndefined, 526\nmasking, 538\noverview of, 534\nreusing pretrained embeddings, 540\nseparable convolution, 474\nsequence-to-sequence models, 510\nsequence-to-vector networks, 501\nSequenceExample protobuf (TensorFlow), 429\nsequences\nforecasting time series, 503-511\nhandling long, 511-523\ninput and output, 501\nRNNS for, 497\nSequential API\nimage classifiers using, 297-307\nregression MLP using, 307\nservice account, 682\nsets, 383, 787\nShannon's information theory, 180\nshort-term memory problems, 514-523\nshortcut connections, 471\nshrinkage, 205\nshuffling-buffer approach, 417\nsigmoid (Logistic) activation function, 143,\n293-294, 302, 332\nsigmoid kernel, 171\nsilhouette coefficient, 246\nsilhouette diagram, 247\nsilhouette score, 246\nsimilarity functions, 159\nsimulated annealing, 125\nsimulated environments, 614\nsingle-shot learning, 495\nSingular Value Decomposition (SVD), 117, 221\nskewed datasets, 89\nskip connections, 337, 471\nSklearn-Deap, 323\nslack variables, 167\nsmoothing term, 340\nSoft Actor-Critic algorithm, 663\nsoft clustering, 240\nsoft margin classification, 154\nsoft voting, 192\nsoftmax function, 148, 294, 299, 470, 482, 488, Softmax Regression, 148\nsoftplus activation function, 293\nspam filters, 1, 2\nspare replicas, 706\nsparse autoencoders, 582\nsparse matrix, 67\nsparse models, 359\nsparse tensors, 383, 785\nsparsity, 582\nsparsity loss, 583\nSpearmint library, 322\nspectral clustering, 259\nIndex | spurious patterns, 774\nstacked autoencoders\noverview of, 572\nstacked denoising autoencoders, 581\nunsupervised pretraining using, 576-579\nusing Keras, 572\nvisualizing Fashion MNIST Dataset, 574\nvisualizing reconstructions, 574\nstacked denoising autoencoders, 581\nstacked generalization, 208\nstacking, 208\nstale gradients, 707\nstandard correlation coefficient, 58\nstandardization, 69\nstart of sequence (SoS) token, 535\nstate-action values, 628\nstateful metrics, 389\nstationary point, 761\nstatistical mode, 193\nstatistical significance, 182\nstep function, 284\nStochastic Gradient Boosting, 207\nStochastic Gradient Descent (SGD), 88, 124\nstochastic neurons, 775\nstochastic policy, 612\nstratified sampling, 53\nstreaming metrics, 389\nstride, 449\nstring kernels, 161\nstring subsequence kernel, 161\nstring tensors, 383, 783\nstrong learners, 190\nstyle mixing, 606\nstyle transfer, 604\nStyleGANs, 567, 604\nSubclassing API, 313\nsubderivatives, 173\nsubgradient vector, 140\nsubsampling, 456\nsubspace, 215\nsummaries (TensorFlow), 317\nsupervised learning\nalgorithms covered, 9\ncommon tasks, 8\ndefined, 8\nSupport Vector Machines (SVMs)\nbenefits of, 153\ndecision function and prediction, 165\ndual problem, 168, 761\nkernelized SVM, 169\nlinear SVM classification, 153\nnonlinear SVM classification, 157-162\nonline SVMs, 172\nSVM regression, 162\ntraining objective, 166\nsupport vectors, 154\nsymbolic differentiation, 768\nsymbolic tensors, 408, 792\nsymmetry, breaking in backpropagation, 291\nsynchronous updates, 706\nT\nt-Distributed Stochastic Neighbor Embedding\n(t-SNE), 233\ntail-heavy histograms, 51\nTalos library, 322\ntarget model, 639\nTD error, 630\nTD target, 630\ntemperature\nin Boltzmann machines, 775\nin text generation, 531\nTemporal Difference Learning (TD Learning), tensor arrays, 383, 786\nTensorBoard, 317\nTensorFlow Addons, 545\nTensorFlow cluster, 711\nTensorFlow Extended (TFX), 440\nTensorFlow Hub, 378, 540\nTensorFlow Lite, 378\nTensorFlow Model Optimization Toolkit (TF-\nMOT), 359\nTensorFlow Playground, 295\nTensorFlow, basics of\narchitecture, 377\nbenefits, xvi, 376\ncommunity support, 379\nfeatures, 376\ngetting help, 379\ninstalling, 296\nlibrary ecosystem, 378\noperating system compatibility, 378\nPyTorch library and, 296\nversions covered, 375\nTensorFlow, CNNs\nconvolution operations, 494\nconvolutional layers, 453 | Index\npooling layer, 458\nTensorFlow, custom models and training\nabout, 375\nactivation functions, initializers, regulariz\u2010\ners, and constraints, 387\ncomputing gradients using Autodiff, 399,\n765-772\nimplementing learning rate scheduling, 363\nlayers, 391\nloss functions, 384\nlosses and metrics, 397\nmetrics, 388\nmodels, 394\nsaving and loading, 385\nspecial data structures, 783-789\ntraining loops, 402\nTensorFlow, data loading and preprocessing\nData API, 414-424\noverview of, 413\npreprocessing input features, 430-439\nTensorFlow Datasets (TFDS) Project, 441, TF Transform, 439\nTFRecord format, 424-430\nTensorFlow, functions and graphs\nAutoGraph and tracing, 407, 791-799\noverview of, 405\nTF Function rules, 409\nTensorFlow, model deployment at scale\ndeploying on AI platforms, 81\ndeploying to mobile and embedded devices,\n685-688\noverview of, 667\nserving TensorFlow models, 668-685\ntraining models across multiple devices,\n701-717\nusing GPUs to speed computations, 689-701\nTensorFlow, NumPy-like operations\nother data structures, 383\ntensors and NumPy, 381\ntensors and operations, 379\ntype conversions, 381\nvariables, 382\nTensorFlow.js, 378\ntensors, 379\nTerm-Frequency \u00d7 Inverse-Document-\nFrequency (TF-IDF), 439\nterminal state, 626\ntest sets, 30, 51\ntesting and validation\ndata mismatch, 32\nhyperparameter tuning, 31\nmodel selection, 31\ntext generation\nbuilding and training models for, 530\nchopping sequential datasets, 528\ngenerating Shakespearean text, 531\noverview of, 526\nsplitting sequential datasets, 527\nstateful RNNs and, 532\ntraining dataset creation, 527\nusing models for, 531\nTF Datasets (TFDS), 414, 441\nTF Functions\ngraphs generated by, 791-799\nrules, 409\nTF Transform (tf.Transform), 414, 439\nTF-Agents library\ncollect driver, 656\ndatasets, 658\ndeep Q-networks (DQNs), 650\nDQN agents, 652\nenvironment specifications, 644\nenvironment wrappers, 645\nenvironments, 643\ninstalling, 643\noverview of, 642\nreplay buffer and observer, 654\ntraining architecture, 649\ntraining loops, 661\ntraining metrics, 655\ntf.keras, 295, 363, 363, 423\ntf.summary package, 319\nTF.Text library, 536\nTFRecord format\ncompressed TFRecord files, 425\nlists of lists using SequenceExample Proto\u2010\nbuf, 429\nloading and parsing examples, 428\noverview of, 424\nprotocol buffers (protobufs), 425\nTensorFlow protobufs, 427\nTheano, 295\ntheoretical information criterion, 267\nthermal equilibrium, 775\nthreshold logic unit (TLU), 284\nTikhonov regularization, 135\ntime series data\nIndex | additional models for, 506\nbaseline metrics, 505\ndeep RNNS, 506\nforecasting several steps ahead, 508\noverview of, 503\nRNNS for, 497\nsimple RNNs, 505\ntime step, 498\ntokenization, 536\ntolerance, 123\nTPUs (tensor processing units), 377\ntrain-dev sets, 32\ntraining data\ndefined, 2\nhold outs, 31\ninsufficient quantity of, 23\nirrelevant features, 27\nnonrepresentative, 25\noverfitting, 27\npoor quality, 26\ntraining dataset creation, 527\nunderfitting, 29\ntraining instances, 2, 215\ntraining models\ndefined, 20\nexample project, 72\nGradient Descent, 118-128\nlearning curves, 130-134\nLinear Regression, 112-117\nLogistic Regression, 142-151\noverview of, 111\nPolynomial Regression, 128-130\nregularized linear models, 134-142\ntraining samples, 2\ntraining set rotation, 185\ntraining sets, 2, 30, 213\ntraining/serving skew, 440\ntrajectories, 649\ntrajectory, 650\ntransfer learning, 324, 345, 481\ntransformations\naffine transformations, 604\nchaining, 415\ncustom, 68\ninverse transformation, 225\npurpose of, 64\ntransformation pipelines, 70\nTransformer architecture, 554\ntransposed convolutional layer, 493\ntrue negative rate (TNR), 97\ntrue positive rate (TPR), 91\ntruncated backpropagation through time, 529\nTuring test, 525\ntying weights, 577\ntype conversions, 381\nU\nuncertainty sampling, 255\nundercomplete autoencoders, 570\nunderfitting, 29\nundiscounted rewards, 656\nunivariate regression problems, 39\nunivariate time series, 503\nunrolling the network through time, 498\nunstable gradients problem, 512\nunsupervised learning\nalgorithms covered, 10\nclustering, 236-260\ncommon tasks, 10\ndefined, 9\nGaussian mixtures model (GMM), 260-275\noverview of, 235\npretraining using stacked autoencoders,\n576-579\nunsupervised pretraining, 349\nupsampling layer, 493\nutility functions, 20\nV\nvalidation sets, 31\nValue Iteration algorithm, 627\nvanishing/exploding gradients problems,\n332-345\nvariables, 382\nvariance\nexplained variance ratio, 222\npreserving, 219\nvariational autoencoders, 586-591\nvariational inference, 272\nvariational parameters, 272\nvector-to-sequence networks, 501\nvectors\ncolumn vectors, 113\nfeature vectors, 113\nmomentum vector, 352\nparameter vectors, 113\nsubgradient vectors, 140\nVGGNet, 470 | Index\nvirtual GPU devices, 695\nvisible units, 775\nvisual attention, 552\nvisualization algorithms, 11\nvocabulary, 432\nvoice recognition, 445\nW\nwall time, 341\nwarmup phase, 708\nWaveNet, 498, 521\nweak learners, 190\nweighted moving average model, 506\nwhite box models, 178\nWide & Deep neural networks, 308\nwisdom of the crowd, 189\nword embeddings, 434\nword tokenization, 536\nWordTrees, 490\nworkspace creation, 42\nX\nXavier initialization, 333\nXception (Extreme Inception), 474\nXGBoost, 208\nY\nYou Only Look Once (YOLO), 489\nZ\nzero padding, 449\nzero-shot learning (ZSL), 564\nZF Net, 466\nIndex | About the Author\nAur\u00e9lien G\u00e9ron is a Machine Learning consultant and lecturer."
  },
  {
    "id": 539,
    "content": "A former Googler, he\nled YouTube\u2019s video classification team from 2013 to 2016. He\u2019s been a founder of and\nCTO at a few different companies: Wifirst, a leading wireless ISP in France; Polycon\u2010\nseil, a consulting firm focused on telecoms, media, and strategy; and Kiwisoft, a con\u2010\nsulting firm focused on Machine Learning and data privacy. Before all that he worked as an engineer in a variety of domains: finance (JP Morgan\nand Soci\u00e9t\u00e9 G\u00e9n\u00e9rale), defense (Canada\u2019s DOD), and healthcare (blood transfusion). He also published a few technical books (on C++, WiFi, and internet architectures)\nand lectured about computer science at a French engineering school. A few fun facts: he taught his three children to count in binary with their fingers (up\nto 1,023), he studied microbiology and evolutionary genetics before going into soft\u2010\nware engineering, and his parachute didn\u2019t open on the second jump. Colophon\nThe animal on the cover of Hands-On Machine Learning with Scikit-Learn, Keras, and\nTensorFlow is the fire salamander (Salamandra salamandra), an amphibian found\nacross most of Europe. Its black, glossy skin features large yellow spots on the head\nand back, signaling the presence of alkaloid toxins. This is a possible source of this\namphibian\u2019s common name: contact with these toxins (which they can also spray\nshort distances) causes convulsions and hyperventilation. Either the painful poisons\nor the moistness of the salamander\u2019s skin (or both) led to a misguided belief that these\ncreatures not only could survive being placed in fire but could extinguish it as well. Fire salamanders live in shaded forests, hiding in moist crevices and under logs near\nthe pools or other freshwater bodies that facilitate their breeding. Though they spend\nmost of their lives on land, they give birth to their young in water. They subsist\nmostly on a diet of insects, spiders, slugs, and worms. Fire salamanders can grow up\nto a foot in length, and in captivity may live as long as 50 years. The fire salamander\u2019s numbers have been reduced by destruction of their forest habi\u2010\ntat and capture for the pet trade, but the greatest threat they face is the susceptibility\nof their moisture-permeable skin to pollutants and microbes. Since 2014, they have\nbecome extinct in parts of the Netherlands and Belgium due to an introduced fungus. Many of the animals on O\u2019Reilly covers are endangered; all of them are important to\nthe world. The cover illustration is by Karen Montgomery, based on an engraving\nfrom Wood\u2019s Illustrated Natural History. The cover fonts are URW Typewriter and\nGuardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad\nCondensed; and the code font is Dalton Maag\u2019s Ubuntu Mono. There\u2019s much more where this came from. Experience books, videos, live online training courses, and more from O\u2019Reilly and our 200+ partners\u2014all in one place. Learn more at oreilly.com/online-learning\n\u00a92019 O\u2019Reilly Media, Inc. O\u2019Reilly is a registered trademark of O\u2019Reilly Media, Inc. | 175"
  }
]